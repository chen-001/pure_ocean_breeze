__updated__ = "2022-11-05 00:13:27"

try:
    import rqdatac

    rqdatac.init()
except Exception:
    print("æš‚æ—¶æœªè¿æ¥ç±³ç­")
from loguru import logger
import os
import time
import datetime
import numpy as np
import pandas as pd
import scipy.io as scio
from sqlalchemy import FLOAT, INT, VARCHAR, BIGINT
from tenacity import retry
import pickledb
import tqdm
from functools import reduce
import dcube as dc
from pure_ocean_breeze.legacy_version.v3p4.state.homeplace import HomePlace

homeplace = HomePlace()
try:
    pro = dc.pro_api(homeplace.api_token)
except Exception:
    print("æš‚æ—¶æœªè¿æ¥æ•°ç«‹æ–¹")
from pure_ocean_breeze.data.database import (
    sqlConfig,
    ClickHouseClient,
    PostgreSQL,
    Questdb,
)
from pure_ocean_breeze.legacy_version.v3p4.data.read_data import read_daily, read_money_flow
from pure_ocean_breeze.legacy_version.v3p4.data.dicts import INDUS_DICT, INDEX_DICT, ZXINDUS_DICT
from pure_ocean_breeze.legacy_version.v3p4.data.tools import ç”Ÿæˆæ¯æ—¥åˆ†ç±»è¡¨, add_suffix, convert_code,drop_duplicates_index
from pure_ocean_breeze.legacy_version.v3p4.labor.process import pure_fama


def database_update_minute_data_to_clickhouse_and_questdb(kind: str) -> None:
    """ä½¿ç”¨ç±³ç­æ›´æ–°åˆ†é’Ÿæ•°æ®è‡³clickhouseå’Œquestdbä¸­

    Parameters
    ----------
    kind : str
        æ›´æ–°è‚¡ç¥¨åˆ†é’Ÿæ•°æ®æˆ–æŒ‡æ•°åˆ†é’Ÿæ•°æ®ï¼Œè‚¡ç¥¨åˆ™'stock'ï¼ŒæŒ‡æ•°åˆ™'index'

    Raises
    ------
    `IOError`
        å¦‚æœæœªæŒ‡å®šè‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼Œå°†æŠ¥é”™
    """
    if kind == "stock":
        code_type = "CS"
    elif kind == "index":
        code_type = "INDX"
    else:
        raise IOError("æ€»å¾—æŒ‡å®šä¸€ç§ç±»å‹å§ï¼Ÿè¯·ä»stockå’Œindexä¸­é€‰ä¸€ä¸ª")
    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user1 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user1}MB")
    # è·å–å…¨éƒ¨è‚¡ç¥¨/æŒ‡æ•°ä»£ç 
    cs = rqdatac.all_instruments(type=code_type, market="cn", date=None)
    codes = list(cs.order_book_id)
    # è·å–ä¸Šæ¬¡æ›´æ–°æˆªæ­¢æ—¶é—´
    chc = ClickHouseClient("minute_data")
    last_date = max(chc.show_all_dates(f"minute_data_{kind}"))
    # æœ¬æ¬¡æ›´æ–°èµ·å§‹æ—¥æœŸ
    start_date = pd.Timestamp(str(last_date)) + pd.Timedelta(days=1)
    start_date = datetime.datetime.strftime(start_date, "%Y-%m-%d")
    # æœ¬æ¬¡æ›´æ–°ç»ˆæ­¢æ—¥æœŸ
    end_date = datetime.datetime.now()
    if end_date.hour < 17:
        end_date = end_date - pd.Timedelta(days=1)
    end_date = datetime.datetime.strftime(end_date, "%Y-%m-%d")
    logger.info(f"æœ¬æ¬¡å°†ä¸‹è½½ä»{start_date}åˆ°{end_date}çš„æ•°æ®")
    # ä¸‹è½½æ•°æ®
    ts = rqdatac.get_price(
        codes,
        start_date=start_date,
        end_date=end_date,
        frequency="1m",
        fields=["volume", "total_turnover", "high", "low", "close", "open"],
        adjust_type="none",
        skip_suspended=False,
        market="cn",
        expect_df=True,
        time_slice=None,
    )
    # è°ƒæ•´æ•°æ®æ ¼å¼
    ts = ts.reset_index()
    ts = ts.rename(
        columns={
            "order_book_id": "code",
            "datetime": "date",
            "volume": "amount",
            "total_turnover": "money",
        }
    )
    ts = ts.sort_values(["code", "date"])
    ts.date = ts.date.dt.strftime("%Y%m%d").astype(int)
    ts = ts.groupby(["code", "date"]).apply(
        lambda x: x.assign(num=list(range(1, x.shape[0] + 1)))
    )
    ts = (np.around(ts.set_index("code"), 2) * 100).astype(int).reset_index()
    ts.code = ts.code.str.replace(".XSHE", ".SZ")
    ts.code = ts.code.str.replace(".XSHG", ".SH")
    # æ•°æ®å†™å…¥æ•°æ®åº“
    ts.to_sql(f"minute_data_{kind}", chc.engine, if_exists="append", index=False)
    ts = ts.set_index("code")
    ts = ts / 100
    ts = ts.reset_index()
    qdb = Questdb()
    ts.date = ts.date.astype(int).astype(str)
    ts.num = ts.num.astype(int).astype(str)
    qdb.write_via_csv(ts, f"minute_data_{kind}")
    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user2 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    user12 = round(user2 - user1, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user2}MBï¼Œæœ¬é¡¹æ›´æ–°æ¶ˆè€—æµé‡{user12}MB")


def database_update_minute_data_to_postgresql(kind: str) -> None:
    """ä½¿ç”¨ç±³ç­æ›´æ–°åˆ†é’Ÿæ•°æ®è‡³postgresqlä¸­

    Parameters
    ----------
    kind : str
        æ›´æ–°è‚¡ç¥¨åˆ†é’Ÿæ•°æ®æˆ–æŒ‡æ•°åˆ†é’Ÿæ•°æ®ï¼Œè‚¡ç¥¨åˆ™'stock'ï¼ŒæŒ‡æ•°åˆ™'index'

    Raises
    ------
    `IOError`
        å¦‚æœæœªæŒ‡å®šè‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼Œå°†æŠ¥é”™
    """
    if kind == "stock":
        code_type = "CS"
    elif kind == "index":
        code_type = "INDX"
    else:
        raise IOError("æ€»å¾—æŒ‡å®šä¸€ç§ç±»å‹å§ï¼Ÿè¯·ä»stockå’Œindexä¸­é€‰ä¸€ä¸ª")
    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user1 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user1}MB")
    # è·å–å…¨éƒ¨è‚¡ç¥¨/æŒ‡æ•°ä»£ç 
    cs = rqdatac.all_instruments(type=code_type, market="cn", date=None)
    codes = list(cs.order_book_id)
    # è·å–ä¸Šæ¬¡æ›´æ–°æˆªæ­¢æ—¶é—´
    qdb = Questdb()
    last_date = max(qdb.show_all_dates(f"minute_data_{kind}"))
    # æœ¬æ¬¡æ›´æ–°èµ·å§‹æ—¥æœŸ
    start_date = pd.Timestamp(str(last_date)) + pd.Timedelta(days=1)
    start_date = datetime.datetime.strftime(start_date, "%Y-%m-%d")
    # æœ¬æ¬¡æ›´æ–°ç»ˆæ­¢æ—¥æœŸ
    end_date = datetime.datetime.now()
    if end_date.hour < 17:
        end_date = end_date - pd.Timedelta(days=1)
    end_date = datetime.datetime.strftime(end_date, "%Y-%m-%d")
    logger.info(f"æœ¬æ¬¡å°†ä¸‹è½½ä»{start_date}åˆ°{end_date}çš„æ•°æ®")
    # ä¸‹è½½æ•°æ®
    ts = rqdatac.get_price(
        codes,
        start_date=start_date,
        end_date=end_date,
        frequency="1m",
        fields=["volume", "total_turnover", "high", "low", "close", "open"],
        adjust_type="none",
        skip_suspended=False,
        market="cn",
        expect_df=True,
        time_slice=None,
    )
    # è°ƒæ•´æ•°æ®æ ¼å¼
    ts = ts.reset_index()
    ts = ts.rename(
        columns={
            "order_book_id": "code",
            "datetime": "date",
            "volume": "amount",
            "total_turnover": "money",
        }
    )
    ts = ts.sort_values(["code", "date"])
    ts.date = ts.date.dt.strftime("%Y%m%d").astype(int)
    ts = ts.groupby(["code", "date"]).apply(
        lambda x: x.assign(num=list(range(1, x.shape[0] + 1)))
    )
    ts.code = ts.code.str.replace(".XSHE", ".SZ")
    ts.code = ts.code.str.replace(".XSHG", ".SH")
    # æ•°æ®å†™å…¥æ•°æ®åº“
    pgdb = PostgreSQL("minute_data")
    ts.to_sql(
        f"minute_data_{kind}",
        pgdb.engine,
        if_exists="append",
        index=False,
        dtype={
            "code": VARCHAR(9),
            "date": INT,
            "open": FLOAT,
            "high": FLOAT,
            "low": FLOAT,
            "close": FLOAT,
            "amount": FLOAT,
            "money": FLOAT,
            "num": INT,
        },
    )
    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user2 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    user12 = round(user2 - user1, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user2}MBï¼Œæœ¬é¡¹æ›´æ–°æ¶ˆè€—æµé‡{user12}MB")


def database_update_minute_data_to_questdb(kind: str) -> None:
    """ä½¿ç”¨ç±³ç­æ›´æ–°åˆ†é’Ÿæ•°æ®è‡³questdbä¸­

    Parameters
    ----------
    kind : str
        æ›´æ–°è‚¡ç¥¨åˆ†é’Ÿæ•°æ®æˆ–æŒ‡æ•°åˆ†é’Ÿæ•°æ®ï¼Œè‚¡ç¥¨åˆ™'stock'ï¼ŒæŒ‡æ•°åˆ™'index'

    Raises
    ------
    `IOError`
        å¦‚æœæœªæŒ‡å®šè‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼Œå°†æŠ¥é”™
    """
    if kind == "stock":
        code_type = "CS"
    elif kind == "index":
        code_type = "INDX"
    else:
        raise IOError("æ€»å¾—æŒ‡å®šä¸€ç§ç±»å‹å§ï¼Ÿè¯·ä»stockå’Œindexä¸­é€‰ä¸€ä¸ª")
    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user1 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user1}MB")
    # è·å–å…¨éƒ¨è‚¡ç¥¨/æŒ‡æ•°ä»£ç 
    cs = rqdatac.all_instruments(type=code_type, market="cn", date=None)
    codes = list(cs.order_book_id)
    # è·å–ä¸Šæ¬¡æ›´æ–°æˆªæ­¢æ—¶é—´
    qdb = Questdb()
    last_date = max(qdb.show_all_dates(f"minute_data_{kind}"))
    # æœ¬æ¬¡æ›´æ–°èµ·å§‹æ—¥æœŸ
    start_date = pd.Timestamp(str(last_date)) + pd.Timedelta(days=1)
    start_date = datetime.datetime.strftime(start_date, "%Y-%m-%d")
    # æœ¬æ¬¡æ›´æ–°ç»ˆæ­¢æ—¥æœŸ
    end_date = datetime.datetime.now()
    if end_date.hour < 17:
        end_date = end_date - pd.Timedelta(days=1)
    end_date = datetime.datetime.strftime(end_date, "%Y-%m-%d")
    logger.info(f"æœ¬æ¬¡å°†ä¸‹è½½ä»{start_date}åˆ°{end_date}çš„æ•°æ®")
    # ä¸‹è½½æ•°æ®
    ts = rqdatac.get_price(
        codes,
        start_date=start_date,
        end_date=end_date,
        frequency="1m",
        fields=["volume", "total_turnover", "high", "low", "close", "open"],
        adjust_type="none",
        skip_suspended=False,
        market="cn",
        expect_df=True,
        time_slice=None,
    )
    # è°ƒæ•´æ•°æ®æ ¼å¼
    ts = ts.reset_index()
    ts = ts.rename(
        columns={
            "order_book_id": "code",
            "datetime": "date",
            "volume": "amount",
            "total_turnover": "money",
        }
    )
    ts = ts.sort_values(["code", "date"])
    ts.date = ts.date.dt.strftime("%Y%m%d").astype(int)
    ts = ts.groupby(["code", "date"]).apply(
        lambda x: x.assign(num=list(range(1, x.shape[0] + 1)))
    )
    ts = ts.ffill().dropna()
    ts.code = ts.code.str.replace(".XSHE", ".SZ")
    ts.code = ts.code.str.replace(".XSHG", ".SH")
    ts.date = ts.date.astype(int).astype(str)
    ts.num = ts.num.astype(int).astype(str)
    # æ•°æ®å†™å…¥æ•°æ®åº“
    qdb = Questdb()
    qdb.write_via_csv(ts, f"minute_data_{kind}")
    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user2 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    user12 = round(user2 - user1, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user2}MBï¼Œæœ¬é¡¹æ›´æ–°æ¶ˆè€—æµé‡{user12}MB")


def database_update_minute_data_to_mysql(kind: str) -> None:
    """ä½¿ç”¨ç±³ç­æ›´æ–°åˆ†é’Ÿæ•°æ®è‡³mmysqlä¸­

    Parameters
    ----------
    kind : str
        æ›´æ–°è‚¡ç¥¨åˆ†é’Ÿæ•°æ®æˆ–æŒ‡æ•°åˆ†é’Ÿæ•°æ®ï¼Œè‚¡ç¥¨åˆ™'stock'ï¼ŒæŒ‡æ•°åˆ™'index'

    Raises
    ------
    `IOError`
        å¦‚æœæœªæŒ‡å®šè‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼Œå°†æŠ¥é”™
    """
    if kind == "stock":
        code_type = "CS"
    elif kind == "index":
        code_type = "INDX"
    else:
        raise IOError("æ€»å¾—æŒ‡å®šä¸€ç§ç±»å‹å§ï¼Ÿè¯·ä»stockå’Œindexä¸­é€‰ä¸€ä¸ª")
    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user1 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user1}MB")
    # è·å–å…¨éƒ¨è‚¡ç¥¨/æŒ‡æ•°ä»£ç 
    cs = rqdatac.all_instruments(type=code_type, market="cn", date=None)
    codes = list(cs.order_book_id)
    # è·å–ä¸Šæ¬¡æ›´æ–°æˆªæ­¢æ—¶é—´
    # è¿æ¥2ä¸ªæ•°æ®åº“
    sqlsa = sqlConfig("minute_data_stock_alter")
    sqlia = sqlConfig("minute_data_index_alter")
    last_date = max(sqlsa.show_tables(full=False))
    # æœ¬æ¬¡æ›´æ–°èµ·å§‹æ—¥æœŸ
    start_date = pd.Timestamp(str(last_date)) + pd.Timedelta(days=1)
    start_date = datetime.datetime.strftime(start_date, "%Y-%m-%d")
    # æœ¬æ¬¡æ›´æ–°ç»ˆæ­¢æ—¥æœŸ
    end_date = datetime.datetime.now()
    if end_date.hour < 17:
        end_date = end_date - pd.Timedelta(days=1)
    end_date = datetime.datetime.strftime(end_date, "%Y-%m-%d")
    logger.info(f"æœ¬æ¬¡å°†ä¸‹è½½ä»{start_date}åˆ°{end_date}çš„æ•°æ®")
    # ä¸‹è½½æ•°æ®
    ts = rqdatac.get_price(
        codes,
        start_date=start_date,
        end_date=end_date,
        frequency="1m",
        fields=["volume", "total_turnover", "high", "low", "close", "open"],
        adjust_type="none",
        skip_suspended=False,
        market="cn",
        expect_df=True,
        time_slice=None,
    )
    # è°ƒæ•´æ•°æ®æ ¼å¼
    ts = ts.reset_index()
    ts = ts.rename(
        columns={
            "order_book_id": "code",
            "datetime": "date",
            "volume": "amount",
            "total_turnover": "money",
        }
    )
    ts = ts.sort_values(["code", "date"])
    ts.date = ts.date.dt.strftime("%Y%m%d").astype(int)
    ts = ts.groupby(["code", "date"]).apply(
        lambda x: x.assign(num=list(range(1, x.shape[0] + 1)))
    )
    ts.code = ts.code.str.replace(".XSHE", ".SZ")
    ts.code = ts.code.str.replace(".XSHG", ".SH")
    codes = list(set(ts.code))
    dates = list(set(ts.date))
    # æ•°æ®å†™å…¥æ•°æ®åº“
    fails = []
    # è‚¡ç¥¨
    if kind == "stock":
        # æŠŠæ¯å¤©å†™å…¥æ¯å¤©æ‰€æœ‰è‚¡ç¥¨ä¸€å¼ è¡¨
        for date in dates:
            dfi = ts[ts.date == date]
            try:
                dfi.drop(columns=["date"]).to_sql(
                    name=str(date),
                    con=sqlsa.engine,
                    if_exists="append",
                    index=False,
                    dtype={
                        "code": VARCHAR(9),
                        "open": FLOAT,
                        "high": FLOAT,
                        "low": FLOAT,
                        "close": FLOAT,
                        "amount": FLOAT,
                        "money": FLOAT,
                        "num": INT,
                    },
                )
            except Exception:
                try:
                    if sqlsa.get_data(date).shape[0] == 0:
                        dfi.drop(columns=["date"]).to_sql(
                            name=str(date),
                            con=sqlsa.engine,
                            if_exists="replace",
                            index=False,
                            dtype={
                                "code": VARCHAR(9),
                                "open": FLOAT,
                                "high": FLOAT,
                                "low": FLOAT,
                                "close": FLOAT,
                                "amount": FLOAT,
                                "money": FLOAT,
                                "num": INT,
                            },
                        )
                except Exception:
                    fails.append(date)
                    logger.warning(f"è‚¡ç¥¨{date}å†™å…¥å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥")
    # æŒ‡æ•°
    else:
        # æŠŠæ¯å¤©å†™å…¥æ¯å¤©æ‰€æœ‰æŒ‡æ•°ä¸€å¼ è¡¨
        for date in dates:
            dfi = ts[ts.date == date]
            try:
                dfi.drop(columns=["date"]).to_sql(
                    name=str(date),
                    con=sqlia.engine,
                    if_exists="append",
                    index=False,
                    dtype={
                        "code": VARCHAR(9),
                        "open": FLOAT,
                        "high": FLOAT,
                        "low": FLOAT,
                        "close": FLOAT,
                        "amount": FLOAT,
                        "money": FLOAT,
                        "num": INT,
                    },
                )
            except Exception:
                try:
                    if sqlia.get_data(date).shape[0] == 0:
                        dfi.drop(columns=["date"]).to_sql(
                            name=str(date),
                            con=sqlia.engine,
                            if_exists="replace",
                            index=False,
                            dtype={
                                "code": VARCHAR(9),
                                "open": FLOAT,
                                "high": FLOAT,
                                "low": FLOAT,
                                "close": FLOAT,
                                "amount": FLOAT,
                                "money": FLOAT,
                                "num": INT,
                            },
                        )
                except Exception:
                    fails.append(date)
                    logger.warning(f"æŒ‡æ•°{date}å†™å…¥å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥")

    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user2 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    user12 = round(user2 - user1, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user2}MBï¼Œæœ¬é¡¹æ›´æ–°æ¶ˆè€—æµé‡{user12}MB")


@retry
def download_single_daily(day):
    """æ›´æ–°å•æ—¥çš„æ•°æ®"""
    try:
        # 8ä¸ªä»·æ ¼ï¼Œäº¤æ˜“çŠ¶æ€ï¼Œæˆäº¤é‡ï¼Œ
        df1 = pro.a_daily(
            trade_date=day,
            fields=[
                "code",
                "trade_date",
                "open",
                "high",
                "low",
                "close",
                "volume",
                "adjopen",
                "adjclose",
                "adjhigh",
                "adjlow",
                "tradestatus",
            ],
        )
        # æ¢æ‰‹ç‡ï¼Œæµé€šè‚¡æœ¬ï¼Œæ¢æ‰‹ç‡è¦é™¤ä»¥100ï¼Œæµé€šè‚¡æœ¬è¦ä¹˜ä»¥10000
        df2 = pro.daily_basic(
            trade_date=day,
            fields=[
                "ts_code",
                "trade_date",
                "turnover_rate_f",
                "float_share",
                "pe",
                "pb",
            ],
        )
        time.sleep(1)
        return df1, df2
    except Exception:
        time.sleep(60)
        # 8ä¸ªä»·æ ¼ï¼Œäº¤æ˜“çŠ¶æ€ï¼Œæˆäº¤é‡ï¼Œ
        df1 = pro.a_daily(
            trade_date=day,
            fields=[
                "code",
                "trade_date",
                "open",
                "high",
                "low",
                "close",
                "volume",
                "adjopen",
                "adjclose",
                "adjhigh",
                "adjlow",
                "tradestatus",
            ],
        )
        # æ¢æ‰‹ç‡ï¼Œæµé€šè‚¡æœ¬ï¼Œæ¢æ‰‹ç‡è¦é™¤ä»¥100ï¼Œæµé€šè‚¡æœ¬è¦ä¹˜ä»¥10000
        df2 = pro.daily_basic(
            trade_date=day,
            fields=[
                "ts_code",
                "trade_date",
                "turnover_rate_f",
                "float_share",
                "pe",
                "pb",
            ],
        )
        time.sleep(1)
        return df1, df2


@retry
def download_calendar(startdate, enddate):
    """æ›´æ–°å•æ—¥çš„æ•°æ®"""
    try:
        # äº¤æ˜“æ—¥å†
        df0 = pro.a_calendar(start_date=startdate, end_date=enddate)
        time.sleep(1)
        return df0
    except Exception:
        time.sleep(60)
        # äº¤æ˜“æ—¥å†
        df0 = pro.a_calendar(start_date=startdate, end_date=enddate)
        time.sleep(1)
        return df0



def database_update_daily_files() -> None:
    """æ›´æ–°æ•°æ®åº“ä¸­çš„æ—¥é¢‘æ•°æ®

    Raises
    ------
    `ValueError`
        å¦‚æœä¸Šæ¬¡æ›´æ–°åˆ°æœ¬æ¬¡æ›´æ–°æ²¡æœ‰æ–°çš„äº¤æ˜“æ—¥ï¼Œå°†æŠ¥é”™
    """
    homeplace = HomePlace()

    def single_file(name):
        df = pd.read_feather(homeplace.daily_data_file + name + ".feather")
        df = df.set_index(list(df.columns)[0])
        startdate = df.index.max() + pd.Timedelta(days=1)
        return startdate

    names = [
        "opens",
        "highs",
        "lows",
        "closes",
        "trs",
        "opens_unadj",
        "highs_unadj",
        "lows_unadj",
        "closes_unadj",
        "sharenums",
        "ages",
        "sts",
        "states",
        "volumes",
        "pb",
        "pe",
    ]
    startdates = list(map(single_file, names))
    startdate = min(startdates)
    startdate = datetime.datetime.strftime(startdate, "%Y%m%d")
    now = datetime.datetime.now()
    if now.hour < 17:
        now = now - pd.Timedelta(days=1)
    now = datetime.datetime.strftime(now, "%Y%m%d")
    logger.info(f"æ—¥é¢‘æ•°æ®ä¸Šæ¬¡æ›´æ–°åˆ°{startdate},æœ¬æ¬¡å°†æ›´æ–°åˆ°{now}")

    # äº¤æ˜“æ—¥å†
    df0 = download_calendar(startdate, now)
    tradedates = sorted(list(set(df0.trade_date)))
    if len(tradedates) > 1:
        # å­˜å‚¨æ¯å¤©æ•°æ®
        df1s = []
        df2s = []
        for day in tqdm.tqdm(tradedates, desc="æ­£åœ¨ä¸‹è½½æ—¥é¢‘æ•°æ®"):
            df1, df2 = download_single_daily(day)
            df1s.append(df1)
            df2s.append(df2)
        # 8ä¸ªä»·æ ¼ï¼Œäº¤æ˜“çŠ¶æ€ï¼Œæˆäº¤é‡ï¼Œ
        df1s = pd.concat(df1s)
        # æ¢æ‰‹ç‡ï¼Œæµé€šè‚¡æœ¬ï¼Œæ¢æ‰‹ç‡è¦é™¤ä»¥100ï¼Œæµé€šè‚¡æœ¬è¦ä¹˜ä»¥10000
        df2s = pd.concat(df2s)
    elif len(tradedates) == 1:
        df1s, df2s = download_single_daily(tradedates[0])
    else:
        raise ValueError("ä»ä¸Šæ¬¡æ›´æ–°åˆ°è¿™æ¬¡æ›´æ–°ï¼Œè¿˜æ²¡æœ‰ç»è¿‡äº¤æ˜“æ—¥ã€‚æ”¾å‡å°±å¥½å¥½ä¼‘æ¯å§ï¼Œåˆ«è·‘ä»£ç äº†ğŸ¤’")
    df1s.tradestatus = (df1s.tradestatus == "äº¤æ˜“") + 0
    df2s = df2s.rename(columns={"ts_code": "code"})
    df1s.trade_date = pd.to_datetime(df1s.trade_date, format="%Y%m%d")
    df2s.trade_date = pd.to_datetime(df2s.trade_date, format="%Y%m%d")
    df1s = df1s.rename(columns={"trade_date": "date"})
    df2s = df2s.rename(columns={"trade_date": "date"})
    both_codes = list(set(df1s.code) & set(df2s.code))
    df1s = df1s[df1s.code.isin(both_codes)]
    df2s = df2s[df2s.code.isin(both_codes)]
    # stè‚¡
    df3 = pro.ashare_st()

    def to_mat(df, row, name, ind="date", col="code"):
        df = df[[ind, col, row]].pivot(index=ind, columns=col, values=row)
        old = pd.read_feather(homeplace.daily_data_file + name + ".feather").set_index(
            "date"
        )
        new = pd.concat([old, df]).drop_duplicates()
        new = drop_duplicates_index(new)
        new = new[sorted(list(new.columns))]
        new.reset_index().to_feather(homeplace.daily_data_file + name + ".feather")
        logger.success(name + "å·²æ›´æ–°")
        return new

    # è‚¡ç¥¨æ—¥è¡Œæƒ…ï¼ˆæœªå¤æƒé«˜å¼€ä½æ”¶ï¼Œå¤æƒé«˜å¼€ä½æ”¶ï¼Œäº¤æ˜“çŠ¶æ€ï¼Œæˆäº¤é‡ï¼‰
    part1 = df1s.copy()
    # æœªå¤æƒå¼€ç›˜ä»·
    opens = to_mat(part1, "open", "opens_unadj")
    # æœªå¤æƒæœ€é«˜ä»·
    highs = to_mat(part1, "high", "highs_unadj")
    # æœªå¤æƒæœ€ä½ä»·
    lows = to_mat(part1, "low", "lows_unadj")
    # æœªå¤æƒæ”¶ç›˜ä»·
    closes = to_mat(part1, "close", "closes_unadj")
    # æˆäº¤é‡
    volumes = to_mat(part1, "volume", "volumes")
    # å¤æƒå¼€ç›˜ä»·
    diopens = to_mat(part1, "adjopen", "opens")
    # å¤æƒæœ€é«˜ä»·
    dihighs = to_mat(part1, "adjhigh", "highs")
    # å¤æƒæœ€ä½ä»·
    dilows = to_mat(part1, "adjlow", "lows")
    # å¤æƒæ”¶ç›˜ä»·
    dicloses = to_mat(part1, "adjclose", "closes")
    # äº¤æ˜“çŠ¶æ€
    status = to_mat(part1, "tradestatus", "states")

    # æ¢æ‰‹ç‡
    part2 = df2s[["date", "code", "turnover_rate_f"]].pivot(
        index="date", columns="code", values="turnover_rate_f"
    )
    part2 = part2 / 100
    part2_old = pd.read_feather(homeplace.daily_data_file + "trs.feather").set_index(
        "date"
    )
    part2_new = pd.concat([part2_old, part2])
    part2_new = part2_new.drop_duplicates()
    part2_new = part2_new[closes.columns]
    part2_new = part2_new[sorted(list(part2_new.columns))]
    part2_new = drop_duplicates_index(part2_new)
    part2_new.reset_index().to_feather(homeplace.daily_data_file + "trs.feather")
    logger.success("æ¢æ‰‹ç‡æ›´æ–°å®Œæˆ")

    # æµé€šè‚¡æ•°
    # è¯»å–æ–°çš„æµé€šè‚¡å˜åŠ¨æ•°
    part3 = df2s[["date", "code", "float_share"]].pivot(
        columns="code", index="date", values="float_share"
    )
    part3 = part3 * 10000
    part3_old = pd.read_feather(
        homeplace.daily_data_file + "sharenums.feather"
    ).set_index("date")
    part3_new = pd.concat([part3_old, part3]).drop_duplicates()
    part3_new = part3_new[closes.columns]
    part3_new = drop_duplicates_index(part3_new)
    part3_new = part3_new[sorted(list(part3_new.columns))]
    part3_new.reset_index().to_feather(homeplace.daily_data_file + "sharenums.feather")
    logger.success("æµé€šè‚¡æ•°æ›´æ–°å®Œæˆ")

    # pb
    partpb = df2s[["date", "code", "pb"]].pivot(
        index="date", columns="code", values="pb"
    )
    partpb_old = pd.read_feather(homeplace.daily_data_file + "pb.feather").set_index(
        "date"
    )
    partpb_new = pd.concat([partpb_old, partpb])
    partpb_new = partpb_new.drop_duplicates()
    partpb_new = partpb_new[closes.columns]
    partpb_new = partpb_new[sorted(list(partpb_new.columns))]
    partpb_new = drop_duplicates_index(partpb_new)
    partpb_new.reset_index().to_feather(homeplace.daily_data_file + "pb.feather")
    logger.success("å¸‚å‡€ç‡æ›´æ–°å®Œæˆ")

    # pe
    partpe = df2s[["date", "code", "pe"]].pivot(
        index="date", columns="code", values="pe"
    )
    partpe_old = pd.read_feather(homeplace.daily_data_file + "pe.feather").set_index(
        "date"
    )
    partpe_new = pd.concat([partpe_old, partpe])
    partpe_new = partpe_new.drop_duplicates()
    partpe_new = partpe_new[closes.columns]
    partpe_new = partpe_new[sorted(list(partpe_new.columns))]
    partpe_new = drop_duplicates_index(partpe_new)
    partpe_new.reset_index().to_feather(homeplace.daily_data_file + "pe.feather")
    logger.success("å¸‚ç›ˆç‡æ›´æ–°å®Œæˆ")

    # st
    part4 = df3[["s_info_windcode", "entry_dt", "remove_dt"]]
    part4 = part4.sort_values("s_info_windcode")
    part4.remove_dt = part4.remove_dt.fillna(now).astype(int)
    part4 = part4.set_index("s_info_windcode").stack()
    part4 = part4.reset_index().assign(
        he=sorted(list(range(int(part4.shape[0] / 2))) * 2)
    )
    part4 = part4.drop(columns=["level_1"])
    part4.columns = ["code", "date", "he"]
    part4.date = pd.to_datetime(part4.date, format="%Y%m%d")

    def single(df):
        full = pd.DataFrame({"date": pd.date_range(df.date.min(), df.date.max())})
        df = pd.merge(full, df, on=["date"], how="left")
        df = df.fillna(method="ffill")
        return df

    tqdm.tqdm.pandas()
    part4 = part4.groupby(["code", "he"]).progress_apply(single)
    part4 = part4[part4.date.isin(list(part2_new.index))]
    part4 = part4.reset_index(drop=True)
    part4 = part4.assign(st=1)

    part4 = part4.drop_duplicates(subset=["date", "code"]).pivot(
        index="date", columns="code", values="st"
    )

    part4_0 = pd.DataFrame(0, columns=part2_new.columns, index=part2_new.index)
    part4_0 = part4_0 + part4
    part4_0 = part4_0.replace(np.nan, 0)
    part4_0 = part4_0[part4_0.index.isin(list(part2_new.index))]
    part4_0 = part4_0.T
    part4_0 = part4_0[part4_0.index.isin(list(part2_new.columns))]
    part4_0 = part4_0.T
    part4_0 = part4_0[closes.columns]
    part4_0 = drop_duplicates_index(part4_0)
    part4_0 = part4_0[sorted(list(part4_0.columns))]
    part4_0.reset_index().to_feather(homeplace.daily_data_file + "sts.feather")
    logger.success("stæ›´æ–°å®Œäº†")

    # ä¸Šå¸‚å¤©æ•°
    part5_close = pd.read_feather(
        homeplace.update_data_file + "BasicFactor_Close.txt"
    ).set_index("index")
    part5_close = part5_close[part5_close.index < 20040101]
    part5_close.index = pd.to_datetime(part5_close.index, format="%Y%m%d")
    part5_close = pd.concat([part5_close, closes]).drop_duplicates()
    part5 = np.sign(part5_close).fillna(method="ffill").cumsum()
    part5 = part5[part5.index.isin(list(part2_new.index))]
    part5 = part5.T
    part5 = part5[part5.index.isin(list(part2_new.columns))]
    part5 = part5.T
    part5 = part5[closes.columns]
    part5 = drop_duplicates_index(part5)
    part5 = part5[sorted(list(part5.columns))]
    part5.reset_index().to_feather(homeplace.daily_data_file + "ages.feather")
    logger.success("ä¸Šå¸‚å¤©æ•°æ›´æ–°å®Œäº†")


@retry
def download_single_day_style(day):
    """æ›´æ–°å•æ—¥çš„æ•°æ®"""
    try:
        style = pro.RMExposureDayGet(
            trade_date=str(day),
            fields="tradeDate,ticker,BETA,MOMENTUM,SIZE,EARNYILD,RESVOL,GROWTH,BTOP,LEVERAGE,LIQUIDTY,SIZENL",
        )
        time.sleep(1)
        return style
    except Exception:
        time.sleep(60)
        style = pro.RMExposureDayGet(
            trade_date=str(day),
            fields="tradeDate,ticker,BETA,MOMENTUM,SIZE,EARNYILD,RESVOL,GROWTH,BTOP,LEVERAGE,LIQUIDTY,SIZENL",
        )
        time.sleep(1)
        return style


def database_update_barra_files():
    fs = os.listdir(homeplace.barra_data_file)[0]
    fs = pd.read_feather(homeplace.barra_data_file + fs)
    fs.columns = ["date"] + list(fs.columns)[1:]
    fs = fs.set_index("date")
    last_date = fs.index.max()
    last_date = datetime.datetime.strftime(last_date, "%Y%m%d")
    now = datetime.datetime.now()
    if now.hour < 17:
        now = now - pd.Timedelta(days=1)
    now = datetime.datetime.strftime(now, "%Y%m%d")
    logger.info(f"é£æ ¼æš´éœ²æ•°æ®ä¸Šæ¬¡æ›´æ–°åˆ°{last_date}ï¼Œæœ¬æ¬¡å°†æ›´æ–°åˆ°{now}")
    df0 = download_calendar(last_date, now)
    tradedates = sorted(list(set(df0.trade_date)))
    style_names = [
        "beta",
        "momentum",
        "size",
        "residualvolatility",
        "earningsyield",
        "growth",
        "booktoprice",
        "leverage",
        "liquidity",
        "nonlinearsize",
    ]
    ds = {k: [] for k in style_names}
    for t in tqdm.tqdm(tradedates):
        style = download_single_day_style(t)
        style.columns = style.columns.str.lower()
        style = style.rename(
            columns={
                "earnyild": "earningsyield",
                "tradedate": "date",
                "ticker": "code",
                "resvol": "residualvolatility",
                "btop": "booktoprice",
                "sizenl": "nonlinearsize",
                "liquidty": "liquidity",
            }
        )
        style.date = pd.to_datetime(style.date, format="%Y%m%d")
        style.code = style.code.apply(add_suffix)
        sts = list(style.columns)[2:]
        for s in sts:
            ds[s].append(style.pivot(columns="code", index="date", values=s))
    for k, v in ds.items():
        old = pd.read_feather(homeplace.barra_data_file + k + ".feather")
        old.columns = ["date"] + list(old.columns)[1:]
        old = old.set_index("date")
        new = pd.concat(v)
        new = pd.concat([old, new])
        new.reset_index().to_feather(homeplace.barra_data_file + k + ".feather")
    logger.success(f"é£æ ¼æš´éœ²æ•°æ®å·²ç»æ›´æ–°åˆ°{now}")


"""æ›´æ–°300ã€500ã€1000è¡Œæƒ…æ•°æ®"""


@retry
def download_single_index(index_code: str):
    if index_code == "000300.SH":
        file = "æ²ªæ·±300"
    elif index_code == "000905.SH":
        file = "ä¸­è¯500"
    elif index_code == "000852.SH":
        file = "ä¸­è¯1000"
    else:
        file = index_code
    try:
        df = (
            pro.index_daily(ts_code=index_code)
            .sort_values("trade_date")
            .rename(columns={"trade_date": "date"})
        )
        df = df[["date", "close"]]
        df.date = pd.to_datetime(df.date, format="%Y%m%d")
        df = df.set_index("date")
        df = df.resample("M").last()
        df.columns = [file]
        return df
    except Exception:
        logger.warning("å°ipäº†ï¼Œè¯·ç­‰å¾…1åˆ†é’Ÿ")
        time.sleep(60)
        df = (
            pro.index_daily(ts_code=index_code)
            .sort_values("trade_date")
            .rename(columns={"trade_date": "date"})
        )
        df = df[["date", "close"]]
        df.date = pd.to_datetime(df.date, format="%Y%m%d")
        df = df.set_index("date")
        df = df.resample("M").last()
        df.columns = [file]
        return df


def database_update_index_three():
    """è¯»å–ä¸‰å¤§æŒ‡æ•°çš„åŸå§‹è¡Œæƒ…æ•°æ®ï¼Œè¿”å›å¹¶ä¿å­˜åœ¨æœ¬åœ°"""
    hs300 = download_single_index("000300.SH")
    zz500 = download_single_index("000905.SH")
    zz1000 = download_single_index("000852.SH")
    res = pd.concat([hs300, zz500, zz1000], axis=1)
    new_date = datetime.datetime.strftime(res.index.max(), "%Y%m%d")
    res.reset_index().to_feather(homeplace.daily_data_file + "3510è¡Œæƒ….feather")
    logger.success(f"3510è¡Œæƒ…æ•°æ®å·²ç»æ›´æ–°è‡³{new_date}")


"""æ›´æ–°ç”³ä¸‡ä¸€çº§è¡Œä¸šçš„è¡Œæƒ…"""


@retry
def download_single_industry_price(ind):
    try:
        df = pro.swindex_daily(code=ind)[["trade_date", "close"]].set_index(
            "trade_date"
        )
        df.columns = [ind]
        time.sleep(1)
        return df
    except Exception:
        time.sleep(60)
        df = pro.swindex_daily(code=ind)[["trade_date", "close"]].set_index(
            "trade_date"
        )
        df.columns = [ind]
        time.sleep(1)
        return df


def database_update_swindustry_prices():
    indus = []
    for ind in list(INDUS_DICT.keys()):
        df = download_single_industry_price(ind=ind)
        indus.append(df)
    indus = pd.concat(indus, axis=1).reset_index()
    indus.columns = ["date"] + list(indus.columns)[1:]
    indus = indus.set_index("date")
    indus = indus.dropna()
    indus.index = pd.to_datetime(indus.index, format="%Y%m%d")
    indus = indus.sort_index()
    indus.reset_index().to_feather(homeplace.daily_data_file + "ç”³ä¸‡å„è¡Œä¸šè¡Œæƒ…æ•°æ®.feather")
    new_date = datetime.datetime.strftime(indus.index.max(), "%Y%m%d")
    logger.success(f"ç”³ä¸‡ä¸€çº§è¡Œä¸šçš„è¡Œæƒ…æ•°æ®å·²ç»æ›´æ–°è‡³{new_date}")


def database_update_zxindustry_prices():
    zxinds = ZXINDUS_DICT
    now = datetime.datetime.now()
    zxprices = []
    for k in list(zxinds.keys()):
        ind = rqdatac.get_price(
            k, start_date="2010-01-01", end_date=now, fields="close"
        )
        ind = (
            ind.rename(columns={"close": zxinds[k]})
            .reset_index(level=1)
            .reset_index(drop=True)
        )
        zxprices.append(ind)
    zxprice = reduce(lambda x, y: pd.merge(x, y, on=["date"], how="outer"), zxprices)
    zxprice.reset_index(drop=True).to_feather(
        homeplace.daily_data_file + "ä¸­ä¿¡å„è¡Œä¸šè¡Œæƒ…æ•°æ®.feather"
    )
    new_date = datetime.datetime.strftime(zxprice.date.max(), "%Y%m%d")
    logger.success(f"ä¸­ä¿¡ä¸€çº§è¡Œä¸šçš„è¡Œæƒ…æ•°æ®å·²ç»æ›´æ–°è‡³{new_date}")


"""æ›´æ–°ç”³ä¸‡ä¸€çº§è¡Œä¸šå“‘å˜é‡"""


@retry
def download_single_swindustry_member(ind):
    try:
        df = pro.index_member(index_code=ind)
        # time.sleep(1)
        return df
    except Exception:
        time.sleep(60)
        df = pro.index_member(index_code=ind)
        time.sleep(1)
        return df


def database_update_swindustry_member():
    dfs = []
    for ind in tqdm.tqdm(INDUS_DICT.keys()):
        ff = download_single_swindustry_member(ind)
        ff = ç”Ÿæˆæ¯æ—¥åˆ†ç±»è¡¨(ff, "con_code", "in_date", "out_date", "index_code")
        khere = ff.index_code.iloc[0]
        ff = ff.assign(khere=1)
        ff.columns = ["code", "index_code", "date", khere]
        ff = ff.drop(columns=["index_code"])
        dfs.append(ff)
    res = pd.merge(dfs[0], dfs[1], on=["code", "date"])
    dfs = pd.concat(dfs)
    trs = read_daily(tr=1, start=20040101)
    dfs = dfs[dfs.date.isin(trs.index)]
    dfs = dfs.sort_values(["date", "code"])
    dfs = dfs[["date", "code"] + list(dfs.columns)[2:]]
    dfs = dfs.fillna(0)
    dfs.reset_index(drop=True).to_feather(
        homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather"
    )
    new_date = dfs.date.max()
    new_date = datetime.datetime.strftime(new_date, "%Y%m%d")
    logger.success(f"ç”³ä¸‡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡(å“‘å˜é‡)å·²ç»æ›´æ–°è‡³{new_date}")


@retry
def download_single_index_member_monthly(code):
    file = homeplace.daily_data_file + INDEX_DICT[code] + "æœˆæˆåˆ†è‚¡.feather"
    old = pd.read_feather(file).set_index("index")
    old_date = old.index.max()
    start_date = old_date + pd.Timedelta(days=1)
    end_date = datetime.datetime.now()
    if start_date >= end_date:
        logger.info(f"{INDEX_DICT[code]}æœˆæˆåˆ†è‚¡æ— éœ€æ›´æ–°ï¼Œä¸Šæ¬¡å·²ç»æ›´æ–°åˆ°äº†{start_date}")
    else:
        start_date, end_date = datetime.datetime.strftime(
            start_date, "%Y%m%d"
        ), datetime.datetime.strftime(end_date, "%Y%m%d")
        logger.info(f"{INDEX_DICT[code]}æœˆæˆåˆ†è‚¡ä¸Šæ¬¡æ›´æ–°åˆ°{start_date},æœ¬æ¬¡å°†æ›´æ–°åˆ°{end_date}")
        try:
            a = pro.index_weight(
                index_code=code, start_date=start_date, end_date=end_date
            )
            if a.shape[0] == 0:
                logger.info(f"{INDEX_DICT[code]}æœˆæˆåˆ†è‚¡æ— éœ€æ›´æ–°ï¼Œä¸Šæ¬¡å·²ç»æ›´æ–°åˆ°äº†{start_date}")
            else:
                time.sleep(1)
                a.trade_date = pd.to_datetime(a.trade_date, format="%Y%m%d")
                a = a.sort_values("trade_date").set_index("trade_date")
                a = (
                    a.groupby("con_code")
                    .resample("M")
                    .last()
                    .drop(columns=["con_code"])
                    .reset_index()
                )
                a = a.assign(num=1)
                a = (
                    a[["trade_date", "con_code", "num"]]
                    .rename(columns={"trade_date": "date", "con_code": "code"})
                    .pivot(columns="code", index="date", values="num")
                )
                a = pd.concat([old, a]).fillna(0)
                a.reset_index().to_feather(file)
                logger.success(f"å·²å°†{INDEX_DICT[code]}æœˆæˆåˆ†è‚¡æ›´æ–°è‡³{end_date}")
        except Exception:
            time.sleep(60)
            a = pro.index_weight(
                index_code=code, start_date=start_date, end_date=end_date
            )
            if a.shape[0] == 0:
                logger.info(f"{INDEX_DICT[code]}æœˆæˆåˆ†è‚¡æ— éœ€æ›´æ–°ï¼Œä¸Šæ¬¡å·²ç»æ›´æ–°åˆ°äº†{start_date}")
            else:
                time.sleep(1)
                a.trade_date = pd.to_datetime(a.trade_date, format="%Y%m%d")
                a = a.sort_values("trade_date").set_index("trade_date")
                a = (
                    a.groupby("con_code")
                    .resample("M")
                    .last()
                    .drop(columns=["con_code"])
                    .reset_index()
                )
                a = a.assign(num=1)
                a = (
                    a[["trade_date", "con_code", "num"]]
                    .rename(columns={"trade_date": "date", "con_code": "code"})
                    .pivot(columns="code", index="date", values="num")
                )
                a = pd.concat([old, a]).fillna(0)
                a.reset_index().to_feather(file)
                logger.success(f"å·²å°†{INDEX_DICT[code]}æœˆæˆåˆ†è‚¡æ›´æ–°è‡³{end_date}")


def database_update_index_members_monthly():
    for k in list(INDEX_DICT.keys()):
        download_single_index_member_monthly(k)


def download_single_index_member(code):
    file = homeplace.daily_data_file + INDEX_DICT[code] + "æ—¥æˆåˆ†è‚¡.feather"
    if code.endswith(".SH"):
        code = code[:6] + ".XSHG"
    elif code.endswith(".SZ"):
        code = code[:6] + ".XSHE"
    now = datetime.datetime.now()
    df = rqdatac.index_components(
        code, start_date="20100101", end_date=now, market="cn"
    )
    ress = []
    for k, v in df.items():
        res = pd.DataFrame(1, index=[pd.Timestamp(k)], columns=v)
        ress.append(res)
    ress = pd.concat(ress)
    ress.columns = [convert_code(i)[0] for i in list(ress.columns)]
    tr = np.sign(read_daily(tr=1, start=20100101))
    tr = np.sign(tr + ress)
    now_str = datetime.datetime.strftime(now, "%Y%m%d")
    tr.reset_index().to_feather(file)
    logger.success(f"å·²å°†{INDEX_DICT[convert_code(code)[0]]}æ—¥æˆåˆ†è‚¡æ›´æ–°è‡³{now_str}")


def database_update_index_members():
    for k in list(INDEX_DICT.keys()):
        download_single_index_member(k)


def database_save_final_factors(df: pd.DataFrame, name: str, order: int) -> None:
    """ä¿å­˜æœ€ç»ˆå› å­çš„å› å­å€¼

    Parameters
    ----------
    df : pd.DataFrame
        æœ€ç»ˆå› å­å€¼
    name : str
        å› å­çš„åå­—ï¼Œå¦‚â€œé€‚åº¦å†’é™©â€
    order : int
        å› å­çš„åºå·
    """
    homeplace = HomePlace()
    path = homeplace.final_factor_file + name + "_" + "å¤šå› å­" + str(order) + ".feather"
    df = df.drop_duplicates().dropna(how="all")
    df.reset_index().to_feather(path)
    final_date = df.index.max()
    final_date = datetime.datetime.strftime(final_date, "%Y%m%d")
    logger.success(f"ä»Šæ—¥è®¡ç®—çš„å› å­å€¼ä¿å­˜ï¼Œæœ€æ–°ä¸€å¤©ä¸º{final_date}")


@retry
def download_single_day_money_flow(
    code: str, start_date: str, end_date: str
) -> pd.DataFrame:
    try:
        df = pro.ashare_moneyflow(
            code=code,
            start_date=start_date,
            end_date=end_date,
            fields="trade_dt,buy_value_exlarge_order,sell_value_exlarge_order,buy_value_large_order,sell_value_large_order,buy_value_med_order,sell_value_med_order,buy_value_small_order,sell_value_small_order",
        ).iloc[::-1, :]
        if df.shape[0] > 0:
            df = df.assign(code=code)
            return df
        time.sleep(0.1)
    except Exception:
        time.sleep(60)
        df = pro.ashare_moneyflow(
            code=code,
            start_date=start_date,
            end_date=end_date,
            fields="trade_dt,buy_value_exlarge_order,sell_value_exlarge_order,buy_value_large_order,sell_value_large_order,buy_value_med_order,sell_value_med_order,buy_value_small_order,sell_value_small_order",
        ).iloc[::-1, :]
        if df.shape[0] > 0:
            df = df.assign(code=code)
            return df
        time.sleep(0.1)


def database_update_money_flow():
    trs = read_daily(tr=1)
    codes = sorted(list(trs.columns))
    codes = [i for i in codes if i[0] != "8"]
    old = read_money_flow(buy=1, small=1)
    old_enddate = old.index.max()
    old_enddate_str = datetime.datetime.strftime(old_enddate, "%Y%m%d")
    new_trs = trs[trs.index > old_enddate]
    start_date = new_trs.index.min()
    start_date_str = datetime.datetime.strftime(start_date, "%Y%m%d")
    now = datetime.datetime.now()
    now_str = datetime.datetime.strftime(now, "%Y%m%d")
    logger.info(f"ä¸Šæ¬¡èµ„é‡‘æµæ•°æ®æ›´æ–°åˆ°{old_enddate_str}ï¼Œæœ¬æ¬¡å°†ä»{start_date_str}æ›´æ–°åˆ°{now_str}")
    dfs = []
    for code in tqdm.tqdm(codes):
        df = download_single_day_money_flow(
            code=code, start_date=start_date_str, end_date=now_str
        )
        dfs.append(df)
    dfs = pd.concat(dfs)
    dfs = dfs.rename(columns={"trade_dt": "date"})
    dfs.date = pd.to_datetime(dfs.date, format="%Y%m%d")
    ws = [i for i in list(dfs.columns) if i not in ["date", "code"]]
    for w in ws:
        old = pd.read_feather(
            homeplace.daily_data_file + w[:-6] + ".feather"
        ).set_index("date")
        new = dfs.pivot(index="date", columns="code", values=w)
        new = pd.concat([old, new])
        new = new[sorted(list(new.columns))]
        new.reset_index().to_feather(homeplace.daily_data_file + w[:-6] + ".feather")
    logger.success(f"å·²ç»å°†èµ„é‡‘æµæ•°æ®æ›´æ–°åˆ°{now_str}")


def database_update_zxindustry_member():
    """æ›´æ–°ä¸­ä¿¡ä¸€çº§è¡Œä¸šçš„æˆåˆ†è‚¡"""
    old_codes = pd.read_feather(homeplace.daily_data_file + "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ä»£ç ç‰ˆ.feather")
    old_names = pd.read_feather(homeplace.daily_data_file + "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡åç§°ç‰ˆ.feather")
    old_enddate = old_codes.date.max()
    old_enddate_str = datetime.datetime.strftime(old_enddate, "%Y%m%d")
    now = datetime.datetime.now()
    now_str = datetime.datetime.strftime(now, "%Y%m%d")
    logger.info(f"ä¸­ä¿¡ä¸€çº§è¡Œä¸šæ•°æ®ï¼Œä¸Šæ¬¡æ›´æ–°åˆ°äº†{old_enddate_str}ï¼Œæœ¬æ¬¡å°†æ›´æ–°è‡³{now_str}")
    start_date = old_enddate + pd.Timedelta(days=1)
    codes = list(
        set(rqdatac.all_instruments(type="CS", market="cn", date=None).order_book_id)
    )
    trs = read_daily(tr=1)
    trs = trs[trs.index > old_enddate]
    dates = list(trs.index)
    dfs_codes = []
    dfs_names = []
    for date in tqdm.tqdm_notebook(dates):
        df = rqdatac.get_instrument_industry(
            codes, source="citics_2019", date=date, level=1
        )
        if df.shape[0] > 0:
            df_code = df.first_industry_code.to_frame(date)
            df_name = df.first_industry_name.to_frame(date)
            dfs_codes.append(df_code)
            dfs_names.append(df_name)
    dfs_codes = pd.concat(dfs_codes, axis=1)
    dfs_names = pd.concat(dfs_names, axis=1)

    def new_get_dummies(df):
        dums = []
        for col in tqdm.tqdm(list(df.columns)):
            series = df[col]
            dum = pd.get_dummies(series)
            dum = dum.reset_index()
            dum = dum.assign(date=col)
            dums.append(dum)
        dums = pd.concat(dums)
        return dums

    dfs_codes = new_get_dummies(dfs_codes)
    dfs_names = new_get_dummies(dfs_names)

    a = read_daily(tr=1, start=20100101)

    def save(df, old, file):
        df = df.rename(columns={"order_book_id": "code"})
        df = df[["date", "code"] + sorted(list(df.columns)[1:-1])]
        df.code = df.code.apply(lambda x: convert_code(x)[0])
        df = pd.concat([old, df], ignore_index=True)
        df = df[df.date.isin(list(a.index))]
        df.reset_index(drop=True).to_feather(homeplace.daily_data_file + file)
        return df

    dfs_codes = save(dfs_codes, old_codes, "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ä»£ç ç‰ˆ.feather")
    dfs_names = save(dfs_names, old_names, "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡åç§°ç‰ˆ.feather")
    logger.success(f"ä¸­ä¿¡ä¸€çº§è¡Œä¸šæ•°æ®å·²ç»æ›´æ–°è‡³{now_str}äº†")


def database_update_idiosyncratic_ret():
    pb = read_daily(pb=1, start=20100101)
    cap = read_daily(flow_cap=1, start=20100101).dropna(how='all')
    fama = pure_fama([cap, pb])
    fama().reset_index().to_feather(homeplace.daily_data_file+"idiosyncratic_ret.feather")
    logger.success("ç‰¹è´¨æ”¶ç›Šç‡å·²ç»æ›´æ–°å®Œæˆ")