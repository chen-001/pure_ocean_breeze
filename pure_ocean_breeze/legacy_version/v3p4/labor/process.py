__updated__ = "2022-11-05 00:16:56"

import warnings

warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd
import knockknock as kk
import os
import tqdm
import scipy.stats as ss
import statsmodels.formula.api as smf
import matplotlib as mpl

mpl.rcParams.update(mpl.rcParamsDefault)
import matplotlib.pyplot as plt

plt.style.use(["science", "no-latex", "notebook"])
plt.rcParams["axes.unicode_minus"] = False

from functools import reduce, lru_cache, partial
from dateutil.relativedelta import relativedelta
from loguru import logger
import datetime
from collections.abc import Iterable
import plotly.express as pe
import plotly.io as pio
from plotly.tools import FigureFactory as FF
import plotly.graph_objects as go
import plotly.tools as plyoo
import pyfinance.ols as po
from texttable import Texttable
from xpinyin import Pinyin
import cufflinks as cf

cf.set_config_file(offline=True)
from typing import Callable, Union
from pure_ocean_breeze.legacy_version.v3p4.data.read_data import (
    read_daily,
    read_market,
    get_industry_dummies,
    read_swindustry_prices,
    read_zxindustry_prices,
    database_read_final_factors,
)
from pure_ocean_breeze.legacy_version.v3p4.state.homeplace import HomePlace

homeplace = HomePlace()
from pure_ocean_breeze.legacy_version.v3p4.state.states import STATES, is_notebook
from pure_ocean_breeze.legacy_version.v3p4.data.database import *
from pure_ocean_breeze.legacy_version.v3p4.data.dicts import INDUS_DICT
from pure_ocean_breeze.legacy_version.v3p4.data.tools import (
    indus_name,
    drop_duplicates_index,
    to_percent,
    to_group,
)
from pure_ocean_breeze.legacy_version.v3p4.labor.comment import (
    comments_on_twins,
    make_relative_comments,
    make_relative_comments_plot,
)


def daily_factor_on300500(
    fac: pd.DataFrame,
    hs300: bool = 0,
    zz500: bool = 0,
    zz800: bool = 0,
    zz1000: bool = 0,
    gz2000: bool = 0,
    other: bool = 0,
) -> pd.DataFrame:
    """è¾“å…¥æ—¥é¢‘æˆ–æœˆé¢‘å› å­å€¼ï¼Œå°†å…¶é™å®šåœ¨æŸæŒ‡æ•°æˆåˆ†è‚¡çš„è‚¡ç¥¨æ± å†…ï¼Œ
    ç›®å‰ä»…æ”¯æŒæ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯800ã€ä¸­è¯1000ã€å›½è¯2000æˆåˆ†è‚¡ï¼Œå’Œé™¤æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä»¥å¤–çš„è‚¡ç¥¨çš„æˆåˆ†è‚¡

    Parameters
    ----------
    fac : pd.DataFrame
        æœªé™å®šè‚¡ç¥¨æ± çš„å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    hs300 : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºæ²ªæ·±300, by default 0
    zz500 : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºä¸­è¯500, by default 0
    zz800 : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºä¸­è¯800, by default 0
    zz1000 : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºä¸­è¯1000, by default 0
    gz2000 : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºå›½è¯2000, by default 0
    other : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºé™¤æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä»¥å¤–çš„è‚¡ç¥¨çš„æˆåˆ†è‚¡, by default 0

    Returns
    -------
    `pd.DataFrame`
        ä»…åŒ…å«æˆåˆ†è‚¡åçš„å› å­å€¼ï¼Œéæˆåˆ†è‚¡çš„å› å­å€¼ä¸ºç©º

    Raises
    ------
    `ValueError`
        å¦‚æœæœªæŒ‡å®šä»»ä½•ä¸€ç§æŒ‡æ•°çš„æˆåˆ†è‚¡ï¼Œå°†æŠ¥é”™
    """
    last = fac.resample("M").last()
    homeplace = HomePlace()
    if fac.shape[0] / last.shape[0] > 2:
        if hs300:
            df = pd.read_feather(
                homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather"
            ).replace(0, np.nan)
            df = df.set_index(list(df.columns)[0])
            df = df * fac
            df = df.dropna(how="all")
        elif zz500:
            df = pd.read_feather(
                homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather"
            ).replace(0, np.nan)
            df = df.set_index(list(df.columns)[0])
            df = df * fac
            df = df.dropna(how="all")
        elif zz800:
            df1 = pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
            df1 = df1.set_index(list(df1.columns)[0])
            df2 = pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
            df2 = df2.set_index(list(df2.columns)[0])
            df = df1 + df2
            df = df.replace(0, np.nan)
            df = df * fac
            df = df.dropna(how="all")
        elif zz1000:
            df = pd.read_feather(
                homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather"
            ).replace(0, np.nan)
            df = df.set_index(list(df.columns)[0])
            df = df * fac
            df = df.dropna(how="all")
        elif gz2000:
            df = pd.read_feather(
                homeplace.daily_data_file + "å›½è¯2000æ—¥æˆåˆ†è‚¡.feather"
            ).replace(0, np.nan)
            df = df.set_index(list(df.columns)[0])
            df = df * fac
            df = df.dropna(how="all")
        elif other:
            tr = read_daily(tr=1).fillna(0).replace(0, 1)
            tr = np.sign(tr)
            df1 = (
                tr * pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
            ).fillna(0)
            df1 = df1.set_index(list(df1.columns)[0])
            df2 = (
                tr * pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
            ).fillna(0)
            df2 = df2.set_index(list(df2.columns)[0])
            df3 = (
                tr * pd.read_feather(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather")
            ).fillna(0)
            df3 = df3.set_index(list(df3.columns)[0])
            df = (1 - df1) * (1 - df2) * (1 - df3) * tr
            df = df.replace(0, np.nan) * fac
            df = df.dropna(how="all")
        else:
            raise ValueError("æ€»å¾—æŒ‡å®šä¸€ä¸‹æ˜¯å“ªä¸ªæˆåˆ†è‚¡å§ğŸ¤’")
    else:
        if hs300:
            df = pd.read_feather(
                homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather"
            ).replace(0, np.nan)
            df = df.set_index(list(df.columns)[0])
            df = df.resample("M").last()
            df = df * fac
            df = df.dropna(how="all")
        elif zz500:
            df = pd.read_feather(
                homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather"
            ).replace(0, np.nan)
            df = df.set_index(list(df.columns)[0])
            df = df.resample("M").last()
            df = df * fac
            df = df.dropna(how="all")
        elif zz800:
            df1 = pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
            df1 = df1.set_index(list(df1.columns)[0])
            df1 = df1.resample("M").last()
            df2 = pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
            df2 = df2.set_index(list(df2.columns)[0])
            df2 = df2.resample("M").last()
            df = df1 + df2
            df = df.replace(0, np.nan)
            df = df * fac
            df = df.dropna(how="all")
        elif zz1000:
            df = pd.read_feather(
                homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather"
            ).replace(0, np.nan)
            df = df.set_index(list(df.columns)[0])
            df = df.resample("M").last()
            df = df * fac
            df = df.dropna(how="all")
        elif gz2000:
            df = pd.read_feather(
                homeplace.daily_data_file + "å›½è¯2000æ—¥æˆåˆ†è‚¡.feather"
            ).replace(0, np.nan)
            df = df.set_index(list(df.columns)[0])
            df = df.resample("M").last()
            df = df * fac
            df = df.dropna(how="all")
        elif other:
            tr = read_daily(tr=1).fillna(0).replace(0, 1).resample("M").last()
            tr = np.sign(tr)
            df1 = (
                tr * pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
            ).fillna(0)
            df1 = df1.set_index(list(df1.columns)[0])
            df1 = df1.resample("M").last()
            df2 = (
                tr * pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
            ).fillna(0)
            df2 = df2.set_index(list(df2.columns)[0])
            df2 = df2.resample("M").last()
            df3 = (
                tr * pd.read_feather(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather")
            ).fillna(0)
            df3 = df3.set_index(list(df3.columns)[0])
            df3 = df3.resample("M").last()
            df = (1 - df1) * (1 - df2) * (1 - df3)
            df = df.replace(0, np.nan) * fac
            df = df.dropna(how="all")
        else:
            raise ValueError("æ€»å¾—æŒ‡å®šä¸€ä¸‹æ˜¯å“ªä¸ªæˆåˆ†è‚¡å§ğŸ¤’")
    return df


def daily_factor_on_industry(
    df: pd.DataFrame, swindustry: bool = 0, zxindustry: bool = 0
) -> dict:
    """å°†ä¸€ä¸ªå› å­å˜ä¸ºä»…åœ¨æŸä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šçš„è‚¡ç¥¨

    Parameters
    ----------
    df : pd.DataFrame
        å…¨å¸‚åœºçš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    swindustry : bool, optional
        é€‰æ‹©ä½¿ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
    zxindustry : bool, optional
        é€‰æ‹©ä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0

    Returns
    -------
    dict
        keyä¸ºè¡Œä¸šä»£ç ï¼Œvalueä¸ºå¯¹åº”çš„è¡Œä¸šä¸Šçš„å› å­å€¼
    """
    df1 = df.resample("M").last()
    if df1.shape[0] * 2 > df.shape[0]:
        daily = 0
        monthly = 1
    else:
        daily = 1
        monthly = 0
    start = int(datetime.datetime.strftime(df.index.min(), "%Y%m%d"))
    ress = get_industry_dummies(
        daily=daily,
        monthly=monthly,
        start=start,
        swindustry=swindustry,
        zxindustry=zxindustry,
    )
    ress = {k: v * df for k, v in ress.items()}
    return ress


def group_test_on_industry(
    df: pd.DataFrame,
    group_num: int = 10,
    net_values_writer: pd.ExcelWriter = None,
    swindustry: bool = 0,
    zxindustry: bool = 0,
) -> pd.DataFrame:
    """åœ¨ç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šæµ‹è¯•æ¯ä¸ªè¡Œä¸šçš„åˆ†ç»„å›æµ‹

    Parameters
    ----------
    df : pd.DataFrame
        å…¨å¸‚åœºçš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    group_num : int, optional
        åˆ†ç»„æ•°é‡, by default 10
    net_values_writer : pd.ExcelWriter, optional
        ç”¨äºå­˜å‚¨å„ä¸ªè¡Œä¸šåˆ†ç»„åŠå¤šç©ºå¯¹å†²å‡€å€¼åºåˆ—çš„excelæ–‡ä»¶, by default None
    swindustry : bool, optional
        é€‰æ‹©ä½¿ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
    zxindustry : bool, optional
        é€‰æ‹©ä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0

    Returns
    -------
    pd.DataFrame
        å„ä¸ªè¡Œä¸šçš„ç»©æ•ˆè¯„ä»·æ±‡æ€»
    """
    dfs = daily_factor_on_industry(df, swindustry=swindustry, zxindustry=zxindustry)

    ks = []
    vs = []
    if swindustry:
        for k, v in dfs.items():
            shen = pure_moonnight(
                v,
                groups_num=group_num,
                net_values_writer=net_values_writer,
                sheetname=INDUS_DICT[k],
                plt_plot=0,
            )
            ks.append(k)
            vs.append(shen.shen.total_comments.T)
        vs = pd.concat(vs)
        vs.index = [INDUS_DICT[i] for i in ks]
    else:
        for k, v in dfs.items():
            shen = pure_moonnight(
                v,
                groups_num=group_num,
                net_values_writer=net_values_writer,
                sheetname=k,
                plt_plot=0,
            )
            ks.append(k)
            vs.append(shen.shen.total_comments.T)
        vs = pd.concat(vs)
        vs.index = ks
    return vs


def rankic_test_on_industry(
    df: pd.DataFrame,
    excel_name: str = "è¡Œä¸šrankic.xlsx",
    png_name: str = "è¡Œä¸šrankicå›¾.png",
    swindustry: bool = 0,
    zxindustry: bool = 0,
) -> pd.DataFrame:
    """ä¸“é—¨è®¡ç®—å› å­å€¼åœ¨å„ä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šçš„Rank ICå€¼ï¼Œå¹¶ç»˜åˆ¶æŸ±çŠ¶å›¾

    Parameters
    ----------
    df : pd.DataFrame
        å…¨å¸‚åœºçš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    excel_name : str, optional
        ç”¨äºä¿å­˜å„ä¸ªè¡Œä¸šRank ICå€¼çš„excelæ–‡ä»¶çš„åå­—, by default 'è¡Œä¸šrankic.xlsx'
    png_name : str, optional
        ç”¨äºä¿å­˜å„ä¸ªè¡Œä¸šRank ICå€¼çš„æŸ±çŠ¶å›¾çš„åå­—, by default 'è¡Œä¸šrankicå›¾.png'
    swindustry : bool, optional
        é€‰æ‹©ä½¿ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
    zxindustry : bool, optional
        é€‰æ‹©ä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0

    Returns
    -------
    pd.DataFrame
        è¡Œä¸šåç§°ä¸å¯¹åº”çš„Rank IC
    """
    vs = group_test_on_industry(df, swindustry=swindustry, zxindustry=zxindustry)
    rankics = vs[["RankIC"]].T
    rankics.to_excel(excel_name)
    rankics.plot(kind="bar")
    plt.show()
    plt.savefig(png_name)
    return rankics


def long_test_on_industry(
    df: pd.DataFrame,
    nums: list,
    pos: bool = 0,
    neg: bool = 0,
    save_stock_list: bool = 0,
    swindustry: bool = 0,
    zxindustry: bool = 0,
) -> list[dict]:
    """å¯¹æ¯ä¸ªç”³ä¸‡/ä¸­ä¿¡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡ï¼Œä½¿ç”¨æŸå› å­æŒ‘é€‰å‡ºæœ€å¤šå¤´çš„nå€¼è‚¡ç¥¨ï¼Œè€ƒå¯Ÿå…¶è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•

    Parameters
    ----------
    df : pd.DataFrame
        ä½¿ç”¨çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    nums : list
        å¤šå¤´æƒ³é€‰å–çš„è‚¡ç¥¨çš„æ•°é‡ï¼Œä¾‹å¦‚[3,4,5]
    pos : bool, optional
        å› å­æ–¹å‘ä¸ºæ­£ï¼Œå³Rank ICä¸ºæ­£ï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºTrue, by default 0
    neg : bool, optional
        å› å­æ–¹å‘ä¸ºè´Ÿï¼Œå³Rank ICä¸ºè´Ÿï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºFalse, by default 0
    save_stock_list : bool, optional
        æ˜¯å¦ä¿å­˜æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•ï¼Œä¼šé™ä½è¿è¡Œé€Ÿåº¦, by default 0
    swindustry : bool, optional
        åœ¨ç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šæµ‹è¯•, by default 0
    zxindusrty : bool, optional
        åœ¨ä¸­ä¿¡ä¸€çº§è¡Œä¸šä¸Šæµ‹è¯•, by default 0
    Returns
    -------
    list[dict]
        è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•

    Raises
    ------
    IOError
        poså’Œnegå¿…é¡»æœ‰ä¸€ä¸ªä¸º1ï¼Œå¦åˆ™å°†æŠ¥é”™
    """
    fac = decap_industry(df, monthly=True)

    if swindustry:
        industry_dummy = pd.read_feather(
            homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather"
        ).fillna(0)
        indus = read_swindustry_prices()
    else:
        industry_dummy = pd.read_feather(
            homeplace.daily_data_file + "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡åç§°ç‰ˆ.feather"
        ).fillna(0)
        indus = read_zxindustry_prices()
    inds = list(industry_dummy.columns)
    ret_next = (
        read_daily(close=1).resample("M").last()
        / read_daily(open=1).resample("M").first()
        - 1
    )
    ages = read_daily(age=1).resample("M").last()
    ages = (ages >= 60) + 0
    ages = ages.replace(0, np.nan)
    ret_next = ret_next * ages
    ret_next_dummy = 1 - ret_next.isna()

    def save_ind(code, num):
        ind = industry_dummy[["date", "code", code]]
        ind = ind.pivot(index="date", columns="code", values=code)
        ind = ind.resample("M").last()
        ind = ind.replace(0, np.nan)
        fi = ind * fac
        fi = fi.dropna(how="all")
        fi = fi.shift(1)
        fi = fi * ret_next_dummy
        fi = fi.dropna(how="all")

        def sing(x):
            if neg:
                thr = x.nsmallest(num).iloc[-1]
            elif pos:
                thr = x.nlargest(num).iloc[-1]
            else:
                raise IOError("æ‚¨éœ€è¦æŒ‡å®šä¸€ä¸‹å› å­æ–¹å‘ğŸ¤’")
            x = (x <= thr) + 0
            return x

        fi = fi.T.apply(sing).T
        fi = fi.replace(0, np.nan)
        fi = fi * ret_next
        ret_long = fi.mean(axis=1)
        return ret_long

    ret_longs = {k: [] for k in nums}
    if is_notebook():
        for num in tqdm.tqdm_notebook(nums):
            for code in inds[2:]:
                df = save_ind(code, num).to_frame(code)
                ret_longs[num] = ret_longs[num] + [df]
    else:
        for num in tqdm.tqdm(nums):
            for code in inds[2:]:
                df = save_ind(code, num).to_frame(code)
                ret_longs[num] = ret_longs[num] + [df]

    indus = indus.resample("M").last().pct_change()

    if swindustry:
        coms = {
            k: indus_name(pd.concat(v, axis=1).dropna(how="all").T).T
            for k, v in ret_longs.items()
        }
        rets = {
            k: (v - indus_name(indus.T).T).dropna(how="all") for k, v in coms.items()
        }
    else:
        coms = {k: pd.concat(v, axis=1).dropna(how="all") for k, v in ret_longs.items()}
        rets = {k: (v - indus).dropna(how="all") for k, v in coms.items()}

    nets = {k: (v + 1).cumprod() for k, v in rets.items()}
    nets = {
        k: v.apply(lambda x: x.dropna() / x.dropna().iloc[0]) for k, v in nets.items()
    }

    def comments_on_twins(nets: pd.Series, rets: pd.Series) -> pd.Series:
        series = nets.copy()
        series1 = rets.copy()
        ret = (series.iloc[-1] - series.iloc[0]) / series.iloc[0]
        duration = (series.index[-1] - series.index[0]).days
        year = duration / 365
        ret_yearly = (series.iloc[-1] / series.iloc[0]) ** (1 / year) - 1
        max_draw = -(series / series.expanding(1).max() - 1).min()
        vol = np.std(series1) * (12**0.5)
        sharpe = ret_yearly / vol
        wins = series1[series1 > 0]
        win_rate = len(wins) / len(series1)
        return pd.Series(
            [ret, ret_yearly, vol, sharpe, win_rate, max_draw],
            index=["æ€»æ”¶ç›Šç‡", "å¹´åŒ–æ”¶ç›Šç‡", "å¹´åŒ–æ³¢åŠ¨ç‡", "ä¿¡æ¯æ¯”ç‡", "èƒœç‡", "æœ€å¤§å›æ’¤ç‡"],
        )

    if swindustry:
        name = "ç”³ä¸‡"
    else:
        name = "ä¸­ä¿¡"
    w = pd.ExcelWriter(f"å„ä¸ª{name}ä¸€çº§è¡Œä¸šå¤šå¤´è¶…é¢ç»©æ•ˆ.xlsx")

    def com_all(df1, df2, num):
        cs = []
        for ind in list(df1.columns):
            c = comments_on_twins(df2[ind].dropna(), df1[ind].dropna()).to_frame(ind)
            cs.append(c)
        res = pd.concat(cs, axis=1).T
        res.to_excel(w, sheet_name=str(num))
        return res

    coms_finals = {k: com_all(rets[k], nets[k], k) for k in rets.keys()}
    w.save()
    w.close()

    rets_save = {k: v.dropna() for k, v in rets.items() if k in nums}
    u = pd.ExcelWriter(f"å„ä¸ª{name}ä¸€çº§è¡Œä¸šæ¯æœˆè¶…é¢æ”¶ç›Šç‡.xlsx")
    for k, v in rets_save.items():
        v.to_excel(u, sheet_name=str(k))
    u.save()
    u.close()

    if save_stock_list:

        def save_ind_stocks(code, num):
            ind = industry_dummy[["date", "code", code]]
            ind = ind.pivot(index="date", columns="code", values=code)
            ind = ind.replace(0, np.nan)
            fi = ind * fac
            fi = fi.dropna(how="all")
            fi = fi.shift(1)
            fi = fi * ret_next_dummy
            fi = fi.dropna(how="all")

            def sing(x):
                if neg:
                    thr = x.nsmallest(num)
                elif pos:
                    thr = x.nlargest(num)
                else:
                    raise IOError("æ‚¨éœ€è¦æŒ‡å®šä¸€ä¸‹å› å­æ–¹å‘ğŸ¤’")
                return tuple(thr.index)

            fi = fi.T.apply(sing)
            return fi

        stocks_longs = {k: {} for k in nums}
        if is_notebook():
            for num in tqdm.tqdm_notebook(nums):
                for code in inds[2:]:
                    stocks_longs[num][code] = save_ind_stocks(code, num)
        else:
            for num in tqdm.tqdm(nums):
                for code in inds[2:]:
                    stocks_longs[num][code] = save_ind_stocks(code, num)

        for num in nums:
            w1 = pd.ExcelWriter(f"å„ä¸ª{name}ä¸€çº§è¡Œä¸šä¹°{num}åªçš„è‚¡ç¥¨åå•.xlsx")
            for k, v in stocks_longs[num].items():
                v = v.T
                v.index = v.index.strftime("%Y/%m/%d")
                v.to_excel(w1, sheet_name=INDUS_DICT[k])
            w1.save()
            w1.close()

        return [coms_finals, rets_save, stocks_longs]
    else:
        return [coms_finals, rets_save]


def long_test_on_swindustry(
    df: pd.DataFrame,
    nums: list,
    pos: bool = 0,
    neg: bool = 0,
    save_stock_list: bool = 0,
) -> list[dict]:
    """å¯¹æ¯ä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡ï¼Œä½¿ç”¨æŸå› å­æŒ‘é€‰å‡ºæœ€å¤šå¤´çš„nå€¼è‚¡ç¥¨ï¼Œè€ƒå¯Ÿå…¶è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•

    Parameters
    ----------
    df : pd.DataFrame
        ä½¿ç”¨çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    nums : list
        å¤šå¤´æƒ³é€‰å–çš„è‚¡ç¥¨çš„æ•°é‡ï¼Œä¾‹å¦‚[3,4,5]
    pos : bool, optional
        å› å­æ–¹å‘ä¸ºæ­£ï¼Œå³Rank ICä¸ºæ­£ï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºTrue, by default 0
    neg : bool, optional
        å› å­æ–¹å‘ä¸ºè´Ÿï¼Œå³Rank ICä¸ºè´Ÿï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºFalse, by default 0
    save_stock_list : bool, optional
        æ˜¯å¦ä¿å­˜æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•ï¼Œä¼šé™ä½è¿è¡Œé€Ÿåº¦, by default 0
    Returns
    -------
    list[dict]
        è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•

    Raises
    ------
    IOError
        poså’Œnegå¿…é¡»æœ‰ä¸€ä¸ªä¸º1ï¼Œå¦åˆ™å°†æŠ¥é”™
    """
    res = long_test_on_industry(
        df=df,
        nums=nums,
        pos=pos,
        neg=neg,
        save_stock_list=save_stock_list,
        swindustry=1,
    )
    return res


def long_test_on_zxindustry(
    df: pd.DataFrame,
    nums: list,
    pos: bool = 0,
    neg: bool = 0,
    save_stock_list: bool = 0,
) -> list[dict]:
    """å¯¹æ¯ä¸ªä¸­ä¿¡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡ï¼Œä½¿ç”¨æŸå› å­æŒ‘é€‰å‡ºæœ€å¤šå¤´çš„nå€¼è‚¡ç¥¨ï¼Œè€ƒå¯Ÿå…¶è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•

    Parameters
    ----------
    df : pd.DataFrame
        ä½¿ç”¨çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    nums : list
        å¤šå¤´æƒ³é€‰å–çš„è‚¡ç¥¨çš„æ•°é‡ï¼Œä¾‹å¦‚[3,4,5]
    pos : bool, optional
        å› å­æ–¹å‘ä¸ºæ­£ï¼Œå³Rank ICä¸ºæ­£ï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºTrue, by default 0
    neg : bool, optional
        å› å­æ–¹å‘ä¸ºè´Ÿï¼Œå³Rank ICä¸ºè´Ÿï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºFalse, by default 0
    save_stock_list : bool, optional
        æ˜¯å¦ä¿å­˜æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•ï¼Œä¼šé™ä½è¿è¡Œé€Ÿåº¦, by default 0
    Returns
    -------
    list[dict]
        è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•

    Raises
    ------
    IOError
        poså’Œnegå¿…é¡»æœ‰ä¸€ä¸ªä¸º1ï¼Œå¦åˆ™å°†æŠ¥é”™
    """
    res = long_test_on_industry(
        df=df,
        nums=nums,
        pos=pos,
        neg=neg,
        save_stock_list=save_stock_list,
        zxindustry=1,
    )
    return res


@kk.desktop_sender(title="å˜¿ï¼Œè¡Œä¸šä¸­æ€§åŒ–åšå®Œå•¦ï½ğŸ›")
def decap(df: pd.DataFrame, daily: bool = 0, monthly: bool = 0) -> pd.DataFrame:
    """å¯¹å› å­åšå¸‚å€¼ä¸­æ€§åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        æœªä¸­æ€§åŒ–çš„å› å­ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    daily : bool, optional
        æœªä¸­æ€§åŒ–å› å­æ˜¯æ—¥é¢‘çš„åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0, by default 0
    monthly : bool, optional
        æœªä¸­æ€§åŒ–å› å­æ˜¯æœˆé¢‘çš„åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0, by default 0

    Returns
    -------
    `pd.DataFrame`
        å¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­

    Raises
    ------
    `NotImplementedError`
        å¦‚æœæœªæŒ‡å®šæ—¥é¢‘æˆ–æœˆé¢‘ï¼Œå°†æŠ¥é”™
    """
    if is_notebook():
        tqdm.tqdm_notebook().pandas()
    else:
        tqdm.tqdm.pandas()
    share = read_daily(sharenum=1)
    undi_close = read_daily(close=1, unadjust=1)
    cap = (share * undi_close).stack().reset_index()
    cap.columns = ["date", "code", "cap"]
    cap.cap = ss.boxcox(cap.cap)[0]

    def single(x):
        x.cap = ss.boxcox(x.cap)[0]
        return x

    cap = cap.groupby(["date"]).apply(single)
    cap = cap.set_index(["date", "code"]).unstack()
    cap.columns = [i[1] for i in list(cap.columns)]
    cap_monthly = cap.resample("M").last()
    last = df.resample("M").last()
    if df.shape[0] / last.shape[0] < 2:
        monthly = True
    else:
        daily = True
    if daily:
        df = (pure_fallmount(df) - (pure_fallmount(cap),))()
    elif monthly:
        df = (pure_fallmount(df) - (pure_fallmount(cap_monthly),))()
    else:
        raise NotImplementedError("å¿…é¡»æŒ‡å®šé¢‘ç‡")
    return df


@kk.desktop_sender(title="å˜¿ï¼Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–åšå®Œå•¦ï½ğŸ›")
def decap_industry(
    df: pd.DataFrame,
    daily: bool = 0,
    monthly: bool = 0,
    swindustry: bool = 0,
    zxindustry: bool = 0,
) -> pd.DataFrame:
    """å¯¹å› å­åšè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        æœªä¸­æ€§åŒ–çš„å› å­ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    daily : bool, optional
        æœªä¸­æ€§åŒ–å› å­æ˜¯æ—¥é¢‘çš„åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0, by default 0
    monthly : bool, optional
        æœªä¸­æ€§åŒ–å› å­æ˜¯æœˆé¢‘çš„åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0, by default 0
    swindustry : bool, optional
        é€‰æ‹©ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
    zxindustry : bool, optional
        é€‰æ‹©ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0

    Returns
    -------
    `pd.DataFrame`
        è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­

    Raises
    ------
    `NotImplementedError`
        å¦‚æœæœªæŒ‡å®šæ—¥é¢‘æˆ–æœˆé¢‘ï¼Œå°†æŠ¥é”™
    """
    start_date = int(datetime.datetime.strftime(df.index.min(), "%Y%m%d"))
    last = df.resample("M").last()
    homeplace = HomePlace()
    if daily == 0 and monthly == 0:
        if df.shape[0] / last.shape[0] < 2:
            monthly = True
        else:
            daily = True
    if monthly:
        cap = read_daily(flow_cap=1, start=start_date).resample("M").last()
    else:
        cap = read_daily(flow_cap=1, start=start_date)
    cap = cap.stack().reset_index()
    cap.columns = ["date", "code", "cap"]
    cap.cap = ss.boxcox(cap.cap)[0]

    def single(x):
        x.cap = ss.boxcox(x.cap)[0]
        return x

    cap = cap.groupby(["date"]).apply(single)
    df = df.stack().reset_index()
    df.columns = ["date", "code", "fac"]
    df = pd.merge(df, cap, on=["date", "code"])

    def neutralize_factors(df):
        """ç»„å†…å¯¹å› å­è¿›è¡Œå¸‚å€¼ä¸­æ€§åŒ–"""
        industry_codes = list(df.columns)
        industry_codes = [i for i in industry_codes if i.startswith("w")]
        industry_codes_str = "+".join(industry_codes)
        ols_result = smf.ols("fac~cap+" + industry_codes_str, data=df).fit()
        ols_w = ols_result.params["cap"]
        ols_b = ols_result.params["Intercept"]
        ols_bs = {}
        for ind in industry_codes:
            ols_bs[ind] = ols_result.params[ind]
        df.fac = df.fac - ols_w * df.cap - ols_b
        for k, v in ols_bs.items():
            df.fac = df.fac - v * df[k]
        df = df[["fac"]]
        return df

    if swindustry:
        file_name = "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather"
    else:
        file_name = "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ä»£ç ç‰ˆ.feather"

    if monthly:
        industry_dummy = (
            pd.read_feather(homeplace.daily_data_file + file_name)
            .fillna(0)
            .set_index("date")
            .groupby("code")
            .resample("M")
            .last()
        )
        industry_dummy = industry_dummy.fillna(0).drop(columns=["code"]).reset_index()
        industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
        col = ["code", "date"] + industry_ws
    elif daily:
        industry_dummy = pd.read_feather(homeplace.daily_data_file + file_name).fillna(
            0
        )
        industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
        col = ["date", "code"] + industry_ws
    else:
        raise NotImplementedError("å¿…é¡»æŒ‡å®šé¢‘ç‡")
    industry_dummy.columns = col
    df = pd.merge(df, industry_dummy, on=["date", "code"])
    df = df.set_index(["date", "code"])
    if is_notebook():
        tqdm.tqdm_notebook().pandas()
    else:
        tqdm.tqdm.pandas()
    df = df.groupby(["date"]).progress_apply(neutralize_factors)
    df = df.unstack()
    df.columns = [i[1] for i in list(df.columns)]
    return df


def deboth(df: pd.DataFrame) -> pd.DataFrame:
    """é€šè¿‡å›æµ‹çš„æ–¹å¼ï¼Œå¯¹æœˆé¢‘å› å­åšè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        æœªä¸­æ€§åŒ–çš„å› å­

    Returns
    -------
    `pd.DataFrame`
        è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­
    """
    shen = pure_moonnight(df, boxcox=1, plt_plot=0, print_comments=0)
    return shen()


def boom_four(
    df: pd.DataFrame, backsee: int = 20, daily: bool = 0, min_periods: int = None
) -> tuple[pd.DataFrame]:
    """ç”Ÿæˆ20å¤©å‡å€¼ï¼Œ20å¤©æ ‡å‡†å·®ï¼ŒåŠäºŒè€…æ­£å‘z-scoreåˆæˆï¼Œæ­£å‘æ’åºåˆæˆï¼Œè´Ÿå‘z-scoreåˆæˆï¼Œè´Ÿå‘æ’åºåˆæˆè¿™6ä¸ªå› å­

    Parameters
    ----------
    df : pd.DataFrame
        åŸæ—¥é¢‘å› å­
    backsee : int, optional
        å›çœ‹å¤©æ•°, by default 20
    daily : bool, optional
        ä¸º1æ˜¯æ¯å¤©éƒ½æ»šåŠ¨ï¼Œä¸º0åˆ™ä»…ä¿ç•™æœˆåº•å€¼, by default 0
    min_periods : int, optional
        rollingæ—¶çš„æœ€å°æœŸ, by default backseeçš„ä¸€åŠ

    Returns
    -------
    `tuple[pd.DataFrame]`
        6ä¸ªå› å­çš„å…ƒç»„
    """
    if min_periods is None:
        min_periods = int(backsee * 0.5)
    if not daily:
        df_mean = (
            df.rolling(backsee, min_periods=min_periods).mean().resample("M").last()
        )
        df_std = df.rolling(backsee, min_periods=min_periods).std().resample("M").last()
        twins_add = (pure_fallmount(df_mean) + (pure_fallmount(df_std),))()
        rtwins_add = df_mean.rank(axis=1) + df_std.rank(axis=1)
        twins_minus = (pure_fallmount(df_mean) + (pure_fallmount(-df_std),))()
        rtwins_minus = df_mean.rank(axis=1) - df_std.rank(axis=1)
    else:
        df_mean = df.rolling(backsee, min_periods=min_periods).mean()
        df_std = df.rolling(backsee, min_periods=min_periods).std()
        twins_add = (pure_fallmount(df_mean) + (pure_fallmount(df_std),))()
        rtwins_add = df_mean.rank(axis=1) + df_std.rank(axis=1)
        twins_minus = (pure_fallmount(df_mean) + (pure_fallmount(-df_std),))()
        rtwins_minus = df_mean.rank(axis=1) - df_std.rank(axis=1)
    return df_mean, df_std, twins_add, rtwins_add, twins_minus, rtwins_minus


def add_cross_standardlize(*args: list) -> pd.DataFrame:
    """å°†ä¼—å¤šå› å­æ¨ªæˆªé¢åšz-scoreæ ‡å‡†åŒ–ä¹‹åç›¸åŠ 

    Returns
    -------
    `pd.DataFrame`
        åˆæˆåçš„å› å­
    """
    fms = [pure_fallmount(i) for i in args]
    one = fms[0]
    others = fms[1:]
    final = one + others
    return final()


def to_tradeends(df: pd.DataFrame) -> pd.DataFrame:
    """å°†æœ€åä¸€ä¸ªè‡ªç„¶æ—¥æ”¹å˜ä¸ºæœ€åä¸€ä¸ªäº¤æ˜“æ—¥

    Parameters
    ----------
    df : pd.DataFrame
        indexä¸ºæ—¶é—´ï¼Œä¸ºæ¯ä¸ªæœˆçš„æœ€åä¸€å¤©

    Returns
    -------
    `pd.DataFrame`
        ä¿®æ”¹ä¸ºäº¤æ˜“æ—¥æ ‡æ³¨åçš„pd.DataFrame
    """
    """"""
    trs = read_daily(tr=1)
    trs = trs.assign(tradeends=list(trs.index))
    trs = trs[["tradeends"]]
    trs = trs.resample("M").last()
    df = pd.concat([trs, df], axis=1)
    df = df.set_index(["tradeends"])
    return df


def market_kind(
    df: pd.DataFrame,
    zhuban: bool = 0,
    chuangye: bool = 0,
    kechuang: bool = 0,
    beijing: bool = 0,
) -> pd.DataFrame:
    """ä¸å®½åŸºæŒ‡æ•°æˆåˆ†è‚¡çš„å‡½æ•°ç±»ä¼¼ï¼Œé™å®šè‚¡ç¥¨åœ¨æŸä¸ªå…·ä½“æ¿å—ä¸Š

    Parameters
    ----------
    df : pd.DataFrame
        åŸå§‹å…¨éƒ¨è‚¡ç¥¨çš„å› å­å€¼
    zhuban : bool, optional
        é™å®šåœ¨ä¸»æ¿èŒƒå›´å†…, by default 0
    chuangye : bool, optional
        é™å®šåœ¨åˆ›ä¸šæ¿èŒƒå›´å†…, by default 0
    kechuang : bool, optional
        é™å®šåœ¨ç§‘åˆ›æ¿èŒƒå›´å†…, by default 0
    beijing : bool, optional
        é™å®šåœ¨åŒ—äº¤æ‰€èŒƒå›´å†…, by default 0

    Returns
    -------
    `pd.DataFrame`
        é™åˆ¶èŒƒå›´åçš„å› å­å€¼ï¼Œå…¶ä½™ä¸ºç©º

    Raises
    ------
    `ValueError`
        å¦‚æœæœªæŒ‡å®šä»»ä½•è‚¡ç¥¨æ± ï¼Œå°†æŠ¥é”™
    """
    trs = read_daily(tr=1)
    codes = list(trs.columns)
    dates = list(trs.index)
    if chuangye and kechuang:
        dummys = [1 if code[:2] in ["30", "68"] else np.nan for code in codes]
    else:
        if zhuban:
            dummys = [1 if code[:2] in ["00", "60"] else np.nan for code in codes]
        elif chuangye:
            dummys = [1 if code.startswith("3") else np.nan for code in codes]
        elif kechuang:
            dummys = [1 if code.startswith("68") else np.nan for code in codes]
        elif beijing:
            dummys = [1 if code.startswith("8") else np.nan for code in codes]
        else:
            raise ValueError("ä½ æ€»å¾—é€‰ä¸€ä¸ªè‚¡ç¥¨æ± å§ï¼ŸğŸ¤’")
    dummy_dict = {k: v for k, v in zip(codes, dummys)}
    dummy_df = pd.DataFrame(dummy_dict, index=dates)
    df = df * dummy_df
    return df


def show_corr(
    fac1: pd.DataFrame,
    fac2: pd.DataFrame,
    method: str = "spearman",
    plt_plot: bool = 1,
    show_series: bool = 0,
) -> float:
    """å±•ç¤ºä¸¤ä¸ªå› å­çš„æˆªé¢ç›¸å…³æ€§

    Parameters
    ----------
    fac1 : pd.DataFrame
        å› å­1
    fac2 : pd.DataFrame
        å› å­2
    method : str, optional
        è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "spearman"
    plt_plot : bool, optional
        æ˜¯å¦ç”»å‡ºç›¸å…³ç³»æ•°çš„æ—¶åºå˜åŒ–å›¾, by default 1
    show_series : bool, optional
        è¿”å›ç›¸å…³æ€§çš„åºåˆ—ï¼Œè€Œéå‡å€¼

    Returns
    -------
    `float`
        å¹³å‡æˆªé¢ç›¸å…³ç³»æ•°
    """
    corr = show_x_with_func(fac1, fac2, lambda x: x.corr(method=method).iloc[0, 1])
    if show_series:
        return corr
    else:
        if plt_plot:
            corr.plot(rot=60)
            plt.show()
        return corr.mean()


def show_corrs(
    factors: list[pd.DataFrame],
    factor_names: list[str] = None,
    print_bool: bool = True,
    show_percent: bool = True,
    method: str = "spearman",
) -> pd.DataFrame:
    """å±•ç¤ºå¾ˆå¤šå› å­ä¸¤ä¸¤ä¹‹é—´çš„æˆªé¢ç›¸å…³æ€§

    Parameters
    ----------
    factors : list[pd.DataFrame]
        æ‰€æœ‰å› å­æ„æˆçš„åˆ—è¡¨, by default None
    factor_names : list[str], optional
        ä¸Šè¿°å› å­ä¾æ¬¡çš„åå­—, by default None
    print_bool : bool, optional
        æ˜¯å¦æ‰“å°å‡ºä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼, by default True
    show_percent : bool, optional
        æ˜¯å¦ä»¥ç™¾åˆ†æ•°çš„å½¢å¼å±•ç¤º, by default True
    method : str, optional
        è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "spearman"

    Returns
    -------
    `pd.DataFrame`
        ä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼
    """
    corrs = []
    for i in range(len(factors)):
        main_i = factors[i]
        follows = factors[i + 1 :]
        corr = [show_corr(main_i, i, plt_plot=False, method=method) for i in follows]
        corr = [np.nan] * (i + 1) + corr
        corrs.append(corr)
    if factor_names is None:
        factor_names = [f"fac{i}" for i in list(range(1, len(factors) + 1))]
    corrs = pd.DataFrame(corrs, columns=factor_names, index=factor_names)
    np.fill_diagonal(corrs.to_numpy(), 1)
    if show_percent:
        pcorrs = corrs.applymap(to_percent)
    else:
        pcorrs = corrs.copy()
    if print_bool:
        print(pcorrs)
    return corrs


def show_cov(
    fac1: pd.DataFrame,
    fac2: pd.DataFrame,
    plt_plot: bool = 1,
    show_series: bool = 0,
) -> float:
    """å±•ç¤ºä¸¤ä¸ªå› å­çš„æˆªé¢ç›¸å…³æ€§

    Parameters
    ----------
    fac1 : pd.DataFrame
        å› å­1
    fac2 : pd.DataFrame
        å› å­2
    method : str, optional
        è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "spearman"
    plt_plot : bool, optional
        æ˜¯å¦ç”»å‡ºç›¸å…³ç³»æ•°çš„æ—¶åºå˜åŒ–å›¾, by default 1
    show_series : bool, optional
        è¿”å›ç›¸å…³æ€§çš„åºåˆ—ï¼Œè€Œéå‡å€¼

    Returns
    -------
    `float`
        å¹³å‡æˆªé¢ç›¸å…³ç³»æ•°
    """
    cov = show_x_with_func(fac1, fac2, lambda x: x.cov().iloc[0, 1])
    if show_series:
        return cov
    else:
        if plt_plot:
            cov.plot(rot=60)
            plt.show()
        return cov.mean()


def show_x_with_func(
    fac1: pd.DataFrame,
    fac2: pd.DataFrame,
    func: Callable,
) -> pd.Series:
    """å±•ç¤ºä¸¤ä¸ªå› å­çš„æŸç§æˆªé¢å…³ç³»

    Parameters
    ----------
    fac1 : pd.DataFrame
        å› å­1
    fac2 : pd.DataFrame
        å› å­2
    func : Callable
        è¦å¯¹ä¸¤ä¸ªå› å­åœ¨æˆªé¢ä¸Šçš„è¿›è¡Œçš„æ“ä½œ

    Returns
    -------
    `pd.Series`
        æˆªé¢å…³ç³»
    """
    the_func = partial(func)
    both1 = fac1.stack().reset_index()
    befo1 = fac2.stack().reset_index()
    both1.columns = ["date", "code", "both"]
    befo1.columns = ["date", "code", "befo"]
    twins = pd.merge(both1, befo1, on=["date", "code"]).set_index(["date", "code"])
    corr = twins.groupby("date").apply(the_func)
    return corr


def show_covs(
    factors: list[pd.DataFrame],
    factor_names: list[str] = None,
    print_bool: bool = True,
    show_percent: bool = True,
    method: str = "spearman",
) -> pd.DataFrame:
    """å±•ç¤ºå¾ˆå¤šå› å­ä¸¤ä¸¤ä¹‹é—´çš„æˆªé¢ç›¸å…³æ€§

    Parameters
    ----------
    factors : list[pd.DataFrame]
        æ‰€æœ‰å› å­æ„æˆçš„åˆ—è¡¨, by default None
    factor_names : list[str], optional
        ä¸Šè¿°å› å­ä¾æ¬¡çš„åå­—, by default None
    print_bool : bool, optional
        æ˜¯å¦æ‰“å°å‡ºä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼, by default True
    show_percent : bool, optional
        æ˜¯å¦ä»¥ç™¾åˆ†æ•°çš„å½¢å¼å±•ç¤º, by default True
    method : str, optional
        è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "spearman"

    Returns
    -------
    `pd.DataFrame`
        ä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼
    """
    corrs = []
    for i in range(len(factors)):
        main_i = factors[i]
        follows = factors[i + 1 :]
        corr = [show_cov(main_i, i, plt_plot=False) for i in follows]
        corr = [np.nan] * (i + 1) + corr
        corrs.append(corr)
    if factor_names is None:
        factor_names = [f"fac{i}" for i in list(range(1, len(factors) + 1))]
    corrs = pd.DataFrame(corrs, columns=factor_names, index=factor_names)
    np.fill_diagonal(corrs.to_numpy(), 1)
    if show_percent:
        pcorrs = corrs.applymap(to_percent)
    else:
        pcorrs = corrs.copy()
    if print_bool:
        print(pcorrs)
    return corrs


def de_cross(
    y: pd.DataFrame, xs: Union[list[pd.DataFrame], pd.DataFrame]
) -> pd.DataFrame:
    """ä½¿ç”¨è‹¥å¹²å› å­å¯¹æŸä¸ªå› å­è¿›è¡Œæ­£äº¤åŒ–å¤„ç†

    Parameters
    ----------
    y : pd.DataFrame
        ç ”ç©¶çš„ç›®æ ‡ï¼Œå›å½’ä¸­çš„y
    xs : Union[list[pd.DataFrame],pd.DataFrame]
        ç”¨äºæ­£äº¤åŒ–çš„è‹¥å¹²å› å­ï¼Œå›å½’ä¸­çš„x

    Returns
    -------
    pd.DataFrame
        æ­£äº¤åŒ–ä¹‹åçš„å› å­
    """
    if not isinstance(xs, list):
        xs = [xs]
    y = pure_fallmount(y)
    xs = [pure_fallmount(i) for i in xs]
    return (y - xs)()


def show_corrs_with_old(
    df: pd.DataFrame = None, method: str = "spearman"
) -> pd.DataFrame:
    """è®¡ç®—æ–°å› å­å’Œå·²æœ‰å› å­çš„ç›¸å…³ç³»æ•°

    Parameters
    ----------
    df : pd.DataFrame, optional
        æ–°å› å­, by default None
    method : str, optional
        æ±‚ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default 'spearman'

    Returns
    -------
    pd.DataFrame
        ç›¸å…³ç³»æ•°çŸ©é˜µ
    """
    if df is not None:
        df0 = df.resample("M").last()
        if df.shape[0] / df0.shape[0] > 2:
            daily = 1
        else:
            daily = 0
    olds = []
    for i in range(1, 100):
        try:
            if daily:
                old = database_read_final_factors(order=i)[0]
            else:
                old = database_read_final_factors(order=i)[0].resample("M").last()
            olds.append(old)
        except Exception:
            break
    if df is not None:
        olds = [df] + olds
        corrs = show_corrs(
            olds, ["new"] + [f"old{i}" for i in range(1, len(olds))], method=method
        )
    else:
        corrs = show_corrs(
            olds, [f"old{i}" for i in range(1, len(olds))], method=method
        )
    return corrs


class pure_moon(object):
    __slots__ = [
        "homeplace",
        "sts_monthly_file",
        "states_monthly_file",
        "factors",
        "codes",
        "tradedays",
        "ages",
        "amounts",
        "closes",
        "opens",
        "capital",
        "states",
        "sts",
        "turnovers",
        "sts_monthly",
        "states_monthly",
        "ages_monthly",
        "tris_monthly",
        "opens_monthly",
        "closes_monthly",
        "rets_monthly",
        "opens_monthly_shift",
        "rets_monthly_begin",
        "limit_ups",
        "limit_downs",
        "data",
        "ic_icir_and_rank",
        "rets_monthly_limit_downs",
        "group_rets",
        "long_short_rets",
        "long_short_net_values",
        "group_net_values",
        "long_short_ret_yearly",
        "long_short_vol_yearly",
        "long_short_info_ratio",
        "long_short_win_times",
        "long_short_win_ratio",
        "retreats",
        "max_retreat",
        "long_short_comments",
        "total_comments",
        "square_rets",
        "cap",
        "cap_value",
        "industry_dummy",
        "industry_codes",
        "industry_codes_str",
        "industry_ws",
        "__factors_out",
        "ics",
        "rankics",
        "factor_turnover_rates",
        "factor_turnover_rate",
        "group_rets_std",
        "group_rets_stds",
        "wind_out",
        "swindustry_dummy",
        "zxindustry_dummy",
        "closes2_monthly",
        "rets_monthly_last",
    ]

    @classmethod
    @lru_cache(maxsize=None)
    def __init__(
        cls,
        no_read_indu: bool = 0,
        swindustry_dummy: pd.DataFrame = None,
        zxindustry_dummy: pd.DataFrame = None,
        read_in_swindustry_dummy: bool = 0,
    ):
        cls.homeplace = HomePlace()
        # å·²ç»ç®—å¥½çš„æœˆåº¦stçŠ¶æ€æ–‡ä»¶
        cls.sts_monthly_file = homeplace.daily_data_file + "sts_monthly.feather"
        # å·²ç»ç®—å¥½çš„æœˆåº¦äº¤æ˜“çŠ¶æ€æ–‡ä»¶
        cls.states_monthly_file = homeplace.daily_data_file + "states_monthly.feather"

        if swindustry_dummy is not None:
            cls.swindustry_dummy = swindustry_dummy
        if zxindustry_dummy is not None:
            cls.zxindustry_dummy = zxindustry_dummy

        def deal_dummy(industry_dummy):
            industry_dummy = industry_dummy.drop(columns=["code"]).reset_index()
            industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
            col = ["code", "date"] + industry_ws
            industry_dummy.columns = col
            industry_dummy = industry_dummy[
                industry_dummy.date >= pd.Timestamp("20100101")
            ]
            return industry_dummy

        if (swindustry_dummy is None) and (zxindustry_dummy is None):

            if not no_read_indu:
                if read_in_swindustry_dummy:
                    cls.swindustry_dummy = (
                        pd.read_feather(
                            cls.homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather"
                        )
                        .fillna(0)
                        .set_index("date")
                        .groupby("code")
                        .resample("M")
                        .last()
                    )
                    cls.swindustry_dummy = deal_dummy(cls.swindustry_dummy)

                cls.zxindustry_dummy = (
                    pd.read_feather(
                        cls.homeplace.daily_data_file + "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ä»£ç ç‰ˆ.feather"
                    )
                    .fillna(0)
                    .set_index("date")
                    .groupby("code")
                    .resample("M")
                    .last()
                    .fillna(0)
                )

                cls.zxindustry_dummy = deal_dummy(cls.zxindustry_dummy)

    @property
    def factors_out(self):
        return self.__factors_out

    def __call__(self):
        """è°ƒç”¨å¯¹è±¡åˆ™è¿”å›å› å­å€¼"""
        return self.factors_out

    @classmethod
    def set_basic_data(
        cls,
        age: pd.DataFrame,
        st: pd.DataFrame,
        state: pd.DataFrame,
        open: pd.DataFrame,
        close: pd.DataFrame,
        capital: pd.DataFrame,
    ):
        # ä¸Šå¸‚å¤©æ•°æ–‡ä»¶
        cls.ages = age
        # stæ—¥å­æ ‡å¿—æ–‡ä»¶
        cls.sts = st.fillna(0)
        # cls.sts = 1 - cls.sts.fillna(0)
        # äº¤æ˜“çŠ¶æ€æ–‡ä»¶
        cls.states = state
        # å¤æƒå¼€ç›˜ä»·æ•°æ®æ–‡ä»¶
        cls.opens = open
        # å¤æƒæ”¶ç›˜ä»·æ•°æ®æ–‡ä»¶
        cls.closes = close
        # æœˆåº•æµé€šå¸‚å€¼æ•°æ®
        cls.capital = capital
        cls.opens = cls.opens.replace(0, np.nan)
        cls.closes = cls.closes.replace(0, np.nan)

    def set_factor_df_date_as_index(self, df):
        """è®¾ç½®å› å­æ•°æ®çš„dataframeï¼Œå› å­è¡¨åˆ—ååº”ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•åº”ä¸ºæ—¶é—´"""
        df = df.reset_index()
        df.columns = ["date"] + list(df.columns)[1:]
        self.factors = df
        self.factors = self.factors.set_index("date")
        self.factors = self.factors.resample("M").last()
        self.factors = self.factors.reset_index()

    @classmethod
    def judge_month_st(cls, df):
        """æ¯”è¾ƒä¸€ä¸ªæœˆå†…stçš„å¤©æ•°ï¼Œå¦‚æœstå¤©æ•°å¤šï¼Œå°±åˆ é™¤æœ¬æœˆï¼Œå¦‚æœæ­£å¸¸å¤šï¼Œå°±ä¿ç•™æœ¬æœˆ"""
        st_count = len(df[df == 1])
        normal_count = len(df[df != 1])
        if st_count >= normal_count:
            return 0
        else:
            return 1

    @classmethod
    def judge_month_state(cls, df):
        """æ¯”è¾ƒä¸€ä¸ªæœˆå†…éæ­£å¸¸äº¤æ˜“çš„å¤©æ•°ï¼Œå¦‚æœéæ­£å¸¸äº¤æ˜“å¤©æ•°å¤šï¼Œå°±åˆ é™¤æœ¬æœˆï¼Œå¦åˆ™ä¿ç•™æœ¬æœˆ"""
        abnormal_count = len(df[df == 0])
        normal_count = len(df[df == 1])
        if abnormal_count >= normal_count:
            return 0
        else:
            return 1

    @classmethod
    def read_add(cls, pridf, df, func):
        """ç”±äºæ•°æ®æ›´æ–°ï¼Œè¿‡å»è®¡ç®—çš„æœˆåº¦çŠ¶æ€å¯èƒ½éœ€è¦è¿½åŠ """
        if pridf.index.max() > df.index.max():
            df_add = pridf[pridf.index > df.index.max()]
            df_add = df_add.resample("M").apply(func)
            df = pd.concat([df, df_add])
            return df
        else:
            return df

    @classmethod
    def daily_to_monthly(cls, pridf, path, func):
        """æŠŠæ—¥åº¦çš„äº¤æ˜“çŠ¶æ€ã€stã€ä¸Šå¸‚å¤©æ•°ï¼Œè½¬åŒ–ä¸ºæœˆåº¦çš„ï¼Œå¹¶ç”Ÿæˆèƒ½å¦äº¤æ˜“çš„åˆ¤æ–­
        è¯»å–æœ¬åœ°å·²ç»ç®—å¥½çš„æ–‡ä»¶ï¼Œå¹¶è¿½åŠ æ–°çš„æ—¶é—´æ®µéƒ¨åˆ†ï¼Œå¦‚æœæœ¬åœ°æ²¡æœ‰å°±ç›´æ¥å…¨éƒ¨é‡æ–°ç®—"""
        try:
            month_df = pd.read_feather(path)
            month_df = month_df.set_index(list(month_df.columns)[0])
            month_df = cls.read_add(pridf, month_df, func)
            month_df.reset_index().to_feather(path)
        except Exception as e:
            if not STATES["NO_LOG"]:
                logger.error("error occurs when read state files")
                logger.error(e)
            print("state file rewritingâ€¦â€¦")
            month_df = pridf.resample("M").apply(func)
            month_df.reset_index().to_feather(path)
        return month_df

    @classmethod
    @lru_cache(maxsize=None)
    def judge_month(cls):
        """ç”Ÿæˆä¸€ä¸ªæœˆç»¼åˆåˆ¤æ–­çš„è¡¨æ ¼"""

        cls.sts_monthly = cls.daily_to_monthly(
            cls.sts, cls.sts_monthly_file, cls.judge_month_st
        )
        cls.states_monthly = cls.daily_to_monthly(
            cls.states, cls.states_monthly_file, cls.judge_month_state
        )
        cls.ages_monthly = cls.ages.resample("M").last()
        cls.ages_monthly = np.sign(cls.ages_monthly.applymap(lambda x: x - 60)).replace(
            -1, 0
        )
        cls.tris_monthly = cls.sts_monthly * cls.states_monthly * cls.ages_monthly
        cls.tris_monthly = cls.tris_monthly.replace(0, np.nan)

    @classmethod
    @lru_cache(maxsize=None)
    def get_rets_month(cls):
        """è®¡ç®—æ¯æœˆçš„æ”¶ç›Šç‡ï¼Œå¹¶æ ¹æ®æ¯æœˆåšå‡ºäº¤æ˜“çŠ¶æ€ï¼Œåšå‡ºåˆ å‡"""
        cls.opens_monthly = cls.opens.resample("M").first()
        cls.closes_monthly = cls.closes.resample("M").last()
        cls.rets_monthly = (cls.closes_monthly - cls.opens_monthly) / cls.opens_monthly
        cls.rets_monthly = cls.rets_monthly * cls.tris_monthly
        cls.rets_monthly = cls.rets_monthly.stack().reset_index()
        cls.rets_monthly.columns = ["date", "code", "ret"]

    @classmethod
    def neutralize_factors(cls, df):
        """ç»„å†…å¯¹å› å­è¿›è¡Œå¸‚å€¼ä¸­æ€§åŒ–"""
        industry_codes = list(df.columns)
        industry_codes = [i for i in industry_codes if i.startswith("w")]
        industry_codes_str = "+".join(industry_codes)
        if len(industry_codes_str) > 0:
            ols_result = smf.ols("fac~cap_size+" + industry_codes_str, data=df).fit()
        else:
            ols_result = smf.ols("fac~cap_size", data=df).fit()
        ols_w = ols_result.params["cap_size"]
        ols_b = ols_result.params["Intercept"]
        ols_bs = {}
        for ind in industry_codes:
            ols_bs[ind] = ols_result.params[ind]
        df.fac = df.fac - ols_w * df.cap_size - ols_b
        for k, v in ols_bs.items():
            df.fac = df.fac - v * df[k]
        df = df[["fac"]]
        return df

    @classmethod
    @lru_cache(maxsize=None)
    def get_log_cap(cls, boxcox=True):
        """è·å¾—å¯¹æ•°å¸‚å€¼"""
        cls.cap = cls.capital.stack().reset_index()
        cls.cap.columns = ["date", "code", "cap_size"]
        if boxcox:

            def single(x):
                x.cap_size = ss.boxcox(x.cap_size)[0]
                return x

            cls.cap = cls.cap.groupby(["date"]).apply(single)
        else:
            cls.cap["cap_size"] = np.log(cls.cap["cap_size"])

    def get_neutral_factors(
        self, zxindustry_dummies=0, swindustry_dummies=0, only_cap=0
    ):
        """å¯¹å› å­è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–"""
        self.factors = self.factors.set_index("date")
        self.factors.index = self.factors.index + pd.DateOffset(months=1)
        self.factors = self.factors.resample("M").last()
        last_date = self.tris_monthly.index.max() + pd.DateOffset(months=1)
        last_date = last_date + pd.tseries.offsets.MonthEnd()
        add_tail = pd.DataFrame(1, index=[last_date], columns=self.tris_monthly.columns)
        tris_monthly = pd.concat([self.tris_monthly, add_tail])
        self.factors = self.factors * tris_monthly
        self.factors.index = self.factors.index - pd.DateOffset(months=1)
        self.factors = self.factors.resample("M").last()
        self.factors = self.factors.stack().reset_index()
        self.factors.columns = ["date", "code", "fac"]
        self.factors = pd.merge(
            self.factors, self.cap, how="inner", on=["date", "code"]
        )
        if not only_cap:
            if swindustry_dummies:
                self.factors = pd.merge(
                    self.factors, self.swindustry_dummy, on=["date", "code"]
                )
            else:
                self.factors = pd.merge(
                    self.factors, self.zxindustry_dummy, on=["date", "code"]
                )
        self.factors = self.factors.set_index(["date", "code"])
        self.factors = self.factors.groupby(["date"]).apply(self.neutralize_factors)
        self.factors = self.factors.reset_index()

    def deal_with_factors(self):
        """åˆ é™¤ä¸ç¬¦åˆäº¤æ˜“æ¡ä»¶çš„å› å­æ•°æ®"""
        self.factors = self.factors.set_index("date")
        self.__factors_out = self.factors.copy()
        self.__factors_out.columns = [i[1] for i in list(self.__factors_out.columns)]
        self.factors.index = self.factors.index + pd.DateOffset(months=1)
        self.factors = self.factors.resample("M").last()
        self.factors = self.factors * self.tris_monthly
        self.factors = self.factors.stack().reset_index()
        self.factors.columns = ["date", "code", "fac"]

    def deal_with_factors_after_neutralize(self):
        """ä¸­æ€§åŒ–ä¹‹åçš„å› å­å¤„ç†æ–¹æ³•"""
        self.factors = self.factors.set_index(["date", "code"])
        self.factors = self.factors.unstack()
        self.__factors_out = self.factors.copy()
        self.__factors_out.columns = [i[1] for i in list(self.__factors_out.columns)]
        self.factors.index = self.factors.index + pd.DateOffset(months=1)
        self.factors = self.factors.resample("M").last()
        self.factors.columns = list(map(lambda x: x[1], list(self.factors.columns)))
        self.factors = self.factors.stack().reset_index()
        self.factors.columns = ["date", "code", "fac"]

    @classmethod
    def find_limit(cls, df, up=1):
        """è®¡ç®—æ¶¨è·Œå¹…è¶…è¿‡9.8%çš„è‚¡ç¥¨ï¼Œå¹¶å°†å…¶å­˜å‚¨è¿›ä¸€ä¸ªé•¿åˆ—è¡¨é‡Œ
        å…¶ä¸­æ—¶é—´åˆ—ï¼Œä¸ºæŸæœˆçš„æœ€åä¸€å¤©ï¼›æ¶¨åœæ—¥è™½ç„¶ä¸ºä¸‹æœˆåˆç¬¬ä¸€å¤©ï¼Œä½†è¿™é‡Œæ ‡æ³¨çš„æ—¶é—´ç»Ÿä¸€ä¸ºä¸Šæœˆæœ€åä¸€å¤©"""
        limit_df = np.sign(df.applymap(lambda x: x - up * 0.098)).replace(
            -1 * up, np.nan
        )
        limit_df = limit_df.stack().reset_index()
        limit_df.columns = ["date", "code", "limit_up_signal"]
        limit_df = limit_df[["date", "code"]]
        return limit_df

    @classmethod
    @lru_cache(maxsize=None)
    def get_limit_ups_downs(cls):
        """æ‰¾æœˆåˆç¬¬ä¸€å¤©å°±æ¶¨åœ"""
        """æˆ–è€…æ˜¯æœˆæœ«è·Œåœçš„è‚¡ç¥¨"""
        cls.opens_monthly_shift = cls.opens_monthly.copy()
        cls.opens_monthly_shift = cls.opens_monthly_shift.shift(-1)
        cls.rets_monthly_begin = (
            cls.opens_monthly_shift - cls.closes_monthly
        ) / cls.closes_monthly
        cls.closes2_monthly = cls.closes.shift(1).resample("M").last()
        cls.rets_monthly_last = (
            cls.closes_monthly - cls.closes2_monthly
        ) / cls.closes2_monthly
        cls.limit_ups = cls.find_limit(cls.rets_monthly_begin, up=1)
        cls.limit_downs = cls.find_limit(cls.rets_monthly_last, up=-1)

    def get_ic_rankic(cls, df):
        """è®¡ç®—ICå’ŒRankIC"""
        df1 = df[["ret", "fac"]]
        ic = df1.corr(method="pearson").iloc[0, 1]
        rankic = df1.corr(method="spearman").iloc[0, 1]
        df2 = pd.DataFrame({"ic": [ic], "rankic": [rankic]})
        return df2

    def get_icir_rankicir(cls, df):
        """è®¡ç®—ICIRå’ŒRankICIR"""
        ic = df.ic.mean()
        rankic = df.rankic.mean()
        icir = ic / np.std(df.ic) * (12 ** (0.5))
        rankicir = rankic / np.std(df.rankic) * (12 ** (0.5))
        return pd.DataFrame(
            {"IC": [ic], "ICIR": [icir], "RankIC": [rankic], "RankICIR": [rankicir]},
            index=["è¯„ä»·æŒ‡æ ‡"],
        )

    def get_ic_icir_and_rank(cls, df):
        """è®¡ç®—ICã€ICIRã€RankICã€RankICIR"""
        df1 = df.groupby("date").apply(cls.get_ic_rankic)
        cls.ics = df1.ic
        cls.rankics = df1.rankic
        cls.ics = cls.ics.reset_index(drop=True, level=1).to_frame()
        cls.rankics = cls.rankics.reset_index(drop=True, level=1).to_frame()
        df2 = cls.get_icir_rankicir(df1)
        df2 = df2.T
        dura = (df.date.max() - df.date.min()).days / 365
        t_value = df2.iloc[3, 0] * (dura ** (1 / 2))
        df3 = pd.DataFrame({"è¯„ä»·æŒ‡æ ‡": [t_value]}, index=["RankICå‡å€¼tå€¼"])
        df4 = pd.concat([df2, df3])
        return df4

    @classmethod
    def get_groups(cls, df, groups_num):
        """ä¾æ®å› å­å€¼ï¼Œåˆ¤æ–­æ˜¯åœ¨ç¬¬å‡ ç»„"""
        if "group" in list(df.columns):
            df = df.drop(columns=["group"])
        df = df.sort_values(["fac"], ascending=True)
        each_group = round(df.shape[0] / groups_num)
        l = list(
            map(
                lambda x, y: [x] * y,
                list(range(1, groups_num + 1)),
                [each_group] * groups_num,
            )
        )
        l = reduce(lambda x, y: x + y, l)
        if len(l) < df.shape[0]:
            l = l + [groups_num] * (df.shape[0] - len(l))
        l = l[: df.shape[0]]
        df.insert(0, "group", l)
        return df

    @classmethod
    def next_month_end(cls, x):
        """æ‰¾åˆ°ä¸‹ä¸ªæœˆæœ€åä¸€å¤©"""
        x1 = x = x + relativedelta(months=1)
        while x1.month == x.month:
            x1 = x1 + relativedelta(days=1)
        return x1 - relativedelta(days=1)

    @classmethod
    def limit_old_to_new(cls, limit, data):
        """è·å–è·Œåœè‚¡åœ¨æ—§æœˆçš„ç»„å·ï¼Œç„¶åå°†æ—¥æœŸè°ƒæ•´åˆ°æ–°æœˆé‡Œ
        æ¶¨åœè‚¡åˆ™è·å¾—æ–°æœˆé‡Œæ¶¨åœè‚¡çš„ä»£ç å’Œæ—¶é—´ï¼Œç„¶åç›´æ¥åˆ å»"""
        data1 = data.copy()
        data1 = data1.reset_index()
        data1.columns = ["data_index"] + list(data1.columns)[1:]
        old = pd.merge(limit, data1, how="inner", on=["date", "code"])
        old = old.set_index("data_index")
        old = old[["group", "date", "code"]]
        old.date = list(map(cls.next_month_end, list(old.date)))
        return old

    def get_data(self, groups_num):
        """æ‹¼æ¥å› å­æ•°æ®å’Œæ¯æœˆæ”¶ç›Šç‡æ•°æ®ï¼Œå¹¶å¯¹æ¶¨åœå’Œè·Œåœè‚¡åŠ ä»¥å¤„ç†"""
        self.data = pd.merge(
            self.rets_monthly, self.factors, how="inner", on=["date", "code"]
        )
        self.ic_icir_and_rank = self.get_ic_icir_and_rank(self.data)
        self.data = self.data.groupby("date").apply(
            lambda x: self.get_groups(x, groups_num)
        )
        self.wind_out = self.data.copy()
        self.factor_turnover_rates = self.data.pivot(
            index="date", columns="code", values="group"
        )
        self.factor_turnover_rates = self.factor_turnover_rates.diff()
        change = ((np.abs(np.sign(self.factor_turnover_rates)) == 1) + 0).sum(axis=1)
        still = ((self.factor_turnover_rates == 0) + 0).sum(axis=1)
        self.factor_turnover_rates = change / (change + still)
        self.factor_turnover_rate = self.factor_turnover_rates.mean()
        self.data = self.data.reset_index(drop=True)
        limit_ups_object = self.limit_old_to_new(self.limit_ups, self.data)
        limit_downs_object = self.limit_old_to_new(self.limit_downs, self.data)
        self.data = self.data.drop(limit_ups_object.index)
        rets_monthly_limit_downs = pd.merge(
            self.rets_monthly, limit_downs_object, how="inner", on=["date", "code"]
        )
        self.data = pd.concat([self.data, rets_monthly_limit_downs])

    def select_data_time(self, time_start, time_end):
        """ç­›é€‰ç‰¹å®šçš„æ—¶é—´æ®µ"""
        if time_start:
            self.data = self.data[self.data.date >= time_start]
        if time_end:
            self.data = self.data[self.data.date <= time_end]

    def make_start_to_one(self, l):
        """è®©å‡€å€¼åºåˆ—çš„ç¬¬ä¸€ä¸ªæ•°å˜æˆ1"""
        min_date = self.factors.date.min()
        add_date = min_date - relativedelta(days=min_date.day)
        add_l = pd.Series([1], index=[add_date])
        l = pd.concat([add_l, l])
        return l

    def to_group_ret(self, l):
        """æ¯ä¸€ç»„çš„å¹´åŒ–æ”¶ç›Šç‡"""
        ret = l[-1] ** (12 / len(l)) - 1
        return ret

    def get_group_rets_net_values(self, groups_num=10, value_weighted=False):
        """è®¡ç®—ç»„å†…æ¯ä¸€æœŸçš„å¹³å‡æ”¶ç›Šï¼Œç”Ÿæˆæ¯æ—¥æ”¶ç›Šç‡åºåˆ—å’Œå‡€å€¼åºåˆ—"""
        if value_weighted:
            cap_value = self.capital.copy()
            cap_value = cap_value.resample("M").last().shift(1)
            cap_value = cap_value * self.tris_monthly
            # cap_value=np.log(cap_value)
            cap_value = cap_value.stack().reset_index()
            cap_value.columns = ["date", "code", "cap_value"]
            self.data = pd.merge(self.data, cap_value, on=["date", "code"])

            def in_g(df):
                df.cap_value = df.cap_value / df.cap_value.sum()
                df.ret = df.ret * df.cap_value
                return df.ret.sum()

            self.group_rets = self.data.groupby(["date", "group"]).apply(in_g)
            self.group_rets_std = "å¸‚å€¼åŠ æƒæš‚æœªè®¾ç½®è¯¥åŠŸèƒ½ï¼Œæ•¬è¯·æœŸå¾…ğŸŒ™"
        else:
            self.group_rets = self.data.groupby(["date", "group"]).apply(
                lambda x: x.ret.mean()
            )
            self.group_rets_stds = self.data.groupby(["date", "group"]).apply(
                lambda x: x.ret.std()
            )
            self.group_rets_std = self.group_rets_stds.groupby("group").mean()
        # dropnaæ˜¯å› ä¸ºå¦‚æœè‚¡ç¥¨è¡Œæƒ…æ•°æ®æ¯”å› å­æ•°æ®çš„æˆªæ­¢æ—¥æœŸæ™šï¼Œè€Œæœ€åä¸€ä¸ªæœˆå‘ç”Ÿæœˆåˆè·Œåœæ—¶ï¼Œä¼šé€ æˆæœ€åæŸç»„å¤šå‡ºä¸€ä¸ªæœˆçš„æ•°æ®
        self.group_rets = self.group_rets.unstack()
        self.group_rets = self.group_rets[
            self.group_rets.index <= self.factors.date.max()
        ]
        self.group_rets.columns = list(map(str, list(self.group_rets.columns)))
        self.group_rets = self.group_rets.add_prefix("group")
        self.long_short_rets = (
            self.group_rets["group1"] - self.group_rets["group" + str(groups_num)]
        )
        self.long_short_net_values = (self.long_short_rets + 1).cumprod()
        if self.long_short_net_values[-1] <= self.long_short_net_values[0]:
            self.long_short_rets = (
                self.group_rets["group" + str(groups_num)] - self.group_rets["group1"]
            )
            self.long_short_net_values = (self.long_short_rets + 1).cumprod()
        self.long_short_net_values = self.make_start_to_one(self.long_short_net_values)
        self.group_rets = self.group_rets.assign(long_short=self.long_short_rets)
        self.group_net_values = self.group_rets.applymap(lambda x: x + 1)
        self.group_net_values = self.group_net_values.cumprod()
        self.group_net_values = self.group_net_values.apply(self.make_start_to_one)
        a = groups_num ** (0.5)
        # åˆ¤æ–­æ˜¯å¦è¦ä¸¤ä¸ªå› å­ç”»è¡¨æ ¼
        if a == int(a):
            self.square_rets = (
                self.group_net_values.iloc[:, :-1].apply(self.to_group_ret).to_numpy()
            )
            self.square_rets = self.square_rets.reshape((int(a), int(a)))
            self.square_rets = pd.DataFrame(
                self.square_rets,
                columns=list(range(1, int(a) + 1)),
                index=list(range(1, int(a) + 1)),
            )
            print("è¿™æ˜¯self.square_rets", self.square_rets)

    def get_long_short_comments(self, on_paper=False):
        """è®¡ç®—å¤šç©ºå¯¹å†²çš„ç›¸å…³è¯„ä»·æŒ‡æ ‡
        åŒ…æ‹¬å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€æœˆåº¦èƒœç‡ã€æœ€å¤§å›æ’¤ç‡"""
        self.long_short_ret_yearly = (
            self.long_short_net_values[-1] ** (12 / len(self.long_short_net_values)) - 1
        )
        self.long_short_vol_yearly = np.std(self.long_short_rets) * (12**0.5)
        self.long_short_info_ratio = (
            self.long_short_ret_yearly / self.long_short_vol_yearly
        )
        self.long_short_win_times = len(self.long_short_rets[self.long_short_rets > 0])
        self.long_short_win_ratio = self.long_short_win_times / len(
            self.long_short_rets
        )
        self.max_retreat = -(
            self.long_short_net_values / self.long_short_net_values.expanding(1).max()
            - 1
        ).min()
        if on_paper:
            self.long_short_comments = pd.DataFrame(
                {
                    "è¯„ä»·æŒ‡æ ‡": [
                        self.long_short_ret_yearly,
                        self.long_short_vol_yearly,
                        self.long_short_info_ratio,
                        self.long_short_win_ratio,
                        self.max_retreat,
                    ]
                },
                index=["å¹´åŒ–æ”¶ç›Šç‡", "å¹´åŒ–æ³¢åŠ¨ç‡", "æ”¶ç›Šæ³¢åŠ¨æ¯”", "æœˆåº¦èƒœç‡", "æœ€å¤§å›æ’¤ç‡"],
            )
        else:
            self.long_short_comments = pd.DataFrame(
                {
                    "è¯„ä»·æŒ‡æ ‡": [
                        self.long_short_ret_yearly,
                        self.long_short_vol_yearly,
                        self.long_short_info_ratio,
                        self.long_short_win_ratio,
                        self.max_retreat,
                    ]
                },
                index=["å¹´åŒ–æ”¶ç›Šç‡", "å¹´åŒ–æ³¢åŠ¨ç‡", "ä¿¡æ¯æ¯”ç‡", "æœˆåº¦èƒœç‡", "æœ€å¤§å›æ’¤ç‡"],
            )

    def get_total_comments(self):
        """ç»¼åˆICã€ICIRã€RankICã€RankICIR,å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€æœˆåº¦èƒœç‡ã€æœ€å¤§å›æ’¤ç‡"""
        self.total_comments = pd.concat(
            [
                self.ic_icir_and_rank,
                self.long_short_comments,
                pd.DataFrame({"è¯„ä»·æŒ‡æ ‡": [self.factor_turnover_rate]}, index=["æœˆå‡æ¢æ‰‹ç‡"]),
            ]
        )

    def plot_net_values(self, y2, filename, iplot=1, ilegend=1):
        """ä½¿ç”¨matplotlibæ¥ç”»å›¾ï¼Œy2ä¸ºæ˜¯å¦å¯¹å¤šç©ºç»„åˆé‡‡ç”¨åŒyè½´"""
        if not iplot:
            fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(33, 8))
            self.group_net_values.plot(secondary_y=y2, rot=60, ax=ax[0])
            self.group_net_values.plot(secondary_y=y2, ax=ax[0])
            b = self.rankics.copy()
            b.index = [int(i.year) if i.month == 1 else "" for i in list(b.index)]
            b.plot(kind="bar", rot=60, ax=ax[1])
            self.factor_turnover_rates.plot(rot=60, ax=ax[2])

            filename_path = filename + ".png"
            if not STATES["NO_SAVE"]:
                plt.savefig(filename_path)
        else:
            tris = pd.concat(
                [self.group_net_values, self.factor_turnover_rates, self.rankics],
                axis=1,
            ).rename(columns={0: "turnover_rate"})
            figs = cf.figures(
                tris,
                [
                    dict(kind="line", y=list(self.group_net_values.columns)),
                    dict(kind="line", y="turnover_rate"),
                    dict(kind="bar", y="rankic"),
                ],
                asList=True,
            )
            comments = (
                self.total_comments.applymap(lambda x: round(x, 4))
                .rename(index={"RankICå‡å€¼tå€¼": "RankIC.t"})
                .reset_index()
            )
            here = pd.concat(
                [
                    comments.iloc[:5, :].reset_index(drop=True),
                    comments.iloc[5:, :].reset_index(drop=True),
                ],
                axis=1,
            )
            here.columns = ["ä¿¡æ¯ç³»æ•°", "ç»“æœ", "ç»©æ•ˆæŒ‡æ ‡", "ç»“æœ"]
            # here=here.to_numpy().tolist()+[['ä¿¡æ¯ç³»æ•°','ç»“æœ','ç»©æ•ˆæŒ‡æ ‡','ç»“æœ']]
            table = FF.create_table(here.iloc[::-1])
            table.update_yaxes(matches=None)
            # table=go.Figure([go.Table(header=dict(values=list(here.columns)),cells=dict(values=here.to_numpy().tolist()))])
            figs.append(table)
            figs = [figs[-1]] + figs[:-1]
            figs[1].update_layout(
                legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
            )
            base_layout = cf.tools.get_base_layout(figs)

            sp = cf.subplots(
                figs,
                shape=(2, 10),
                base_layout=base_layout,
                vertical_spacing=0.15,
                horizontal_spacing=0.03,
                shared_yaxes=False,
                specs=[
                    [
                        {"rowspan": 2, "colspan": 3},
                        None,
                        None,
                        {"rowspan": 2, "colspan": 4},
                        None,
                        None,
                        None,
                        {"colspan": 3},
                        None,
                        None,
                    ],
                    [
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        {"colspan": 3},
                        None,
                        None,
                    ],
                ],
                subplot_titles=["å‡€å€¼æ›²çº¿", "æœˆæ¢æ‰‹ç‡", "Rank ICæ—¶åºå›¾", "ç»©æ•ˆæŒ‡æ ‡"],
            )
            sp["layout"].update(showlegend=ilegend)
            # los=sp['layout']['annotations']
            # los[0]['font']['color']='#000000'
            # los[1]['font']['color']='#000000'
            # los[2]['font']['color']='#000000'
            # los[3]['font']['color']='#000000'
            # los[-1]['font']['color']='#ffffff'
            # los[-2]['font']['color']='#ffffff'
            # los[-3]['font']['color']='#ffffff'
            # los[-4]['font']['color']='#ffffff'
            # los[0]['text']=los[0]['text'][3:-4]
            # los[1]['text']=los[1]['text'][3:-4]
            # los[2]['text']=los[2]['text'][3:-4]
            # los[3]['text']=los[3]['text'][3:-4]
            # los[-1]['text']='<b>'+los[-1]['text']+'</b>'
            # los[-2]['text']='<b>'+los[-2]['text']+'</b>'
            # los[-3]['text']='<b>'+los[-3]['text']+'</b>'
            # los[-4]['text']='<b>'+los[-4]['text']+'</b>'
            # sp['layout']['annotations']=los
            # print(sp['layout']['annotations'])
            # sp['layout']['annotations'][0]['yanchor']='top'
            cf.iplot(sp)
            # tris=pd.concat([self.group_net_values,self.rankics,self.factor_turnover_rates],axis=1).rename(columns={0:'turnover_rate'})
            # sp=plyoo.make_subplots(rows=2,cols=8,vertical_spacing=.15,horizontal_spacing=.03,
            #                specs=[[{'rowspan':2,'colspan':2,'type':'domain'},None,{'rowspan':2,'colspan':4,'type':'xy'},None,None,None,{'colspan':2,'type':'xy'},None],
            #                       [None,None,None,None,None,None,{'colspan':2,'type':'xy'},None]],
            #                subplot_titles=['å‡€å€¼æ›²çº¿','Rank ICæ—¶åºå›¾','æœˆæ¢æ‰‹ç‡','ç»©æ•ˆæŒ‡æ ‡'])
            # comments=self.total_comments.applymap(lambda x:round(x,4)).rename(index={'RankICå‡å€¼tå€¼':'RankIC.t'}).reset_index()
            # here=pd.concat([comments.iloc[:5,:].reset_index(drop=True),comments.iloc[5:,:].reset_index(drop=True)],axis=1)
            # here.columns=['ä¿¡æ¯ç³»æ•°','ç»“æœ','ç»©æ•ˆæŒ‡æ ‡','ç»“æœ']
            # table=FF.create_table(here)
            # sp.add_trace(table)

    def plotly_net_values(self, filename):
        """ä½¿ç”¨plotly.expressç”»å›¾"""
        fig = pe.line(self.group_net_values)
        filename_path = filename + ".html"
        pio.write_html(fig, filename_path, auto_open=True)

    @classmethod
    @lru_cache(maxsize=None)
    def prerpare(cls):
        """é€šç”¨æ•°æ®å‡†å¤‡"""
        cls.judge_month()
        cls.get_rets_month()

    @kk.desktop_sender(title="å˜¿ï¼Œå›æµ‹ç»“æŸå•¦ï½ğŸ—“")
    def run(
        self,
        groups_num=10,
        neutralize=False,
        boxcox=False,
        value_weighted=False,
        y2=False,
        plt_plot=True,
        plotly_plot=False,
        filename="åˆ†ç»„å‡€å€¼å›¾",
        time_start=None,
        time_end=None,
        print_comments=True,
        comments_writer=None,
        net_values_writer=None,
        rets_writer=None,
        comments_sheetname=None,
        net_values_sheetname=None,
        rets_sheetname=None,
        on_paper=False,
        sheetname=None,
        zxindustry_dummies=0,
        swindustry_dummies=0,
        only_cap=0,
        iplot=1,
        ilegend=1,
    ):
        """è¿è¡Œå›æµ‹éƒ¨åˆ†"""
        if comments_writer and not (comments_sheetname or sheetname):
            raise IOError("æŠŠtotal_commentsè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
        if net_values_writer and not (net_values_sheetname or sheetname):
            raise IOError("æŠŠgroup_net_valuesè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
        if rets_writer and not (rets_sheetname or sheetname):
            raise IOError("æŠŠgroup_retsè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
        if neutralize:
            self.get_log_cap()
            self.get_neutral_factors(
                swindustry_dummies=swindustry_dummies,
                zxindustry_dummies=zxindustry_dummies,
            )
            self.deal_with_factors_after_neutralize()
        elif boxcox:
            self.get_log_cap(boxcox=True)
            self.get_neutral_factors(
                swindustry_dummies=swindustry_dummies,
                zxindustry_dummies=zxindustry_dummies,
                only_cap=only_cap,
            )
            self.deal_with_factors_after_neutralize()
        else:
            self.deal_with_factors()
        self.get_limit_ups_downs()
        self.get_data(groups_num)
        self.select_data_time(time_start, time_end)
        self.get_group_rets_net_values(
            groups_num=groups_num, value_weighted=value_weighted
        )
        self.get_long_short_comments(on_paper=on_paper)
        self.get_total_comments()
        if on_paper:
            group1_ttest = ss.ttest_1samp(self.group_rets.group1, 0).pvalue
            group10_ttest = ss.ttest_1samp(
                self.group_rets[f"group{groups_num}"], 0
            ).pvalue
            group_long_short_ttest = ss.ttest_1samp(self.long_short_rets, 0).pvalue
            group1_ret = self.group_rets.group1.mean()
            group10_ret = self.group_rets[f"group{groups_num}"].mean()
            group_long_short_ret = self.long_short_rets.mean()
            papers = pd.DataFrame(
                {
                    "è¯„ä»·æŒ‡æ ‡": [
                        group1_ttest,
                        group10_ttest,
                        group_long_short_ttest,
                        group1_ret,
                        group10_ret,
                        group_long_short_ret,
                    ]
                },
                index=[
                    "åˆ†ç»„1på€¼",
                    f"åˆ†ç»„{groups_num}på€¼",
                    f"åˆ†ç»„1-åˆ†ç»„{groups_num}på€¼",
                    "åˆ†ç»„1æ”¶ç›Šç‡",
                    f"åˆ†ç»„{groups_num}æ”¶ç›Šç‡",
                    f"åˆ†ç»„1-åˆ†ç»„{groups_num}æ”¶ç›Šç‡",
                ],
            )
            self.total_comments = pd.concat([papers, self.total_comments])

        if plt_plot:
            if not STATES["NO_PLOT"]:
                if filename:
                    self.plot_net_values(
                        y2=y2, filename=filename, iplot=iplot, ilegend=bool(ilegend)
                    )
                else:
                    self.plot_net_values(
                        y2=y2,
                        filename=self.factors_file.split(".")[-2].split("/")[-1]
                        + str(groups_num)
                        + "åˆ†ç»„",
                        iplot=iplot,
                        ilegend=bool(ilegend),
                    )
                plt.show()
        if plotly_plot:
            if not STATES["NO_PLOT"]:
                if filename:
                    self.plotly_net_values(filename=filename)
                else:
                    self.plotly_net_values(
                        filename=self.factors_file.split(".")[-2].split("/")[-1]
                        + str(groups_num)
                        + "åˆ†ç»„"
                    )
        if print_comments:
            if not STATES["NO_COMMENT"]:
                tb = Texttable()
                tb.set_cols_width([8] * 4 + [12] + [8] * 2 + [7] * 2 + [8] + [10])
                tb.set_cols_dtype(["f"] * 11)
                tb.header(list(self.total_comments.T.columns))
                tb.add_rows(self.total_comments.T.to_numpy(), header=False)
                print(tb.draw())
        if sheetname:
            if comments_writer:
                if not on_paper:
                    total_comments = self.total_comments.copy()
                    tc = list(total_comments.è¯„ä»·æŒ‡æ ‡)
                    tc[0] = str(round(tc[0] * 100, 2)) + "%"
                    tc[1] = str(round(tc[1], 2))
                    tc[2] = str(round(tc[2] * 100, 2)) + "%"
                    tc[3] = str(round(tc[3], 2))
                    tc[4] = str(round(tc[4], 2))
                    tc[5] = str(round(tc[5] * 100, 2)) + "%"
                    tc[6] = str(round(tc[6] * 100, 2)) + "%"
                    tc[7] = str(round(tc[7], 2))
                    tc[8] = str(round(tc[8] * 100, 2)) + "%"
                    tc[9] = str(round(tc[9] * 100, 2)) + "%"
                    tc[10] = str(round(tc[10] * 100, 2)) + "%"
                    new_total_comments = pd.DataFrame(
                        {sheetname: tc}, index=total_comments.index
                    )
                    new_total_comments.T.to_excel(comments_writer, sheet_name=sheetname)
                else:
                    self.total_comments.rename(columns={"è¯„ä»·æŒ‡æ ‡": sheetname}).to_excel(
                        comments_writer, sheet_name=sheetname
                    )
            if net_values_writer:
                groups_net_values = self.group_net_values.copy()
                groups_net_values.index = groups_net_values.index.strftime("%Y/%m/%d")
                groups_net_values.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(groups_net_values.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                groups_net_values.to_excel(net_values_writer, sheet_name=sheetname)
            if rets_writer:
                group_rets = self.group_rets.copy()
                group_rets.index = group_rets.index.strftime("%Y/%m/%d")
                group_rets.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(group_rets.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                group_rets.to_excel(rets_writer, sheet_name=sheetname)
        else:
            if comments_writer and comments_sheetname:
                total_comments = self.total_comments.copy()
                tc = list(total_comments.è¯„ä»·æŒ‡æ ‡)
                tc[0] = str(round(tc[0] * 100, 2)) + "%"
                tc[1] = str(round(tc[1], 2))
                tc[2] = str(round(tc[2] * 100, 2)) + "%"
                tc[3] = str(round(tc[3], 2))
                tc[4] = str(round(tc[4], 2))
                tc[5] = str(round(tc[5] * 100, 2)) + "%"
                tc[6] = str(round(tc[6] * 100, 2)) + "%"
                tc[7] = str(round(tc[7], 2))
                tc[8] = str(round(tc[8] * 100, 2)) + "%"
                tc[9] = str(round(tc[9] * 100, 2)) + "%"
                tc[10] = str(round(tc[10] * 100, 2)) + "%"
                new_total_comments = pd.DataFrame(
                    {comments_sheetname: tc}, index=total_comments.index
                )
                new_total_comments.T.to_excel(
                    comments_writer, sheet_name=comments_sheetname
                )
            if net_values_writer and net_values_sheetname:
                groups_net_values = self.group_net_values.copy()
                groups_net_values.index = groups_net_values.index.strftime("%Y/%m/%d")
                groups_net_values.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(groups_net_values.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                groups_net_values.to_excel(
                    net_values_writer, sheet_name=net_values_sheetname
                )
            if rets_writer and rets_sheetname:
                group_rets = self.group_rets.copy()
                group_rets.index = group_rets.index.strftime("%Y/%m/%d")
                group_rets.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(group_rets.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                group_rets.to_excel(rets_writer, sheet_name=rets_sheetname)


class pure_moonnight(object):
    """å°è£…é€‰è‚¡æ¡†æ¶"""

    __slots__ = ["shen"]

    def __init__(
        self,
        factors: pd.DataFrame,
        groups_num: int = 10,
        neutralize: bool = 0,
        boxcox: bool = 1,
        value_weighted: bool = 0,
        y2: bool = 0,
        plt_plot: bool = 1,
        plotly_plot: bool = 0,
        filename: str = "åˆ†ç»„å‡€å€¼å›¾",
        time_start: int = None,
        time_end: int = None,
        print_comments: bool = 1,
        comments_writer: pd.ExcelWriter = None,
        net_values_writer: pd.ExcelWriter = None,
        rets_writer: pd.ExcelWriter = None,
        comments_sheetname: str = None,
        net_values_sheetname: str = None,
        rets_sheetname: str = None,
        on_paper: bool = 0,
        sheetname: str = None,
        zxindustry_dummies: bool = 0,
        swindustry_dummies: bool = 0,
        ages: pd.DataFrame = None,
        sts: pd.DataFrame = None,
        states: pd.DataFrame = None,
        opens: pd.DataFrame = None,
        closes: pd.DataFrame = None,
        capitals: pd.DataFrame = None,
        swindustry_dummy: pd.DataFrame = None,
        zxindustry_dummy: pd.DataFrame = None,
        no_read_indu: bool = 0,
        only_cap: bool = 0,
        iplot: bool = 1,
        ilegend: bool = 1,
    ) -> None:
        """ä¸€é”®å›æµ‹æ¡†æ¶ï¼Œæµ‹è¯•å•å› å­çš„æœˆé¢‘è°ƒä»“çš„åˆ†ç»„è¡¨ç°
        æ¯æœˆæœˆåº•è®¡ç®—å› å­å€¼ï¼Œæœˆåˆç¬¬ä¸€å¤©å¼€ç›˜æ—¶ä¹°å…¥ï¼Œæœˆæœ«æ”¶ç›˜æœ€åä¸€å¤©æ”¶ç›˜æ—¶å–å‡º
        å‰”é™¤ä¸Šå¸‚ä¸è¶³60å¤©çš„ï¼Œåœç‰Œå¤©æ•°è¶…è¿‡ä¸€åŠçš„ï¼Œstå¤©æ•°è¶…è¿‡ä¸€åŠçš„
        æœˆæœ«æ”¶ç›˜è·Œåœçš„ä¸å–å‡ºï¼Œæœˆåˆå¼€ç›˜æ¶¨åœçš„ä¸ä¹°å…¥
        ç”±æœ€å¥½ç»„å’Œæœ€å·®ç»„çš„å¤šç©ºç»„åˆæ„æˆå¤šç©ºå¯¹å†²ç»„

        Parameters
        ----------
        factors : pd.DataFrame
            è¦ç”¨äºæ£€æµ‹çš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
        groups_num : int, optional
            åˆ†ç»„æ•°é‡, by default 10
        neutralize : bool, optional
            å¯¹æµé€šå¸‚å€¼å–è‡ªç„¶å¯¹æ•°ï¼Œä»¥å®Œæˆè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 0
        boxcox : bool, optional
            å¯¹æµé€šå¸‚å€¼åšæˆªé¢boxcoxå˜æ¢ï¼Œä»¥å®Œæˆè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 1
        value_weighted : bool, optional
            æ˜¯å¦ç”¨æµé€šå¸‚å€¼åŠ æƒ, by default 0
        y2 : bool, optional
            ç”»å›¾æ—¶æ˜¯å¦å¯ç”¨ç¬¬äºŒyè½´, by default 0
        plt_plot : bool, optional
            å°†åˆ†ç»„å‡€å€¼æ›²çº¿ç”¨matplotlibç”»å‡ºæ¥, by default 1
        plotly_plot : bool, optional
            å°†åˆ†ç»„å‡€å€¼æ›²çº¿ç”¨plotlyç”»å‡ºæ¥, by default 0
        filename : str, optional
            åˆ†ç»„å‡€å€¼æ›²çº¿çš„å›¾ä¿å­˜çš„åç§°, by default "åˆ†ç»„å‡€å€¼å›¾"
        time_start : int, optional
            å›æµ‹èµ·å§‹æ—¶é—´ï¼ˆæ­¤å‚æ•°å·²åºŸå¼ƒï¼Œè¯·åœ¨å› å­ä¸Šç›´æ¥æˆªæ–­ï¼‰, by default None
        time_end : int, optional
            å›æµ‹ç»ˆæ­¢æ—¶é—´ï¼ˆæ­¤å‚æ•°å·²åºŸå¼ƒï¼Œè¯·åœ¨å› å­ä¸Šç›´æ¥æˆªæ–­ï¼‰, by default None
        print_comments : bool, optional
            æ˜¯å¦æ‰“å°å‡ºè¯„ä»·æŒ‡æ ‡, by default 1
        comments_writer : pd.ExcelWriter, optional
            ç”¨äºè®°å½•è¯„ä»·æŒ‡æ ‡çš„xlsxæ–‡ä»¶, by default None
        net_values_writer : pd.ExcelWriter, optional
            ç”¨äºè®°å½•å‡€å€¼åºåˆ—çš„xlsxæ–‡ä»¶, by default None
        rets_writer : pd.ExcelWriter, optional
            ç”¨äºè®°å½•æ”¶ç›Šç‡åºåˆ—çš„xlsxæ–‡ä»¶, by default None
        comments_sheetname : str, optional
            åœ¨è®°å½•è¯„ä»·æŒ‡æ ‡çš„xlsxæ–‡ä»¶ä¸­ï¼Œè¯¥å·¥ä½œè¡¨çš„åç§°, by default None
        net_values_sheetname : str, optional
            åœ¨è®°å½•å‡€å€¼åºåˆ—çš„xlsxæ–‡ä»¶ä¸­ï¼Œè¯¥å·¥ä½œè¡¨çš„åç§°, by default None
        rets_sheetname : str, optional
            åœ¨è®°å½•æ”¶ç›Šç‡åºåˆ—çš„xlsxæ–‡ä»¶ä¸­ï¼Œè¯¥å·¥ä½œè¡¨çš„åç§°, by default None
        on_paper : bool, optional
            ä½¿ç”¨å­¦æœ¯åŒ–è¯„ä»·æŒ‡æ ‡, by default 0
        sheetname : str, optional
            å„ä¸ªpd.Excelwriterä¸­å·¥ä½œè¡¨çš„ç»Ÿä¸€åç§°, by default None
        zxindustry_dummies : bool, optional
            è¡Œä¸šä¸­æ€§åŒ–æ—¶ï¼Œé€‰ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0
        swindustry_dummies : bool, optional
            è¡Œä¸šä¸­æ€§åŒ–æ—¶ï¼Œé€‰ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
        ages : pd.DataFrame, optional
            è¾“å…¥è‚¡ç¥¨ä¸Šå¸‚å¤©æ•°çš„æ•°æ®ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯å¤©æ•°, by default None
        sts : pd.DataFrame, optional
            è¾“å…¥è‚¡ç¥¨æ¯å¤©æ˜¯å¦stçš„æ•°æ®ï¼Œæ˜¯stè‚¡å³ä¸º1ï¼Œå¦åˆ™ä¸º0ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯0æˆ–1, by default None
        states : pd.DataFrame, optional
            è¾“å…¥è‚¡ç¥¨æ¯å¤©äº¤æ˜“çŠ¶æ€çš„æ•°æ®ï¼Œæ­£å¸¸äº¤æ˜“ä¸º1ï¼Œå¦åˆ™ä¸º0ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯0æˆ–1, by default None
        opens : pd.DataFrame, optional
            è¾“å…¥è‚¡ç¥¨çš„å¤æƒå¼€ç›˜ä»·æ•°æ®ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯ä»·æ ¼, by default None
        closes : pd.DataFrame, optional
            è¾“å…¥è‚¡ç¥¨çš„å¤æƒæ”¶ç›˜ä»·æ•°æ®ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯ä»·æ ¼, by default None
        capitals : pd.DataFrame, optional
            è¾“å…¥è‚¡ç¥¨çš„æ¯æœˆæœˆæœ«æµé€šå¸‚å€¼æ•°æ®ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯æµé€šå¸‚å€¼, by default None
        swindustry_dummy : pd.DataFrame, optioanl
            ç†Ÿäººè‚¡ç¥¨çš„æ¯æœˆæœˆæœ«çš„ç”³ä¸‡ä¸€çº§è¡Œä¸šå“‘å˜é‡ï¼Œè¡¨åŒ…å«33åˆ—ï¼Œç¬¬ä¸€åˆ—ä¸ºè‚¡ç¥¨ä»£ç ï¼Œåä¸º`code`ï¼Œç¬¬äºŒåˆ—ä¸ºæœˆæœ«æœ€åä¸€å¤©çš„æ—¥æœŸï¼Œåä¸º`date`
            å…¶ä½™31åˆ—ï¼Œä¸ºå„ä¸ªè¡Œä¸šçš„å“‘å˜é‡ï¼Œåä¸º`w1`ã€`w2`ã€`w3`â€¦â€¦`w31`, by default None
        zxindustry_dummy : pd.DataFrame, optioanl
            ç†Ÿäººè‚¡ç¥¨çš„æ¯æœˆæœˆæœ«çš„ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ï¼Œè¡¨åŒ…å«32åˆ—ï¼Œç¬¬ä¸€åˆ—ä¸ºè‚¡ç¥¨ä»£ç ï¼Œåä¸º`code`ï¼Œç¬¬äºŒåˆ—ä¸ºæœˆæœ«æœ€åä¸€å¤©çš„æ—¥æœŸï¼Œåä¸º`date`
            å…¶ä½™30åˆ—ï¼Œä¸ºå„ä¸ªè¡Œä¸šçš„å“‘å˜é‡ï¼Œåä¸º`w1`ã€`w2`ã€`w3`â€¦â€¦`w30`, by default None
        no_read_indu : bool, optional
            ä¸è¯»å…¥è¡Œä¸šæ•°æ®, by default 0
        only_cap : bool, optional
            ä»…åšå¸‚å€¼ä¸­æ€§åŒ–, by default 0
        iplot : bool, optional
            ä½¿ç”¨cufflinkså‘ˆç°å›æµ‹ç»“æœ, by default 1
        ilegend : bool, optional
            ä½¿ç”¨cufflinksç»˜å›¾æ—¶ï¼Œæ˜¯å¦æ˜¾ç¤ºå›¾ä¾‹, by default 1
        """

        if not isinstance(factors, pd.DataFrame):
            factors = factors()
        start = datetime.datetime.strftime(factors.index.min(), "%Y%m%d")
        if ages is None:
            ages = read_daily(age=1, start=start)
        if sts is None:
            sts = read_daily(st=1, start=start)
        if states is None:
            states = read_daily(state=1, start=start)
        if opens is None:
            opens = read_daily(open=1, start=start)
        if closes is None:
            closes = read_daily(close=1, start=start)
        if capitals is None:
            capitals = read_daily(flow_cap=1, start=start).resample("M").last()
        if comments_writer is None and sheetname is not None:
            from pure_ocean_breeze.legacy_version.v3p4.state.states import COMMENTS_WRITER

            comments_writer = COMMENTS_WRITER
        if net_values_writer is None and sheetname is not None:
            from pure_ocean_breeze.legacy_version.v3p4.state.states import NET_VALUES_WRITER

            net_values_writer = NET_VALUES_WRITER
        if not on_paper:
            from pure_ocean_breeze.legacy_version.v3p4.state.states import ON_PAPER

            on_paper = ON_PAPER
        from pure_ocean_breeze.legacy_version.v3p4.state.states import MOON_START

        if MOON_START is not None:
            factors = factors[factors.index >= pd.Timestamp(str(MOON_START))]
        from pure_ocean_breeze.legacy_version.v3p4.state.states import MOON_END

        if MOON_END is not None:
            factors = factors[factors.index <= pd.Timestamp(str(MOON_END))]
        if boxcox + neutralize == 0:
            no_read_indu = 1
        if only_cap + no_read_indu > 0:
            only_cap = no_read_indu = 1
        if iplot:
            print_comments = 0
        self.shen = pure_moon(
            no_read_indu=no_read_indu,
            swindustry_dummy=swindustry_dummy,
            zxindustry_dummy=zxindustry_dummy,
            read_in_swindustry_dummy=swindustry_dummies,
        )
        self.shen.set_basic_data(
            age=ages,
            st=sts,
            state=states,
            open=opens,
            close=closes,
            capital=capitals,
        )
        self.shen.set_factor_df_date_as_index(factors)
        self.shen.prerpare()
        self.shen.run(
            groups_num=groups_num,
            neutralize=neutralize,
            boxcox=boxcox,
            value_weighted=value_weighted,
            y2=y2,
            plt_plot=plt_plot,
            plotly_plot=plotly_plot,
            filename=filename,
            time_start=time_start,
            time_end=time_end,
            print_comments=print_comments,
            comments_writer=comments_writer,
            net_values_writer=net_values_writer,
            rets_writer=rets_writer,
            comments_sheetname=comments_sheetname,
            net_values_sheetname=net_values_sheetname,
            rets_sheetname=rets_sheetname,
            on_paper=on_paper,
            sheetname=sheetname,
            swindustry_dummies=swindustry_dummies,
            zxindustry_dummies=zxindustry_dummies,
            only_cap=only_cap,
            iplot=iplot,
            ilegend=ilegend,
        )

    def __call__(self) -> pd.DataFrame:
        """å¦‚æœåšäº†è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ï¼Œåˆ™è¿”å›è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­æ•°æ®

        Returns
        -------
        `pd.DataFrame`
            å¦‚æœåšäº†è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ï¼Œåˆ™è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­æ•°æ®ï¼Œå¦åˆ™è¿”å›åŸå› å­æ•°æ®
        """
        return self.shen.factors_out

    def comments_ten(self) -> pd.DataFrame:
        """å¯¹å›æµ‹çš„ååˆ†ç»„ç»“æœåˆ†åˆ«ç»™å‡ºè¯„ä»·

        Returns
        -------
        `pd.DataFrame`
            è¯„ä»·æŒ‡æ ‡åŒ…æ‹¬å¹´åŒ–æ”¶ç›Šç‡ã€æ€»æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€å¹´åŒ–å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤ç‡ã€èƒœç‡
        """
        rets_cols = list(self.shen.group_rets.columns)
        rets_cols = rets_cols[:-1]
        coms = []
        for i in rets_cols:
            ret = self.shen.group_rets[i]
            net = self.shen.group_net_values[i]
            com = comments_on_twins(net, ret)
            com = com.to_frame(i)
            coms.append(com)
        df = pd.concat(coms, axis=1)
        return df.T


class pure_fall(object):
    # DONEï¼šä¿®æ”¹ä¸ºå› å­æ–‡ä»¶åå¯ä»¥å¸¦â€œæ—¥é¢‘_â€œï¼Œä¹Ÿå¯ä»¥ä¸å¸¦â€œæ—¥é¢‘_â€œ
    def __init__(self, daily_path: str) -> None:
        """ä¸€ä¸ªä½¿ç”¨mysqlä¸­çš„åˆ†é’Ÿæ•°æ®ï¼Œæ¥æ›´æ–°å› å­å€¼çš„æ¡†æ¶

        Parameters
        ----------
        daily_path : str
            æ—¥é¢‘å› å­å€¼å­˜å‚¨æ–‡ä»¶çš„åå­—ï¼Œè¯·ä»¥'.feather'ç»“å°¾
        """
        self.homeplace = HomePlace()
        # å°†åˆ†é’Ÿæ•°æ®æ‹¼æˆä¸€å¼ æ—¥é¢‘å› å­è¡¨
        self.daily_factors = None
        self.daily_factors_path = self.homeplace.factor_data_file + "æ—¥é¢‘_" + daily_path

    def __call__(self, monthly=False):
        """ä¸ºäº†é˜²æ­¢å±æ€§åå¤ªå¤šï¼Œå¿˜è®°äº†è¦è°ƒç”¨å“ªä¸ªæ‰æ˜¯ç»“æœï¼Œå› æ­¤å¯ä»¥ç›´æ¥è¾“å‡ºæœˆåº¦æ•°æ®è¡¨"""
        if monthly:
            return self.monthly_factors.copy()
        else:
            try:
                return self.daily_factors.copy()
            except Exception:
                return self.monthly_factors.copy()

    def __add__(self, selfas):
        """å°†å‡ ä¸ªå› å­æˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œå› å­å€¼ç›¸åŠ """
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac2s = []
        if not isinstance(selfas, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfas} is changed into Iterable")
            selfas = (selfas,)
        for selfa in selfas:
            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2s.append(fac2)
        for i in fac2s:
            fac1 = fac1 + i
        new_pure = pure_fall()
        new_pure.monthly_factors = fac1
        return new_pure

    def __mul__(self, selfas):
        """å°†å‡ ä¸ªå› å­æ¨ªæˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œä½¿å…¶éƒ½ä¸ºæ­£æ•°ï¼Œç„¶åå› å­å€¼ç›¸ä¹˜"""
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac1 = fac1 - fac1.min()
        fac2s = []
        if not isinstance(selfas, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfas} is changed into Iterable")
            selfas = (selfas,)
        for selfa in selfas:
            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2 = fac2 - fac2.min()
            fac2s.append(fac2)
        for i in fac2s:
            fac1 = fac1 * i
        new_pure = pure_fall()
        new_pure.monthly_factors = fac1
        return new_pure

    def __truediv__(self, selfa):
        """ä¸¤ä¸ªä¸€æ­£ä¸€å‰¯çš„å› å­ï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸å‡"""
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
        fac = fac1 - fac2
        new_pure = pure_fall()
        new_pure.monthly_factors = fac
        return new_pure

    def __floordiv__(self, selfa):
        """ä¸¤ä¸ªå› å­ä¸€æ­£ä¸€è´Ÿï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸é™¤"""
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
        fac1 = fac1 - fac1.min()
        fac2 = fac2 - fac2.min()
        fac = fac1 / fac2
        fac = fac.replace(np.inf, np.nan)
        new_pure = pure_fall()
        new_pure.monthly_factors = fac
        return new_pure

    @kk.desktop_sender(title="å˜¿ï¼Œæ­£äº¤åŒ–ç»“æŸå•¦ï½ğŸ¬")
    def __sub__(self, selfa):
        """ç”¨ä¸»å› å­å‰”é™¤å…¶ä»–ç›¸å…³å› å­ã€ä¼ ç»Ÿå› å­ç­‰
        selfaå¯ä»¥ä¸ºå¤šä¸ªå› å­å¯¹è±¡ç»„æˆçš„å…ƒç»„æˆ–åˆ—è¡¨ï¼Œæ¯ä¸ªè¾…åŠ©å› å­åªéœ€è¦æœ‰æœˆåº¦å› å­æ–‡ä»¶è·¯å¾„å³å¯"""
        if is_notebook():
            tqdm.tqdm_notebook().pandas()
        else:
            tqdm.tqdm.pandas()
        if not isinstance(selfa, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfa} is changed into Iterable")
            selfa = (selfa,)
        fac_main = self.wide_to_long(self.monthly_factors, "fac")
        fac_helps = [i.monthly_factors for i in selfa]
        help_names = ["help" + str(i) for i in range(1, (len(fac_helps) + 1))]
        fac_helps = list(map(self.wide_to_long, fac_helps, help_names))
        fac_helps = pd.concat(fac_helps, axis=1)
        facs = pd.concat([fac_main, fac_helps], axis=1).dropna()
        facs = facs.groupby("date").progress_apply(
            lambda x: self.de_in_group(x, help_names)
        )
        facs = facs.unstack()
        facs.columns = list(map(lambda x: x[1], list(facs.columns)))
        return facs

    def __gt__(self, selfa):
        """ç”¨äºè¾“å‡º25åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
        x = self.monthly_factors.copy()
        y = selfa.monthly_factors.copy()
        x = x.stack().reset_index()
        y = y.stack().reset_index()
        x.columns = ["date", "code", "fac"]
        y.columns = ["date", "code", "fac"]
        shen = pure_moon()
        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 5))
        x = (
            x.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupx"})
        )
        xy = pd.merge(x, y, on=["date", "code"])
        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 5))
        xy = (
            xy.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupy"})
        )
        xy = xy.assign(fac=xy.groupx * 5 + xy.groupy)
        xy = xy[["date", "code", "fac"]]
        xy = xy.set_index(["date", "code"]).unstack()
        xy.columns = [i[1] for i in list(xy.columns)]
        new_pure = pure_fall()
        new_pure.monthly_factors = xy
        return new_pure

    def __rshift__(self, selfa):
        """ç”¨äºè¾“å‡º100åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
        x = self.monthly_factors.copy()
        y = selfa.monthly_factors.copy()
        x = x.stack().reset_index()
        y = y.stack().reset_index()
        x.columns = ["date", "code", "fac"]
        y.columns = ["date", "code", "fac"]
        shen = pure_moon()
        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 10))
        x = (
            x.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupx"})
        )
        xy = pd.merge(x, y, on=["date", "code"])
        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 10))
        xy = (
            xy.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupy"})
        )
        xy = xy.assign(fac=xy.groupx * 10 + xy.groupy)
        xy = xy[["date", "code", "fac"]]
        xy = xy.set_index(["date", "code"]).unstack()
        xy.columns = [i[1] for i in list(xy.columns)]
        new_pure = pure_fall()
        new_pure.monthly_factors = xy
        return new_pure

    def wide_to_long(self, df, i):
        """å°†å®½æ•°æ®è½¬åŒ–ä¸ºé•¿æ•°æ®ï¼Œç”¨äºå› å­è¡¨è½¬åŒ–å’Œæ‹¼æ¥"""
        df = df.stack().reset_index()
        df.columns = ["date", "code", i]
        df = df.set_index(["date", "code"])
        return df

    def de_in_group(self, df, help_names):
        """å¯¹æ¯ä¸ªæ—¶é—´ï¼Œåˆ†åˆ«åšå›å½’ï¼Œå‰”é™¤ç›¸å…³å› å­"""
        ols_order = "fac~" + "+".join(help_names)
        ols_result = smf.ols(ols_order, data=df).fit()
        params = {i: ols_result.params[i] for i in help_names}
        predict = [params[i] * df[i] for i in help_names]
        predict = reduce(lambda x, y: x + y, predict)
        df.fac = df.fac - predict - ols_result.params["Intercept"]
        df = df[["fac"]]
        return df

    def standardlize_in_cross_section(self, df):
        """
        åœ¨æ¨ªæˆªé¢ä¸Šåšæ ‡å‡†åŒ–
        è¾“å…¥çš„dfåº”ä¸ºï¼Œåˆ—åæ˜¯è‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•æ˜¯æ—¶é—´
        """
        df = df.T
        df = (df - df.mean()) / df.std()
        df = df.T
        return df

    def get_single_day_factor(self, func: Callable, day: int) -> pd.DataFrame:
        """è®¡ç®—å•æ—¥çš„å› å­å€¼ï¼Œé€šè¿‡sqlæ•°æ®åº“ï¼Œè¯»å–å•æ—¥çš„æ•°æ®ï¼Œç„¶åè®¡ç®—å› å­å€¼"""
        sql = sqlConfig("minute_data_stock_alter")
        df = sql.get_data(str(day))
        the_func = partial(func)
        df = df.groupby(["code"]).apply(the_func).to_frame()
        df.columns = [str(day)]
        df = df.T
        df.index = pd.to_datetime(df.index, format="%Y%m%d")
        return df

    @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
    def get_daily_factors_alter(self, func: Callable) -> None:
        """ç”¨mysqlé€æ—¥æ›´æ–°åˆ†é’Ÿæ•°æ®æ„é€ çš„å› å­

        Parameters
        ----------
        func : Callable
            æ„é€ åˆ†é’Ÿæ•°æ®ä½¿ç”¨çš„å‡½æ•°

        Raises
        ------
        `IOError`
            å¦‚æœæ²¡æœ‰å†å²å› å­æ•°æ®ï¼Œå°†æŠ¥é”™
        """
        """é€šè¿‡minute_data_stock_alteræ•°æ®åº“ä¸€å¤©ä¸€å¤©è®¡ç®—å› å­å€¼"""
        try:
            try:
                self.daily_factors = pd.read_feather(self.daily_factors_path)
            except Exception:
                self.daily_factors_path = self.daily_factors_path.split("æ—¥é¢‘_")
                self.daily_factors_path = (
                    self.daily_factors_path[0] + self.daily_factors_path[1]
                )
                self.daily_factors = pd.read_feather(self.daily_factors_path)
            self.daily_factors = self.daily_factors.rename(
                columns={list(self.daily_factors.columns)[0]: "date"}
            )
            self.daily_factors = self.daily_factors.drop_duplicates(
                subset=["date"], keep="last"
            )
            self.daily_factors = self.daily_factors.set_index("date")
            sql = sqlConfig("minute_data_stock_alter")
            now_minute_datas = sql.show_tables(full=False)
            now_minute_data = now_minute_datas[-1]
            now_minute_data = pd.Timestamp(now_minute_data)
            if self.daily_factors.index.max() < now_minute_data:
                if not STATES["NO_LOG"]:
                    logger.info(
                        f"ä¸Šæ¬¡å­˜å‚¨çš„å› å­å€¼åˆ°{self.daily_factors.index.max()}ï¼Œè€Œåˆ†é’Ÿæ•°æ®æœ€æ–°åˆ°{now_minute_data}ï¼Œå¼€å§‹æ›´æ–°â€¦â€¦"
                    )
                old_end = datetime.datetime.strftime(
                    self.daily_factors.index.max(), "%Y%m%d"
                )
                now_minute_datas = [i for i in now_minute_datas if i > old_end]
                dfs = []
                for c in tqdm.tqdm(now_minute_datas, desc="æ¡‚æ£¹å…®å…°æ¡¨ï¼Œå‡»ç©ºæ˜å…®é‚æµå…‰ğŸŒŠ"):
                    df = self.get_single_day_factor(func, c)
                    dfs.append(df)
                dfs = pd.concat(dfs)
                dfs = dfs.sort_index()
                self.daily_factors = pd.concat([self.daily_factors, dfs])
                self.daily_factors = self.daily_factors.dropna(how="all")
                self.daily_factors = self.daily_factors[
                    self.daily_factors.index >= pd.Timestamp(str(STATES["START"]))
                ]
            self.daily_factors = self.daily_factors.reset_index()
            self.daily_factors = self.daily_factors.rename(
                columns={list(self.daily_factors.columns)[0]: "date"}
            )
            self.daily_factors = self.daily_factors.drop_duplicates(
                subset=["date"], keep="first"
            )
            self.daily_factors = self.daily_factors.set_index("date")
            self.daily_factors.reset_index().to_feather(self.daily_factors_path)
            if not STATES["NO_LOG"]:
                logger.success("æ›´æ–°å·²å®Œæˆ")

        except Exception:
            raise IOError(
                "æ‚¨è¿˜æ²¡æœ‰è¯¥å› å­çš„åˆçº§æ•°æ®ï¼Œæš‚æ—¶ä¸èƒ½æ›´æ–°ã€‚è¯·å…ˆä½¿ç”¨pure_fall_frequentæˆ–pure_fall_flexibleè®¡ç®—å†å²å› å­å€¼ã€‚"
            )


class pure_fallmount(pure_fall):
    """ç»§æ‰¿è‡ªçˆ¶ç±»ï¼Œä¸“ä¸ºåšå› å­æˆªé¢æ ‡å‡†åŒ–ä¹‹åç›¸åŠ å’Œå› å­å‰”é™¤å…¶ä»–è¾…åŠ©å› å­çš„ä½œç”¨"""

    def __init__(self, monthly_factors):
        """è¾“å…¥æœˆåº¦å› å­å€¼ï¼Œä»¥è®¾å®šæ–°çš„å¯¹è±¡"""
        super(pure_fall, self).__init__()
        self.monthly_factors = monthly_factors

    def __call__(self, monthly=False):
        """ä¸ºäº†é˜²æ­¢å±æ€§åå¤ªå¤šï¼Œå¿˜è®°äº†è¦è°ƒç”¨å“ªä¸ªæ‰æ˜¯ç»“æœï¼Œå› æ­¤å¯ä»¥ç›´æ¥è¾“å‡ºæœˆåº¦æ•°æ®è¡¨"""
        if monthly:
            return self.monthly_factors.copy()
        else:
            try:
                return self.daily_factors.copy()
            except Exception:
                return self.monthly_factors.copy()

    def __add__(self, selfas):
        """è¿”å›ä¸€ä¸ªå¯¹è±¡ï¼Œè€Œéä¸€ä¸ªè¡¨æ ¼ï¼Œå¦‚éœ€è¡¨æ ¼è¯·è°ƒç”¨å¯¹è±¡"""
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac2s = []
        if not isinstance(selfas, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfas} is changed into Iterable")
            selfas = (selfas,)
        for selfa in selfas:
            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2s.append(fac2)
        for i in fac2s:
            fac1 = fac1 + i
        new_pure = pure_fallmount(fac1)
        return new_pure

    def __mul__(self, selfas):
        """å°†å‡ ä¸ªå› å­æ¨ªæˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œä½¿å…¶éƒ½ä¸ºæ­£æ•°ï¼Œç„¶åå› å­å€¼ç›¸ä¹˜"""
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac1 = fac1 - fac1.min()
        fac2s = []
        if not isinstance(selfas, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfas} is changed into Iterable")
            selfas = (selfas,)
        for selfa in selfas:
            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2 = fac2 - fac2.min()
            fac2s.append(fac2)
        for i in fac2s:
            fac1 = fac1 * i
        new_pure = pure_fall()
        new_pure.monthly_factors = fac1
        return new_pure

    def __sub__(self, selfa):
        """è¿”å›å¯¹è±¡ï¼Œå¦‚éœ€è¡¨æ ¼ï¼Œè¯·è°ƒç”¨å¯¹è±¡"""
        if is_notebook():
            tqdm.tqdm_notebook().pandas()
        else:
            tqdm.tqdm.pandas()
        if not isinstance(selfa, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfa} is changed into Iterable")
            selfa = (selfa,)
        fac_main = self.wide_to_long(self.monthly_factors, "fac")
        fac_helps = [i.monthly_factors for i in selfa]
        help_names = ["help" + str(i) for i in range(1, (len(fac_helps) + 1))]
        fac_helps = list(map(self.wide_to_long, fac_helps, help_names))
        fac_helps = pd.concat(fac_helps, axis=1)
        facs = pd.concat([fac_main, fac_helps], axis=1).dropna()
        facs = facs.groupby("date").progress_apply(
            lambda x: self.de_in_group(x, help_names)
        )
        facs = facs.unstack()
        facs.columns = list(map(lambda x: x[1], list(facs.columns)))
        new_pure = pure_fallmount(facs)
        return new_pure

    def __gt__(self, selfa):
        """ç”¨äºè¾“å‡º25åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
        x = self.monthly_factors.copy()
        y = selfa.monthly_factors.copy()
        x = x.stack().reset_index()
        y = y.stack().reset_index()
        x.columns = ["date", "code", "fac"]
        y.columns = ["date", "code", "fac"]
        shen = pure_moon()
        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 5))
        x = (
            x.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupx"})
        )
        xy = pd.merge(x, y, on=["date", "code"])
        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 5))
        xy = (
            xy.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupy"})
        )
        xy = xy.assign(fac=xy.groupx * 5 + xy.groupy)
        xy = xy[["date", "code", "fac"]]
        xy = xy.set_index(["date", "code"]).unstack()
        xy.columns = [i[1] for i in list(xy.columns)]
        new_pure = pure_fallmount(xy)
        return new_pure

    def __rshift__(self, selfa):
        """ç”¨äºè¾“å‡º100åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
        x = self.monthly_factors.copy()
        y = selfa.monthly_factors.copy()
        x = x.stack().reset_index()
        y = y.stack().reset_index()
        x.columns = ["date", "code", "fac"]
        y.columns = ["date", "code", "fac"]
        shen = pure_moon()
        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 10))
        x = (
            x.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupx"})
        )
        xy = pd.merge(x, y, on=["date", "code"])
        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 10))
        xy = (
            xy.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupy"})
        )
        xy = xy.assign(fac=xy.groupx * 10 + xy.groupy)
        xy = xy[["date", "code", "fac"]]
        xy = xy.set_index(["date", "code"]).unstack()
        xy.columns = [i[1] for i in list(xy.columns)]
        new_pure = pure_fallmount(xy)
        return new_pure


class pure_fall_frequent(object):
    """å¯¹å•åªè‚¡ç¥¨å•æ—¥è¿›è¡Œæ“ä½œ"""

    def __init__(
        self,
        factor_file: str,
        startdate: int = None,
        enddate: int = None,
        kind: str = "stock",
        clickhouse: bool = 0,
        questdb: bool = 0,
        ignore_history_in_questdb: bool = 0,
        groupby_target: list = ["date", "code"],
    ) -> None:
        """åŸºäºclickhouseçš„åˆ†é’Ÿæ•°æ®ï¼Œè®¡ç®—å› å­å€¼ï¼Œæ¯å¤©çš„å› å­å€¼åªç”¨åˆ°å½“æ—¥çš„æ•°æ®

        Parameters
        ----------
        factor_file : str
            ç”¨äºä¿å­˜å› å­å€¼çš„æ–‡ä»¶åï¼Œéœ€ä¸ºfeatheræ–‡ä»¶ï¼Œä»¥'.feather'ç»“å°¾
        startdate : int, optional
            èµ·å§‹æ—¶é—´ï¼Œå½¢å¦‚20121231ï¼Œä¸ºå¼€åŒºé—´, by default None
        enddate : int, optional
            æˆªæ­¢æ—¶é—´ï¼Œå½¢å¦‚20220814ï¼Œä¸ºé—­åŒºé—´ï¼Œä¸ºç©ºåˆ™è®¡ç®—åˆ°æœ€è¿‘æ•°æ®, by default None
        kind : str, optional
            ç±»å‹ä¸ºè‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼ŒæŒ‡æ•°ä¸º'index', by default "stock"
        clickhouse : bool, optional
            ä½¿ç”¨clickhouseä½œä¸ºæ•°æ®æºï¼Œå¦‚æœpostgresqlä¸æœ¬å‚æ•°éƒ½ä¸º0ï¼Œå°†ä¾ç„¶ä»clickhouseä¸­è¯»å–, by default 0
        questdb : bool, optional
            ä½¿ç”¨questdbä½œä¸ºæ•°æ®æº, by default 0
        ignore_history_in_questdb : bool, optional
            æ‰“æ–­åé‡æ–°ä»å¤´è®¡ç®—ï¼Œæ¸…é™¤åœ¨questdbä¸­çš„è®°å½•
        groupby_target: list, optional
            groupbyè®¡ç®—æ—¶ï¼Œåˆ†ç»„çš„ä¾æ®ï¼Œä½¿ç”¨æ­¤å‚æ•°æ—¶ï¼Œè‡ªå®šä¹‰å‡½æ•°çš„éƒ¨åˆ†ï¼Œå¦‚æœæŒ‡å®šæŒ‰ç…§['date']åˆ†ç»„groupbyè®¡ç®—ï¼Œ
            åˆ™è¿”å›æ—¶ï¼Œåº”å½“è¿”å›ä¸€ä¸ªä¸¤åˆ—çš„dataframeï¼Œç¬¬ä¸€åˆ—ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç¬¬äºŒåˆ—ä¸ºä¸ºå› å­å€¼, by default ['date','code']
        """
        homeplace = HomePlace()
        self.kind = kind
        self.groupby_target = groupby_target
        if clickhouse == 0 and questdb == 0:
            clickhouse = 1
        self.clickhouse = clickhouse
        self.questdb = questdb
        if clickhouse == 1:
            # è¿æ¥clickhouse
            self.chc = ClickHouseClient("minute_data")
        elif questdb == 1:
            self.chc = Questdb()
        # å°†è®¡ç®—åˆ°ä¸€åŠçš„å› å­ï¼Œå­˜å…¥questdbä¸­ï¼Œé¿å…ä¸­é€”è¢«æ‰“æ–­åé‡æ–°è®¡ç®—ï¼Œè¡¨åå³ä¸ºå› å­æ–‡ä»¶åçš„æ±‰è¯­æ‹¼éŸ³
        pinyin = Pinyin()
        self.factor_file_pinyin = pinyin.get_pinyin(
            factor_file.replace(".feather", ""), ""
        )
        self.factor_steps = Questdb()
        # å®Œæ•´çš„å› å­æ–‡ä»¶è·¯å¾„
        factor_file = homeplace.factor_data_file + factor_file
        self.factor_file = factor_file
        # è¯»å…¥ä¹‹å‰çš„å› å­
        if os.path.exists(factor_file):
            factor_old = pd.read_feather(self.factor_file)
            factor_old.columns = ["date"] + list(factor_old.columns)[1:]
            factor_old = factor_old.drop_duplicates(subset=["date"])
            factor_old = factor_old.set_index("date")
            self.factor_old = factor_old
            # å·²ç»ç®—å¥½çš„æ—¥å­
            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
            self.dates_old = dates_old
        elif (not ignore_history_in_questdb) and self.factor_file_pinyin in list(
            self.factor_steps.get_data("show tables").table
        ):
            logger.info(
                f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œç°åœ¨å°†è¯»å–ä¸Šæ¬¡çš„æ•°æ®ï¼Œç»§ç»­è®¡ç®—"
            )
            factor_old = self.factor_steps.get_data(
                f"select * from {self.factor_file_pinyin}"
            )
            # åˆ¤æ–­ä¸€ä¸‹æ¯å¤©æ˜¯å¦ç”Ÿæˆå¤šä¸ªæ•°æ®ï¼Œå•ä¸ªæ•°æ®å°±ä»¥floatå½¢å¼å­˜å‚¨ï¼Œå¤šä¸ªæ•°æ®ä»¥listå½¢å¼å­˜å‚¨
            if "f0" in list(factor_old.columns):
                factor_old = factor_old[factor_old.f0 != "date"]
                factor_old.columns = ["date", "code", "fac"]
                try:
                    factor_old.fac = factor_old.fac.apply(
                        lambda x: [float(i) for i in x[1:-1].split(" ") if i != ""]
                    )
                except Exception:
                    factor_old.fac = factor_old.fac.apply(
                        lambda x: [float(i) for i in x[1:-1].split(", ") if i != ""]
                    )
            factor_old = factor_old.pivot(index="date", columns="code", values="fac")
            factor_old.index = pd.to_datetime(factor_old.index)
            factor_old = factor_old.sort_index()
            factor_old = drop_duplicates_index(factor_old)
            self.factor_old = factor_old
            # å·²ç»ç®—å¥½çš„æ—¥å­
            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
            self.dates_old = dates_old
        elif ignore_history_in_questdb and self.factor_file_pinyin in list(
            self.factor_steps.get_data("show tables").table
        ):
            logger.info(
                f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œä½†æ‚¨é€‰æ‹©é‡æ–°è®¡ç®—ï¼Œæ‰€ä»¥æ­£åœ¨åˆ é™¤åŸæ¥çš„æ•°æ®ï¼Œä»å¤´è®¡ç®—"
            )
            factor_old = self.factor_steps.do_order(
                f"drop table {self.factor_file_pinyin}"
            )
            self.factor_old = None
            self.dates_old = []
            logger.info("åˆ é™¤å®Œæ¯•ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
        else:
            self.factor_old = None
            self.dates_old = []
            logger.info("è¿™ä¸ªå› å­ä»¥å‰æ²¡æœ‰ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
        # è¯»å–å½“å‰æ‰€æœ‰çš„æ—¥å­
        dates_all = self.chc.show_all_dates(f"minute_data_{kind}")
        dates_all = [int(i) for i in dates_all]
        if startdate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i >= startdate]
        if enddate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i <= enddate]
        self.dates_all = dates_all
        # éœ€è¦æ–°è¡¥å……çš„æ—¥å­
        self.dates_new = sorted([i for i in dates_all if i not in self.dates_old])
        if len(self.dates_new) == 0:
            ...
        elif len(self.dates_new) == 1:
            self.dates_new_intervals = [[pd.Timestamp(str(self.dates_new[0]))]]
            print(f"åªç¼ºä¸€å¤©{self.dates_new[0]}")
        else:
            dates = [pd.Timestamp(str(i)) for i in self.dates_new]
            intervals = [[]] * len(dates)
            interbee = 0
            intervals[0] = intervals[0] + [dates[0]]
            for i in range(len(dates) - 1):
                val1 = dates[i]
                val2 = dates[i + 1]
                if val2 - val1 < pd.Timedelta(days=30):
                    ...
                else:
                    interbee = interbee + 1
                intervals[interbee] = intervals[interbee] + [val2]
            intervals = [i for i in intervals if len(i) > 0]
            print(f"å…±{len(intervals)}ä¸ªæ—¶é—´åŒºé—´ï¼Œåˆ†åˆ«æ˜¯")
            for date in intervals:
                print(f"ä»{date[0]}åˆ°{date[-1]}")
            self.dates_new_intervals = intervals
        self.factor_new = []

    def __call__(self) -> pd.DataFrame:
        """è·å¾—ç»è¿ç®—äº§ç”Ÿçš„å› å­

        Returns
        -------
        `pd.DataFrame`
            ç»è¿ç®—äº§ç”Ÿçš„å› å­å€¼
        """
        return self.factor.copy()

    def select_one_calculate(
        self,
        date: pd.Timestamp,
        func: Callable,
        fields: str = "*",
        show_time: bool = 0,
    ) -> None:
        the_func = partial(func)
        if not isinstance(date, int):
            date = int(datetime.datetime.strftime(date, "%Y%m%d"))
        # å¼€å§‹è®¡ç®—å› å­å€¼
        if self.clickhouse == 1:
            sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={date * 100} order by code,date,num"
        else:
            sql_order = (
                f"select {fields} from minute_data_{self.kind} where date='{date}'"
            )
        if show_time:
            df = self.chc.get_data_show_time(sql_order)
        else:
            df = self.chc.get_data(sql_order)
        if self.clickhouse == 1:
            df = ((df.set_index("code")) / 100).reset_index()
        else:
            df.num = df.num.astype(int)
            df.date = df.date.astype(int)
            df = df.sort_values(["date", "num"])
        df = df.groupby(self.groupby_target).apply(the_func)
        if self.groupby_target == ["date", "code"]:
            df = df.to_frame("fac").reset_index()
            df.columns = ["date", "code", "fac"]
        else:
            df = df.reset_index()
        df = df.pivot(columns="code", index="date", values="fac")
        df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
        return df

    def select_many_calculate(
        self,
        dates: list[pd.Timestamp],
        func: Callable,
        fields: str = "*",
        chunksize: int = 10,
        show_time: bool = 0,
        tqdm_inside: bool = 0,
    ) -> None:
        the_func = partial(func)
        dates = [int(datetime.datetime.strftime(i, "%Y%m%d")) for i in dates]
        # å°†éœ€è¦æ›´æ–°çš„æ—¥å­åˆ†å—ï¼Œæ¯200å¤©ä¸€ç»„ï¼Œä¸€èµ·è¿ç®—
        dates_new_len = len(dates)
        cut_points = list(range(0, dates_new_len, chunksize)) + [dates_new_len - 1]
        if cut_points[-1] == cut_points[-2]:
            cut_points = cut_points[:-1]
        cut_first = cut_points[0]
        cuts = tuple(zip(cut_points[:-1], cut_points[1:]))
        print(f"å…±{len(cuts)}æ®µ")
        factor_new = []
        df_first = self.select_one_calculate(
            date=dates[0],
            func=func,
            fields=fields,
            show_time=show_time,
        )
        factor_new.append(df_first)
        to_save = df_first.stack().reset_index()
        to_save.columns = ["date", "code", "fac"]
        self.factor_steps.write_via_csv(to_save, self.factor_file_pinyin)

        if tqdm_inside == 1:
            # å¼€å§‹è®¡ç®—å› å­å€¼
            for date1, date2 in cuts:
                if self.clickhouse == 1:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1] * 100} and date<={dates[date2] * 100} order by code,date,num"
                else:
                    sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{dates[date1]} and cast(date as int)<={dates[date2]}"
                if show_time:
                    df = self.chc.get_data_show_time(sql_order)
                else:
                    df = self.chc.get_data(sql_order)
                if self.clickhouse == 1:
                    df = ((df.set_index("code")) / 100).reset_index()
                else:
                    df.num = df.num.astype(int)
                    df.date = df.date.astype(int)
                    df = df.sort_values(["date", "num"])
                if is_notebook():
                    tqdm.tqdm_notebook().pandas()
                else:
                    tqdm.tqdm.pandas()
                df = df.groupby(self.groupby_target).progress_apply(the_func)
                df = df.to_frame("fac").reset_index()
                df.columns = ["date", "code", "fac"]
                df = df.pivot(columns="code", index="date", values="fac")
                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                factor_new.append(df)
                to_save = df.stack().reset_index()
                to_save.columns = ["date", "code", "fac"]
                self.factor_steps.write_via_csv(to_save, self.factor_file_pinyin)
        else:
            if is_notebook():
                # å¼€å§‹è®¡ç®—å› å­å€¼
                for date1, date2 in tqdm.tqdm_notebook(cuts, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
                    if self.clickhouse == 1:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1] * 100} and date<={dates[date2] * 100} order by code,date,num"
                    else:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1]} and date<={dates[date2]} order by code,date,num"
                    if show_time:
                        df = self.chc.get_data_show_time(sql_order)
                    else:
                        df = self.chc.get_data(sql_order)
                    if self.clickhouse == 1:
                        df = ((df.set_index("code")) / 100).reset_index()
                    df = df.groupby(self.groupby_target).apply(the_func)
                    if self.groupby_target == ["date", "code"]:
                        df = df.to_frame("fac").reset_index()
                        df.columns = ["date", "code", "fac"]
                    else:
                        df = df.reset_index()
                    df = df.pivot(columns="code", index="date", values="fac")
                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                    factor_new.append(df)
                    to_save = df.stack().reset_index()
                    to_save.columns = ["date", "code", "fac"]
                    self.factor_steps.write_via_csv(to_save, self.factor_file_pinyin)
            else:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                for date1, date2 in tqdm.tqdm(cuts, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
                    if self.clickhouse == 1:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1] * 100} and date<={dates[date2] * 100} order by code,date,num"
                    else:
                        sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{dates[date1]} and cast(date as int)<={dates[date2]}"
                    if show_time:
                        df = self.chc.get_data_show_time(sql_order)
                    else:
                        df = self.chc.get_data(sql_order)
                    if self.clickhouse == 1:
                        df = ((df.set_index("code")) / 100).reset_index()
                    else:
                        df.num = df.num.astype(int)
                        df.date = df.date.astype(int)
                        df = df.sort_values(["date", "num"])
                    df = df.groupby(self.groupby_target).apply(the_func)
                    if self.groupby_target == ["date", "code"]:
                        df = df.to_frame("fac").reset_index()
                        df.columns = ["date", "code", "fac"]
                    else:
                        df = df.reset_index()
                    df = df.pivot(columns="code", index="date", values="fac")
                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                    factor_new.append(df)
                    to_save = df.stack().reset_index()
                    to_save.columns = ["date", "code", "fac"]
                    self.factor_steps.write_via_csv(to_save, self.factor_file_pinyin)
        factor_new = pd.concat(factor_new)
        return factor_new

    def select_any_calculate(
        self,
        dates: list[pd.Timestamp],
        func: Callable,
        fields: str = "*",
        chunksize: int = 10,
        show_time: bool = 0,
        tqdm_inside: bool = 0,
    ) -> None:
        if len(dates) == 1:
            res = self.select_one_calculate(
                dates[0],
                func=func,
                fields=fields,
                show_time=show_time,
            )
        else:
            res = self.select_many_calculate(
                dates=dates,
                func=func,
                fields=fields,
                chunksize=chunksize,
                show_time=show_time,
                tqdm_inside=tqdm_inside,
            )
        return res

    @staticmethod
    def for_cross_via_str(func):
        """è¿”å›å€¼ä¸ºä¸¤å±‚çš„listï¼Œæ¯ä¸€ä¸ªé‡Œå±‚çš„å°listä¸ºå•ä¸ªè‚¡ç¥¨åœ¨è¿™ä¸€å¤©çš„è¿”å›å€¼
        ä¾‹å¦‚
        ```python
        return [[0.11,0.24,0.55],[2.59,1.99,0.43],[1.32,8.88,7.77]â€¦â€¦]
        ```
        ä¸Šä¾‹ä¸­ï¼Œæ¯ä¸ªè‚¡ç¥¨ä¸€å¤©è¿”å›ä¸‰ä¸ªå› å­å€¼ï¼Œé‡Œå±‚çš„listæŒ‰ç…§è‚¡ç¥¨ä»£ç é¡ºåºæ’åˆ—"""

        def full_run(df, *args, **kwargs):
            codes = sorted(list(set(df.code)))
            res = func(df, *args, **kwargs)
            if isinstance(res[0], list):
                kind = 1
                res = [",".join(i) for i in res]
            else:
                kind = 0
            df = pd.DataFrame({"code": codes, "fac": res})
            if kind:
                df.fac = df.fac.apply(lambda x: [float(i) for i in x.split(",")])
            return df

        return full_run

    @staticmethod
    def for_cross_via_zip(func):
        """è¿”å›å€¼ä¸ºå¤šä¸ªpd.Seriesï¼Œæ¯ä¸ªpd.Seriesçš„indexä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå•ä¸ªå› å­å€¼
        ä¾‹å¦‚
        ```python
        return (
                    pd.Series([1.54,8.77,9.99â€¦â€¦],index=['000001.SZ','000002.SZ','000004.SZ'â€¦â€¦]),
                    pd.Series([3.54,6.98,9.01â€¦â€¦],index=['000001.SZ','000002.SZ','000004.SZ'â€¦â€¦]),
                )
        ```
        ä¸Šä¾‹ä¸­ï¼Œæ¯ä¸ªè‚¡ç¥¨ä¸€å¤©è¿”å›ä¸¤ä¸ªå› å­å€¼ï¼Œæ¯ä¸ªpd.Serieså¯¹åº”ä¸€ä¸ªå› å­å€¼
        """

        def full_run(df, *args, **kwargs):
            res = func(df, *args, **kwargs)
            res = pd.concat(res, axis=1)
            res.columns = [f"fac{i}" for i in range(len(res.columns))]
            res = res.assign(fac=list(zip(*[res[i] for i in list(res.columns)])))
            res = res[["fac"]].reset_index()
            res.columns = ["code", "fac"]
            return res

        return full_run

    @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
    def get_daily_factors(
        self,
        func: Callable,
        fields: str = "*",
        chunksize: int = 10,
        show_time: bool = 0,
        tqdm_inside: bool = 0,
    ) -> None:
        """æ¯æ¬¡æŠ½å–chunksizeå¤©çš„æˆªé¢ä¸Šå…¨éƒ¨è‚¡ç¥¨çš„åˆ†é’Ÿæ•°æ®
        å¯¹æ¯å¤©çš„è‚¡ç¥¨çš„æ•°æ®è®¡ç®—å› å­å€¼

        Parameters
        ----------
        func : Callable
            ç”¨äºè®¡ç®—å› å­å€¼çš„å‡½æ•°
        fields : str, optional
            è‚¡ç¥¨æ•°æ®æ¶‰åŠåˆ°å“ªäº›å­—æ®µï¼Œæ’é™¤ä¸å¿…è¦çš„å­—æ®µï¼Œå¯ä»¥èŠ‚çº¦è¯»å–æ•°æ®çš„æ—¶é—´ï¼Œå½¢å¦‚'date,code,num,close,amount,open'
            æå–å‡ºçš„æ•°æ®ï¼Œè‡ªåŠ¨æŒ‰ç…§code,date,numæ’åºï¼Œå› æ­¤code,date,numæ˜¯å¿…ä¸å¯å°‘çš„å­—æ®µ, by default "*"
        chunksize : int, optional
            æ¯æ¬¡è¯»å–çš„æˆªé¢ä¸Šçš„å¤©æ•°, by default 10
        show_time : bool, optional
            å±•ç¤ºæ¯æ¬¡è¯»å–æ•°æ®æ‰€éœ€è¦çš„æ—¶é—´, by default 0
        tqdm_inside : bool, optional
            å°†è¿›åº¦æ¡åŠ åœ¨å†…éƒ¨ï¼Œè€Œéå¤–éƒ¨ï¼Œå»ºè®®ä»…chunksizeè¾ƒå¤§æ—¶ä½¿ç”¨, by default 0
        """
        if len(self.dates_new) > 0:
            for interval in self.dates_new_intervals:
                df = self.select_any_calculate(
                    dates=interval,
                    func=func,
                    fields=fields,
                    chunksize=chunksize,
                    show_time=show_time,
                    tqdm_inside=tqdm_inside,
                )
                self.factor_new.append(df)
            self.factor_new = pd.concat(self.factor_new)
            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
            self.factor = pd.concat([self.factor_old, self.factor_new]).sort_index()
            self.factor = self.factor.dropna(how="all")
            self.factor = self.factor.reset_index()
            self.factor = self.factor.rename(
                columns={list(self.factor.columns)[0]: "date"}
            )
            self.factor = self.factor.drop_duplicates(subset=["date"], keep="first")
            self.factor = self.factor.set_index("date")
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            # å­˜å…¥æœ¬åœ°
            self.factor.reset_index().to_feather(self.factor_file)
            logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
            # åˆ é™¤å­˜å‚¨åœ¨questdbçš„ä¸­é€”å¤‡ä»½æ•°æ®
            self.factor_steps.do_order(f"drop table {self.factor_file_pinyin}")
            logger.info("å¤‡ä»½åœ¨questdbçš„è¡¨æ ¼å·²åˆ é™¤")

        else:
            self.factor = self.factor_old
            self.factor = self.factor.reset_index()
            self.factor = self.factor.rename(
                columns={list(self.factor.columns)[0]: "date"}
            )
            self.factor = self.factor.drop_duplicates(subset=["date"], keep="first")
            self.factor = self.factor.set_index("date")
            # å­˜å…¥æœ¬åœ°
            self.factor.reset_index().to_feather(self.factor_file)
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            logger.info(f"å½“å‰æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼å·²ç»æ˜¯æœ€æ–°çš„äº†")


class pure_fall_flexible(object):
    def __init__(
        self,
        factor_file: str,
        startdate: int = None,
        enddate: int = None,
        kind: str = "stock",
        clickhouse: bool = 0,
        questdb: bool = 0,
    ) -> None:
        """åŸºäºclickhouseçš„åˆ†é’Ÿæ•°æ®ï¼Œè®¡ç®—å› å­å€¼ï¼Œæ¯å¤©çš„å› å­å€¼ç”¨åˆ°å¤šæ—¥çš„æ•°æ®ï¼Œæˆ–è€…ç”¨åˆ°æˆªé¢çš„æ•°æ®
        å¯¹ä¸€æ®µæ—¶é—´çš„æˆªé¢æ•°æ®è¿›è¡Œæ“ä½œï¼Œåœ¨get_daily_factorsçš„funcå‡½æ•°ä¸­
        è¯·å†™å…¥df=df.groupby([xxx]).apply(fff)ä¹‹ç±»çš„è¯­å¥
        ç„¶åå•ç‹¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä½œä¸ºè¦applyçš„fffï¼Œå¯ä»¥åœ¨applyä¸ŠåŠ è¿›åº¦æ¡

        Parameters
        ----------
        factor_file : str
            ç”¨äºå­˜å‚¨å› å­çš„æ–‡ä»¶åç§°ï¼Œè¯·ä»¥'.feather'ç»“å°¾
        startdate : int, optional
            è®¡ç®—å› å­çš„èµ·å§‹æ—¥æœŸï¼Œå½¢å¦‚20220816, by default None
        enddate : int, optional
            è®¡ç®—å› å­çš„ç»ˆæ­¢æ—¥æœŸï¼Œå½¢å¦‚20220816, by default None
        kind : str, optional
            æŒ‡å®šè®¡ç®—è‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼ŒæŒ‡æ•°åˆ™ä¸º'index', by default "stock"
        clickhouse : bool, optional
            ä½¿ç”¨clickhouseä½œä¸ºæ•°æ®æºï¼Œå¦‚æœpostgresqlä¸æœ¬å‚æ•°éƒ½ä¸º0ï¼Œå°†ä¾ç„¶ä»clickhouseä¸­è¯»å–, by default 0
        questdb : bool, optional
            ä½¿ç”¨questdbä½œä¸ºæ•°æ®æº, by default 0
        """
        homeplace = HomePlace()
        self.kind = kind
        if clickhouse == 0 and questdb == 0:
            clickhouse = 1
        self.clickhouse = clickhouse
        self.questdb = questdb
        if clickhouse == 1:
            # è¿æ¥clickhouse
            self.chc = ClickHouseClient("minute_data")
        elif questdb:
            self.chc = Questdb()
        # å®Œæ•´çš„å› å­æ–‡ä»¶è·¯å¾„
        factor_file = homeplace.factor_data_file + factor_file
        self.factor_file = factor_file
        # è¯»å…¥ä¹‹å‰çš„å› å­
        if os.path.exists(factor_file):
            factor_old = pd.read_feather(self.factor_file)
            factor_old.columns = ["date"] + list(factor_old.columns)[1:]
            factor_old = factor_old.drop_duplicates(subset=["date"])
            factor_old = factor_old.set_index("date")
            self.factor_old = factor_old
            # å·²ç»ç®—å¥½çš„æ—¥å­
            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
            self.dates_old = dates_old
        else:
            self.factor_old = None
            self.dates_old = []
            logger.info("è¿™ä¸ªå› å­ä»¥å‰æ²¡æœ‰ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
        # è¯»å–å½“å‰æ‰€æœ‰çš„æ—¥å­
        dates_all = self.chc.show_all_dates(f"minute_data_{kind}")
        if startdate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i >= startdate]
        if enddate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i <= enddate]
        self.dates_all = dates_all
        # éœ€è¦æ–°è¡¥å……çš„æ—¥å­
        self.dates_new = sorted([i for i in dates_all if i not in dates_old])

    def __call__(self) -> pd.DataFrame:
        """ç›´æ¥è¿”å›å› å­å€¼çš„pd.DataFrame

        Returns
        -------
        `pd.DataFrame`
            è®¡ç®—å‡ºçš„å› å­å€¼
        """
        return self.factor.copy()

    @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
    def get_daily_factors(
        self,
        func: Callable,
        fields: str = "*",
        chunksize: int = 250,
        show_time: bool = 0,
        tqdm_inside: bool = 0,
    ) -> None:
        """æ¯æ¬¡æŠ½å–chunksizeå¤©çš„æˆªé¢ä¸Šå…¨éƒ¨è‚¡ç¥¨çš„åˆ†é’Ÿæ•°æ®
        ä¾ç…§å®šä¹‰çš„å‡½æ•°è®¡ç®—å› å­å€¼

        Parameters
        ----------
        func : Callable
            ç”¨äºè®¡ç®—å› å­å€¼çš„å‡½æ•°
        fields : str, optional
            è‚¡ç¥¨æ•°æ®æ¶‰åŠåˆ°å“ªäº›å­—æ®µï¼Œæ’é™¤ä¸å¿…è¦çš„å­—æ®µï¼Œå¯ä»¥èŠ‚çº¦è¯»å–æ•°æ®çš„æ—¶é—´ï¼Œå½¢å¦‚'date,code,num,close,amount,open'
            æå–å‡ºçš„æ•°æ®ï¼Œè‡ªåŠ¨æŒ‰ç…§code,date,numæ’åºï¼Œå› æ­¤code,date,numæ˜¯å¿…ä¸å¯å°‘çš„å­—æ®µ, by default "*"
        chunksize : int, optional
            æ¯æ¬¡è¯»å–çš„æˆªé¢ä¸Šçš„å¤©æ•°, by default 10
        show_time : bool, optional
            å±•ç¤ºæ¯æ¬¡è¯»å–æ•°æ®æ‰€éœ€è¦çš„æ—¶é—´, by default 0
        tqdm_inside : bool, optional
            å°†è¿›åº¦æ¡åŠ åœ¨å†…éƒ¨ï¼Œè€Œéå¤–éƒ¨ï¼Œå»ºè®®ä»…chunksizeè¾ƒå¤§æ—¶ä½¿ç”¨, by default 0
        """
        the_func = partial(func)
        # å°†éœ€è¦æ›´æ–°çš„æ—¥å­åˆ†å—ï¼Œæ¯200å¤©ä¸€ç»„ï¼Œä¸€èµ·è¿ç®—
        dates_new_len = len(self.dates_new)
        if dates_new_len > 0:
            cut_points = list(range(0, dates_new_len, chunksize)) + [dates_new_len - 1]
            if cut_points[-1] == cut_points[-2]:
                cut_points = cut_points[:-1]
            self.cut_points = cut_points
            self.factor_new = []
            # å¼€å§‹è®¡ç®—å› å­å€¼
            if tqdm_inside:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                for date1, date2 in cut_points:
                    if self.clickhouse == 1:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]*100} and date<={self.dates_new[date2]*100} order by code,date,num"
                    else:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]} and date<={self.dates_new[date2]}"
                    if show_time:
                        df = self.chc.get_data_show_time(sql_order)
                    else:
                        df = self.chc.get_data(sql_order)
                    if self.clickhouse == 1:
                        df = ((df.set_index("code")) / 100).reset_index()
                    if is_notebook():
                        tqdm.tqdm_notebook().pandas()
                    else:
                        tqdm.tqdm.pandas()
                    df = the_func(df)
                    if isinstance(df, pd.Series):
                        df = df.reset_index()
                    df.columns = ["date", "code", "fac"]
                    df = df.pivot(columns="code", index="date", values="fac")
                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                    self.factor_new.append(df)
            else:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                for date1, date2 in tqdm.tqdm(cut_points, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
                    if self.clickhouse == 1:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]*100} and date<={self.dates_new[date2]*100} order by code,date,num"
                    else:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]} and date<={self.dates_new[date2]} order by code,date,num"
                    if show_time:
                        df = self.chc.get_data_show_time(sql_order)
                    else:
                        df = self.chc.get_data(sql_order)
                    if self.clickhouse == 1:
                        df = ((df.set_index("code")) / 100).reset_index()
                    df = the_func(df)
                    if isinstance(df, pd.Series):
                        df = df.reset_index()
                    df.columns = ["date", "code", "fac"]
                    df = df.pivot(columns="code", index="date", values="fac")
                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                    self.factor_new.append(df)
            self.factor_new = pd.concat(self.factor_new)
            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
            self.factor = pd.concat([self.factor_old, self.factor_new]).sort_index()
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            # å­˜å…¥æœ¬åœ°
            self.factor.reset_index().to_feather(self.factor_file)
            logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
        elif dates_new_len == 1:
            print("å…±1å¤©")
            if tqdm_inside:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                if self.clickhouse == 1:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]*100} order by code,date,num"
                else:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]} order by code,date,num"
                if show_time:
                    df = self.chc.get_data_show_time(sql_order)
                else:
                    df = self.chc.get_data(sql_order)
                if self.clickhouse == 1:
                    df = ((df.set_index("code")) / 100).reset_index()
                tqdm.tqdm.pandas()
                df = the_func(df)
                if isinstance(df, pd.Series):
                    df = df.reset_index()
                df.columns = ["date", "code", "fac"]
                df = df.pivot(columns="code", index="date", values="fac")
                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
            else:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                if self.clickhouse == 1:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]*100} order by code,date,num"
                else:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]} order by code,date,num"
                if show_time:
                    df = self.chc.get_data_show_time(sql_order)
                else:
                    df = self.chc.get_data(sql_order)
                if self.clickhouse == 1:
                    df = ((df.set_index("code")) / 100).reset_index()
                df = the_func(df)
                if isinstance(df, pd.Series):
                    df = df.reset_index()
                df.columns = ["date", "code", "fac"]
                df = df.pivot(columns="code", index="date", values="fac")
                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                self.factor_new.append(df)
            self.factor_new = df
            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
            self.factor = (
                pd.concat([self.factor_old, self.factor_new])
                .sort_index()
                .drop_duplicates()
            )
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            # å­˜å…¥æœ¬åœ°
            self.factor.reset_index().to_feather(self.factor_file)
            logger.info(f"è¡¥å……{self.dates_new[0]}æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
        else:
            self.factor = self.factor_old
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            logger.info(f"å½“å‰æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼å·²ç»æ˜¯æœ€æ–°çš„äº†")


class pure_coldwinter(object):
    # DONE: å¯ä»¥è‡ªç”±æ·»åŠ å…¶ä»–è¦å‰”é™¤çš„å› å­ï¼Œæˆ–è€…æ›¿æ¢æŸäº›è¦å‰”é™¤çš„å› å­

    def __init__(
        self,
        facs_dict: dict = None,
        momentum: bool = 1,
        earningsyield: bool = 1,
        growth: bool = 1,
        liquidity: bool = 1,
        size: bool = 1,
        leverage: bool = 1,
        beta: bool = 1,
        nonlinearsize: bool = 1,
        residualvolatility: bool = 1,
        booktoprice: bool = 1,
    ) -> None:
        """è¯»å…¥10ç§å¸¸ç”¨é£æ ¼å› å­ï¼Œå¹¶å¯ä»¥é¢å¤–åŠ å…¥å…¶ä»–å› å­

        Parameters
        ----------
        facs_dict : dict, optional
            é¢å¤–åŠ å…¥çš„å› å­ï¼Œåå­—ä¸ºkeyï¼Œå› å­çŸ©é˜µä¸ºvalueï¼Œå½¢å¦‚`{'åè½¬': ret20, 'æ¢æ‰‹': tr20}`, by default None
        momentum : bool, optional
            æ˜¯å¦åˆ å»åŠ¨é‡å› å­, by default 1
        earningsyield : bool, optional
            æ˜¯å¦åˆ å»ç›ˆåˆ©å› å­, by default 1
        growth : bool, optional
            æ˜¯å¦åˆ å»æˆé•¿å› å­, by default 1
        liquidity : bool, optional
            æ˜¯å¦åˆ å»æµåŠ¨æ€§å› å­, by default 1
        size : bool, optional
            æ˜¯å¦åˆ å»è§„æ¨¡å› å­, by default 1
        leverage : bool, optional
            æ˜¯å¦åˆ å»æ æ†å› å­, by default 1
        beta : bool, optional
            æ˜¯å¦åˆ å»è´å¡”å› å­, by default 1
        nonlinearsize : bool, optional
            æ˜¯å¦åˆ å»éçº¿æ€§å¸‚å€¼å› å­, by default 1
        residualvolatility : bool, optional
            æ˜¯å¦åˆ å»æ®‹å·®æ³¢åŠ¨ç‡å› å­, by default 1
        booktoprice : bool, optional
            æ˜¯å¦åˆ å»è´¦é¢å¸‚å€¼æ¯”å› å­, by default 1
        """
        self.homeplace = HomePlace()
        # barraå› å­æ•°æ®
        styles = os.listdir(self.homeplace.barra_data_file)
        styles = [i for i in styles if i.endswith(".feather")]
        barras = {}
        for s in styles:
            k = s.split(".")[0]
            v = pd.read_feather(self.homeplace.barra_data_file + s)
            v.columns = ["date"] + list(v.columns)[1:]
            v = v.set_index("date")
            barras[k] = v
        rename_dict = {
            "fac": "å› å­è‡ªèº«",
            "size": "å¸‚å€¼",
            "nonlinearsize": "éçº¿æ€§å¸‚å€¼",
            "booktoprice": "ä¼°å€¼",
            "earningsyield": "ç›ˆåˆ©",
            "growth": "æˆé•¿",
            "leverage": "æ æ†",
            "liquidity": "æµåŠ¨æ€§",
            "momentum": "åŠ¨é‡",
            "residualvolatility": "æ³¢åŠ¨ç‡",
            "beta": "è´å¡”",
        }
        if momentum == 0:
            barras = {k: v for k, v in barras.items() if k != "momentum"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "momentum"}
        if earningsyield == 0:
            barras = {k: v for k, v in barras.items() if k != "earningsyield"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "earningsyield"}
        if growth == 0:
            barras = {k: v for k, v in barras.items() if k != "growth"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "growth"}
        if liquidity == 0:
            barras = {k: v for k, v in barras.items() if k != "liquidity"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "liquidity"}
        if size == 0:
            barras = {k: v for k, v in barras.items() if k != "size"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "size"}
        if leverage == 0:
            barras = {k: v for k, v in barras.items() if k != "leverage"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "leverage"}
        if beta == 0:
            barras = {k: v for k, v in barras.items() if k != "beta"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "beta"}
        if nonlinearsize == 0:
            barras = {k: v for k, v in barras.items() if k != "nonlinearsize"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "nonlinearsize"}
        if residualvolatility == 0:
            barras = {k: v for k, v in barras.items() if k != "residualvolatility"}
            rename_dict = {
                k: v for k, v in rename_dict.items() if k != "residualvolatility"
            }
        if booktoprice == 0:
            barras = {k: v for k, v in barras.items() if k != "booktoprice"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "booktoprice"}
        if facs_dict is not None:
            barras.update(facs_dict)
        self.barras = barras
        self.rename_dict = rename_dict
        sort_names = list(rename_dict.values())
        if facs_dict is not None:
            sort_names = sort_names + list(facs_dict.keys())
        sort_names = [i for i in sort_names if i != "å› å­è‡ªèº«"]
        self.sort_names = sort_names

    def __call__(self):
        """è¿”å›çº¯å‡€å› å­å€¼"""
        return self.snow_fac

    def set_factors_df_wide(self, df):
        """ä¼ å…¥å› å­æ•°æ®ï¼Œæ—¶é—´ä¸ºç´¢å¼•ï¼Œä»£ç ä¸ºåˆ—å"""
        df1 = df.copy()
        # df1.index=df1.index-pd.DateOffset(months=1)
        df1 = df1.resample("M").last()
        df1 = df1.stack().reset_index()
        df1.columns = ["date", "code", "fac"]
        self.factors = df1.copy()

    def daily_to_monthly(self, df):
        """å°†æ—¥åº¦çš„barraå› å­æœˆåº¦åŒ–"""
        df = df.resample("M").last()
        return df

    def get_monthly_barras_industrys(self):
        """å°†barraå› å­å’Œè¡Œä¸šå“‘å˜é‡å˜æˆæœˆåº¦æ•°æ®"""
        for key, value in self.barras.items():
            self.barras[key] = self.daily_to_monthly(value)

    def wide_to_long(self, df, name):
        """å°†å®½æ•°æ®å˜æˆé•¿æ•°æ®ï¼Œä¾¿äºåç»­æ‹¼æ¥"""
        df = df.stack().reset_index()
        df.columns = ["date", "code", name]
        df = df.set_index(["date", "code"])
        return df

    def get_wide_barras_industrys(self):
        """å°†barraå› å­å’Œè¡Œä¸šå“‘å˜é‡éƒ½å˜æˆé•¿æ•°æ®"""
        for key, value in self.barras.items():
            self.barras[key] = self.wide_to_long(value, key)

    def get_corr_pri_ols_pri(self):
        """æ‹¼æ¥barraå› å­å’Œè¡Œä¸šå“‘å˜é‡ï¼Œç”Ÿæˆç”¨äºæ±‚ç›¸å…³ç³»æ•°å’Œçº¯å‡€å› å­çš„æ•°æ®è¡¨"""
        if self.factors.shape[0] > 1:
            self.factors = self.factors.set_index(["date", "code"])
        self.corr_pri = pd.concat(
            [self.factors] + list(self.barras.values()), axis=1
        ).dropna()

    # DONE: ä¿®æ”¹é£æ ¼å› å­å±•ç¤ºé¡ºåºè‡³æŠ¥å‘Šçš„é¡ºåº
    def get_corr(self):
        """è®¡ç®—æ¯ä¸€æœŸçš„ç›¸å…³ç³»æ•°ï¼Œå†æ±‚å¹³å‡å€¼"""
        self.corr_by_step = self.corr_pri.groupby(["date"]).apply(
            lambda x: x.corr().head(1)
        )
        self.__corr = self.corr_by_step.mean()
        self.__corr = self.__corr.rename(index=self.rename_dict)
        self.__corr = self.__corr.to_frame("ç›¸å…³ç³»æ•°").T

        self.__corr = self.__corr[self.sort_names]
        self.__corr = self.__corr.T

    @property
    def corr(self) -> pd.DataFrame:
        """å› å­å’Œ10ä¸ªå¸¸ç”¨é£æ ¼å› å­çš„ç›¸å…³ç³»æ•°

        Returns
        -------
        pd.DataFrame
            å› å­å’Œ10ä¸ªå¸¸ç”¨é£æ ¼å› å­çš„ç›¸å…³ç³»æ•°
        """
        return self.__corr.copy()

    def ols_in_group(self, df):
        """å¯¹æ¯ä¸ªæ—¶é—´æ®µè¿›è¡Œå›å½’ï¼Œå¹¶è®¡ç®—æ®‹å·®"""
        xs = list(df.columns)
        xs = [i for i in xs if i != "fac"]
        xs_join = "+".join(xs)
        ols_formula = "fac~" + xs_join
        ols_result = smf.ols(ols_formula, data=df).fit()
        ols_ws = {i: ols_result.params[i] for i in xs}
        ols_b = ols_result.params["Intercept"]
        to_minus = [ols_ws[i] * df[i] for i in xs]
        to_minus = reduce(lambda x, y: x + y, to_minus)
        df = df.assign(snow_fac=df.fac - to_minus - ols_b)
        df = df[["snow_fac"]]
        df = df.rename(columns={"snow_fac": "fac"})
        return df

    def get_snow_fac(self):
        """è·å¾—çº¯å‡€å› å­"""
        self.snow_fac = self.corr_pri.groupby(["date"]).apply(self.ols_in_group)
        self.snow_fac = self.snow_fac.unstack()
        self.snow_fac.columns = list(map(lambda x: x[1], list(self.snow_fac.columns)))

    def run(self):
        """è¿è¡Œä¸€äº›å¿…è¦çš„å‡½æ•°"""
        self.get_monthly_barras_industrys()
        self.get_wide_barras_industrys()
        self.get_corr_pri_ols_pri()
        self.get_corr()
        self.get_snow_fac()


class pure_snowtrain(object):
    """ç›´æ¥è¿”å›çº¯å‡€å› å­"""

    def __init__(
        self,
        factors: pd.DataFrame,
        facs_dict: dict = None,
        momentum: bool = 1,
        earningsyield: bool = 1,
        growth: bool = 1,
        liquidity: bool = 1,
        size: bool = 1,
        leverage: bool = 1,
        beta: bool = 1,
        nonlinearsize: bool = 1,
        residualvolatility: bool = 1,
        booktoprice: bool = 1,
    ) -> None:
        """è®¡ç®—å› å­å€¼ä¸10ç§å¸¸ç”¨é£æ ¼å› å­ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶è¿›è¡Œçº¯å‡€åŒ–ï¼Œå¯ä»¥é¢å¤–åŠ å…¥å…¶ä»–å› å­

        Parameters
        ----------
        factors : pd.DataFrame
            è¦è€ƒå¯Ÿçš„å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        facs_dict : dict, optional
            _description_, by default None
        momentum : bool, optional
            _description_, by default 1
        earningsyield : bool, optional
            _description_, by default 1
        growth : bool, optional
            _description_, by default 1
        liquidity : bool, optional
            _description_, by default 1
        size : bool, optional
            _description_, by default 1
        leverage : bool, optional
            _description_, by default 1
        beta : bool, optional
            _description_, by default 1
        nonlinearsize : bool, optional
            _description_, by default 1
        residualvolatility : bool, optional
            _description_, by default 1
        booktoprice : bool, optional
            _description_, by default 1
        """
        self.winter = pure_coldwinter(
            facs_dict=facs_dict,
            momentum=momentum,
            earningsyield=earningsyield,
            growth=growth,
            liquidity=liquidity,
            size=size,
            leverage=leverage,
            beta=beta,
            nonlinearsize=nonlinearsize,
            residualvolatility=residualvolatility,
            booktoprice=booktoprice,
        )
        self.winter.set_factors_df_wide(factors.copy())
        self.winter.run()
        self.corr = self.winter.corr

    def __call__(self) -> pd.DataFrame:
        """è·å¾—çº¯å‡€åŒ–ä¹‹åçš„å› å­å€¼

        Returns
        -------
        pd.DataFrame
            çº¯å‡€åŒ–ä¹‹åçš„å› å­å€¼
        """
        return self.winter.snow_fac.copy()


class pure_newyear(object):
    """è½¬ä¸ºç”Ÿæˆ25åˆ†ç»„å’Œç™¾åˆ†ç»„çš„æ”¶ç›ŠçŸ©é˜µè€Œå°è£…"""

    def __init__(
        self,
        facx: pd.DataFrame,
        facy: pd.DataFrame,
        group_num_single: int,
        namex: str = "ä¸»",
        namey: str = "æ¬¡",
    ) -> None:
        """æ¡ä»¶åŒå˜é‡æ’åºæ³•ï¼Œå…ˆå¯¹æ‰€æœ‰è‚¡ç¥¨ï¼Œä¾ç…§å› å­facxè¿›è¡Œæ’åº
        ç„¶ååœ¨æ¯ä¸ªç»„å†…ï¼Œä¾ç…§facyè¿›è¡Œæ’åºï¼Œæœ€åç»Ÿè®¡å„ä¸ªç»„å†…çš„å¹³å‡æ”¶ç›Šç‡

        Parameters
        ----------
        facx : pd.DataFrame
            é¦–å…ˆè¿›è¡Œæ’åºçš„å› å­ï¼Œé€šå¸¸ä¸ºæ§åˆ¶å˜é‡ï¼Œç›¸å½“äºæ­£äº¤åŒ–ä¸­çš„è‡ªå˜é‡
            indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        facy : pd.DataFrame
            åœ¨facxçš„å„ä¸ªç»„å†…ï¼Œä¾ç…§facyè¿›è¡Œæ’åºï¼Œä¸ºä¸»è¦è¦ç ”ç©¶çš„å› å­
            indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        group_num_single : int
            å•ä¸ªå› å­åˆ†æˆå‡ ç»„ï¼Œé€šå¸¸ä¸º5æˆ–10
        namex : str, optional
            facxè¿™ä¸€å› å­çš„åå­—, by default "ä¸»"
        namey : str, optional
            facyè¿™ä¸€å› å­çš„åå­—, by default "æ¬¡"
        """
        homex = pure_fallmount(facx)
        homey = pure_fallmount(facy)
        if group_num_single == 5:
            homexy = homex > homey
        elif group_num_single == 10:
            homexy = homex >> homey
        shen = pure_moonnight(
            homexy(), group_num_single**2, plt_plot=False, print_comments=False
        )
        sq = shen.shen.square_rets.copy()
        sq.index = [namex + str(i) for i in list(sq.index)]
        sq.columns = [namey + str(i) for i in list(sq.columns)]
        self.square_rets = sq

    def __call__(self) -> pd.DataFrame:
        """è°ƒç”¨å¯¹è±¡æ—¶ï¼Œè¿”å›æœ€ç»ˆç»“æœï¼Œæ­£æ–¹å½¢çš„åˆ†ç»„å¹´åŒ–æ”¶ç›Šç‡è¡¨

        Returns
        -------
        `pd.DataFrame`
            æ¯ä¸ªç»„çš„å¹´åŒ–æ”¶ç›Šç‡
        """
        return self.square_rets.copy()


class pure_dawn(object):
    """
    å› å­åˆ‡å‰²è®ºçš„æ¯æ¡†æ¶ï¼Œå¯ä»¥å¯¹ä¸¤ä¸ªå› å­è¿›è¡Œç±»ä¼¼äºå› å­åˆ‡å‰²çš„æ“ä½œ
    å¯ç”¨äºæ´¾ç”Ÿä»»ä½•"ä»¥ä¸¤ä¸ªå› å­ç”Ÿæˆä¸€ä¸ªå› å­"çš„å­ç±»
    ä½¿ç”¨ä¸¾ä¾‹
    cutå‡½æ•°é‡Œï¼Œå¿…é¡»å¸¦æœ‰è¾“å…¥å˜é‡df,dfæœ‰ä¸¤ä¸ªcolumnsï¼Œä¸€ä¸ªåä¸º'fac1'ï¼Œä¸€ä¸ªåä¸º'fac2'ï¼Œdfæ˜¯æœ€è¿‘ä¸€ä¸ªå›çœ‹æœŸå†…çš„æ•°æ®
    ```python
    class Cut(pure_dawn):

    def cut(self,df):
        df=df.sort_values('fac1')
        df=df.assign(fac3=df.fac1*df.fac2)
        ret0=df.fac2.iloc[:4].mean()
        ret1=df.fac2.iloc[4:8].mean()
        ret2=df.fac2.iloc[8:12].mean()
        ret3=df.fac2.iloc[12:16].mean()
        ret4=df.fac2.iloc[16:].mean()
        aret0=df.fac3.iloc[:4].mean()
        aret1=df.fac3.iloc[4:8].mean()
        aret2=df.fac3.iloc[8:12].mean()
        aret3=df.fac3.iloc[12:16].mean()
        aret4=df.fac3.iloc[16:].mean()
        return ret0,ret1,ret2,ret3,ret4,aret0,aret1,aret2,aret3,aret4

    cut=Cut(ct,ret_inday)
    cut.run(cut.cut)

    cut0=get_value(cut(),0)
    cut1=get_value(cut(),1)
    cut2=get_value(cut(),2)
    cut3=get_value(cut(),3)
    cut4=get_value(cut(),4)
    ```
    """

    def __init__(self, fac1: pd.DataFrame, fac2: pd.DataFrame, *args: list) -> None:
        """å‡ ä¸ªå› å­çš„æ“ä½œï¼Œæ¯ä¸ªæœˆæ“ä½œä¸€æ¬¡

        Parameters
        ----------
        fac1 : pd.DataFrame
            å› å­å€¼1ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        fac2 : pd.DataFrame
            å› å­2ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        """
        self.fac1 = fac1
        self.fac1 = self.fac1.stack().reset_index()
        self.fac1.columns = ["date", "code", "fac1"]
        self.fac2 = fac2
        self.fac2 = self.fac2.stack().reset_index()
        self.fac2.columns = ["date", "code", "fac2"]
        fac_all = pd.merge(self.fac1, self.fac2, on=["date", "code"])
        for i, fac in enumerate(args):
            fac = fac.stack().reset_index()
            fac.columns = ["date", "code", f"fac{i+3}"]
            fac_all = pd.merge(fac_all, fac, on=["date", "code"])
        fac_all = fac_all.sort_values(["date", "code"])
        self.fac = fac_all.copy()

    def __call__(self) -> pd.DataFrame:
        """è¿”å›æœ€ç»ˆæœˆåº¦å› å­å€¼

        Returns
        -------
        `pd.DataFrame`
            æœ€ç»ˆå› å­å€¼
        """
        return self.fac.copy()

    def get_fac_long_and_tradedays(self):
        """å°†ä¸¤ä¸ªå› å­çš„çŸ©é˜µè½¬åŒ–ä¸ºé•¿åˆ—è¡¨"""
        self.tradedays = sorted(list(set(self.fac.date)))

    def get_month_starts_and_ends(self, backsee=20):
        """è®¡ç®—å‡ºæ¯ä¸ªæœˆå›çœ‹æœŸé—´çš„èµ·ç‚¹æ—¥å’Œç»ˆç‚¹æ—¥"""
        self.month_ends = [
            i
            for i, j in zip(self.tradedays[:-1], self.tradedays[1:])
            if i.month != j.month
        ]
        self.month_ends.append(self.tradedays[-1])
        self.month_starts = [
            self.find_begin(self.tradedays, i, backsee=backsee) for i in self.month_ends
        ]
        self.month_starts[0] = self.tradedays[0]

    def find_begin(self, tradedays, end_day, backsee=20):
        """æ‰¾å‡ºå›çœ‹è‹¥å¹²å¤©çš„å¼€å§‹æ—¥ï¼Œé»˜è®¤ä¸º20"""
        end_day_index = tradedays.index(end_day)
        start_day_index = end_day_index - backsee + 1
        start_day = tradedays[start_day_index]
        return start_day

    def make_monthly_factors_single_code(self, df, func):
        """
        å¯¹å•ä¸€è‚¡ç¥¨æ¥è®¡ç®—æœˆåº¦å› å­
        funcä¸ºå•æœˆæ‰§è¡Œçš„å‡½æ•°ï¼Œè¿”å›å€¼åº”ä¸ºæœˆåº¦å› å­ï¼Œå¦‚ä¸€ä¸ªfloatæˆ–ä¸€ä¸ªlist
        dfä¸ºä¸€ä¸ªè‚¡ç¥¨çš„å››åˆ—è¡¨ï¼ŒåŒ…å«æ—¶é—´ã€ä»£ç ã€å› å­1å’Œå› å­2
        """
        res = {}
        for start, end in zip(self.month_starts, self.month_ends):
            this_month = df[(df.date >= start) & (df.date <= end)]
            res[end] = func(this_month)
        dates = list(res.keys())
        corrs = list(res.values())
        part = pd.DataFrame({"date": dates, "corr": corrs})
        return part

    def get_monthly_factor(self, func):
        """è¿è¡Œè‡ªå·±å†™çš„å‡½æ•°ï¼Œè·å¾—æœˆåº¦å› å­"""
        if is_notebook():
            tqdm.tqdm_notebook().pandas()
        else:
            tqdm.tqdm.pandas(desc="when the dawn comes, tonight will be a memory too.")
        self.fac = self.fac.groupby(["code"]).progress_apply(
            lambda x: self.make_monthly_factors_single_code(x, func)
        )
        self.fac = (
            self.fac.reset_index(level=1, drop=True)
            .reset_index()
            .set_index(["date", "code"])
            .unstack()
        )
        self.fac.columns = [i[1] for i in list(self.fac.columns)]
        self.fac = self.fac.resample("M").last()

    @kk.desktop_sender(title="å˜¿ï¼Œåˆ‡å‰²å®Œæˆå•¦ğŸ›")
    def run(self, func: Callable, backsee: int = 20) -> None:
        """æ‰§è¡Œè®¡ç®—çš„æ¡†æ¶ï¼Œäº§ç”Ÿå› å­å€¼

        Parameters
        ----------
        func : Callable
            æ¯ä¸ªæœˆè¦è¿›è¡Œçš„æ“ä½œ
        backsee : int, optional
            å›çœ‹æœŸï¼Œå³æ¯ä¸ªæœˆæœˆåº•å¯¹è¿‡å»å¤šå°‘å¤©è¿›è¡Œè®¡ç®—, by default 20
        """
        self.get_fac_long_and_tradedays()
        self.get_month_starts_and_ends(backsee=backsee)
        self.get_monthly_factor(func)


def follow_tests(
    fac: pd.DataFrame,
    comments_writer: pd.ExcelWriter = None,
    net_values_writer: pd.ExcelWriter = None,
    pos: bool = 0,
    neg: bool = 0,
    swindustry: bool = 0,
    zxindustry: bool = 0,
    nums: list[int] = [3],
):
    """å› å­å®Œæˆå…¨Aæµ‹è¯•åï¼Œè¿›è¡Œçš„ä¸€äº›å¿…è¦çš„åç»­æµ‹è¯•ï¼ŒåŒ…æ‹¬å„ä¸ªåˆ†ç»„è¡¨ç°ã€ç›¸å…³ç³»æ•°ä¸çº¯å‡€åŒ–ã€3510çš„å¤šç©ºå’Œå¤šå¤´ã€å„ä¸ªè¡Œä¸šRank ICã€å„ä¸ªè¡Œä¸šä¹°3åªè¶…é¢è¡¨ç°

    Parameters
    ----------
    fac : pd.DataFrame
        è¦è¿›è¡Œåç»­æµ‹è¯•çš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯å› å­å€¼
    comments_writer : pd.ExcelWriter, optional
        å†™å…¥è¯„ä»·æŒ‡æ ‡çš„excel, by default None
    net_values_writer : pd.ExcelWriter, optional
        å†™å…¥å‡€å€¼åºåˆ—çš„excel, by default None
    pos : bool, optional
        å› å­çš„æ–¹å‘ä¸ºæ­£, by default 0
    neg : bool, optional
        å› å­çš„æ–¹å‘ä¸ºè´Ÿ, by default 0
    swindustry : bool, optional
        ä½¿ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
    zxindustry : bool, optional
        ä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0
    nums : list[int], optional
        å„ä¸ªè¡Œä¸šä¹°å‡ åªè‚¡ç¥¨, by default [3]

    Raises
    ------
    IOError
        å¦‚æœæœªæŒ‡å®šå› å­æ­£è´Ÿæ–¹å‘ï¼Œå°†æŠ¥é”™
    """
    if comments_writer is None:
        from pure_ocean_breeze.legacy_version.v3p4.state.states import COMMENTS_WRITER

        comments_writer = COMMENTS_WRITER
    if net_values_writer is None:
        from pure_ocean_breeze.legacy_version.v3p4.state.states import NET_VALUES_WRITER

        net_values_writer = NET_VALUES_WRITER

    shen = pure_moonnight(fac)
    shen.comments_ten().to_excel(comments_writer, sheet_name="ååˆ†ç»„")
    """ç›¸å…³ç³»æ•°ä¸çº¯å‡€åŒ–"""
    pure_fac = pure_snowtrain(fac)
    pure_fac.corr.to_excel(comments_writer, sheet_name="ç›¸å…³ç³»æ•°")
    shen = pure_moonnight(
        pure_fac(),
        comments_writer=comments_writer,
        net_values_writer=net_values_writer,
        sheetname="çº¯å‡€",
    )
    """3510å¤šç©ºå’Œå¤šå¤´"""
    # 300
    fi300 = daily_factor_on300500(fac, hs300=1)
    shen = pure_moonnight(
        fi300,
        comments_writer=comments_writer,
        net_values_writer=net_values_writer,
        sheetname="300å¤šç©º",
    )
    if pos:
        make_relative_comments(shen.shen.group_rets.group10, hs300=1).to_excel(
            comments_writer, sheet_name="300è¶…é¢"
        )
        make_relative_comments_plot(shen.shen.group_rets.group10, hs300=1).to_excel(
            net_values_writer, sheet_name="300è¶…é¢"
        )
    elif neg:
        make_relative_comments(shen.shen.group_rets.group1, hs300=1).to_excel(
            comments_writer, sheet_name="300è¶…é¢"
        )
        make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1).to_excel(
            net_values_writer, sheet_name="300è¶…é¢"
        )
    else:
        raise IOError("è¯·æŒ‡å®šå› å­çš„æ–¹å‘æ˜¯æ­£æ˜¯è´ŸğŸ¤’")
    # 500
    fi500 = daily_factor_on300500(fac, zz500=1)
    shen = pure_moonnight(
        fi500,
        comments_writer=comments_writer,
        net_values_writer=net_values_writer,
        sheetname="500å¤šç©º",
    )
    if pos:
        make_relative_comments(shen.shen.group_rets.group10, zz500=1).to_excel(
            comments_writer, sheet_name="500è¶…é¢"
        )
        make_relative_comments_plot(shen.shen.group_rets.group10, zz500=1).to_excel(
            net_values_writer, sheet_name="500è¶…é¢"
        )
    else:
        make_relative_comments(shen.shen.group_rets.group1, zz500=1).to_excel(
            comments_writer, sheet_name="500è¶…é¢"
        )
        make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1).to_excel(
            net_values_writer, sheet_name="500è¶…é¢"
        )
    # 1000
    fi1000 = daily_factor_on300500(fac, zz1000=1)
    shen = pure_moonnight(
        fi1000,
        comments_writer=comments_writer,
        net_values_writer=net_values_writer,
        sheetname="1000å¤šç©º",
    )
    if pos:
        make_relative_comments(shen.shen.group_rets.group10, zz1000=1).to_excel(
            comments_writer, sheet_name="1000è¶…é¢"
        )
        make_relative_comments_plot(shen.shen.group_rets.group10, zz1000=1).to_excel(
            net_values_writer, sheet_name="1000è¶…é¢"
        )
    else:
        make_relative_comments(shen.shen.group_rets.group1, zz1000=1).to_excel(
            comments_writer, sheet_name="1000è¶…é¢"
        )
        make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1).to_excel(
            net_values_writer, sheet_name="1000è¶…é¢"
        )
    # å„è¡Œä¸šRank IC
    rankics = rankic_test_on_industry(fac, comments_writer)
    # ä¹°3åªè¶…é¢è¡¨ç°
    rets = long_test_on_industry(
        fac, nums, pos=pos, neg=neg, swindustry=swindustry, zxindustry=zxindustry
    )
    logger.success("å› å­åç»­çš„å¿…è¦æµ‹è¯•å…¨éƒ¨å®Œæˆ")


class pure_helper(object):
    def __init__(
        self,
        df_main: pd.DataFrame,
        df_helper: pd.DataFrame,
        func: Callable = None,
        group: int = 10,
    ) -> None:
        """ä½¿ç”¨å› å­bçš„å€¼å¤§å°ï¼Œå¯¹å› å­aè¿›è¡Œåˆ†ç»„ï¼Œå¹¶å¯ä»¥åœ¨ç»„å†…è¿›è¡ŒæŸç§æ“ä½œ

        Parameters
        ----------
        df_main : pd.DataFrame
            è¦è¢«åˆ†ç»„å¹¶è¿›è¡Œæ“ä½œçš„å› å­
        df_helper : pd.DataFrame
            ç”¨æ¥åšåˆ†ç»„çš„ä¾æ®
        func : Callable, optional
            åˆ†ç»„åï¼Œç»„å†…è¦è¿›è¡Œçš„æ“ä½œ, by default None
        group : int, optional
            è¦åˆ†çš„ç»„æ•°, by default 10
        """
        self.df_main = df_main
        self.df_helper = df_helper
        self.func = func
        self.group = group
        if self.func is None:
            self.__data = self.sort_a_with_b()
        else:
            self.__data = self.sort_a_with_b_func()

    @property
    def data(self):
        return self.__data

    def __call__(self) -> pd.DataFrame:
        return self.data

    def sort_a_with_b(self):
        dfb = to_group(self.df_helper, group=self.group)
        dfb = dfb.stack().reset_index()
        dfb.columns = ["date", "code", "group"]
        dfa = self.df_main.stack().reset_index()
        dfa.columns = ["date", "code", "target"]
        df = pd.merge(dfa, dfb, on=["date", "code"])
        return df

    def sort_a_with_b_func(self):
        the_func = partial(self.func)
        df = self.sort_a_with_b().drop(columns=["code"])
        df = (
            df.groupby(["date", "group"])
            .apply(the_func)
            .drop(columns=["group"])
            .reset_index()
        )
        df = df.pivot(index="date", columns="group", values="target")
        df.columns = [f"group{str(int(i+1))}" for i in list(df.columns)]
        return df


class pure_fama(object):
    # @lru_cache(maxsize=None)
    def __init__(
        self,
        factors: list[pd.DataFrame],
        minus_group: Union[list, float] = 3,
        backsee: int = 20,
        rets: pd.DataFrame = None,
        value_weighted: bool = 1,
        add_market: bool = 1,
        add_market_series: pd.Series = None,
        factors_names: list = None,
        betas_rets: bool = 0,
    ) -> None:
        """ä½¿ç”¨famaä¸‰å› å­çš„æ–¹æ³•ï¼Œå°†ä¸ªè‚¡çš„æ”¶ç›Šç‡ï¼Œæ‹†åˆ†å‡ºå„ä¸ªå› å­å¸¦æ¥çš„æ”¶ç›Šç‡ä»¥åŠç‰¹è´¨çš„æ”¶ç›Šç‡
        åˆ†åˆ«è®¡ç®—æ¯ä¸€æœŸï¼Œå„ä¸ªå› å­æ”¶ç›Šç‡çš„å€¼ï¼Œè¶…é¢æ”¶ç›Šç‡ï¼Œå› å­çš„æš´éœ²ï¼Œä»¥åŠç‰¹è´¨æ”¶ç›Šç‡

        Parameters
        ----------
        factors : list[pd.DataFrame]
            ç”¨äºè§£é‡Šæ”¶ç›Šçš„å„ä¸ªå› å­å€¼ï¼Œæ¯ä¸€ä¸ªéƒ½æ˜¯indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼çš„dataframe
        minus_group : Union[list, float], optional
            æ¯ä¸€ä¸ªå› å­å°†æˆªé¢ä¸Šçš„è‚¡ç¥¨åˆ†ä¸ºå‡ ç»„, by default 3
        backsee : int, optional
            åšæ—¶åºå›å½’æ—¶ï¼Œå›çœ‹çš„å¤©æ•°, by default 20
        rets : pd.DataFrame, optional
            æ¯åªä¸ªè‚¡çš„æ”¶ç›Šç‡ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºæ”¶ç›Šç‡ï¼Œé»˜è®¤ä½¿ç”¨å½“æ—¥æ—¥é—´æ”¶ç›Šç‡, by default None
        value_weighted : bool, optional
            æ˜¯å¦ä½¿ç”¨æµé€šå¸‚å€¼åŠ æƒ, by default 1
        add_market : bool, optional
            æ˜¯å¦åŠ å…¥å¸‚åœºæ”¶ç›Šç‡å› å­ï¼Œé»˜è®¤ä½¿ç”¨ä¸­è¯å…¨æŒ‡çš„æ¯æ—¥æ—¥é—´æ”¶ç›Šç‡, by default 1
        add_market_series : bool, optional
            åŠ å…¥çš„å¸‚åœºæ”¶ç›Šç‡çš„æ•°æ®ï¼Œå¦‚æœæ²¡æŒ‡å®šï¼Œåˆ™ä½¿ç”¨ä¸­è¯å…¨æŒ‡çš„æ—¥é—´æ”¶ç›Šç‡, by default None
        factors_names : list, optional
            å„ä¸ªå› å­çš„åå­—ï¼Œé»˜è®¤ä¸ºfac0(å¸‚åœºæ”¶ç›Šç‡å› å­ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»fac1å¼€å§‹),fac1,fac2,fac3, by default None
        betas_rets : bool, optional
            æ˜¯å¦è®¡ç®—æ¯åªä¸ªè‚¡çš„ç”±äºæš´éœ²åœ¨æ¯ä¸ªå› å­ä¸Šæ‰€å¸¦æ¥çš„æ”¶ç›Šç‡, by default 0
        """
        start = max(
            [int(datetime.datetime.strftime(i.index.min(), "%Y%m%d")) for i in factors]
        )
        self.backsee = backsee
        self.factors = factors
        self.factors_names = factors_names
        if isinstance(minus_group, int):
            minus_group = [minus_group] * len(factors)
        self.minus_group = minus_group
        if rets is None:
            closes = read_daily(close=1, start=start)
            rets = closes / closes.shift(1) - 1
        self.rets = rets
        self.factors_group = [
            to_group(i, group=j) for i, j in zip(self.factors, self.minus_group)
        ]
        self.factors_group_long = [(i == 0) + 0 for i in self.factors_group]
        self.factors_group_short = [
            (i == (j - 1)) + 0 for i, j in zip(self.factors_group, self.minus_group)
        ]
        self.value_weighted = value_weighted
        if value_weighted:
            self.cap = read_daily(flow_cap=1, start=start)
            self.factors_group_long = [self.cap * i for i in self.factors_group_long]
            self.factors_group_short = [self.cap * i for i in self.factors_group_short]
            self.factors_group_long = [
                (i.T / i.T.sum()).T for i in self.factors_group_long
            ]
            self.factors_group_short = [
                (i.T / i.T.sum()).T for i in self.factors_group_short
            ]
            self.factors_rets_long = [
                (self.rets * i).sum(axis=1).to_frame(f"fac{num+1}")
                for num, i in enumerate(self.factors_group_long)
            ]
            self.factors_rets_short = [
                (self.rets * i).sum(axis=1).to_frame(f"fac{num+1}")
                for num, i in enumerate(self.factors_group_short)
            ]
        else:
            self.factors_rets_long = [
                (self.rets * i).mean(axis=1).to_frame(f"fac{num+1}")
                for num, i in enumerate(self.factors_group_long)
            ]
            self.factors_rets_short = [
                (self.rets * i).mean(axis=1).to_frame(f"fac{num+1}")
                for num, i in enumerate(self.factors_group_short)
            ]
        self.rets_long = pd.concat(self.factors_rets_long, axis=1)
        self.rets_short = pd.concat(self.factors_rets_short, axis=1)
        self.__factors_rets = self.rets_long - self.rets_short
        if add_market_series is not None:
            add_market = 1
        self.add_market = add_market
        if add_market:
            if add_market_series is None:
                closes = read_market(close=1, every_stock=0, start=start).to_frame(
                    "fac0"
                )
            else:
                closes = add_market_series.to_frame("fac0")
            rets = closes / closes.shift(1) - 1
            self.__factors_rets = pd.concat([rets, self.__factors_rets], axis=1)
            if factors_names is not None:
                factors_names = ["å¸‚åœº"] + factors_names
        self.__data = self.make_df(self.rets, self.__factors_rets)
        if is_notebook():
            tqdm.tqdm_notebook().pandas()
        else:
            tqdm.tqdm.pandas()
        self.__coefficients = (
            self.__data.groupby("code").progress_apply(self.ols_in).reset_index()
        )
        self.__coefficients = self.__coefficients.rename(
            columns={
                i: "co" + i for i in list(self.__coefficients.columns) if "fac" in i
            }
        )
        self.__data = pd.merge(
            self.__data.reset_index(), self.__coefficients, on=["date", "code"]
        )
        betas = [
            self.__data[i] * self.__data["co" + i]
            for i in list(self.__data.columns)
            if i.startswith("fac")
        ]
        betas = sum(betas)
        self.__data = self.__data.assign(
            idiosyncratic=self.__data.ret - self.__data.intercept - betas
        )
        self.__idiosyncratic = self.__data.pivot(
            index="date", columns="code", values="idiosyncratic"
        )
        self.__alphas = self.__data.pivot(
            index="date", columns="code", values="intercept"
        )
        if factors_names is None:
            self.__betas = {
                i: self.__data.pivot(index="date", columns="code", values=i)
                for i in list(self.__data.columns)
                if i.startswith("fac")
            }
        else:
            facs = [i for i in list(self.__data.columns) if i.startswith("fac")]
            self.__betas = {
                factors_names[num]: self.__data.pivot(
                    index="date", columns="code", values=i
                )
                for num, i in enumerate(facs)
            }
        if betas_rets:
            if add_market:
                if add_market_series is None:
                    factors = [read_market(close=1, start=start)] + factors
                else:
                    factors = [
                        pd.DataFrame(
                            {k: add_market_series for k in list(factors[0].columns)},
                            index=factors[0].index,
                        )
                    ] + factors
            self.__betas_rets = {
                d1[0]: d1[1] * d2 for d1, d2 in zip(self.__betas, factors)
            }
        else:
            self.__betas_rets = "æ‚¨å¦‚æœæƒ³è®¡ç®—å„ä¸ªè‚¡ç¥¨åœ¨å„ä¸ªå› å­çš„æ”¶ç›Šç‡ï¼Œè¯·å…ˆæŒ‡å®šbetas_retså‚æ•°ä¸ºTrue"

    @property
    def idiosyncratic(self):
        return self.__idiosyncratic

    @property
    def data(self):
        return self.__data

    @property
    def alphas(self):
        return self.__alphas

    @property
    def betas(self):
        return self.__betas

    @property
    def betas_rets(self):
        return self.__betas_rets

    @property
    def factors_rets(self):
        return self.__factors_rets

    @property
    def coefficients(self):
        return self.__coefficients

    def __call__(self):
        return self.idiosyncratic

    def make_df(self, rets, facs):
        rets = rets.stack().reset_index()
        rets.columns = ["date", "code", "ret"]
        facs = facs.reset_index()
        facs.columns = ["date"] + list(facs.columns)[1:]
        df = pd.merge(rets, facs, on=["date"])
        df = df.set_index("date")
        return df

    def ols_in(self, df):
        try:
            if self.add_market:
                x = df[["fac0"] + [f"fac{i+1}" for i in range(len(self.factors))]]
            else:
                x = df[[f"fac{i+1}" for i in range(len(self.factors))]]
            ols = po.PandasRollingOLS(
                y=df[["ret"]],
                x=x,
                window=self.backsee,
            )
            betas = ols.beta
            alpha = ols.alpha
            return pd.concat([alpha, betas], axis=1)
        except Exception:
            # æœ‰äº›æ•°æ®æ€»å…±ä¸è¶³ï¼Œé‚£å°±è·³è¿‡
            ...


class pure_rollingols(object):
    def __init__(
        self,
        y: pd.DataFrame,
        xs: Union[list[pd.DataFrame], pd.DataFrame],
        backsee: int = 20,
        factors_names: list[str] = None,
    ) -> None:
        """ä½¿ç”¨è‹¥å¹²ä¸ªdataframeï¼Œå¯¹åº”çš„è‚¡ç¥¨è¿›è¡ŒæŒ‡å®šçª—å£çš„æ—¶åºæ»šåŠ¨å›å½’

        Parameters
        ----------
        y : pd.DataFrame
            æ»šåŠ¨å›å½’ä¸­çš„å› å˜é‡yï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
        xs : Union[list[pd.DataFrame], pd.DataFrame]
            æ»šåŠ¨å›å½’ä¸­çš„è‡ªå˜é‡xiï¼Œæ¯ä¸€ä¸ªdataframeï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
        backsee : int, optional
            æ»šåŠ¨å›å½’çš„æ—¶é—´çª—å£, by default 20
        factors_names : list[str], optional
            xsä¸­ï¼Œæ¯ä¸ªå› å­çš„åå­—, by default None
        """
        self.backsee = backsee
        self.y = y
        if not isinstance(xs, list):
            xs = [xs]
        self.xs = xs
        y = y.stack().reset_index()
        xs = [i.stack().reset_index() for i in xs]
        y.columns = ["date", "code", "y"]
        xs = [
            i.rename(
                columns={list(i.columns)[1]: "code", list(i.columns)[2]: f"x{j+1}"}
            )
            for j, i in enumerate(xs)
        ]
        xs = [y] + xs
        xs = reduce(lambda x, y: pd.merge(x, y, on=["date", "code"]), xs)
        xs = xs.set_index("date")
        self.__data = xs
        self.haha = xs
        if is_notebook():
            tqdm.tqdm_notebook().pandas()
        else:
            tqdm.tqdm.pandas()
        self.__coefficients = (
            self.__data.groupby("code").progress_apply(self.ols_in).reset_index()
        )
        self.__coefficients = self.__coefficients.rename(
            columns={i: "co" + i for i in list(self.__coefficients.columns) if "x" in i}
        )
        self.__data = pd.merge(
            self.__data.reset_index(), self.__coefficients, on=["date", "code"]
        )
        betas = [
            self.__data[i] * self.__data["co" + i]
            for i in list(self.__data.columns)
            if i.startswith("x")
        ]
        betas = sum(betas)
        self.__data = self.__data.assign(
            residual=self.__data.y - self.__data.intercept - betas
        )
        self.__residual = self.__data.pivot(
            index="date", columns="code", values="residual"
        )
        self.__alphas = self.__data.pivot(
            index="date", columns="code", values="intercept"
        )
        if factors_names is None:
            self.__betas = {
                i: self.__data.pivot(index="date", columns="code", values=i)
                for i in list(self.__data.columns)
                if i.startswith("x")
            }
        else:
            facs = [i for i in list(self.__data.columns) if i.startswith("x")]
            self.__betas = {
                factors_names[num]: self.__data.pivot(
                    index="date", columns="code", values=i
                )
                for num, i in enumerate(facs)
            }
        if len(list(self.__betas)) == 1:
            self.__betas = list(self.__betas.values())[0]

    @property
    def residual(self):
        return self.__residual

    @property
    def data(self):
        return self.__data

    @property
    def alphas(self):
        return self.__alphas

    @property
    def betas(self):
        return self.__betas

    @property
    def coefficients(self):
        return self.__coefficients

    def ols_in(self, df):
        try:
            ols = po.PandasRollingOLS(
                y=df[["y"]],
                x=df[[f"x{i+1}" for i in range(len(self.xs))]],
                window=self.backsee,
            )
            betas = ols.beta
            alpha = ols.alpha
            return pd.concat([alpha, betas], axis=1)

        except Exception:
            # æœ‰äº›æ•°æ®æ€»å…±ä¸è¶³ï¼Œé‚£å°±è·³è¿‡
            ...


def test_on_300500(
    df: pd.DataFrame,
    hs300: bool = 0,
    zz500: bool = 0,
    zz1000: bool = 0,
    gz2000: bool = 0,
    iplot: bool = 1,
) -> pd.Series:
    """å¯¹å› å­åœ¨æŒ‡æ•°æˆåˆ†è‚¡å†…è¿›è¡Œå¤šç©ºå’Œå¤šå¤´æµ‹è¯•

    Parameters
    ----------
    df : pd.DataFrame
        å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    hs300 : bool, optional
        åœ¨æ²ªæ·±300æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
    zz500 : bool, optional
        åœ¨ä¸­è¯500æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
    zz1000 : bool, optional
        åœ¨ä¸­è¯1000æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
    gz1000 : bool, optional
        åœ¨å›½è¯2000æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
    iplot : bol,optional
        å¤šç©ºå›æµ‹çš„æ—¶å€™ï¼Œæ˜¯å¦ä½¿ç”¨cufflinksç»˜ç”»

    Returns
    -------
    pd.Series
        å¤šå¤´ç»„åœ¨è¯¥æŒ‡æ•°ä¸Šçš„è¶…é¢æ”¶ç›Šåºåˆ—
    """
    fi300 = daily_factor_on300500(
        df, hs300=hs300, zz500=zz500, zz1000=zz1000, gz2000=gz2000
    )
    shen = pure_moonnight(fi300, iplot=iplot)
    if (
        shen.shen.group_net_values.group1.iloc[-1]
        > shen.shen.group_net_values.group10.iloc[-1]
    ):
        print(
            make_relative_comments(
                shen.shen.group_rets.group1,
                hs300=hs300,
                zz500=zz500,
                zz1000=zz1000,
                gz2000=gz2000,
            )
        )
        abrets = make_relative_comments_plot(
            shen.shen.group_rets.group1,
            hs300=hs300,
            zz500=zz500,
            zz1000=zz1000,
            gz2000=gz2000,
        )
        return abrets
    else:
        print(
            make_relative_comments(
                shen.shen.group_rets.group10,
                hs300=hs300,
                zz500=zz500,
                zz1000=zz1000,
                gz2000=gz2000,
            )
        )
        abrets = make_relative_comments_plot(
            shen.shen.group_rets.group10,
            hs300=hs300,
            zz500=zz500,
            zz1000=zz1000,
            gz2000=gz2000,
        )
        return abrets


def test_on_index_four(
    df: pd.DataFrame, iplot: bool = 1, gz2000: bool = 0, boxcox: bool = 1
) -> pd.DataFrame:
    """å¯¹å› å­åŒæ—¶åœ¨æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ã€å›½è¯2000è¿™4ä¸ªæŒ‡æ•°æˆåˆ†è‚¡å†…è¿›è¡Œå¤šç©ºå’Œå¤šå¤´è¶…é¢æµ‹è¯•

    Parameters
    ----------
    df : pd.DataFrame
        å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    iplot : bol,optional
        å¤šç©ºå›æµ‹çš„æ—¶å€™ï¼Œæ˜¯å¦ä½¿ç”¨cufflinksç»˜ç”»
    gz2000 : bool, optional
        æ˜¯å¦è¿›è¡Œå›½è¯2000ä¸Šçš„æµ‹è¯•, by default 0
    boxcox : bool, optional
        æ˜¯å¦è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–å¤„ç†, by default 1

    Returns
    -------
    pd.DataFrame
        å¤šå¤´ç»„åœ¨å„ä¸ªæŒ‡æ•°ä¸Šçš„è¶…é¢æ”¶ç›Šåºåˆ—
    """
    fi300 = daily_factor_on300500(df, hs300=1)
    shen = pure_moonnight(fi300, iplot=iplot, boxcox=boxcox)
    if (
        shen.shen.group_net_values.group1.iloc[-1]
        > shen.shen.group_net_values.group10.iloc[-1]
    ):
        com300, net300 = make_relative_comments(
            shen.shen.group_rets.group1, hs300=1, show_nets=1
        )
        fi500 = daily_factor_on300500(df, zz500=1)
        shen = pure_moonnight(fi500, iplot=iplot, boxcox=boxcox)
        com500, net500 = make_relative_comments(
            shen.shen.group_rets.group1, zz500=1, show_nets=1
        )
        fi1000 = daily_factor_on300500(df, zz1000=1)
        shen = pure_moonnight(fi1000, iplot=iplot, boxcox=boxcox)
        com1000, net1000 = make_relative_comments(
            shen.shen.group_rets.group1, zz1000=1, show_nets=1
        )
        if gz2000:
            fi2000 = daily_factor_on300500(df, gz2000=1)
            shen = pure_moonnight(fi2000, iplot=iplot, boxcox=boxcox)
            com2000, net2000 = make_relative_comments(
                shen.shen.group_rets.group1, gz2000=1, show_nets=1
            )
    else:
        com300, net300 = make_relative_comments(
            shen.shen.group_rets.group10, hs300=1, show_nets=1
        )
        fi500 = daily_factor_on300500(df, zz500=1)
        shen = pure_moonnight(fi500, iplot=iplot, boxcox=boxcox)
        com500, net500 = make_relative_comments(
            shen.shen.group_rets.group10, zz500=1, show_nets=1
        )
        fi1000 = daily_factor_on300500(df, zz1000=1)
        shen = pure_moonnight(fi1000, iplot=iplot, boxcox=boxcox)
        com1000, net1000 = make_relative_comments(
            shen.shen.group_rets.group10, zz1000=1, show_nets=1
        )
        if gz2000:
            fi2000 = daily_factor_on300500(df, gz2000=1)
            shen = pure_moonnight(fi2000, iplot=iplot, boxcox=boxcox)
            com2000, net2000 = make_relative_comments(
                shen.shen.group_rets.group10, gz2000=1, show_nets=1
            )
    com300 = com300.to_frame("300è¶…é¢")
    com500 = com500.to_frame("500è¶…é¢")
    com1000 = com1000.to_frame("1000è¶…é¢")
    if gz2000:
        com2000 = com2000.to_frame("2000è¶…é¢")
        coms = pd.concat([com300, com500, com1000, com2000], axis=1)
    else:
        coms = pd.concat([com300, com500, com1000], axis=1)
    coms = np.around(coms, 3)
    if gz2000:
        nets = pd.concat([net300, net500, net1000, net2000], axis=1)
        nets.columns = ["300è¶…é¢", "500è¶…é¢", "1000è¶…é¢", "2000è¶…é¢"]
    else:
        nets = pd.concat([net300, net500, net1000], axis=1)
        nets.columns = ["300è¶…é¢", "500è¶…é¢", "1000è¶…é¢"]
    coms = coms.reset_index()
    if iplot:
        figs = cf.figures(
            nets,
            [dict(kind="line", y=list(nets.columns))],
            asList=True,
        )
        coms = coms.rename(columns={list(coms)[0]: "ç»©æ•ˆæŒ‡æ ‡"})
        table = FF.create_table(coms.iloc[::-1])
        table.update_yaxes(matches=None)
        figs.append(table)
        figs = [figs[-1]] + figs[:-1]
        figs[1].update_layout(
            legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
        )
        base_layout = cf.tools.get_base_layout(figs)
        if gz2000:
            sp = cf.subplots(
                figs,
                shape=(2, 10),
                base_layout=base_layout,
                vertical_spacing=0.15,
                horizontal_spacing=0.03,
                shared_yaxes=False,
                specs=[
                    [
                        None,
                        {"rowspan": 2, "colspan": 4},
                        None,
                        None,
                        None,
                        {"rowspan": 2, "colspan": 5},
                        None,
                        None,
                        None,
                        None,
                    ],
                    [
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                    ],
                ],
            )
        else:
            sp = cf.subplots(
                figs,
                shape=(2, 10),
                base_layout=base_layout,
                vertical_spacing=0.15,
                horizontal_spacing=0.03,
                shared_yaxes=False,
                specs=[
                    [
                        None,
                        {"rowspan": 2, "colspan": 3},
                        None,
                        None,
                        {"rowspan": 2, "colspan": 6},
                        None,
                        None,
                        None,
                        None,
                        None,
                    ],
                    [
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                    ],
                ],
            )
        sp["layout"].update(showlegend=True)
        cf.iplot(sp)
    else:
        nets.plot()
        plt.show()
        tb = Texttable()
        tb.set_cols_width([8] + [7] + [8] * 2 + [7] * 2 + [8])
        tb.set_cols_dtype(["f"] * 7)
        tb.header(list(coms.T.reset_index().columns))
        tb.add_rows(coms.T.reset_index().to_numpy(), header=True)
        print(tb.draw())


class pure_star(object):
    def __init__(
        self,
        fac: pd.Series,
        code: str = None,
        price_opens: pd.Series = None,
        iplot: bool = 1,
        comments_writer: pd.ExcelWriter = None,
        net_values_writer: pd.ExcelWriter = None,
        sheetname: str = None,
    ):
        """æ‹©æ—¶å›æµ‹æ¡†æ¶ï¼Œè¾“å…¥ä»“ä½æ¯”ä¾‹æˆ–ä¿¡å·å€¼ï¼Œä¾æ®ä¿¡å·ä¹°å…¥å¯¹åº”çš„è‚¡ç¥¨æˆ–æŒ‡æ•°ï¼Œå¹¶è€ƒå¯Ÿç»å¯¹æ”¶ç›Šã€è¶…é¢æ”¶ç›Šå’ŒåŸºå‡†æ”¶ç›Š
        å›æµ‹æ–¹å¼ä¸ºï¼Œtæ—¥æ”¶ç›˜æ—¶è·å¾—ä¿¡å·ï¼Œt+1æ—¥å¼€ç›˜æ—¶ä»¥å¼€ç›˜ä»·ä¹°å…¥ï¼Œt+2å¼€ç›˜æ—¶ä»¥å¼€ç›˜ä»·å–å‡º

        Parameters
        ----------
        fac : pd.Series
            ä»“ä½æ¯”ä¾‹åºåˆ—ï¼Œæˆ–ä¿¡å·åºåˆ—ï¼Œè¾“å…¥ä¿¡å·åºåˆ—æ—¶å³ä¸º0å’Œ1ï¼Œè¾“å…¥ä»“ä½æ¯”ä¾‹æ—¶ï¼Œå°†æ¯ä¸€æœŸçš„æ”¶ç›ŠæŒ‰ç…§å¯¹åº”æ¯”ä¾‹ç¼©å°
        code : str, optional
            å›æµ‹çš„èµ„äº§ä»£ç ï¼Œå¯ä»¥ä¸ºè‚¡ç¥¨ä»£ç æˆ–åŸºé‡‘ä»£ç , by default None
        price_opens : pd.Series, optional
            èµ„äº§çš„å¼€ç›˜ä»·åºåˆ—, by default None
        iplot : bool, optional
            ä½¿ç”¨cufflinkså‘ˆç°å›æµ‹ç»©æ•ˆå’Œèµ°åŠ¿å›¾, by default 1
        comments_writer : pd.ExcelWriter, optional
            ç»©æ•ˆè¯„ä»·çš„å­˜å‚¨æ–‡ä»¶, by default None
        net_values_writer : pd.ExcelWriter, optional
            å‡€å€¼åºåˆ—çš„å­˜å‚¨æ–‡ä»¶, by default None
        sheetname : str, optional
            å­˜å‚¨æ–‡ä»¶çš„å·¥ä½œè¡¨çš„åå­—, by default None
        """
        if code is not None:
            x1 = code.split(".")[0]
            x2 = code.split(".")[1]
            if (x1[0] == "0" or x1[:2] == "30") and x2 == "SZ":
                kind = "stock"
            elif x1[0] == "6" and x2 == "SH":
                kind = "stock"
            else:
                kind = "index"
            self.kind = kind
            if kind == "index":
                qdb = Questdb()
                price_opens = qdb.get_data(
                    f"select date,num,close from minute_data_{kind} where code='{code}'"
                )
                price_opens = price_opens[price_opens.num == "1"]
                price_opens = price_opens.set_index("date").close
                price_opens.index = pd.to_datetime(price_opens.index, format="%Y%m%d")
            else:
                price_opens = read_daily(open=1)[code]
        price_opens = price_opens[price_opens.index >= fac.index.min()]
        self.price_opens = price_opens
        self.price_rets = price_opens.pct_change()
        self.fac = fac
        self.fac_rets = (self.fac.shift(2) * self.price_rets).dropna()
        self.ab_rets = (self.fac_rets - self.price_rets).dropna()
        self.price_rets = self.price_rets.dropna()
        self.fac_nets = (1 + self.fac_rets).cumprod()
        self.ab_nets = (1 + self.ab_rets).cumprod()
        self.price_nets = (1 + self.price_rets).cumprod()
        self.fac_nets = self.fac_nets / self.fac_nets.iloc[0]
        self.ab_nets = self.ab_nets / self.ab_nets.iloc[0]
        self.price_nets = self.price_nets / self.price_nets.iloc[0]
        self.fac_comments = comments_on_twins(self.fac_nets, self.fac_rets)
        self.ab_comments = comments_on_twins(self.ab_nets, self.ab_rets)
        self.price_comments = comments_on_twins(self.price_nets, self.price_rets)
        self.total_comments = pd.concat(
            [self.fac_comments, self.ab_comments, self.price_comments], axis=1
        )
        self.total_nets = pd.concat(
            [self.fac_nets, self.ab_nets, self.price_nets], axis=1
        )
        self.total_rets = pd.concat(
            [self.fac_rets, self.ab_rets, self.price_rets], axis=1
        )
        self.total_comments.columns = (
            self.total_nets.columns
        ) = self.total_rets.columns = ["å› å­ç»å¯¹", "å› å­è¶…é¢", "ä¹°å…¥æŒæœ‰"]
        self.total_comments = np.around(self.total_comments, 3)
        self.iplot = iplot
        self.plot()
        if comments_writer is None and sheetname is not None:
            from pure_ocean_breeze.legacy_version.v3p4.state.states import COMMENTS_WRITER

            comments_writer = COMMENTS_WRITER
            self.total_comments.to_excel(comments_writer, sheet_name=sheetname)
        if net_values_writer is None and sheetname is not None:
            from pure_ocean_breeze.legacy_version.v3p4.state.states import NET_VALUES_WRITER

            net_values_writer = NET_VALUES_WRITER
            self.total_nets.to_excel(net_values_writer, sheet_name=sheetname)

    def plot(self):
        coms = self.total_comments.copy().reset_index()
        if self.iplot:
            figs = cf.figures(
                self.total_nets,
                [dict(kind="line", y=list(self.total_nets.columns))],
                asList=True,
            )
            coms = coms.rename(columns={list(coms)[0]: "ç»©æ•ˆæŒ‡æ ‡"})
            table = FF.create_table(coms.iloc[::-1])
            table.update_yaxes(matches=None)
            figs.append(table)
            figs = [figs[-1]] + figs[:-1]
            figs[1].update_layout(
                legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
            )
            base_layout = cf.tools.get_base_layout(figs)
            sp = cf.subplots(
                figs,
                shape=(2, 10),
                base_layout=base_layout,
                vertical_spacing=0.15,
                horizontal_spacing=0.03,
                shared_yaxes=False,
                specs=[
                    [
                        None,
                        {"rowspan": 2, "colspan": 3},
                        None,
                        None,
                        {"rowspan": 2, "colspan": 6},
                        None,
                        None,
                        None,
                        None,
                        None,
                    ],
                    [
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                    ],
                ],
            )
            sp["layout"].update(showlegend=True)
            cf.iplot(sp)
        else:
            self.total_nets.plot()
            plt.show()
            tb = Texttable()
            tb.set_cols_width([8] + [7] + [8] * 2 + [7] * 2 + [8])
            tb.set_cols_dtype(["f"] * 7)
            tb.header(list(coms.T.reset_index().columns))
            tb.add_rows(coms.T.reset_index().to_numpy(), header=True)
            print(tb.draw())
