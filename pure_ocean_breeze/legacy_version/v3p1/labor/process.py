__updated__ = "2022-09-13 16:44:33"

import numpy as np
import pandas as pd
import knockknock as kk
import os
import tqdm
import scipy.stats as ss
import scipy.io as scio
import statsmodels.formula.api as smf
import matplotlib as mpl

mpl.rcParams.update(mpl.rcParamsDefault)
import matplotlib.pyplot as plt

plt.style.use(["science", "no-latex", "notebook"])
plt.rcParams["axes.unicode_minus"] = False
from functools import reduce, lru_cache, partial
from dateutil.relativedelta import relativedelta
from loguru import logger
import datetime
from collections import Iterable
import plotly.express as pe
import plotly.io as pio
from typing import Callable, Union
from pure_ocean_breeze.legacy_version.v3p1.data.read_data import read_daily, get_industry_dummies
from pure_ocean_breeze.legacy_version.v3p1.state.homeplace import HomePlace

homeplace = HomePlace()
from pure_ocean_breeze.legacy_version.v3p1.state.decorators import *
from pure_ocean_breeze.legacy_version.v3p1.state.states import STATES
from pure_ocean_breeze.legacy_version.v3p1.data.database import *
from pure_ocean_breeze.legacy_version.v3p1.data.dicts import INDUS_DICT
from pure_ocean_breeze.legacy_version.v3p1.data.tools import indus_name


def daily_factor_on300500(
    fac: pd.DataFrame,
    hs300: bool = 0,
    zz500: bool = 0,
    zz800: bool = 0,
    zz1000: bool = 0,
    gz2000: bool = 0,
    other: bool = 0,
) -> pd.DataFrame:
    """è¾“å…¥æ—¥é¢‘æˆ–æœˆé¢‘å› å­å€¼ï¼Œå°†å…¶é™å®šåœ¨æŸæŒ‡æ•°æˆåˆ†è‚¡çš„è‚¡ç¥¨æ± å†…ï¼Œ
    ç›®å‰ä»…æ”¯æŒæ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯800ã€ä¸­è¯1000ã€å›½è¯2000æˆåˆ†è‚¡ï¼Œå’Œé™¤æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä»¥å¤–çš„è‚¡ç¥¨çš„æˆåˆ†è‚¡

    Parameters
    ----------
    fac : pd.DataFrame
        æœªé™å®šè‚¡ç¥¨æ± çš„å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    hs300 : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºæ²ªæ·±300, by default 0
    zz500 : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºä¸­è¯500, by default 0
    zz800 : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºä¸­è¯800, by default 0
    zz1000 : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºä¸­è¯1000, by default 0
    gz2000 : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºå›½è¯2000, by default 0
    other : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºé™¤æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä»¥å¤–çš„è‚¡ç¥¨çš„æˆåˆ†è‚¡, by default 0

    Returns
    -------
    `pd.DataFrame`
        ä»…åŒ…å«æˆåˆ†è‚¡åçš„å› å­å€¼ï¼Œéæˆåˆ†è‚¡çš„å› å­å€¼ä¸ºç©º

    Raises
    ------
    `ValueError`
        å¦‚æœæœªæŒ‡å®šä»»ä½•ä¸€ç§æŒ‡æ•°çš„æˆåˆ†è‚¡ï¼Œå°†æŠ¥é”™
    """
    last = fac.resample("M").last()
    homeplace = HomePlace()
    if fac.shape[0] / last.shape[0] > 2:
        if hs300:
            df = (
                pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
                .set_index("index")
                .replace(0, np.nan)
            )
            df = df * fac
            df = df.dropna(how="all")
        elif zz500:
            df = (
                pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
                .set_index("index")
                .replace(0, np.nan)
            )
            df = df * fac
            df = df.dropna(how="all")
        elif zz800:
            df1 = pd.read_feather(
                homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather"
            ).set_index("index")
            df2 = pd.read_feather(
                homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather"
            ).set_index("index")
            df = df1 + df2
            df = df.replace(0, np.nan)
            df = df * fac
            df = df.dropna(how="all")
        elif zz1000:
            df = (
                pd.read_feather(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather")
                .set_index("index")
                .replace(0, np.nan)
            )
            df = df * fac
            df = df.dropna(how="all")
        elif gz2000:
            df = (
                pd.read_feather(homeplace.daily_data_file + "å›½è¯2000æ—¥æˆåˆ†è‚¡.feather")
                .set_index("index")
                .replace(0, np.nan)
            )
            df = df * fac
            df = df.dropna(how="all")
        elif other:
            tr = read_daily(tr=1).fillna(0).replace(0, 1)
            tr = np.sign(tr)
            df1 = (
                tr
                * pd.read_feather(
                    homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather"
                ).set_index("index")
            ).fillna(0)
            df2 = (
                tr
                * pd.read_feather(
                    homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather"
                ).set_index("index")
            ).fillna(0)
            df3 = (
                tr
                * pd.read_feather(
                    homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather"
                ).set_index("index")
            ).fillna(0)
            df = (1 - df1) * (1 - df2) * (1 - df3) * tr
            df = df.replace(0, np.nan) * fac
            df = df.dropna(how="all")
        else:
            raise ValueError("æ€»å¾—æŒ‡å®šä¸€ä¸‹æ˜¯å“ªä¸ªæˆåˆ†è‚¡å§ğŸ¤’")
    else:
        if hs300:
            df = (
                pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
                .set_index("index")
                .replace(0, np.nan)
                .resample("M")
                .last()
            )
            df = df * fac
            df = df.dropna(how="all")
        elif zz500:
            df = (
                pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
                .set_index("index")
                .replace(0, np.nan)
                .resample("M")
                .last()
            )
            df = df * fac
            df = df.dropna(how="all")
        elif zz800:
            df1 = (
                pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
                .set_index("index")
                .resample("M")
                .last()
            )
            df2 = (
                pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
                .set_index("index")
                .resample("M")
                .last()
            )
            df = df1 + df2
            df = df.replace(0, np.nan)
            df = df * fac
            df = df.dropna(how="all")
        elif zz1000:
            df = (
                pd.read_feather(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather")
                .set_index("index")
                .replace(0, np.nan)
                .resample("M")
                .last()
            )
            df = df * fac
            df = df.dropna(how="all")
        elif gz2000:
            df = (
                pd.read_feather(homeplace.daily_data_file + "å›½è¯2000æ—¥æˆåˆ†è‚¡.feather")
                .set_index("index")
                .replace(0, np.nan)
                .resample("M")
                .last()
            )
            df = df * fac
            df = df.dropna(how="all")
        elif other:
            tr = read_daily(tr=1).fillna(0).replace(0, 1).resample("M").last()
            tr = np.sign(tr)
            df1 = (
                tr
                * pd.read_feather(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.feather")
                .set_index("index")
                .resample("M")
                .last()
            ).fillna(0)
            df2 = (
                tr
                * pd.read_feather(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.feather")
                .set_index("index")
                .resample("M")
                .last()
            ).fillna(0)
            df3 = (
                tr
                * pd.read_feather(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.feather")
                .set_index("index")
                .resample("M")
                .last()
            ).fillna(0)
            df = (1 - df1) * (1 - df2) * (1 - df3)
            df = df.replace(0, np.nan) * fac
            df = df.dropna(how="all")
        else:
            raise ValueError("æ€»å¾—æŒ‡å®šä¸€ä¸‹æ˜¯å“ªä¸ªæˆåˆ†è‚¡å§ğŸ¤’")
    return df


def daily_factor_on_swindustry(df: pd.DataFrame) -> dict:
    """å°†ä¸€ä¸ªå› å­å˜ä¸ºä»…åœ¨æŸä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šçš„è‚¡ç¥¨

    Parameters
    ----------
    df : pd.DataFrame
        å…¨å¸‚åœºçš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 

    Returns
    -------
    dict
        keyä¸ºè¡Œä¸šä»£ç ï¼Œvalueä¸ºå¯¹åº”çš„è¡Œä¸šä¸Šçš„å› å­å€¼
    """
    df1 = df.resample("M").last()
    if df1.shape[0] * 2 > df.shape[0]:
        daily = 0
        monthly = 1
    else:
        daily = 1
        monthly = 0
    start = int(datetime.datetime.strftime(df.index.min(), "%Y%m%d"))
    ress = get_industry_dummies(daily=daily, monthly=monthly, start=start)
    ress = {k: v * df for k, v in ress.items()}
    return ress


def group_test_on_swindustry(
    df: pd.DataFrame, group_num: int = 10, net_values_writer: pd.ExcelWriter = None
) -> pd.DataFrame:
    """åœ¨ç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šæµ‹è¯•æ¯ä¸ªè¡Œä¸šçš„åˆ†ç»„å›æµ‹

    Parameters
    ----------
    df : pd.DataFrame
        å…¨å¸‚åœºçš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    group_num : int, optional
        åˆ†ç»„æ•°é‡, by default 10
    net_values_writer : pd.ExcelWriter, optional
        ç”¨äºå­˜å‚¨å„ä¸ªè¡Œä¸šåˆ†ç»„åŠå¤šç©ºå¯¹å†²å‡€å€¼åºåˆ—çš„excelæ–‡ä»¶, by default None

    Returns
    -------
    pd.DataFrame
        å„ä¸ªè¡Œä¸šçš„ç»©æ•ˆè¯„ä»·æ±‡æ€»
    """
    dfs = daily_factor_on_swindustry(df)
    ks = []
    vs = []
    for k, v in dfs.items():
        shen = pure_moonnight(
            v,
            groups_num=group_num,
            net_values_writer=net_values_writer,
            sheetname=INDUS_DICT[k],
            plt_plot=0,
        )
        ks.append(k)
        vs.append(shen.shen.total_comments.T)
    vs = pd.concat(vs)
    vs.index = [INDUS_DICT[i] for i in ks]
    return vs


def rankic_test_on_swindustry(
    df: pd.DataFrame, excel_name: str = "è¡Œä¸šrankic.xlsx", png_name: str = "è¡Œä¸šrankicå›¾.png"
) -> pd.DataFrame:
    """ä¸“é—¨è®¡ç®—å› å­å€¼åœ¨å„ä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šçš„Rank ICå€¼ï¼Œå¹¶ç»˜åˆ¶æŸ±çŠ¶å›¾

    Parameters
    ----------
    df : pd.DataFrame
        å…¨å¸‚åœºçš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    excel_name : str, optional
        ç”¨äºä¿å­˜å„ä¸ªè¡Œä¸šRank ICå€¼çš„excelæ–‡ä»¶çš„åå­—, by default 'è¡Œä¸šrankic.xlsx'
    png_name : str, optional
        ç”¨äºä¿å­˜å„ä¸ªè¡Œä¸šRank ICå€¼çš„æŸ±çŠ¶å›¾çš„åå­—, by default 'è¡Œä¸šrankicå›¾.png'

    Returns
    -------
    pd.DataFrame
        è¡Œä¸šåç§°ä¸å¯¹åº”çš„Rank IC
    """
    vs = group_test_on_swindustry(df)
    rankics = vs[["RankIC"]].T
    rankics.to_excel(excel_name)
    rankics.plot(kind="bar")
    plt.show()
    plt.savefig(png_name)
    return rankics


def long_test_on_swindustry(
    df: pd.DataFrame,
    nums: list,
    pos: bool = 0,
    neg: bool = 0,
    save_stock_list: bool = 0,
) -> list[dict]:
    """å¯¹æ¯ä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡ï¼Œä½¿ç”¨æŸå› å­æŒ‘é€‰å‡ºæœ€å¤šå¤´çš„nå€¼è‚¡ç¥¨ï¼Œè€ƒå¯Ÿå…¶è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•

    Parameters
    ----------
    df : pd.DataFrame
        ä½¿ç”¨çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    nums : list
        å¤šå¤´æƒ³é€‰å–çš„è‚¡ç¥¨çš„æ•°é‡ï¼Œä¾‹å¦‚[3,4,5]
    pos : bool, optional
        å› å­æ–¹å‘ä¸ºæ­£ï¼Œå³Rank ICä¸ºæ­£ï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºTrue, by default 0
    neg : bool, optional
        å› å­æ–¹å‘ä¸ºè´Ÿï¼Œå³Rank ICä¸ºè´Ÿï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºFalse, by default 0
    save_stock_list:bool, optional
        æ˜¯å¦ä¿å­˜æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•ï¼Œä¼šé™ä½è¿è¡Œé€Ÿåº¦, by default 0

    Returns
    -------
    list[dict]
        è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•

    Raises
    ------
    IOError
        poså’Œnegå¿…é¡»æœ‰ä¸€ä¸ªä¸º1ï¼Œå¦åˆ™å°†æŠ¥é”™
    """
    fac = decap_industry(fac, monthly=True)
    industry_dummy = pd.read_feather(
        homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather"
    ).fillna(0)
    inds = list(industry_dummy.columns)
    ret_next = (
        read_daily(close=1).resample("M").last()
        / read_daily(open=1).resample("M").first()
        - 1
    )
    ages = read_daily(age=1)
    ages = (ages >= 60) + 0
    ages = ages.replace(0, np.nan)
    ret_next = ret_next * ages
    ret_next_dummy = 1 - ret_next.isna()

    def save_ind(code, num):
        ind = industry_dummy[["date", "code", code]]
        ind = ind.pivot(index="date", columns="code", values=code)
        ind = ind.resample("M").last()
        ind = ind.replace(0, np.nan)
        fi = ind * fac
        fi = fi.dropna(how="all")
        fi = fi.shift(1)
        fi = fi * ret_next_dummy
        fi = fi.dropna(how="all")

        def sing(x):
            if neg:
                thr = x.nsmallest(num).iloc[-1]
            elif pos:
                thr = x.nlargest(num).iloc[-1]
            else:
                raise IOError("æ‚¨éœ€è¦æŒ‡å®šä¸€ä¸‹å› å­æ–¹å‘ğŸ¤’")
            x = (x <= thr) + 0
            return x

        fi = fi.T.apply(sing).T
        fi = fi.replace(0, np.nan)
        fi = fi * ret_next
        ret_long = fi.mean(axis=1)
        return ret_long

    ret_longs = {k: {} for k in nums}
    for num in tqdm.tqdm(nums):
        for code in inds[2:]:
            ret_longs[num][code] = save_ind(code, num)

    coms = {
        k: indus_name(pd.concat(v, axis=1).dropna(how="all").T).T
        for k, v in ret_longs.items()
    }
    indus = indus.resample("M").last().pct_change()
    rets = {k: (v - indus_name(indus.T).T).dropna(how="all") for k, v in coms.items()}
    nets = {k: (v + 1).cumprod() for k, v in rets.items()}
    nets = {
        k: v.apply(lambda x: x.dropna() / x.dropna().iloc[0]) for k, v in nets.items()
    }

    def comments_on_twins(nets: pd.Series, rets: pd.Series) -> pd.Series:
        series = nets.copy()
        series1 = rets.copy()
        ret = (series.iloc[-1] - series.iloc[0]) / series.iloc[0]
        duration = (series.index[-1] - series.index[0]).days
        year = duration / 365
        ret_yearly = (series.iloc[-1] / series.iloc[0]) ** (1 / year) - 1
        max_draw = -(series / series.expanding(1).max() - 1).min()
        vol = np.std(series1) * (12**0.5)
        sharpe = ret_yearly / vol
        wins = series1[series1 > 0]
        win_rate = len(wins) / len(series1)
        return pd.Series(
            [ret, ret_yearly, vol, sharpe, win_rate, max_draw],
            index=["æ€»æ”¶ç›Šç‡", "å¹´åŒ–æ”¶ç›Šç‡", "å¹´åŒ–æ³¢åŠ¨ç‡", "ä¿¡æ¯æ¯”ç‡", "èƒœç‡", "æœ€å¤§å›æ’¤ç‡"],
        )

    w = pd.ExcelWriter("å„ä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šå¤šå¤´è¶…é¢ç»©æ•ˆ.xlsx")

    def com_all(df1, df2, num):
        cs = []
        for ind in list(df1.columns):
            c = comments_on_twins(df2[ind].dropna(), df1[ind].dropna()).to_frame(ind)
            cs.append(c)
        res = pd.concat(cs, axis=1).T
        res.to_excel(w, sheet_name=str(num))
        return res

    coms_finals = {k: com_all(rets[k], nets[k], k) for k in rets.keys()}
    w.save()
    w.close()

    rets_save = {k: v.dropna() for k, v in rets.items() if k in nums}
    u = pd.ExcelWriter("å„ä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šæ¯æœˆè¶…é¢æ”¶ç›Šç‡.xlsx")
    for k, v in rets_save.items():
        v.to_excel(u, sheet_name=str(k))
    u.save()
    u.close()

    if save_stock_list:

        def save_ind_stocks(code, num):
            ind = industry_dummy[["date", "code", code]]
            ind = ind.pivot(index="date", columns="code", values=code)
            ind = ind.replace(0, np.nan)
            fi = ind * fac
            fi = fi.dropna(how="all")
            fi = fi.shift(1)
            fi = fi * ret_next_dummy
            fi = fi.dropna(how="all")

            def sing(x):
                if neg:
                    thr = x.nsmallest(num)
                elif pos:
                    thr = x.nlargest(num)
                else:
                    raise IOError("æ‚¨éœ€è¦æŒ‡å®šä¸€ä¸‹å› å­æ–¹å‘ğŸ¤’")
                return tuple(thr.index)

            fi = fi.T.apply(sing)
            return fi

        stocks_longs = {k: {} for k in nums}
        for num in tqdm.tqdm(nums):
            for code in inds[2:]:
                stocks_longs[num][code] = save_ind_stocks(code, num)

        for num in nums:
            w1 = pd.ExcelWriter(f"å„ä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šä¹°{num}åªçš„è‚¡ç¥¨åå•.xlsx")
            for k, v in stocks_longs[num].items():
                v = v.T
                v.index = v.index.strftime("%Y/%m/%d")
                v.to_excel(w1, sheet_name=INDUS_DICT[k])
            w1.save()
            w1.close()

        return [coms_finals, rets_save, stocks_longs]
    else:
        return [coms_finals, rets_save]


def select_max(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:
    """ä¸¤ä¸ªcolumnsä¸indexå®Œå…¨ç›¸åŒçš„dfï¼Œæ¯ä¸ªå€¼éƒ½æŒ‘å‡ºè¾ƒå¤§å€¼

    Parameters
    ----------
    df1 : pd.DataFrame
        ç¬¬ä¸€ä¸ªdf
    df2 : pd.DataFrame
        ç¬¬äºŒä¸ªdf

    Returns
    -------
    `pd.DataFrame`
        ä¸¤ä¸ªdfæ¯ä¸ªvalueä¸­çš„è¾ƒå¤§è€…
    """
    return (df1 + df2 + np.abs(df1 - df2)) / 2


def select_min(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:
    """ä¸¤ä¸ªcolumnsä¸indexå®Œå…¨ç›¸åŒçš„dfï¼Œæ¯ä¸ªå€¼éƒ½æŒ‘å‡ºè¾ƒå°å€¼

    Parameters
    ----------
    df1 : pd.DataFrame
        ç¬¬ä¸€ä¸ªdf
    df2 : pd.DataFrame
        ç¬¬äºŒä¸ªdf

    Returns
    -------
    `pd.DataFrame`
        ä¸¤ä¸ªdfæ¯ä¸ªvalueä¸­çš„è¾ƒå°è€…
    """
    return (df1 + df2 - np.abs(df1 - df2)) / 2


@kk.desktop_sender(title="å˜¿ï¼Œè¡Œä¸šä¸­æ€§åŒ–åšå®Œå•¦ï½ğŸ›")
def decap(df: pd.DataFrame, daily: bool = 0, monthly: bool = 0) -> pd.DataFrame:
    """å¯¹å› å­åšå¸‚å€¼ä¸­æ€§åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        æœªä¸­æ€§åŒ–çš„å› å­ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    daily : bool, optional
        æœªä¸­æ€§åŒ–å› å­æ˜¯æ—¥é¢‘çš„åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0, by default 0
    monthly : bool, optional
        æœªä¸­æ€§åŒ–å› å­æ˜¯æœˆé¢‘çš„åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0, by default 0

    Returns
    -------
    `pd.DataFrame`
        å¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­

    Raises
    ------
    `NotImplementedError`
        å¦‚æœæœªæŒ‡å®šæ—¥é¢‘æˆ–æœˆé¢‘ï¼Œå°†æŠ¥é”™
    """
    tqdm.tqdm.pandas()
    share = read_daily("AllStock_DailyAShareNum.mat")
    undi_close = read_daily("AllStock_DailyClose.mat")
    cap = (share * undi_close).stack().reset_index()
    cap.columns = ["date", "code", "cap"]
    cap.cap = ss.boxcox(cap.cap)[0]

    def single(x):
        x.cap = ss.boxcox(x.cap)[0]
        return x

    cap = cap.groupby(["date"]).apply(single)
    cap = cap.set_index(["date", "code"]).unstack()
    cap.columns = [i[1] for i in list(cap.columns)]
    cap_monthly = cap.resample("M").last()
    last = df.resample("M").last()
    if df.shape[0] / last.shape[0] < 2:
        monthly = True
    else:
        daily = True
    if daily:
        df = (pure_fallmount(df) - (pure_fallmount(cap),))()
    elif monthly:
        df = (pure_fallmount(df) - (pure_fallmount(cap_monthly),))()
    else:
        raise NotImplementedError("å¿…é¡»æŒ‡å®šé¢‘ç‡")
    return df


@kk.desktop_sender(title="å˜¿ï¼Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–åšå®Œå•¦ï½ğŸ›")
def decap_industry(
    df: pd.DataFrame, daily: bool = 0, monthly: bool = 0
) -> pd.DataFrame:
    """å¯¹å› å­åšè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        æœªä¸­æ€§åŒ–çš„å› å­ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    daily : bool, optional
        æœªä¸­æ€§åŒ–å› å­æ˜¯æ—¥é¢‘çš„åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0, by default 0
    monthly : bool, optional
        æœªä¸­æ€§åŒ–å› å­æ˜¯æœˆé¢‘çš„åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0, by default 0

    Returns
    -------
    `pd.DataFrame`
        è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­

    Raises
    ------
    `NotImplementedError`
        å¦‚æœæœªæŒ‡å®šæ—¥é¢‘æˆ–æœˆé¢‘ï¼Œå°†æŠ¥é”™
    """
    last = df.resample("M").last()
    homeplace = HomePlace()
    share = read_daily("AllStock_DailyAShareNum.mat")
    undi_close = read_daily("AllStock_DailyClose.mat")
    cap = (share * undi_close).stack().reset_index()
    cap.columns = ["date", "code", "cap"]
    cap.cap = ss.boxcox(cap.cap)[0]

    def single(x):
        x.cap = ss.boxcox(x.cap)[0]
        return x

    cap = cap.groupby(["date"]).apply(single)
    df = df.stack().reset_index()
    df.columns = ["date", "code", "fac"]
    df = pd.merge(df, cap, on=["date", "code"])
    if daily == 0 and monthly == 0:
        if df.shape[0] / last.shape[0] < 2:
            monthly = True
        else:
            daily = True

    def neutralize_factors(df):
        """ç»„å†…å¯¹å› å­è¿›è¡Œå¸‚å€¼ä¸­æ€§åŒ–"""
        industry_codes = list(df.columns)
        industry_codes = [i for i in industry_codes if i.startswith("w")]
        industry_codes_str = "+".join(industry_codes)
        ols_result = smf.ols("fac~cap+" + industry_codes_str, data=df).fit()
        ols_w = ols_result.params["cap"]
        ols_b = ols_result.params["Intercept"]
        ols_bs = {}
        for ind in industry_codes:
            ols_bs[ind] = ols_result.params[ind]
        df.fac = df.fac - ols_w * df.cap - ols_b
        for k, v in ols_bs.items():
            df.fac = df.fac - v * df[k]
        df = df[["fac"]]
        return df

    if monthly:
        industry_dummy = (
            pd.read_feather(homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather")
            .set_index("date")
            .groupby("code")
            .resample("M")
            .last()
        )
        industry_dummy = industry_dummy.fillna(0).drop(columns=["code"]).reset_index()
        industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
        col = ["code", "date"] + industry_ws
    elif daily:
        industry_dummy = pd.read_feather(
            homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather"
        ).fillna(0)
        industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
        col = ["date", "code"] + industry_ws
    else:
        raise NotImplementedError("å¿…é¡»æŒ‡å®šé¢‘ç‡")
    industry_dummy.columns = col
    df = pd.merge(df, industry_dummy, on=["date", "code"])
    df = df.set_index(["date", "code"])
    tqdm.tqdm.pandas()
    df = df.groupby(["date"]).progress_apply(neutralize_factors)
    df = df.unstack()
    df.columns = [i[1] for i in list(df.columns)]
    return df


def deboth(df: pd.DataFrame) -> pd.DataFrame:
    """é€šè¿‡å›æµ‹çš„æ–¹å¼ï¼Œå¯¹æœˆé¢‘å› å­åšè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        æœªä¸­æ€§åŒ–çš„å› å­

    Returns
    -------
    `pd.DataFrame`
        è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­
    """
    shen = pure_moonnight(df, boxcox=1, plt_plot=0, print_comments=0)
    return shen()


def detect_nan(df: pd.DataFrame) -> bool:
    """æ£€æŸ¥ä¸€ä¸ªpd.DataFrameä¸­æ˜¯å¦å­˜åœ¨ç©ºå€¼

    Parameters
    ----------
    df : pd.DataFrame
        å¾…æ£€æŸ¥çš„pd.DataFrame

    Returns
    -------
    `bool`
        æ£€æŸ¥ç»“æœï¼Œæœ‰ç©ºå€¼ä¸ºTrueï¼Œå¦åˆ™ä¸ºFalse
    """
    x = np.sum(df.to_numpy().flatten())
    if np.isnan(x):
        print("å­˜åœ¨ç©ºå€¼")
        return True
    else:
        print("ä¸å­˜åœ¨ç©ºå€¼")
        return False


def boom_four(
    df: pd.DataFrame, backsee: int = 20, daily: bool = 0, min_periods: int = None
) -> tuple[pd.DataFrame]:
    """ç”Ÿæˆ20å¤©å‡å€¼ï¼Œ20å¤©æ ‡å‡†å·®ï¼ŒåŠäºŒè€…æ­£å‘z-scoreåˆæˆï¼Œæ­£å‘æ’åºåˆæˆï¼Œè´Ÿå‘z-scoreåˆæˆï¼Œè´Ÿå‘æ’åºåˆæˆè¿™6ä¸ªå› å­

    Parameters
    ----------
    df : pd.DataFrame
        åŸæ—¥é¢‘å› å­
    backsee : int, optional
        å›çœ‹å¤©æ•°, by default 20
    daily : bool, optional
        ä¸º1æ˜¯æ¯å¤©éƒ½æ»šåŠ¨ï¼Œä¸º0åˆ™ä»…ä¿ç•™æœˆåº•å€¼, by default 0
    min_periods : int, optional
        rollingæ—¶çš„æœ€å°æœŸ, by default backseeçš„ä¸€åŠ

    Returns
    -------
    `tuple[pd.DataFrame]`
        6ä¸ªå› å­çš„å…ƒç»„
    """
    if min_periods is None:
        min_periods = int(backsee * 0.5)
    if not daily:
        df_mean = (
            df.rolling(backsee, min_periods=min_periods).mean().resample("M").last()
        )
        df_std = df.rolling(backsee, min_periods=min_periods).std().resample("M").last()
        twins_add = (pure_fallmount(df_mean) + (pure_fallmount(df_std),))()
        rtwins_add = df_mean.rank(axis=1) + df_std.rank(axis=1)
        twins_minus = (pure_fallmount(df_mean) + (pure_fallmount(-df_std),))()
        rtwins_minus = df_mean.rank(axis=1) - df_std.rank(axis=1)
    else:
        df_mean = df.rolling(backsee, min_periods=min_periods).mean()
        df_std = df.rolling(backsee, min_periods=min_periods).std()
        twins_add = (pure_fallmount(df_mean) + (pure_fallmount(df_std),))()
        rtwins_add = df_mean.rank(axis=1) + df_std.rank(axis=1)
        twins_minus = (pure_fallmount(df_mean) + (pure_fallmount(-df_std),))()
        rtwins_minus = df_mean.rank(axis=1) - df_std.rank(axis=1)
    return df_mean, df_std, twins_add, rtwins_add, twins_minus, rtwins_minus


def get_abs(df: pd.DataFrame, median: bool = 0, square: bool = 0) -> pd.DataFrame:
    """å‡å€¼è·ç¦»åŒ–ï¼šè®¡ç®—å› å­ä¸æˆªé¢å‡å€¼çš„è·ç¦»

    Parameters
    ----------
    df : pd.DataFrame
        æœªå‡å€¼è·ç¦»åŒ–çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    median : bool, optional
        ä¸º1åˆ™è®¡ç®—åˆ°ä¸­ä½æ•°çš„è·ç¦», by default 0
    square : bool, optional
        ä¸º1åˆ™è®¡ç®—è·ç¦»çš„å¹³æ–¹, by default 0

    Returns
    -------
    `pd.DataFrame`
        _description_
    """
    if not square:
        if median:
            return np.abs((df.T - df.T.median()).T)
        else:
            return np.abs((df.T - df.T.mean()).T)
    else:
        if median:
            return ((df.T - df.T.median()).T) ** 2
        else:
            return ((df.T - df.T.mean()).T) ** 2


def add_cross_standardlize(*args: list) -> pd.DataFrame:
    """å°†ä¼—å¤šå› å­æ¨ªæˆªé¢åšz-scoreæ ‡å‡†åŒ–ä¹‹åç›¸åŠ 

    Returns
    -------
    `pd.DataFrame`
        åˆæˆåçš„å› å­
    """
    fms = [pure_fallmount(i) for i in args]
    one = fms[0]
    others = fms[1:]
    final = one + others
    return final()


def get_normal(df: pd.DataFrame) -> pd.DataFrame:
    """å°†å› å­æ¨ªæˆªé¢æ­£æ€åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        åŸå§‹å› å­ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 

    Returns
    -------
    `pd.DataFrame`
        æ¯ä¸ªæ¨ªæˆªé¢éƒ½å‘ˆç°æ­£æ€åˆ†å¸ƒçš„å› å­
    """
    df = df.replace(0, np.nan)
    df = df.T.apply(lambda x: ss.boxcox(x)[0]).T
    return df


def coin_reverse(
    ret20: pd.DataFrame, vol20: pd.DataFrame, mean: bool = 1, positive_negtive: bool = 0
) -> pd.DataFrame:
    """çƒé˜Ÿç¡¬å¸æ³•ï¼šæ ¹æ®vol20çš„å¤§å°ï¼Œç¿»è½¬ä¸€åŠret20ï¼ŒæŠŠvol20è¾ƒå¤§çš„éƒ¨åˆ†ï¼Œç»™ret20æ·»åŠ è´Ÿå·

    Parameters
    ----------
    ret20 : pd.DataFrame
        è¦è¢«ç¿»è½¬çš„å› å­ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    vol20 : pd.DataFrame
        ç¿»è½¬çš„ä¾æ®ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    mean : bool, optional
        ä¸º1åˆ™ä»¥æ˜¯å¦å¤§äºæˆªé¢å‡å€¼ä¸ºæ ‡å‡†ç¿»è½¬ï¼Œå¦åˆ™ä»¥æ˜¯å¦å¤§äºæˆªé¢ä¸­ä½æ•°ä¸ºæ ‡å‡†, by default 1
    positive_negtive : bool, optional
        æ˜¯å¦æˆªé¢ä¸Šæ­£è´Ÿå€¼çš„ä¸¤éƒ¨åˆ†ï¼Œå„ç¿»è½¬ä¸€åŠ, by default 0

    Returns
    -------
    `pd.DataFrame`
        ç¿»è½¬åçš„å› å­å€¼
    """
    if positive_negtive:
        if not mean:
            down20 = np.sign(ret20)
            down20 = down20.replace(1, np.nan)
            down20 = down20.replace(-1, 1)

            vol20_down = down20 * vol20
            vol20_down = (vol20_down.T - vol20_down.T.median()).T
            vol20_down = np.sign(vol20_down)
            ret20_down = ret20[ret20 < 0]
            ret20_down = vol20_down * ret20_down

            up20 = np.sign(ret20)
            up20 = up20.replace(-1, np.nan)

            vol20_up = up20 * vol20
            vol20_up = (vol20_up.T - vol20_up.T.median()).T
            vol20_up = np.sign(vol20_up)
            ret20_up = ret20[ret20 > 0]
            ret20_up = vol20_up * ret20_up

            ret20_up = ret20_up.replace(np.nan, 0)
            ret20_down = ret20_down.replace(np.nan, 0)
            new_ret20 = ret20_up + ret20_down
            new_ret20_tr = new_ret20.replace(0, np.nan)
            return new_ret20_tr
        else:
            down20 = np.sign(ret20)
            down20 = down20.replace(1, np.nan)
            down20 = down20.replace(-1, 1)

            vol20_down = down20 * vol20
            vol20_down = (vol20_down.T - vol20_down.T.mean()).T
            vol20_down = np.sign(vol20_down)
            ret20_down = ret20[ret20 < 0]
            ret20_down = vol20_down * ret20_down

            up20 = np.sign(ret20)
            up20 = up20.replace(-1, np.nan)

            vol20_up = up20 * vol20
            vol20_up = (vol20_up.T - vol20_up.T.mean()).T
            vol20_up = np.sign(vol20_up)
            ret20_up = ret20[ret20 > 0]
            ret20_up = vol20_up * ret20_up

            ret20_up = ret20_up.replace(np.nan, 0)
            ret20_down = ret20_down.replace(np.nan, 0)
            new_ret20 = ret20_up + ret20_down
            new_ret20_tr = new_ret20.replace(0, np.nan)
            return new_ret20_tr
    else:
        if not mean:
            vol20_dummy = np.sign((vol20.T - vol20.T.median()).T)
            ret20 = ret20 * vol20_dummy
            return ret20
        else:
            vol20_dummy = np.sign((vol20.T - vol20.T.mean()).T)
            ret20 = ret20 * vol20_dummy
            return ret20


def multidfs_to_one(*args: list) -> pd.DataFrame:
    """å¾ˆå¤šä¸ªdfï¼Œå„æœ‰ä¸€éƒ¨åˆ†ï¼Œå…¶ä½™ä½ç½®éƒ½æ˜¯ç©ºï¼Œ
    æƒ³æŠŠå„è‡ªdfæœ‰å€¼çš„éƒ¨åˆ†ä¿ç•™ï¼Œéƒ½æ²¡æœ‰å€¼çš„éƒ¨åˆ†ç»§ç»­è®¾ä¸ºç©º

    Returns
    -------
    `pd.DataFrame`
        åˆå¹¶åçš„df
    """
    dfs = [i.fillna(0) for i in args]
    background = np.sign(np.abs(np.sign(sum(dfs))) + 1).replace(1, 0)
    dfs = [(i + background).fillna(0) for i in dfs]
    df_nans = [i.isna() for i in dfs]
    nan = reduce(lambda x, y: x * y, df_nans)
    nan = nan.replace(1, np.nan)
    nan = nan.replace(0, 1)
    df_final = sum(dfs) * nan
    return df_final


def to_tradeends(df: pd.DataFrame) -> pd.DataFrame:
    """å°†æœ€åä¸€ä¸ªè‡ªç„¶æ—¥æ”¹å˜ä¸ºæœ€åä¸€ä¸ªäº¤æ˜“æ—¥

    Parameters
    ----------
    df : pd.DataFrame
        indexä¸ºæ—¶é—´ï¼Œä¸ºæ¯ä¸ªæœˆçš„æœ€åä¸€å¤©

    Returns
    -------
    `pd.DataFrame`
        ä¿®æ”¹ä¸ºäº¤æ˜“æ—¥æ ‡æ³¨åçš„pd.DataFrame
    """
    """"""
    trs = read_daily(tr=1)
    trs = trs.assign(tradeends=list(trs.index))
    trs = trs[["tradeends"]]
    trs = trs.resample("M").last()
    df = pd.concat([trs, df], axis=1)
    df = df.set_index(["tradeends"])
    return df


def market_kind(
    df: pd.DataFrame,
    zhuban: bool = 0,
    chuangye: bool = 0,
    kechuang: bool = 0,
    beijing: bool = 0,
) -> pd.DataFrame:
    """ä¸å®½åŸºæŒ‡æ•°æˆåˆ†è‚¡çš„å‡½æ•°ç±»ä¼¼ï¼Œé™å®šè‚¡ç¥¨åœ¨æŸä¸ªå…·ä½“æ¿å—ä¸Š

    Parameters
    ----------
    df : pd.DataFrame
        åŸå§‹å…¨éƒ¨è‚¡ç¥¨çš„å› å­å€¼
    zhuban : bool, optional
        é™å®šåœ¨ä¸»æ¿èŒƒå›´å†…, by default 0
    chuangye : bool, optional
        é™å®šåœ¨åˆ›ä¸šæ¿èŒƒå›´å†…, by default 0
    kechuang : bool, optional
        é™å®šåœ¨ç§‘åˆ›æ¿èŒƒå›´å†…, by default 0
    beijing : bool, optional
        é™å®šåœ¨åŒ—äº¤æ‰€èŒƒå›´å†…, by default 0

    Returns
    -------
    `pd.DataFrame`
        é™åˆ¶èŒƒå›´åçš„å› å­å€¼ï¼Œå…¶ä½™ä¸ºç©º

    Raises
    ------
    `ValueError`
        å¦‚æœæœªæŒ‡å®šä»»ä½•è‚¡ç¥¨æ± ï¼Œå°†æŠ¥é”™
    """
    trs = read_daily(tr=1)
    codes = list(trs.columns)
    dates = list(trs.index)
    if chuangye and kechuang:
        dummys = [1 if code[:2] in ["30", "68"] else np.nan for code in codes]
    else:
        if zhuban:
            dummys = [1 if code[:2] in ["00", "60"] else np.nan for code in codes]
        elif chuangye:
            dummys = [1 if code.startswith("3") else np.nan for code in codes]
        elif kechuang:
            dummys = [1 if code.startswith("68") else np.nan for code in codes]
        elif beijing:
            dummys = [1 if code.startswith("8") else np.nan for code in codes]
        else:
            raise ValueError("ä½ æ€»å¾—é€‰ä¸€ä¸ªè‚¡ç¥¨æ± å§ï¼ŸğŸ¤’")
    dummy_dict = {k: v for k, v in zip(codes, dummys)}
    dummy_df = pd.DataFrame(dummy_dict, index=dates)
    df = df * dummy_df
    return df


def to_percent(x: float) -> Union[float, str]:
    """æŠŠå°æ•°è½¬åŒ–ä¸º2ä½å°æ•°çš„ç™¾åˆ†æ•°

    Parameters
    ----------
    x : float
        è¦è½¬æ¢çš„å°æ•°

    Returns
    -------
    Union[float,str]
        ç©ºå€¼åˆ™ä¾ç„¶ä¸ºç©ºï¼Œå¦åˆ™è¿”å›å¸¦%çš„å­—ç¬¦ä¸²
    """
    if np.isnan(x):
        return x
    else:
        x = str(round(x * 100, 2)) + "%"
        return x


def show_corr(
    fac1: pd.DataFrame, fac2: pd.DataFrame, method: str = "spearman", plt_plot: bool = 1
) -> float:
    """å±•ç¤ºä¸¤ä¸ªå› å­çš„æˆªé¢ç›¸å…³æ€§

    Parameters
    ----------
    fac1 : pd.DataFrame
        å› å­1
    fac2 : pd.DataFrame
        å› å­2
    method : str, optional
        è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "spearman"
    plt_plot : bool, optional
        æ˜¯å¦ç”»å‡ºç›¸å…³ç³»æ•°çš„æ—¶åºå˜åŒ–å›¾, by default 1

    Returns
    -------
    `float`
        å¹³å‡æˆªé¢ç›¸å…³ç³»æ•°
    """
    both1 = fac1.stack().reset_index()
    befo1 = fac2.stack().reset_index()
    both1.columns = ["date", "code", "both"]
    befo1.columns = ["date", "code", "befo"]
    twins = pd.merge(both1, befo1, on=["date", "code"]).set_index(["date", "code"])
    corr = twins.groupby("date").apply(lambda x: x.corr(method=method).iloc[0, 1])
    if plt_plot:
        corr.plot(rot=60)
        plt.show()
    return corr.mean()


def show_corrs(
    factors: list[pd.DataFrame],
    factor_names: list[str] = None,
    print_bool: bool = True,
    show_percent: bool = True,
) -> pd.DataFrame:
    """å±•ç¤ºå¾ˆå¤šå› å­ä¸¤ä¸¤ä¹‹é—´çš„æˆªé¢ç›¸å…³æ€§

    Parameters
    ----------
    factors : list[pd.DataFrame]
        æ‰€æœ‰å› å­æ„æˆçš„åˆ—è¡¨, by default None
    factor_names : list[str], optional
        ä¸Šè¿°å› å­ä¾æ¬¡çš„åå­—, by default None
    print_bool : bool, optional
        æ˜¯å¦æ‰“å°å‡ºä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼, by default True
    show_percent : bool, optional
        æ˜¯å¦ä»¥ç™¾åˆ†æ•°çš„å½¢å¼å±•ç¤º, by default True

    Returns
    -------
    `pd.DataFrame`
        ä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼
    """
    corrs = []
    for i in range(len(factors)):
        main_i = factors[i]
        follows = factors[i + 1 :]
        corr = [show_corr(main_i, i, plt_plot=False) for i in follows]
        corr = [np.nan] * (i + 1) + corr
        corrs.append(corr)
    if factor_names is None:
        factor_names = [f"fac{i}" for i in list(range(1, len(factors) + 1))]
    corrs = pd.DataFrame(corrs, columns=factor_names, index=factor_names)
    np.fill_diagonal(corrs.to_numpy(), 1)
    if show_percent:
        pcorrs = corrs.applymap(to_percent)
    else:
        pcorrs = corrs.copy()
    if print_bool:
        print(pcorrs)
    return corrs


def calc_exp_list(window: int, half_life: int) -> np.ndarray:
    """ç”ŸæˆåŠè¡°åºåˆ—

    Parameters
    ----------
    window : int
        çª—å£æœŸ
    half_life : int
        åŠè¡°æœŸ

    Returns
    -------
    `np.ndarray`
        åŠè¡°åºåˆ—
    """
    exp_wt = np.asarray([0.5 ** (1 / half_life)] * window) ** np.arange(window)
    return exp_wt[::-1] / np.sum(exp_wt)


def calcWeightedStd(series: pd.Series, weights: Union[pd.Series, np.ndarray]) -> float:
    """è®¡ç®—åŠè¡°åŠ æƒæ ‡å‡†å·®

    Parameters
    ----------
    series : pd.Series
        ç›®æ ‡åºåˆ—
    weights : Union[pd.Series,np.ndarray]
        æƒé‡åºåˆ—

    Returns
    -------
    `float`
        åŠè¡°åŠ æƒæ ‡å‡†å·®
    """
    weights /= np.sum(weights)
    return np.sqrt(np.sum((series - np.mean(series)) ** 2 * weights))


def get_list_std(delta_sts: list[pd.DataFrame]) -> pd.DataFrame:
    """åŒä¸€å¤©å¤šä¸ªå› å­ï¼Œè®¡ç®—è¿™äº›å› å­åœ¨å½“å¤©çš„æ ‡å‡†å·®

    Parameters
    ----------
    delta_sts : list[pd.DataFrame]
        å¤šä¸ªå› å­æ„æˆçš„listï¼Œæ¯ä¸ªå› å­indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 

    Returns
    -------
    `pd.DataFrame`
        æ¯å¤©æ¯åªè‚¡ç¥¨å¤šä¸ªå› å­çš„æ ‡å‡†å·®
    """
    delta_sts_mean = sum(delta_sts) / len(delta_sts)
    delta_sts_std = [(i - delta_sts_mean) ** 2 for i in delta_sts]
    delta_sts_std = sum(delta_sts_std)
    delta_sts_std = delta_sts_std**0.5 / len(delta_sts)
    return delta_sts_std


class pure_moon(object):
    __slots__ = [
        "homeplace" "path_prefix",
        "codes_path",
        "tradedays_path",
        "ages_path",
        "sts_path",
        "states_path",
        "opens_path",
        "closes_path",
        # "highs_path",
        # "lows_path",
        "pricloses_path",
        "flowshares_path",
        # "amounts_path",
        # "turnovers_path",
        "factors_file",
        "sts_monthly_file",
        "states_monthly_file",
        "sts_monthly_by10_file",
        "states_monthly_by10_file",
        "factors",
        "codes",
        "tradedays",
        "ages",
        "amounts",
        "closes",
        "flowshares",
        "highs",
        "lows",
        "opens",
        "pricloses",
        "states",
        "sts",
        "turnovers",
        "sts_monthly",
        "states_monthly",
        "ages_monthly",
        "tris_monthly",
        "opens_monthly",
        "closes_monthly",
        "rets_monthly",
        "opens_monthly_shift",
        "rets_monthly_begin",
        "limit_ups",
        "limit_downs",
        "data",
        "ic_icir_and_rank",
        "rets_monthly_limit_downs",
        "group_rets",
        "long_short_rets",
        "long_short_net_values",
        "group_net_values",
        "long_short_ret_yearly",
        "long_short_vol_yearly",
        "long_short_info_ratio",
        "long_short_win_times",
        "long_short_win_ratio",
        "retreats",
        "max_retreat",
        "long_short_comments",
        "total_comments",
        "square_rets",
        "cap",
        "cap_value",
        "industry_dummy",
        "industry_codes",
        "industry_codes_str",
        "industry_ws",
        "factors_out",
        "pricloses_copy",
        "flowshares_copy",
    ]

    @classmethod
    @lru_cache(maxsize=None)
    def __init__(cls):
        now = datetime.datetime.now()
        now = datetime.datetime.strftime(now, format="%Y-%m-%d %H:%M:%S")
        cls.homeplace = HomePlace()
        # logger.add('pure_moon'+now+'.log')
        # ç»å¯¹è·¯å¾„å‰ç¼€
        cls.path_prefix = cls.homeplace.daily_data_file
        # è‚¡ç¥¨ä»£ç æ–‡ä»¶
        cls.codes_path = "AllStockCode.mat"
        # äº¤æ˜“æ—¥æœŸæ–‡ä»¶
        cls.tradedays_path = "TradingDate_Daily.mat"
        # ä¸Šå¸‚å¤©æ•°æ–‡ä»¶
        cls.ages_path = "AllStock_DailyListedDate.mat"
        # stæ—¥å­æ ‡å¿—æ–‡ä»¶
        cls.sts_path = "AllStock_DailyST.mat"
        # äº¤æ˜“çŠ¶æ€æ–‡ä»¶
        cls.states_path = "AllStock_DailyStatus.mat"
        # å¤æƒå¼€ç›˜ä»·æ•°æ®æ–‡ä»¶
        cls.opens_path = "AllStock_DailyOpen_dividend.mat"
        # å¤æƒæ”¶ç›˜ä»·æ•°æ®æ–‡ä»¶
        cls.closes_path = "AllStock_DailyClose_dividend.mat"
        # å¤æƒæœ€é«˜ä»·æ•°æ®æ–‡ä»¶
        # cls.highs_path = "Allstock_DailyHigh_dividend.mat"
        # å¤æƒæœ€ä½ä»·æ•°æ®æ–‡ä»¶
        # cls.lows_path = "Allstock_DailyLow_dividend.mat"
        # ä¸å¤æƒæ”¶ç›˜ä»·æ•°æ®æ–‡ä»¶
        cls.pricloses_path = "AllStock_DailyClose.mat"
        # æµé€šè‚¡æœ¬æ•°æ®æ–‡ä»¶
        cls.flowshares_path = "AllStock_DailyAShareNum.mat"
        # æˆäº¤é‡æ•°æ®æ–‡ä»¶
        # cls.amounts_path = "AllStock_DailyVolume.mat"
        # æ¢æ‰‹ç‡æ•°æ®æ–‡ä»¶
        # cls.turnovers_path = "AllStock_DailyTR.mat"
        # å› å­æ•°æ®æ–‡ä»¶
        cls.factors_file = ""
        # å·²ç»ç®—å¥½çš„æœˆåº¦stçŠ¶æ€æ–‡ä»¶
        cls.sts_monthly_file = "sts_monthly.feather"
        # å·²ç»ç®—å¥½çš„æœˆåº¦äº¤æ˜“çŠ¶æ€æ–‡ä»¶
        cls.states_monthly_file = "states_monthly.feather"
        # å·²ç»ç®—å¥½çš„æœˆåº¦st_by10çŠ¶æ€æ–‡ä»¶
        cls.sts_monthly_by10_file = "sts_monthly_by10.feather"
        # å·²ç»ç®—å¥½çš„æœˆåº¦äº¤æ˜“çŠ¶æ€æ–‡ä»¶
        cls.states_monthly_by10_file = "states_monthly_by10.feather"
        # æ‹¼æ¥ç»å¯¹è·¯å¾„å‰ç¼€å’Œç›¸å¯¹è·¯å¾„
        dirs = dir(cls)
        dirs.remove("new_path")
        dirs.remove("set_factor_file")
        dirs = [i for i in dirs if i.endswith("path")] + [
            i for i in dirs if i.endswith("file")
        ]
        dirs_values = list(map(lambda x, y: getattr(x, y), [cls] * len(dirs), dirs))
        dirs_values = list(
            map(lambda x, y: x + y, [cls.path_prefix] * len(dirs), dirs_values)
        )
        for attr, value in zip(dirs, dirs_values):
            setattr(cls, attr, value)
        cls.industry_dummy = (
            pd.read_feather(cls.homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather")
            .set_index("date")
            .groupby("code")
            .resample("M")
            .last()
        )
        cls.industry_dummy = cls.industry_dummy.drop(columns=["code"]).reset_index()
        cls.industry_ws = [f"w{i}" for i in range(1, cls.industry_dummy.shape[1] - 1)]
        col = ["code", "date"] + cls.industry_ws
        cls.industry_dummy.columns = col
        cls.industry_dummy = cls.industry_dummy[
            cls.industry_dummy.date >= pd.Timestamp("2010-01-01")
        ]

    def __call__(self, fallmount=0):
        """è°ƒç”¨å¯¹è±¡åˆ™è¿”å›å› å­å€¼"""
        df = self.factors_out.copy()
        df.columns = list(map(lambda x: x[1], list(df.columns)))
        if fallmount == 0:
            return df
        else:
            return pure_fallmount(df)

    @params_setter(slogan=None)
    def set_factor_file(self, factors_file):
        """è®¾ç½®å› å­æ–‡ä»¶çš„è·¯å¾„ï¼Œå› å­æ–‡ä»¶åˆ—ååº”ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•ä¸ºæ—¶é—´"""
        self.factors_file = factors_file
        self.factors = pd.read_feather(self.factors_file)
        self.factors = self.factors.set_index("date")
        self.factors = self.factors.resample("M").last()
        self.factors = self.factors.reset_index()

    @params_setter(slogan=None)
    def set_factor_df_date_as_index(self, df):
        """è®¾ç½®å› å­æ•°æ®çš„dataframeï¼Œå› å­è¡¨åˆ—ååº”ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•åº”ä¸ºæ—¶é—´"""
        df = df.reset_index()
        df.columns = ["date"] + list(df.columns)[1:]
        self.factors = df
        self.factors = self.factors.set_index("date")
        self.factors = self.factors.resample("M").last()
        self.factors = self.factors.reset_index()

    @params_setter(slogan=None)
    def set_factor_df_wide(self, df):
        """ä»dataframeè¯»å…¥å› å­å®½æ•°æ®"""
        if isinstance(df, pure_fallmount):
            df = df()
        self.factors = df.copy()
        self.factors = self.factors.set_index("date")
        self.factors = self.factors.resample("M").last()
        self.factors = self.factors.reset_index()

    @classmethod
    @lru_cache(maxsize=None)
    @history_remain(slogan=None)
    def new_path(cls, **kwargs):
        """ä¿®æ”¹æ—¥é¢‘æ•°æ®æ–‡ä»¶çš„è·¯å¾„ï¼Œä¾¿äºæ›´æ–°æ•°æ®
        è¦ä¿®æ”¹çš„è·¯å¾„ä»¥å­—å…¸å½¢å¼ä¼ å…¥ï¼Œé”®ä¸ºå±æ€§åï¼Œå€¼ä¸ºè¦è®¾ç½®çš„æ–°è·¯å¾„"""
        for key, value in kwargs.items():
            setattr(cls, key, value)

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def col_and_index(cls):
        """è¯»å–è‚¡ç¥¨ä»£ç ï¼Œä½œä¸ºæœªæ¥è¡¨æ ¼çš„è¡Œå
        è¯»å–äº¤æ˜“æ—¥å†ï¼Œä½œä¸ºæœªæ¥è¡¨æ ¼çš„ç´¢å¼•"""
        cls.codes = list(scio.loadmat(cls.codes_path).values())[3]
        cls.tradedays = list(scio.loadmat(cls.tradedays_path).values())[3].astype(str)
        cls.codes = cls.codes.flatten().tolist()
        cls.codes = list(map(lambda x: x[0], cls.codes))
        cls.tradedays = cls.tradedays[0].tolist()

    @classmethod
    @tool_box(slogan=None)
    def loadmat(cls, path):
        """é‡å†™ä¸€ä¸ªåŠ è½½matæ–‡ä»¶çš„å‡½æ•°ï¼Œä»¥ä½¿ä»£ç æ›´ç®€æ´"""
        return list(scio.loadmat(path).values())[3]

    @classmethod
    @tool_box(slogan=None)
    def make_df(cls, data):
        """å°†è¯»å…¥çš„æ•°æ®ï¼Œå’Œè‚¡ç¥¨ä»£ç ä¸æ—¶é—´æ‹¼æ¥ï¼Œåšæˆdataframe"""
        data = pd.DataFrame(data, columns=cls.codes, index=cls.tradedays)
        data.index = pd.to_datetime(data.index, format="%Y%m%d")
        data = data[data.index >= pd.Timestamp("2010-01-01")]
        return data

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def load_all_files(cls):
        """åŠ å…¨éƒ¨çš„matæ–‡ä»¶"""
        attrs = dir(cls)
        attrs = [i for i in attrs if i.endswith("path")]
        attrs.remove("codes_path")
        attrs.remove("tradedays_path")
        attrs.remove("new_path")
        for attr in attrs:
            new_attr = attr[:-5]
            setattr(cls, new_attr, cls.make_df(cls.loadmat(getattr(cls, attr))))
        cls.opens = cls.opens.replace(0, np.nan)
        cls.closes = cls.closes.replace(0, np.nan)
        cls.pricloses_copy = cls.pricloses.copy()
        cls.flowshares_copy = cls.flowshares.copy()

    @classmethod
    @tool_box(slogan=None)
    def judge_month_st(cls, df):
        """æ¯”è¾ƒä¸€ä¸ªæœˆå†…stçš„å¤©æ•°ï¼Œå¦‚æœstå¤©æ•°å¤šï¼Œå°±åˆ é™¤æœ¬æœˆï¼Œå¦‚æœæ­£å¸¸å¤šï¼Œå°±ä¿ç•™æœ¬æœˆ"""
        st_count = len(df[df == 1])
        normal_count = len(df[df != 1])
        if st_count >= normal_count:
            return 0
        else:
            return 1

    @classmethod
    @tool_box(slogan=None)
    def judge_month_st_by10(cls, df):
        """æ¯”è¾ƒä¸€ä¸ªæœˆå†…æ­£å¸¸äº¤æ˜“çš„å¤©æ•°ï¼Œå¦‚æœå°‘äº10å¤©ï¼Œå°±åˆ é™¤æœ¬æœˆ"""
        normal_count = len(df[df != 1])
        if normal_count < 10:
            return 0
        else:
            return 1

    @classmethod
    @tool_box(slogan=None)
    def judge_month_state(cls, df):
        """æ¯”è¾ƒä¸€ä¸ªæœˆå†…éæ­£å¸¸äº¤æ˜“çš„å¤©æ•°ï¼Œå¦‚æœéæ­£å¸¸äº¤æ˜“å¤©æ•°å¤šï¼Œå°±åˆ é™¤æœ¬æœˆï¼Œå¦åˆ™ä¿ç•™æœ¬æœˆ"""
        abnormal_count = len(df[df == 0])
        normal_count = len(df[df == 1])
        if abnormal_count >= normal_count:
            return 0
        else:
            return 1

    @classmethod
    @tool_box(slogan=None)
    def judge_month_state_by10(cls, df):
        """æ¯”è¾ƒä¸€ä¸ªæœˆå†…æ­£å¸¸äº¤æ˜“å¤©æ•°ï¼Œå¦‚æœå°‘äº10å¤©ï¼Œå°±åˆ é™¤æœ¬æœˆ"""
        normal_count = len(df[df == 1])
        if normal_count < 10:
            return 0
        else:
            return 1

    @classmethod
    @tool_box(slogan=None)
    def read_add(cls, pridf, df, func):
        """ç”±äºæ•°æ®æ›´æ–°ï¼Œè¿‡å»è®¡ç®—çš„æœˆåº¦çŠ¶æ€å¯èƒ½éœ€è¦è¿½åŠ """
        # if not STATES['NO_LOG']:
        #     logger.info(f'this is max_index of pridf{pridf.index.max()}')
        #     logger.info(f'this is max_index of df{df.index.max()}')
        if pridf.index.max() > df.index.max():
            df_add = pridf[pridf.index > df.index.max()]
            df_add = df_add.resample("M").apply(func)
            df = pd.concat([df, df_add])
            return df
        else:
            return df

    @classmethod
    @tool_box(slogan=None)
    def write_feather(cls, df, path):
        """å°†ç®—å‡ºæ¥çš„æ•°æ®å­˜å…¥æœ¬åœ°ï¼Œä»¥å…é€ æˆé‡å¤è¿ç®—"""
        df1 = df.copy()
        df1 = df1.reset_index()
        df1.to_feather(path)

    @classmethod
    @tool_box(slogan=None)
    def daily_to_monthly(cls, pridf, path, func):
        """æŠŠæ—¥åº¦çš„äº¤æ˜“çŠ¶æ€ã€stã€ä¸Šå¸‚å¤©æ•°ï¼Œè½¬åŒ–ä¸ºæœˆåº¦çš„ï¼Œå¹¶ç”Ÿæˆèƒ½å¦äº¤æ˜“çš„åˆ¤æ–­
        è¯»å–æœ¬åœ°å·²ç»ç®—å¥½çš„æ–‡ä»¶ï¼Œå¹¶è¿½åŠ æ–°çš„æ—¶é—´æ®µéƒ¨åˆ†ï¼Œå¦‚æœæœ¬åœ°æ²¡æœ‰å°±ç›´æ¥å…¨éƒ¨é‡æ–°ç®—"""
        try:
            # if not STATES['NO_LOG']:
            #     logger.info('try to read the prepared state file')
            month_df = pd.read_feather(path).set_index("index")
            # if not STATES['NO_LOG']:
            #     logger.info('state file load success')
            month_df = cls.read_add(pridf, month_df, func)
            # if not STATES['NO_LOG']:
            #     logger.info('adding after state file has finish')
            cls.write_feather(month_df, path)
            # if not STATES['NO_LOG']:
            #     logger.info('the feather is new now')
        except Exception as e:
            if not STATES["NO_LOG"]:
                logger.error("error occurs when read state files")
                logger.error(e)
            print("state file rewritingâ€¦â€¦")
            month_df = pridf.resample("M").apply(func)
            cls.write_feather(month_df, path)
        return month_df

    @classmethod
    @tool_box(slogan=None)
    def daily_to_monthly_by10(cls, pridf, path, func):
        """æŠŠæ—¥åº¦çš„äº¤æ˜“çŠ¶æ€ã€stã€ä¸Šå¸‚å¤©æ•°ï¼Œè½¬åŒ–ä¸ºæœˆåº¦çš„ï¼Œå¹¶ç”Ÿæˆèƒ½å¦äº¤æ˜“çš„åˆ¤æ–­
        è¯»å–æœ¬åœ°å·²ç»ç®—å¥½çš„æ–‡ä»¶ï¼Œå¹¶è¿½åŠ æ–°çš„æ—¶é—´æ®µéƒ¨åˆ†ï¼Œå¦‚æœæœ¬åœ°æ²¡æœ‰å°±ç›´æ¥å…¨éƒ¨é‡æ–°ç®—"""
        try:
            month_df = pd.read_feather(path).set_index("date")
            month_df = cls.read_add(pridf, month_df, func)
            cls.write_feather(month_df, path)
        except Exception:
            print("rewriting")
            month_df = pridf.resample("M").apply(func)
            cls.write_feather(month_df, path)
        return month_df

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def judge_month(cls):
        """ç”Ÿæˆä¸€ä¸ªæœˆç»¼åˆåˆ¤æ–­çš„è¡¨æ ¼"""
        cls.sts_monthly = cls.daily_to_monthly(
            cls.sts, cls.sts_monthly_file, cls.judge_month_st
        )
        cls.states_monthly = cls.daily_to_monthly(
            cls.states, cls.states_monthly_file, cls.judge_month_state
        )
        cls.ages_monthly = cls.ages.resample("M").last()
        cls.ages_monthly = np.sign(cls.ages_monthly.applymap(lambda x: x - 60)).replace(
            -1, 0
        )
        cls.tris_monthly = cls.sts_monthly * cls.states_monthly * cls.ages_monthly
        cls.tris_monthly = cls.tris_monthly.replace(0, np.nan)

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def judge_month_by10(cls):
        """ç”Ÿæˆä¸€ä¸ªæœˆç»¼åˆåˆ¤æ–­çš„è¡¨æ ¼"""
        cls.sts_monthly = cls.daily_to_monthly(
            cls.sts, cls.sts_monthly_by10_file, cls.judge_month_st_by10
        )
        cls.states_monthly = cls.daily_to_monthly(
            cls.states, cls.states_monthly_by10_file, cls.judge_month_state_by10
        )
        cls.ages_monthly = cls.ages.resample("M").last()
        cls.ages_monthly = np.sign(cls.ages_monthly.applymap(lambda x: x - 60)).replace(
            -1, 0
        )
        cls.tris_monthly = cls.sts_monthly * cls.states_monthly * cls.ages_monthly
        cls.tris_monthly = cls.tris_monthly.replace(0, np.nan)

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def get_rets_month(cls):
        """è®¡ç®—æ¯æœˆçš„æ”¶ç›Šç‡ï¼Œå¹¶æ ¹æ®æ¯æœˆåšå‡ºäº¤æ˜“çŠ¶æ€ï¼Œåšå‡ºåˆ å‡"""
        cls.opens_monthly = cls.opens.resample("M").first()
        cls.closes_monthly = cls.closes.resample("M").last()
        cls.rets_monthly = (cls.closes_monthly - cls.opens_monthly) / cls.opens_monthly
        cls.rets_monthly = cls.rets_monthly * cls.tris_monthly
        cls.rets_monthly = cls.rets_monthly.stack().reset_index()
        cls.rets_monthly.columns = ["date", "code", "ret"]

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def neutralize_factors(cls, df):
        """ç»„å†…å¯¹å› å­è¿›è¡Œå¸‚å€¼ä¸­æ€§åŒ–"""
        industry_codes = list(df.columns)
        industry_codes = [i for i in industry_codes if i.startswith("w")]
        industry_codes_str = "+".join(industry_codes)
        ols_result = smf.ols("fac~cap_size+" + industry_codes_str, data=df).fit()
        ols_w = ols_result.params["cap_size"]
        ols_b = ols_result.params["Intercept"]
        ols_bs = {}
        for ind in industry_codes:
            ols_bs[ind] = ols_result.params[ind]
        df.fac = df.fac - ols_w * df.cap_size - ols_b
        for k, v in ols_bs.items():
            df.fac = df.fac - v * df[k]
        df = df[["fac"]]
        return df

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def get_log_cap(cls, boxcox=False):
        """è·å¾—å¯¹æ•°å¸‚å€¼"""
        try:
            cls.pricloses = cls.pricloses.replace(0, np.nan)
            cls.flowshares = cls.flowshares.replace(0, np.nan)
            cls.pricloses = cls.pricloses.resample("M").last()
        except Exception:
            cls.pricloses = cls.pricloses_copy.copy()
            cls.pricloses = cls.pricloses.replace(0, np.nan)
            cls.flowshares = cls.flowshares.replace(0, np.nan)
            cls.pricloses = cls.pricloses.resample("M").last()
        cls.pricloses = cls.pricloses.stack().reset_index()
        cls.pricloses.columns = ["date", "code", "priclose"]
        try:
            cls.flowshares = cls.flowshares.resample("M").last()
        except Exception:
            cls.flowshares = cls.flowshares_copy.copy()
            cls.flowshares = cls.flowshares.resample("M").last()
        cls.flowshares = cls.flowshares.stack().reset_index()
        cls.flowshares.columns = ["date", "code", "flowshare"]
        cls.flowshares = pd.merge(cls.flowshares, cls.pricloses, on=["date", "code"])
        cls.cap = cls.flowshares.assign(
            cap_size=cls.flowshares.flowshare * cls.flowshares.priclose
        )
        if boxcox:

            def single(x):
                x.cap_size = ss.boxcox(x.cap_size)[0]
                return x

            cls.cap = cls.cap.groupby(["date"]).apply(single)
        else:
            cls.cap["cap_size"] = np.log(cls.cap["cap_size"])

    @main_process(slogan=None)
    def get_neutral_factors(self):
        """å¯¹å› å­è¿›è¡Œå¸‚å€¼ä¸­æ€§åŒ–"""
        self.factors = self.factors.set_index("date")
        self.factors.index = self.factors.index + pd.DateOffset(months=1)
        self.factors = self.factors.resample("M").last()
        last_date = self.tris_monthly.index.max() + pd.DateOffset(months=1)
        last_date = last_date + pd.tseries.offsets.MonthEnd()
        add_tail = pd.DataFrame(1, index=[last_date], columns=self.tris_monthly.columns)
        tris_monthly = pd.concat([self.tris_monthly, add_tail])
        self.factors = self.factors * tris_monthly
        self.factors.index = self.factors.index - pd.DateOffset(months=1)
        self.factors = self.factors.resample("M").last()
        self.factors = self.factors.stack().reset_index()
        self.factors.columns = ["date", "code", "fac"]
        self.factors = pd.merge(
            self.factors, self.cap, how="inner", on=["date", "code"]
        )
        self.factors = pd.merge(self.factors, self.industry_dummy, on=["date", "code"])
        self.factors = self.factors.set_index(["date", "code"])
        self.factors = self.factors.groupby(["date"]).apply(self.neutralize_factors)
        self.factors = self.factors.reset_index()

    @main_process(slogan=None)
    def deal_with_factors(self):
        """åˆ é™¤ä¸ç¬¦åˆäº¤æ˜“æ¡ä»¶çš„å› å­æ•°æ®"""
        self.factors = self.factors.set_index("date")
        self.factors_out = self.factors.copy()
        self.factors.index = self.factors.index + pd.DateOffset(months=1)
        self.factors = self.factors.resample("M").last()
        self.factors = self.factors * self.tris_monthly
        self.factors = self.factors.stack().reset_index()
        self.factors.columns = ["date", "code", "fac"]

    @main_process(slogan=None)
    def deal_with_factors_after_neutralize(self):
        """ä¸­æ€§åŒ–ä¹‹åçš„å› å­å¤„ç†æ–¹æ³•"""
        self.factors = self.factors.set_index(["date", "code"])
        self.factors = self.factors.unstack()
        self.factors_out = self.factors.copy()
        self.factors.index = self.factors.index + pd.DateOffset(months=1)
        self.factors = self.factors.resample("M").last()
        self.factors.columns = list(map(lambda x: x[1], list(self.factors.columns)))
        self.factors = self.factors.stack().reset_index()
        self.factors.columns = ["date", "code", "fac"]

    @classmethod
    @tool_box(slogan=None)
    def find_limit(cls, df, up=1):
        """è®¡ç®—æ¶¨è·Œå¹…è¶…è¿‡9.8%çš„è‚¡ç¥¨ï¼Œå¹¶å°†å…¶å­˜å‚¨è¿›ä¸€ä¸ªé•¿åˆ—è¡¨é‡Œ
        å…¶ä¸­æ—¶é—´åˆ—ï¼Œä¸ºæŸæœˆçš„æœ€åä¸€å¤©ï¼›æ¶¨åœæ—¥è™½ç„¶ä¸ºä¸‹æœˆåˆç¬¬ä¸€å¤©ï¼Œä½†è¿™é‡Œæ ‡æ³¨çš„æ—¶é—´ç»Ÿä¸€ä¸ºä¸Šæœˆæœ€åä¸€å¤©"""
        limit_df = np.sign(df.applymap(lambda x: x - up * 0.098)).replace(
            -1 * up, np.nan
        )
        limit_df = limit_df.stack().reset_index()
        limit_df.columns = ["date", "code", "limit_up_signal"]
        limit_df = limit_df[["date", "code"]]
        return limit_df

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def get_limit_ups_downs(cls):
        """æ‰¾æœˆåˆç¬¬ä¸€å¤©å°±æ¶¨åœ"""
        """æˆ–è€…æ˜¯æœˆæœ«è·Œåœçš„è‚¡ç¥¨"""
        cls.opens_monthly_shift = cls.opens_monthly.copy()
        cls.opens_monthly_shift = cls.opens_monthly_shift.shift(-1)
        cls.rets_monthly_begin = (
            cls.opens_monthly_shift - cls.closes_monthly
        ) / cls.closes_monthly
        cls.closes2_monthly = cls.closes.shift(1).resample("M").last()
        cls.rets_monthly_last = (
            cls.closes_monthly - cls.closes2_monthly
        ) / cls.closes2_monthly
        cls.limit_ups = cls.find_limit(cls.rets_monthly_begin, up=1)
        cls.limit_downs = cls.find_limit(cls.rets_monthly_last, up=-1)

    @classmethod
    @tool_box(slogan=None)
    def get_ic_rankic(cls, df):
        """è®¡ç®—ICå’ŒRankIC"""
        df1 = df[["ret", "fac"]]
        ic = df1.corr(method="pearson").iloc[0, 1]
        rankic = df1.corr(method="spearman").iloc[0, 1]
        df2 = pd.DataFrame({"ic": [ic], "rankic": [rankic]})
        return df2

    @classmethod
    @tool_box(slogan=None)
    def get_icir_rankicir(cls, df):
        """è®¡ç®—ICIRå’ŒRankICIR"""
        ic = df.ic.mean()
        rankic = df.rankic.mean()
        icir = ic / np.std(df.ic) * (12 ** (0.5))
        rankicir = rankic / np.std(df.rankic) * (12 ** (0.5))
        return pd.DataFrame(
            {"IC": [ic], "ICIR": [icir], "RankIC": [rankic], "RankICIR": [rankicir]},
            index=["è¯„ä»·æŒ‡æ ‡"],
        )

    @classmethod
    @tool_box(slogan=None)
    def get_ic_icir_and_rank(cls, df):
        """è®¡ç®—ICã€ICIRã€RankICã€RankICIR"""
        df1 = df.groupby("date").apply(cls.get_ic_rankic)
        df2 = cls.get_icir_rankicir(df1)
        df2 = df2.T
        dura = (df.date.max() - df.date.min()).days / 365
        t_value = df2.iloc[3, 0] * (dura ** (1 / 2))
        df3 = pd.DataFrame({"è¯„ä»·æŒ‡æ ‡": [t_value]}, index=["RankICå‡å€¼tå€¼"])
        df4 = pd.concat([df2, df3])
        return df4

    @classmethod
    @tool_box(slogan=None)
    def get_groups(cls, df, groups_num):
        """ä¾æ®å› å­å€¼ï¼Œåˆ¤æ–­æ˜¯åœ¨ç¬¬å‡ ç»„"""
        if "group" in list(df.columns):
            df = df.drop(columns=["group"])
        df = df.sort_values(["fac"], ascending=True)
        each_group = round(df.shape[0] / groups_num)
        l = list(
            map(
                lambda x, y: [x] * y,
                list(range(1, groups_num + 1)),
                [each_group] * groups_num,
            )
        )
        l = reduce(lambda x, y: x + y, l)
        if len(l) < df.shape[0]:
            l = l + [groups_num] * (df.shape[0] - len(l))
        l = l[: df.shape[0]]
        df.insert(0, "group", l)
        return df

    @classmethod
    @tool_box(slogan=None)
    def next_month_end(cls, x):
        """æ‰¾åˆ°ä¸‹ä¸ªæœˆæœ€åä¸€å¤©"""
        x1 = x = x + relativedelta(months=1)
        while x1.month == x.month:
            x1 = x1 + relativedelta(days=1)
        return x1 - relativedelta(days=1)

    @classmethod
    @tool_box(slogan=None)
    def limit_old_to_new(cls, limit, data):
        """è·å–è·Œåœè‚¡åœ¨æ—§æœˆçš„ç»„å·ï¼Œç„¶åå°†æ—¥æœŸè°ƒæ•´åˆ°æ–°æœˆé‡Œ
        æ¶¨åœè‚¡åˆ™è·å¾—æ–°æœˆé‡Œæ¶¨åœè‚¡çš„ä»£ç å’Œæ—¶é—´ï¼Œç„¶åç›´æ¥åˆ å»"""
        data1 = data.copy()
        data1 = data1.reset_index()
        data1.columns = ["data_index"] + list(data1.columns)[1:]
        old = pd.merge(limit, data1, how="inner", on=["date", "code"])
        old = old.set_index("data_index")
        old = old[["group", "date", "code"]]
        old.date = list(map(cls.next_month_end, list(old.date)))
        return old

    @main_process(slogan=None)
    def get_data(self, groups_num):
        """æ‹¼æ¥å› å­æ•°æ®å’Œæ¯æœˆæ”¶ç›Šç‡æ•°æ®ï¼Œå¹¶å¯¹æ¶¨åœå’Œè·Œåœè‚¡åŠ ä»¥å¤„ç†"""
        self.data = pd.merge(
            self.rets_monthly, self.factors, how="inner", on=["date", "code"]
        )
        self.ic_icir_and_rank = self.get_ic_icir_and_rank(self.data)
        self.data = self.data.groupby("date").apply(
            lambda x: self.get_groups(x, groups_num)
        )
        self.data = self.data.reset_index(drop=True)
        limit_ups_object = self.limit_old_to_new(self.limit_ups, self.data)
        limit_downs_object = self.limit_old_to_new(self.limit_downs, self.data)
        self.data = self.data.drop(limit_ups_object.index)
        rets_monthly_limit_downs = pd.merge(
            self.rets_monthly, limit_downs_object, how="inner", on=["date", "code"]
        )
        self.data = pd.concat([self.data, rets_monthly_limit_downs])

    @main_process(slogan=None)
    def select_data_time(self, time_start, time_end):
        """ç­›é€‰ç‰¹å®šçš„æ—¶é—´æ®µ"""
        if time_start:
            self.data = self.data[self.data.date >= time_start]
        if time_end:
            self.data = self.data[self.data.date <= time_end]

    @tool_box(slogan=None)
    def make_start_to_one(self, l):
        """è®©å‡€å€¼åºåˆ—çš„ç¬¬ä¸€ä¸ªæ•°å˜æˆ1"""
        min_date = self.factors.date.min()
        add_date = min_date - relativedelta(days=min_date.day)
        add_l = pd.Series([1], index=[add_date])
        l = pd.concat([add_l, l])
        return l

    @tool_box(slogan=None)
    def to_group_ret(self, l):
        """æ¯ä¸€ç»„çš„å¹´åŒ–æ”¶ç›Šç‡"""
        ret = l[-1] ** (12 / len(l)) - 1
        return ret

    @main_process(slogan=None)
    def get_group_rets_net_values(self, groups_num=10, value_weighted=False):
        """è®¡ç®—ç»„å†…æ¯ä¸€æœŸçš„å¹³å‡æ”¶ç›Šï¼Œç”Ÿæˆæ¯æ—¥æ”¶ç›Šç‡åºåˆ—å’Œå‡€å€¼åºåˆ—"""
        if value_weighted:
            cap_value = self.pricloses_copy * self.flowshares_copy
            cap_value = cap_value.resample("M").last().shift(1)
            cap_value = cap_value * self.tris_monthly
            # cap_value=np.log(cap_value)
            cap_value = cap_value.stack().reset_index()
            cap_value.columns = ["date", "code", "cap_value"]
            self.data = pd.merge(self.data, cap_value, on=["date", "code"])

            def in_g(df):
                df.cap_value = df.cap_value / df.cap_value.sum()
                df.ret = df.ret * df.cap_value
                return df.ret.sum()

            self.group_rets = self.data.groupby(["date", "group"]).apply(in_g)
        else:
            self.group_rets = self.data.groupby(["date", "group"]).apply(
                lambda x: x.ret.mean()
            )
        # dropnaæ˜¯å› ä¸ºå¦‚æœè‚¡ç¥¨è¡Œæƒ…æ•°æ®æ¯”å› å­æ•°æ®çš„æˆªæ­¢æ—¥æœŸæ™šï¼Œè€Œæœ€åä¸€ä¸ªæœˆå‘ç”Ÿæœˆåˆè·Œåœæ—¶ï¼Œä¼šé€ æˆæœ€åæŸç»„å¤šå‡ºä¸€ä¸ªæœˆçš„æ•°æ®
        self.group_rets = self.group_rets.unstack()
        self.group_rets = self.group_rets[
            self.group_rets.index <= self.factors.date.max()
        ]
        self.group_rets.columns = list(map(str, list(self.group_rets.columns)))
        self.group_rets = self.group_rets.add_prefix("group")
        self.long_short_rets = (
            self.group_rets["group1"] - self.group_rets["group" + str(groups_num)]
        )
        self.long_short_net_values = (self.long_short_rets + 1).cumprod()
        if self.long_short_net_values[-1] <= self.long_short_net_values[0]:
            self.long_short_rets = (
                self.group_rets["group" + str(groups_num)] - self.group_rets["group1"]
            )
            self.long_short_net_values = (self.long_short_rets + 1).cumprod()
        self.long_short_net_values = self.make_start_to_one(self.long_short_net_values)
        self.group_rets = self.group_rets.assign(long_short=self.long_short_rets)
        self.group_net_values = self.group_rets.applymap(lambda x: x + 1)
        self.group_net_values = self.group_net_values.cumprod()
        self.group_net_values = self.group_net_values.apply(self.make_start_to_one)
        a = groups_num ** (0.5)
        # åˆ¤æ–­æ˜¯å¦è¦ä¸¤ä¸ªå› å­ç”»è¡¨æ ¼
        if a == int(a):
            self.square_rets = (
                self.group_net_values.iloc[:, :-1].apply(self.to_group_ret).to_numpy()
            )
            self.square_rets = self.square_rets.reshape((int(a), int(a)))
            self.square_rets = pd.DataFrame(
                self.square_rets,
                columns=list(range(1, int(a) + 1)),
                index=list(range(1, int(a) + 1)),
            )
            print("è¿™æ˜¯self.square_rets", self.square_rets)

    @main_process(slogan=None)
    def get_long_short_comments(self, on_paper=False):
        """è®¡ç®—å¤šç©ºå¯¹å†²çš„ç›¸å…³è¯„ä»·æŒ‡æ ‡
        åŒ…æ‹¬å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€æœˆåº¦èƒœç‡ã€æœ€å¤§å›æ’¤ç‡"""
        self.long_short_ret_yearly = (
            self.long_short_net_values[-1] ** (12 / len(self.long_short_net_values)) - 1
        )
        self.long_short_vol_yearly = np.std(self.long_short_rets) * (12**0.5)
        self.long_short_info_ratio = (
            self.long_short_ret_yearly / self.long_short_vol_yearly
        )
        self.long_short_win_times = len(self.long_short_rets[self.long_short_rets > 0])
        self.long_short_win_ratio = self.long_short_win_times / len(
            self.long_short_rets
        )
        self.max_retreat = -(
            self.long_short_net_values / self.long_short_net_values.expanding(1).max()
            - 1
        ).min()
        if on_paper:
            self.long_short_comments = pd.DataFrame(
                {
                    "è¯„ä»·æŒ‡æ ‡": [
                        self.long_short_ret_yearly,
                        self.long_short_vol_yearly,
                        self.long_short_info_ratio,
                        self.long_short_win_ratio,
                        self.max_retreat,
                    ]
                },
                index=["å¹´åŒ–æ”¶ç›Šç‡", "å¹´åŒ–æ³¢åŠ¨ç‡", "æ”¶ç›Šæ³¢åŠ¨æ¯”", "æœˆåº¦èƒœç‡", "æœ€å¤§å›æ’¤ç‡"],
            )
        else:
            self.long_short_comments = pd.DataFrame(
                {
                    "è¯„ä»·æŒ‡æ ‡": [
                        self.long_short_ret_yearly,
                        self.long_short_vol_yearly,
                        self.long_short_info_ratio,
                        self.long_short_win_ratio,
                        self.max_retreat,
                    ]
                },
                index=["å¹´åŒ–æ”¶ç›Šç‡", "å¹´åŒ–æ³¢åŠ¨ç‡", "ä¿¡æ¯æ¯”ç‡", "æœˆåº¦èƒœç‡", "æœ€å¤§å›æ’¤ç‡"],
            )

    @main_process(slogan=None)
    def get_total_comments(self):
        """ç»¼åˆICã€ICIRã€RankICã€RankICIR,å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€æœˆåº¦èƒœç‡ã€æœ€å¤§å›æ’¤ç‡"""
        self.total_comments = pd.concat(
            [self.ic_icir_and_rank, self.long_short_comments]
        )

    @main_process(slogan=None)
    def plot_net_values(self, y2, filename):
        """ä½¿ç”¨matplotlibæ¥ç”»å›¾ï¼Œy2ä¸ºæ˜¯å¦å¯¹å¤šç©ºç»„åˆé‡‡ç”¨åŒyè½´"""
        self.group_net_values.plot(secondary_y=y2, rot=60)
        filename_path = filename + ".png"
        if not STATES["NO_SAVE"]:
            plt.savefig(filename_path)

    @main_process(slogan=None)
    def plotly_net_values(self, filename):
        """ä½¿ç”¨plotly.expressç”»å›¾"""
        fig = pe.line(self.group_net_values)
        filename_path = filename + ".html"
        pio.write_html(fig, filename_path, auto_open=True)

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def prerpare(cls):
        """é€šç”¨æ•°æ®å‡†å¤‡"""
        cls.col_and_index()
        cls.load_all_files()
        cls.judge_month()
        cls.get_rets_month()

    @kk.desktop_sender(title="å˜¿ï¼Œå›æµ‹ç»“æŸå•¦ï½ğŸ—“")
    def run(
        self,
        groups_num=10,
        neutralize=False,
        boxcox=False,
        value_weighted=False,
        y2=False,
        plt_plot=True,
        plotly_plot=False,
        filename="åˆ†ç»„å‡€å€¼å›¾",
        time_start=None,
        time_end=None,
        print_comments=True,
        comments_writer=None,
        net_values_writer=None,
        rets_writer=None,
        comments_sheetname=None,
        net_values_sheetname=None,
        rets_sheetname=None,
        on_paper=False,
        sheetname=None,
    ):
        """è¿è¡Œå›æµ‹éƒ¨åˆ†"""
        if comments_writer and not (comments_sheetname or sheetname):
            raise IOError("æŠŠtotal_commentsè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
        if net_values_writer and not (net_values_sheetname or sheetname):
            raise IOError("æŠŠgroup_net_valuesè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
        if rets_writer and not (rets_sheetname or sheetname):
            raise IOError("æŠŠgroup_retsè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
        if neutralize:
            self.get_log_cap()
            self.get_neutral_factors()
            self.deal_with_factors_after_neutralize()
        elif boxcox:
            self.get_log_cap(boxcox=True)
            self.get_neutral_factors()
            self.deal_with_factors_after_neutralize()
        else:
            self.deal_with_factors()
        self.get_limit_ups_downs()
        self.get_data(groups_num)
        self.select_data_time(time_start, time_end)
        self.get_group_rets_net_values(
            groups_num=groups_num, value_weighted=value_weighted
        )
        self.get_long_short_comments(on_paper=on_paper)
        self.get_total_comments()
        if plt_plot:
            if not STATES["NO_PLOT"]:
                if filename:
                    self.plot_net_values(y2=y2, filename=filename)
                else:
                    self.plot_net_values(
                        y2=y2,
                        filename=self.factors_file.split(".")[-2].split("/")[-1]
                        + str(groups_num)
                        + "åˆ†ç»„",
                    )
                plt.show()
        if plotly_plot:
            if not STATES["NO_PLOT"]:
                if filename:
                    self.plotly_net_values(filename=filename)
                else:
                    self.plotly_net_values(
                        filename=self.factors_file.split(".")[-2].split("/")[-1]
                        + str(groups_num)
                        + "åˆ†ç»„"
                    )
        if print_comments:
            if not STATES["NO_COMMENT"]:
                print(self.total_comments)
        if sheetname:
            if comments_writer:
                total_comments = self.total_comments.copy()
                tc = list(total_comments.è¯„ä»·æŒ‡æ ‡)
                tc[0] = str(round(tc[0] * 100, 2)) + "%"
                tc[1] = str(round(tc[1], 2))
                tc[2] = str(round(tc[2] * 100, 2)) + "%"
                tc[3] = str(round(tc[3], 2))
                tc[4] = str(round(tc[4], 2))
                tc[5] = str(round(tc[5] * 100, 2)) + "%"
                tc[6] = str(round(tc[6] * 100, 2)) + "%"
                tc[7] = str(round(tc[7], 2))
                tc[8] = str(round(tc[8] * 100, 2)) + "%"
                tc[9] = str(round(tc[9] * 100, 2)) + "%"
                new_total_comments = pd.DataFrame(
                    {sheetname: tc}, index=total_comments.index
                )
                new_total_comments.T.to_excel(comments_writer, sheet_name=sheetname)
            if net_values_writer:
                groups_net_values = self.group_net_values.copy()
                groups_net_values.index = groups_net_values.index.strftime("%Y/%m/%d")
                groups_net_values.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(groups_net_values.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                groups_net_values.to_excel(net_values_writer, sheet_name=sheetname)
            if rets_writer:
                group_rets = self.group_rets.copy()
                group_rets.index = group_rets.index.strftime("%Y/%m/%d")
                group_rets.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(group_rets.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                group_rets.to_excel(rets_writer, sheet_name=sheetname)
        else:
            if comments_writer and comments_sheetname:
                total_comments = self.total_comments.copy()
                tc = list(total_comments.è¯„ä»·æŒ‡æ ‡)
                tc[0] = str(round(tc[0] * 100, 2)) + "%"
                tc[1] = str(round(tc[1], 2))
                tc[2] = str(round(tc[2] * 100, 2)) + "%"
                tc[3] = str(round(tc[3], 2))
                tc[4] = str(round(tc[4], 2))
                tc[5] = str(round(tc[5] * 100, 2)) + "%"
                tc[6] = str(round(tc[6] * 100, 2)) + "%"
                tc[7] = str(round(tc[7], 2))
                tc[8] = str(round(tc[8] * 100, 2)) + "%"
                tc[9] = str(round(tc[9] * 100, 2)) + "%"
                new_total_comments = pd.DataFrame(
                    {comments_sheetname: tc}, index=total_comments.index
                )
                new_total_comments.T.to_excel(
                    comments_writer, sheet_name=comments_sheetname
                )
            if net_values_writer and net_values_sheetname:
                groups_net_values = self.group_net_values.copy()
                groups_net_values.index = groups_net_values.index.strftime("%Y/%m/%d")
                groups_net_values.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(groups_net_values.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                groups_net_values.to_excel(
                    net_values_writer, sheet_name=net_values_sheetname
                )
            if rets_writer and rets_sheetname:
                group_rets = self.group_rets.copy()
                group_rets.index = group_rets.index.strftime("%Y/%m/%d")
                group_rets.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(group_rets.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                group_rets.to_excel(rets_writer, sheet_name=rets_sheetname)


class pure_moonnight(object):
    """å°è£…é€‰è‚¡æ¡†æ¶"""

    __slots__ = ["shen"]

    def __init__(
        self,
        factors: pd.DataFrame,
        groups_num: int = 10,
        neutralize: bool = 0,
        boxcox: bool = 1,
        by10: bool = 0,
        value_weighted: bool = 0,
        y2: bool = 0,
        plt_plot: bool = 1,
        plotly_plot: bool = 0,
        filename: str = "åˆ†ç»„å‡€å€¼å›¾",
        time_start: int = None,
        time_end: int = None,
        print_comments: bool = 1,
        comments_writer: pd.ExcelWriter = None,
        net_values_writer: pd.ExcelWriter = None,
        rets_writer: pd.ExcelWriter = None,
        comments_sheetname: str = None,
        net_values_sheetname: str = None,
        rets_sheetname: str = None,
        on_paper: bool = 0,
        sheetname: str = None,
    ) -> None:
        """ä¸€é”®å›æµ‹æ¡†æ¶ï¼Œæµ‹è¯•å•å› å­çš„æœˆé¢‘è°ƒä»“çš„åˆ†ç»„è¡¨ç°
        æ¯æœˆæœˆåº•è®¡ç®—å› å­å€¼ï¼Œæœˆåˆç¬¬ä¸€å¤©å¼€ç›˜æ—¶ä¹°å…¥ï¼Œæœˆæœ«æ”¶ç›˜æœ€åä¸€å¤©æ”¶ç›˜æ—¶å–å‡º
        å‰”é™¤ä¸Šå¸‚ä¸è¶³60å¤©çš„ï¼Œåœç‰Œå¤©æ•°è¶…è¿‡ä¸€åŠçš„ï¼Œstå¤©æ•°è¶…è¿‡ä¸€åŠçš„
        æœˆæœ«æ”¶ç›˜è·Œåœçš„ä¸å–å‡ºï¼Œæœˆåˆå¼€ç›˜æ¶¨åœçš„ä¸ä¹°å…¥
        ç”±æœ€å¥½ç»„å’Œæœ€å·®ç»„çš„å¤šç©ºç»„åˆæ„æˆå¤šç©ºå¯¹å†²ç»„

        Parameters
        ----------
        factors : pd.DataFrame
            è¦ç”¨äºæ£€æµ‹çš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
        groups_num : int, optional
            åˆ†ç»„æ•°é‡, by default 10
        neutralize : bool, optional
            å¯¹æµé€šå¸‚å€¼å–è‡ªç„¶å¯¹æ•°ï¼Œä»¥å®Œæˆè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 0
        boxcox : bool, optional
            å¯¹æµé€šå¸‚å€¼åšæˆªé¢boxcoxå˜æ¢ï¼Œä»¥å®Œæˆè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 1
        by10 : bool, optional
            æ¯å¤©stå’Œåœç‰ŒçŠ¶æ€æœˆåº¦åŒ–æ—¶ï¼Œä»¥10å¤©ä½œä¸ºæ ‡å‡†, by default 0
        value_weighted : bool, optional
            æ˜¯å¦ç”¨æµé€šå¸‚å€¼åŠ æƒ, by default 0
        y2 : bool, optional
            ç”»å›¾æ—¶æ˜¯å¦å¯ç”¨ç¬¬äºŒyè½´, by default 0
        plt_plot : bool, optional
            å°†åˆ†ç»„å‡€å€¼æ›²çº¿ç”¨matplotlibç”»å‡ºæ¥, by default 1
        plotly_plot : bool, optional
            å°†åˆ†ç»„å‡€å€¼æ›²çº¿ç”¨plotlyç”»å‡ºæ¥, by default 0
        filename : str, optional
            åˆ†ç»„å‡€å€¼æ›²çº¿çš„å›¾ä¿å­˜çš„åç§°, by default "åˆ†ç»„å‡€å€¼å›¾"
        time_start : int, optional
            å›æµ‹èµ·å§‹æ—¶é—´ï¼ˆæ­¤å‚æ•°å·²åºŸå¼ƒï¼Œè¯·åœ¨å› å­ä¸Šç›´æ¥æˆªæ–­ï¼‰, by default None
        time_end : int, optional
            å›æµ‹ç»ˆæ­¢æ—¶é—´ï¼ˆæ­¤å‚æ•°å·²åºŸå¼ƒï¼Œè¯·åœ¨å› å­ä¸Šç›´æ¥æˆªæ–­ï¼‰, by default None
        print_comments : bool, optional
            æ˜¯å¦æ‰“å°å‡ºè¯„ä»·æŒ‡æ ‡, by default 1
        comments_writer : pd.ExcelWriter, optional
            ç”¨äºè®°å½•è¯„ä»·æŒ‡æ ‡çš„xlsxæ–‡ä»¶, by default None
        net_values_writer : pd.ExcelWriter, optional
            ç”¨äºè®°å½•å‡€å€¼åºåˆ—çš„xlsxæ–‡ä»¶, by default None
        rets_writer : pd.ExcelWriter, optional
            ç”¨äºè®°å½•æ”¶ç›Šç‡åºåˆ—çš„xlsxæ–‡ä»¶, by default None
        comments_sheetname : str, optional
            åœ¨è®°å½•è¯„ä»·æŒ‡æ ‡çš„xlsxæ–‡ä»¶ä¸­ï¼Œè¯¥å·¥ä½œè¡¨çš„åç§°, by default None
        net_values_sheetname : str, optional
            åœ¨è®°å½•å‡€å€¼åºåˆ—çš„xlsxæ–‡ä»¶ä¸­ï¼Œè¯¥å·¥ä½œè¡¨çš„åç§°, by default None
        rets_sheetname : str, optional
            åœ¨è®°å½•æ”¶ç›Šç‡åºåˆ—çš„xlsxæ–‡ä»¶ä¸­ï¼Œè¯¥å·¥ä½œè¡¨çš„åç§°, by default None
        on_paper : bool, optional
            ä½¿ç”¨å­¦æœ¯åŒ–è¯„ä»·æŒ‡æ ‡, by default 0
        sheetname : str, optional
            å„ä¸ªpd.Excelwriterä¸­å·¥ä½œè¡¨çš„ç»Ÿä¸€åç§°, by default None
        """

        if isinstance(factors, pure_fallmount):
            factors = factors().copy()
        self.shen = pure_moon()
        self.shen.set_factor_df_date_as_index(factors)
        self.shen.prerpare()
        self.shen.run(
            groups_num=groups_num,
            neutralize=neutralize,
            boxcox=boxcox,
            value_weighted=value_weighted,
            y2=y2,
            plt_plot=plt_plot,
            plotly_plot=plotly_plot,
            filename=filename,
            time_start=time_start,
            time_end=time_end,
            print_comments=print_comments,
            comments_writer=comments_writer,
            net_values_writer=net_values_writer,
            rets_writer=rets_writer,
            comments_sheetname=comments_sheetname,
            net_values_sheetname=net_values_sheetname,
            rets_sheetname=rets_sheetname,
            on_paper=on_paper,
            sheetname=sheetname,
        )

    def __call__(self) -> pd.DataFrame:
        """å¦‚æœåšäº†è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ï¼Œåˆ™è¿”å›è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­æ•°æ®

        Returns
        -------
        `pd.DataFrame`
            å¦‚æœåšäº†è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ï¼Œåˆ™è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­æ•°æ®ï¼Œå¦åˆ™è¿”å›åŸå› å­æ•°æ®
        """
        df = self.shen.factors_out.copy()
        df.columns = list(map(lambda x: x[1], list(df.columns)))
        return df


class pure_fall(object):
    # DONEï¼šä¿®æ”¹ä¸ºå› å­æ–‡ä»¶åå¯ä»¥å¸¦â€œæ—¥é¢‘_â€œï¼Œä¹Ÿå¯ä»¥ä¸å¸¦â€œæ—¥é¢‘_â€œ
    def __init__(self, daily_path: str) -> None:
        """ä¸€ä¸ªä½¿ç”¨mysqlä¸­çš„åˆ†é’Ÿæ•°æ®ï¼Œæ¥æ›´æ–°å› å­å€¼çš„æ¡†æ¶

        Parameters
        ----------
        daily_path : str
            æ—¥é¢‘å› å­å€¼å­˜å‚¨æ–‡ä»¶çš„åå­—ï¼Œè¯·ä»¥'.feather'ç»“å°¾
        """
        self.homeplace = HomePlace()
        # å°†åˆ†é’Ÿæ•°æ®æ‹¼æˆä¸€å¼ æ—¥é¢‘å› å­è¡¨
        self.daily_factors = None
        self.daily_factors_path = self.homeplace.factor_data_file + "æ—¥é¢‘_" + daily_path

    def __call__(self, monthly=False):
        """ä¸ºäº†é˜²æ­¢å±æ€§åå¤ªå¤šï¼Œå¿˜è®°äº†è¦è°ƒç”¨å“ªä¸ªæ‰æ˜¯ç»“æœï¼Œå› æ­¤å¯ä»¥ç›´æ¥è¾“å‡ºæœˆåº¦æ•°æ®è¡¨"""
        if monthly:
            return self.monthly_factors.copy()
        else:
            try:
                return self.daily_factors.copy()
            except Exception:
                return self.monthly_factors.copy()

    def __add__(self, selfas):
        """å°†å‡ ä¸ªå› å­æˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œå› å­å€¼ç›¸åŠ """
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac2s = []
        if not isinstance(selfas, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfas} is changed into Iterable")
            selfas = (selfas,)
        for selfa in selfas:
            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2s.append(fac2)
        for i in fac2s:
            fac1 = fac1 + i
        new_pure = pure_fall()
        new_pure.monthly_factors = fac1
        return new_pure

    def __mul__(self, selfas):
        """å°†å‡ ä¸ªå› å­æ¨ªæˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œä½¿å…¶éƒ½ä¸ºæ­£æ•°ï¼Œç„¶åå› å­å€¼ç›¸ä¹˜"""
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac1 = fac1 - fac1.min()
        fac2s = []
        if not isinstance(selfas, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfas} is changed into Iterable")
            selfas = (selfas,)
        for selfa in selfas:
            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2 = fac2 - fac2.min()
            fac2s.append(fac2)
        for i in fac2s:
            fac1 = fac1 * i
        new_pure = pure_fall()
        new_pure.monthly_factors = fac1
        return new_pure

    def __truediv__(self, selfa):
        """ä¸¤ä¸ªä¸€æ­£ä¸€å‰¯çš„å› å­ï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸å‡"""
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
        fac = fac1 - fac2
        new_pure = pure_fall()
        new_pure.monthly_factors = fac
        return new_pure

    def __floordiv__(self, selfa):
        """ä¸¤ä¸ªå› å­ä¸€æ­£ä¸€è´Ÿï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸é™¤"""
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
        fac1 = fac1 - fac1.min()
        fac2 = fac2 - fac2.min()
        fac = fac1 / fac2
        fac = fac.replace(np.inf, np.nan)
        new_pure = pure_fall()
        new_pure.monthly_factors = fac
        return new_pure

    @kk.desktop_sender(title="å˜¿ï¼Œæ­£äº¤åŒ–ç»“æŸå•¦ï½ğŸ¬")
    def __sub__(self, selfa):
        """ç”¨ä¸»å› å­å‰”é™¤å…¶ä»–ç›¸å…³å› å­ã€ä¼ ç»Ÿå› å­ç­‰
        selfaå¯ä»¥ä¸ºå¤šä¸ªå› å­å¯¹è±¡ç»„æˆçš„å…ƒç»„æˆ–åˆ—è¡¨ï¼Œæ¯ä¸ªè¾…åŠ©å› å­åªéœ€è¦æœ‰æœˆåº¦å› å­æ–‡ä»¶è·¯å¾„å³å¯"""
        tqdm.tqdm.pandas()
        if not isinstance(selfa, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfa} is changed into Iterable")
            selfa = (selfa,)
        fac_main = self.wide_to_long(self.monthly_factors, "fac")
        fac_helps = [i.monthly_factors for i in selfa]
        help_names = ["help" + str(i) for i in range(1, (len(fac_helps) + 1))]
        fac_helps = list(map(self.wide_to_long, fac_helps, help_names))
        fac_helps = pd.concat(fac_helps, axis=1)
        facs = pd.concat([fac_main, fac_helps], axis=1).dropna()
        facs = facs.groupby("date").progress_apply(
            lambda x: self.de_in_group(x, help_names)
        )
        facs = facs.unstack()
        facs.columns = list(map(lambda x: x[1], list(facs.columns)))
        return facs

    def __gt__(self, selfa):
        """ç”¨äºè¾“å‡º25åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
        x = self.monthly_factors.copy()
        y = selfa.monthly_factors.copy()
        x = x.stack().reset_index()
        y = y.stack().reset_index()
        x.columns = ["date", "code", "fac"]
        y.columns = ["date", "code", "fac"]
        shen = pure_moon()
        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 5))
        x = (
            x.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupx"})
        )
        xy = pd.merge(x, y, on=["date", "code"])
        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 5))
        xy = (
            xy.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupy"})
        )
        xy = xy.assign(fac=xy.groupx * 5 + xy.groupy)
        xy = xy[["date", "code", "fac"]]
        xy = xy.set_index(["date", "code"]).unstack()
        xy.columns = [i[1] for i in list(xy.columns)]
        new_pure = pure_fall()
        new_pure.monthly_factors = xy
        return new_pure

    def __rshift__(self, selfa):
        """ç”¨äºè¾“å‡º100åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
        x = self.monthly_factors.copy()
        y = selfa.monthly_factors.copy()
        x = x.stack().reset_index()
        y = y.stack().reset_index()
        x.columns = ["date", "code", "fac"]
        y.columns = ["date", "code", "fac"]
        shen = pure_moon()
        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 10))
        x = (
            x.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupx"})
        )
        xy = pd.merge(x, y, on=["date", "code"])
        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 10))
        xy = (
            xy.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupy"})
        )
        xy = xy.assign(fac=xy.groupx * 10 + xy.groupy)
        xy = xy[["date", "code", "fac"]]
        xy = xy.set_index(["date", "code"]).unstack()
        xy.columns = [i[1] for i in list(xy.columns)]
        new_pure = pure_fall()
        new_pure.monthly_factors = xy
        return new_pure

    def wide_to_long(self, df, i):
        """å°†å®½æ•°æ®è½¬åŒ–ä¸ºé•¿æ•°æ®ï¼Œç”¨äºå› å­è¡¨è½¬åŒ–å’Œæ‹¼æ¥"""
        df = df.stack().reset_index()
        df.columns = ["date", "code", i]
        df = df.set_index(["date", "code"])
        return df

    def de_in_group(self, df, help_names):
        """å¯¹æ¯ä¸ªæ—¶é—´ï¼Œåˆ†åˆ«åšå›å½’ï¼Œå‰”é™¤ç›¸å…³å› å­"""
        ols_order = "fac~" + "+".join(help_names)
        ols_result = smf.ols(ols_order, data=df).fit()
        params = {i: ols_result.params[i] for i in help_names}
        predict = [params[i] * df[i] for i in help_names]
        predict = reduce(lambda x, y: x + y, predict)
        df.fac = df.fac - predict - ols_result.params["Intercept"]
        df = df[["fac"]]
        return df

    def standardlize_in_cross_section(self, df):
        """
        åœ¨æ¨ªæˆªé¢ä¸Šåšæ ‡å‡†åŒ–
        è¾“å…¥çš„dfåº”ä¸ºï¼Œåˆ—åæ˜¯è‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•æ˜¯æ—¶é—´
        """
        df = df.T
        df = (df - df.mean()) / df.std()
        df = df.T
        return df

    def get_single_day_factor(self, func: Callable, day: int) -> pd.DataFrame:
        """è®¡ç®—å•æ—¥çš„å› å­å€¼ï¼Œé€šè¿‡sqlæ•°æ®åº“ï¼Œè¯»å–å•æ—¥çš„æ•°æ®ï¼Œç„¶åè®¡ç®—å› å­å€¼"""
        sql = sqlConfig("minute_data_stock_alter")
        df = sql.get_data(str(day))
        the_func = partial(func)
        df = df.groupby(["code"]).apply(the_func).to_frame()
        df.columns = [str(day)]
        df = df.T
        df.index = pd.to_datetime(df.index, format="%Y%m%d")
        return df

    @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
    def get_daily_factors_alter(self, func: Callable) -> None:
        """ç”¨mysqlé€æ—¥æ›´æ–°åˆ†é’Ÿæ•°æ®æ„é€ çš„å› å­

        Parameters
        ----------
        func : Callable
            æ„é€ åˆ†é’Ÿæ•°æ®ä½¿ç”¨çš„å‡½æ•°

        Raises
        ------
        `IOError`
            å¦‚æœæ²¡æœ‰å†å²å› å­æ•°æ®ï¼Œå°†æŠ¥é”™
        """
        """é€šè¿‡minute_data_stock_alteræ•°æ®åº“ä¸€å¤©ä¸€å¤©è®¡ç®—å› å­å€¼"""
        try:
            try:
                self.daily_factors = pd.read_feather(self.daily_factors_path)
            except Exception:
                self.daily_factors_path = self.daily_factors_path.split("æ—¥é¢‘_")
                self.daily_factors_path = (
                    self.daily_factors_path[0] + self.daily_factors_path[1]
                )
                self.daily_factors = pd.read_feather(self.daily_factors_path)
            self.daily_factors = self.daily_factors.drop_duplicates(subset=['date'],keep='last')
            self.daily_factors = self.daily_factors.set_index("date")
            sql = sqlConfig("minute_data_stock_alter")
            now_minute_datas = sql.show_tables(full=False)
            now_minute_data = now_minute_datas[-1]
            now_minute_data = pd.Timestamp(now_minute_data)
            if self.daily_factors.index.max() < now_minute_data:
                if not STATES["NO_LOG"]:
                    logger.info(
                        f"ä¸Šæ¬¡å­˜å‚¨çš„å› å­å€¼åˆ°{self.daily_factors.index.max()}ï¼Œè€Œåˆ†é’Ÿæ•°æ®æœ€æ–°åˆ°{now_minute_data}ï¼Œå¼€å§‹æ›´æ–°â€¦â€¦"
                    )
                old_end = datetime.datetime.strftime(
                    self.daily_factors.index.max(), "%Y%m%d"
                )
                now_minute_datas = [i for i in now_minute_datas if i > old_end]
                dfs = []
                for c in tqdm.tqdm(now_minute_datas, desc="æ¡‚æ£¹å…®å…°æ¡¨ï¼Œå‡»ç©ºæ˜å…®é‚æµå…‰ğŸŒŠ"):
                    df = self.get_single_day_factor(func, c)
                    dfs.append(df)
                dfs = pd.concat(dfs)
                dfs = dfs.sort_index()
                self.daily_factors = pd.concat([self.daily_factors, dfs])
                self.daily_factors = self.daily_factors.dropna(how="all")
                self.daily_factors = self.daily_factors[
                    self.daily_factors.index >= pd.Timestamp(str(STATES["START"]))
                ]
                self.daily_factors.reset_index().to_feather(self.daily_factors_path)
                if not STATES["NO_LOG"]:
                    logger.success("æ›´æ–°å·²å®Œæˆ")

        except Exception:
            raise IOError(
                "æ‚¨è¿˜æ²¡æœ‰è¯¥å› å­çš„åˆçº§æ•°æ®ï¼Œæš‚æ—¶ä¸èƒ½æ›´æ–°ã€‚è¯·å…ˆä½¿ç”¨pure_fall_frequentæˆ–pure_fall_flexibleè®¡ç®—å†å²å› å­å€¼ã€‚"
            )


class pure_fallmount(pure_fall):
    """ç»§æ‰¿è‡ªçˆ¶ç±»ï¼Œä¸“ä¸ºåšå› å­æˆªé¢æ ‡å‡†åŒ–ä¹‹åç›¸åŠ å’Œå› å­å‰”é™¤å…¶ä»–è¾…åŠ©å› å­çš„ä½œç”¨"""

    def __init__(self, monthly_factors):
        """è¾“å…¥æœˆåº¦å› å­å€¼ï¼Œä»¥è®¾å®šæ–°çš„å¯¹è±¡"""
        super(pure_fall, self).__init__()
        self.monthly_factors = monthly_factors

    def __call__(self, monthly=False):
        """ä¸ºäº†é˜²æ­¢å±æ€§åå¤ªå¤šï¼Œå¿˜è®°äº†è¦è°ƒç”¨å“ªä¸ªæ‰æ˜¯ç»“æœï¼Œå› æ­¤å¯ä»¥ç›´æ¥è¾“å‡ºæœˆåº¦æ•°æ®è¡¨"""
        if monthly:
            return self.monthly_factors.copy()
        else:
            try:
                return self.daily_factors.copy()
            except Exception:
                return self.monthly_factors.copy()

    def __add__(self, selfas):
        """è¿”å›ä¸€ä¸ªå¯¹è±¡ï¼Œè€Œéä¸€ä¸ªè¡¨æ ¼ï¼Œå¦‚éœ€è¡¨æ ¼è¯·è°ƒç”¨å¯¹è±¡"""
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac2s = []
        if not isinstance(selfas, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfas} is changed into Iterable")
            selfas = (selfas,)
        for selfa in selfas:
            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2s.append(fac2)
        for i in fac2s:
            fac1 = fac1 + i
        new_pure = pure_fallmount(fac1)
        return new_pure

    def __mul__(self, selfas):
        """å°†å‡ ä¸ªå› å­æ¨ªæˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œä½¿å…¶éƒ½ä¸ºæ­£æ•°ï¼Œç„¶åå› å­å€¼ç›¸ä¹˜"""
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac1 = fac1 - fac1.min()
        fac2s = []
        if not isinstance(selfas, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfas} is changed into Iterable")
            selfas = (selfas,)
        for selfa in selfas:
            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2 = fac2 - fac2.min()
            fac2s.append(fac2)
        for i in fac2s:
            fac1 = fac1 * i
        new_pure = pure_fall()
        new_pure.monthly_factors = fac1
        return new_pure

    def __sub__(self, selfa):
        """è¿”å›å¯¹è±¡ï¼Œå¦‚éœ€è¡¨æ ¼ï¼Œè¯·è°ƒç”¨å¯¹è±¡"""
        tqdm.tqdm.pandas()
        if not isinstance(selfa, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfa} is changed into Iterable")
            selfa = (selfa,)
        fac_main = self.wide_to_long(self.monthly_factors, "fac")
        fac_helps = [i.monthly_factors for i in selfa]
        help_names = ["help" + str(i) for i in range(1, (len(fac_helps) + 1))]
        fac_helps = list(map(self.wide_to_long, fac_helps, help_names))
        fac_helps = pd.concat(fac_helps, axis=1)
        facs = pd.concat([fac_main, fac_helps], axis=1).dropna()
        facs = facs.groupby("date").progress_apply(
            lambda x: self.de_in_group(x, help_names)
        )
        facs = facs.unstack()
        facs.columns = list(map(lambda x: x[1], list(facs.columns)))
        new_pure = pure_fallmount(facs)
        return new_pure

    def __gt__(self, selfa):
        """ç”¨äºè¾“å‡º25åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
        x = self.monthly_factors.copy()
        y = selfa.monthly_factors.copy()
        x = x.stack().reset_index()
        y = y.stack().reset_index()
        x.columns = ["date", "code", "fac"]
        y.columns = ["date", "code", "fac"]
        shen = pure_moon()
        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 5))
        x = (
            x.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupx"})
        )
        xy = pd.merge(x, y, on=["date", "code"])
        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 5))
        xy = (
            xy.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupy"})
        )
        xy = xy.assign(fac=xy.groupx * 5 + xy.groupy)
        xy = xy[["date", "code", "fac"]]
        xy = xy.set_index(["date", "code"]).unstack()
        xy.columns = [i[1] for i in list(xy.columns)]
        new_pure = pure_fallmount(xy)
        return new_pure

    def __rshift__(self, selfa):
        """ç”¨äºè¾“å‡º100åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
        x = self.monthly_factors.copy()
        y = selfa.monthly_factors.copy()
        x = x.stack().reset_index()
        y = y.stack().reset_index()
        x.columns = ["date", "code", "fac"]
        y.columns = ["date", "code", "fac"]
        shen = pure_moon()
        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 10))
        x = (
            x.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupx"})
        )
        xy = pd.merge(x, y, on=["date", "code"])
        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 10))
        xy = (
            xy.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupy"})
        )
        xy = xy.assign(fac=xy.groupx * 10 + xy.groupy)
        xy = xy[["date", "code", "fac"]]
        xy = xy.set_index(["date", "code"]).unstack()
        xy.columns = [i[1] for i in list(xy.columns)]
        new_pure = pure_fallmount(xy)
        return new_pure


class pure_fall_frequent(object):
    """å¯¹å•åªè‚¡ç¥¨å•æ—¥è¿›è¡Œæ“ä½œ"""

    def __init__(
        self,
        factor_file: str,
        startdate: int = None,
        enddate: int = None,
        kind: str = "stock",
        clickhouse: bool = 0,
        postgresql: bool = 0,
        questdb: bool = 0,
    ) -> None:
        """åŸºäºclickhouseçš„åˆ†é’Ÿæ•°æ®ï¼Œè®¡ç®—å› å­å€¼ï¼Œæ¯å¤©çš„å› å­å€¼åªç”¨åˆ°å½“æ—¥çš„æ•°æ®

        Parameters
        ----------
        factor_file : str
            ç”¨äºä¿å­˜å› å­å€¼çš„æ–‡ä»¶åï¼Œéœ€ä¸ºfeatheræ–‡ä»¶ï¼Œä»¥'.feather'ç»“å°¾
        startdate : int, optional
            èµ·å§‹æ—¶é—´ï¼Œå½¢å¦‚20121231ï¼Œä¸ºå¼€åŒºé—´, by default None
        enddate : int, optional
            æˆªæ­¢æ—¶é—´ï¼Œå½¢å¦‚20220814ï¼Œä¸ºé—­åŒºé—´ï¼Œä¸ºç©ºåˆ™è®¡ç®—åˆ°æœ€è¿‘æ•°æ®, by default None
        kind : str, optional
            ç±»å‹ä¸ºè‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼ŒæŒ‡æ•°ä¸º'index', by default "stock"
        clickhouse : bool, optional
            ä½¿ç”¨clickhouseä½œä¸ºæ•°æ®æºï¼Œå¦‚æœpostgresqlä¸æœ¬å‚æ•°éƒ½ä¸º0ï¼Œå°†ä¾ç„¶ä»clickhouseä¸­è¯»å–, by default 0
        postgresql : bool, optional
            ä½¿ç”¨postgresqlä½œä¸ºæ•°æ®æº, by default 0
        questdb : bool, optional
            ä½¿ç”¨questdbä½œä¸ºæ•°æ®æº, by default 0
        """
        homeplace = HomePlace()
        self.kind = kind
        if clickhouse == 0 and postgresql == 0 and questdb == 0:
            clickhouse = 1
        self.clickhouse = clickhouse
        self.postgresql = postgresql
        self.questdb = questdb
        if clickhouse == 1:
            # è¿æ¥clickhouse
            self.chc = ClickHouseClient("minute_data")
        elif postgresql == 1:
            self.chc = PostgreSQL("minute_data")
        else:
            self.chc = Questdb()
        # å®Œæ•´çš„å› å­æ–‡ä»¶è·¯å¾„
        factor_file = homeplace.factor_data_file + factor_file
        self.factor_file = factor_file
        # è¯»å…¥ä¹‹å‰çš„å› å­
        if os.path.exists(factor_file):
            factor_old = pd.read_feather(self.factor_file)
            factor_old.columns = ["date"] + list(factor_old.columns)[1:]
            factor_old = factor_old.drop_duplicates(subset=["date"])
            factor_old = factor_old.set_index("date")
            self.factor_old = factor_old
            # å·²ç»ç®—å¥½çš„æ—¥å­
            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
            self.dates_old = dates_old
        else:
            self.factor_old = None
            self.dates_old = []
            logger.info("è¿™ä¸ªå› å­ä»¥å‰æ²¡æœ‰ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
        # è¯»å–å½“å‰æ‰€æœ‰çš„æ—¥å­
        dates_all = self.chc.show_all_dates(f"minute_data_{kind}")
        if startdate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i >= startdate]
        if enddate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i <= enddate]
        self.dates_all = dates_all
        # éœ€è¦æ–°è¡¥å……çš„æ—¥å­
        self.dates_new = sorted([i for i in dates_all if i not in self.dates_old])
        if len(self.dates_old) == 0:
            ...
        else:
            self.dates_new = self.dates_new[1:]

    def __call__(self) -> pd.DataFrame:
        """è·å¾—ç»è¿ç®—äº§ç”Ÿçš„å› å­

        Returns
        -------
        `pd.DataFrame`
            ç»è¿ç®—äº§ç”Ÿçš„å› å­å€¼
        """
        return self.factor.copy()

    @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
    def get_daily_factors(
        self,
        func: Callable,
        fields: str = "*",
        chunksize: int = 10,
        show_time: bool = 0,
        tqdm_inside: bool = 0,
    ) -> None:
        """æ¯æ¬¡æŠ½å–chunksizeå¤©çš„æˆªé¢ä¸Šå…¨éƒ¨è‚¡ç¥¨çš„åˆ†é’Ÿæ•°æ®
        å¯¹æ¯å¤©çš„è‚¡ç¥¨çš„æ•°æ®è®¡ç®—å› å­å€¼

        Parameters
        ----------
        func : Callable
            ç”¨äºè®¡ç®—å› å­å€¼çš„å‡½æ•°
        fields : str, optional
            è‚¡ç¥¨æ•°æ®æ¶‰åŠåˆ°å“ªäº›å­—æ®µï¼Œæ’é™¤ä¸å¿…è¦çš„å­—æ®µï¼Œå¯ä»¥èŠ‚çº¦è¯»å–æ•°æ®çš„æ—¶é—´ï¼Œå½¢å¦‚'date,code,num,close,amount,open'
            æå–å‡ºçš„æ•°æ®ï¼Œè‡ªåŠ¨æŒ‰ç…§code,date,numæ’åºï¼Œå› æ­¤code,date,numæ˜¯å¿…ä¸å¯å°‘çš„å­—æ®µ, by default "*"
        chunksize : int, optional
            æ¯æ¬¡è¯»å–çš„æˆªé¢ä¸Šçš„å¤©æ•°, by default 10
        show_time : bool, optional
            å±•ç¤ºæ¯æ¬¡è¯»å–æ•°æ®æ‰€éœ€è¦çš„æ—¶é—´, by default 0
        tqdm_inside : bool, optional
            å°†è¿›åº¦æ¡åŠ åœ¨å†…éƒ¨ï¼Œè€Œéå¤–éƒ¨ï¼Œå»ºè®®ä»…chunksizeè¾ƒå¤§æ—¶ä½¿ç”¨, by default 0
        """
        the_func = partial(func)
        # å°†éœ€è¦æ›´æ–°çš„æ—¥å­åˆ†å—ï¼Œæ¯200å¤©ä¸€ç»„ï¼Œä¸€èµ·è¿ç®—
        dates_new_len = len(self.dates_new)
        if dates_new_len > 1:
            cut_points = list(range(0, dates_new_len, chunksize)) + [dates_new_len - 1]
            if cut_points[-1] == cut_points[-2]:
                cut_points = cut_points[:-1]
            cuts = tuple(zip(cut_points[:-1], cut_points[1:]))
            print(f"å…±{len(cuts)}æ®µ")
            self.cut_points = cut_points
            self.factor_new = []
            if tqdm_inside:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                for date1, date2 in cuts:
                    if self.clickhouse == 1:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]*100} and date<={self.dates_new[date2]*100} order by code,date,num"
                    else:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]} and date<={self.dates_new[date2]} order by code,date,num"
                    if show_time:
                        df = self.chc.get_data_show_time(sql_order)
                    else:
                        df = self.chc.get_data(sql_order)
                    if self.clickhouse == 1:
                        df = ((df.set_index("code")) / 100).reset_index()
                    tqdm.tqdm.pandas()
                    df = df.groupby(["date", "code"]).progress_apply(the_func)
                    df = df.to_frame("fac").reset_index()
                    df.columns = ["date", "code", "fac"]
                    df = df.pivot(columns="code", index="date", values="fac")
                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                    self.factor_new.append(df)
            else:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                for date1, date2 in tqdm.tqdm(cuts, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
                    if self.clickhouse == 1:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]*100} and date<={self.dates_new[date2]*100} order by code,date,num"
                    else:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]} and date<={self.dates_new[date2]} order by code,date,num"
                    if show_time:
                        df = self.chc.get_data_show_time(sql_order)
                    else:
                        df = self.chc.get_data(sql_order)
                    if self.clickhouse == 1:
                        df = ((df.set_index("code")) / 100).reset_index()
                    df = df.groupby(["date", "code"]).apply(the_func)
                    df = df.to_frame("fac").reset_index()
                    df.columns = ["date", "code", "fac"]
                    df = df.pivot(columns="code", index="date", values="fac")
                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                    self.factor_new.append(df)
            self.factor_new = pd.concat(self.factor_new)
            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
            self.factor = pd.concat([self.factor_old, self.factor_new]).sort_index()
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            # å­˜å…¥æœ¬åœ°
            self.factor.reset_index().to_feather(self.factor_file)
            logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
        elif dates_new_len == 1:
            print("å…±1å¤©")
            if tqdm_inside:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                if self.clickhouse == 1:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]*100} order by code,date,num"
                else:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]} order by code,date,num"
                if show_time:
                    df = self.chc.get_data_show_time(sql_order)
                else:
                    df = self.chc.get_data(sql_order)
                if self.clickhouse == 1:
                    df = ((df.set_index("code")) / 100).reset_index()
                tqdm.tqdm.pandas()
                df = df.groupby(["date", "code"]).progress_apply(the_func)
                df = df.to_frame("fac").reset_index()
                df.columns = ["date", "code", "fac"]
                df = df.pivot(columns="code", index="date", values="fac")
                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
            else:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                if self.clickhouse == 1:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]*100} order by code,date,num"
                else:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]} order by code,date,num"
                if show_time:
                    df = self.chc.get_data_show_time(sql_order)
                else:
                    df = self.chc.get_data(sql_order)
                if self.clickhouse == 1:
                    df = ((df.set_index("code")) / 100).reset_index()
                df = df.groupby(["date", "code"]).apply(the_func)
                df = df.to_frame("fac").reset_index()
                df.columns = ["date", "code", "fac"]
                df = df.pivot(columns="code", index="date", values="fac")
                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
            self.factor_new = df
            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
            self.factor = (
                pd.concat([self.factor_old, self.factor_new])
                .sort_index()
                .drop_duplicates()
            )
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            # å­˜å…¥æœ¬åœ°
            self.factor.reset_index().to_feather(self.factor_file)
            logger.info(f"è¡¥å……{self.dates_new[0]}æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
        else:
            self.factor = self.factor_old
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            logger.info(f"å½“å‰æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼å·²ç»æ˜¯æœ€æ–°çš„äº†")


class pure_fall_flexible(object):
    def __init__(
        self,
        factor_file: str,
        startdate: int = None,
        enddate: int = None,
        kind: str = "stock",
        clickhouse: bool = 0,
        postgresql: bool = 0,
        questdb: bool = 0,
    ) -> None:
        """åŸºäºclickhouseçš„åˆ†é’Ÿæ•°æ®ï¼Œè®¡ç®—å› å­å€¼ï¼Œæ¯å¤©çš„å› å­å€¼ç”¨åˆ°å¤šæ—¥çš„æ•°æ®ï¼Œæˆ–è€…ç”¨åˆ°æˆªé¢çš„æ•°æ®
        å¯¹ä¸€æ®µæ—¶é—´çš„æˆªé¢æ•°æ®è¿›è¡Œæ“ä½œï¼Œåœ¨get_daily_factorsçš„funcå‡½æ•°ä¸­
        è¯·å†™å…¥df=df.groupby([xxx]).apply(fff)ä¹‹ç±»çš„è¯­å¥
        ç„¶åå•ç‹¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä½œä¸ºè¦applyçš„fffï¼Œå¯ä»¥åœ¨applyä¸ŠåŠ è¿›åº¦æ¡

        Parameters
        ----------
        factor_file : str
            ç”¨äºå­˜å‚¨å› å­çš„æ–‡ä»¶åç§°ï¼Œè¯·ä»¥'.feather'ç»“å°¾
        startdate : int, optional
            è®¡ç®—å› å­çš„èµ·å§‹æ—¥æœŸï¼Œå½¢å¦‚20220816, by default None
        enddate : int, optional
            è®¡ç®—å› å­çš„ç»ˆæ­¢æ—¥æœŸï¼Œå½¢å¦‚20220816, by default None
        kind : str, optional
            æŒ‡å®šè®¡ç®—è‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼ŒæŒ‡æ•°åˆ™ä¸º'index', by default "stock"
        clickhouse : bool, optional
            ä½¿ç”¨clickhouseä½œä¸ºæ•°æ®æºï¼Œå¦‚æœpostgresqlä¸æœ¬å‚æ•°éƒ½ä¸º0ï¼Œå°†ä¾ç„¶ä»clickhouseä¸­è¯»å–, by default 0
        postgresql : bool, optional
            ä½¿ç”¨postgresqlä½œä¸ºæ•°æ®æº, by default 0
        questdb : bool, optional
            ä½¿ç”¨questdbä½œä¸ºæ•°æ®æº, by default 0
        """
        homeplace = HomePlace()
        self.kind = kind
        if clickhouse == 0 and postgresql == 0 and questdb == 0:
            clickhouse = 1
        self.clickhouse = clickhouse
        self.postgresql = postgresql
        self.questdb = questdb
        if clickhouse == 1:
            # è¿æ¥clickhouse
            self.chc = ClickHouseClient("minute_data")
        elif postgresql == 1:
            self.chc = PostgreSQL("minute_data")
        else:
            self.chc = Questdb()
        # å®Œæ•´çš„å› å­æ–‡ä»¶è·¯å¾„
        factor_file = homeplace.factor_data_file + factor_file
        self.factor_file = factor_file
        # è¯»å…¥ä¹‹å‰çš„å› å­
        if os.path.exists(factor_file):
            factor_old = pd.read_feather(self.factor_file)
            factor_old.columns = ["date"] + list(factor_old.columns)[1:]
            factor_old = factor_old.drop_duplicates(subset=["date"])
            factor_old = factor_old.set_index("date")
            self.factor_old = factor_old
            # å·²ç»ç®—å¥½çš„æ—¥å­
            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
            self.dates_old = dates_old
        else:
            self.factor_old = None
            self.dates_old = []
            logger.info("è¿™ä¸ªå› å­ä»¥å‰æ²¡æœ‰ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
        # è¯»å–å½“å‰æ‰€æœ‰çš„æ—¥å­
        dates_all = self.chc.show_all_dates(f"minute_data_{kind}")
        if startdate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i >= startdate]
        if enddate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i <= enddate]
        self.dates_all = dates_all
        # éœ€è¦æ–°è¡¥å……çš„æ—¥å­
        self.dates_new = sorted([i for i in dates_all if i not in dates_old])

    def __call__(self) -> pd.DataFrame:
        """ç›´æ¥è¿”å›å› å­å€¼çš„pd.DataFrame

        Returns
        -------
        `pd.DataFrame`
            è®¡ç®—å‡ºçš„å› å­å€¼
        """
        return self.factor.copy()

    @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
    def get_daily_factors(
        self,
        func: Callable,
        fields: str = "*",
        chunksize: int = 250,
        show_time: bool = 0,
        tqdm_inside: bool = 0,
    ) -> None:
        """æ¯æ¬¡æŠ½å–chunksizeå¤©çš„æˆªé¢ä¸Šå…¨éƒ¨è‚¡ç¥¨çš„åˆ†é’Ÿæ•°æ®
        ä¾ç…§å®šä¹‰çš„å‡½æ•°è®¡ç®—å› å­å€¼

        Parameters
        ----------
        func : Callable
            ç”¨äºè®¡ç®—å› å­å€¼çš„å‡½æ•°
        fields : str, optional
            è‚¡ç¥¨æ•°æ®æ¶‰åŠåˆ°å“ªäº›å­—æ®µï¼Œæ’é™¤ä¸å¿…è¦çš„å­—æ®µï¼Œå¯ä»¥èŠ‚çº¦è¯»å–æ•°æ®çš„æ—¶é—´ï¼Œå½¢å¦‚'date,code,num,close,amount,open'
            æå–å‡ºçš„æ•°æ®ï¼Œè‡ªåŠ¨æŒ‰ç…§code,date,numæ’åºï¼Œå› æ­¤code,date,numæ˜¯å¿…ä¸å¯å°‘çš„å­—æ®µ, by default "*"
        chunksize : int, optional
            æ¯æ¬¡è¯»å–çš„æˆªé¢ä¸Šçš„å¤©æ•°, by default 10
        show_time : bool, optional
            å±•ç¤ºæ¯æ¬¡è¯»å–æ•°æ®æ‰€éœ€è¦çš„æ—¶é—´, by default 0
        tqdm_inside : bool, optional
            å°†è¿›åº¦æ¡åŠ åœ¨å†…éƒ¨ï¼Œè€Œéå¤–éƒ¨ï¼Œå»ºè®®ä»…chunksizeè¾ƒå¤§æ—¶ä½¿ç”¨, by default 0
        """
        the_func = partial(func)
        # å°†éœ€è¦æ›´æ–°çš„æ—¥å­åˆ†å—ï¼Œæ¯200å¤©ä¸€ç»„ï¼Œä¸€èµ·è¿ç®—
        dates_new_len = len(self.dates_new)
        if dates_new_len > 0:
            cut_points = list(range(0, dates_new_len, chunksize)) + [dates_new_len - 1]
            if cut_points[-1] == cut_points[-2]:
                cut_points = cut_points[:-1]
            self.cut_points = cut_points
            self.factor_new = []
            # å¼€å§‹è®¡ç®—å› å­å€¼
            if tqdm_inside:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                for date1, date2 in cut_points:
                    if self.clickhouse == 1:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]*100} and date<={self.dates_new[date2]*100} order by code,date,num"
                    else:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]} and date<={self.dates_new[date2]} order by code,date,num"
                    if show_time:
                        df = self.chc.get_data_show_time(sql_order)
                    else:
                        df = self.chc.get_data(sql_order)
                    if self.clickhouse == 1:
                        df = ((df.set_index("code")) / 100).reset_index()
                    tqdm.tqdm.pandas()
                    df = the_func(df)
                    if isinstance(df, pd.Series):
                        df = df.reset_index()
                    df.columns = ["date", "code", "fac"]
                    df = df.pivot(columns="code", index="date", values="fac")
                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                    self.factor_new.append(df)
            else:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                for date1, date2 in tqdm.tqdm(cut_points, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
                    if self.clickhouse == 1:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]*100} and date<={self.dates_new[date2]*100} order by code,date,num"
                    else:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]} and date<={self.dates_new[date2]} order by code,date,num"
                    if show_time:
                        df = self.chc.get_data_show_time(sql_order)
                    else:
                        df = self.chc.get_data(sql_order)
                    if self.clickhouse == 1:
                        df = ((df.set_index("code")) / 100).reset_index()
                    df = the_func(df)
                    if isinstance(df, pd.Series):
                        df = df.reset_index()
                    df.columns = ["date", "code", "fac"]
                    df = df.pivot(columns="code", index="date", values="fac")
                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                    self.factor_new.append(df)
            self.factor_new = pd.concat(self.factor_new)
            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
            self.factor = pd.concat([self.factor_old, self.factor_new]).sort_index()
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            # å­˜å…¥æœ¬åœ°
            self.factor.reset_index().to_feather(self.factor_file)
            logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
        elif dates_new_len == 1:
            print("å…±1å¤©")
            if tqdm_inside:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                if self.clickhouse == 1:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]*100} order by code,date,num"
                else:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]} order by code,date,num"
                if show_time:
                    df = self.chc.get_data_show_time(sql_order)
                else:
                    df = self.chc.get_data(sql_order)
                if self.clickhouse == 1:
                    df = ((df.set_index("code")) / 100).reset_index()
                tqdm.tqdm.pandas()
                df = the_func(df)
                if isinstance(df, pd.Series):
                    df = df.reset_index()
                df.columns = ["date", "code", "fac"]
                df = df.pivot(columns="code", index="date", values="fac")
                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
            else:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                if self.clickhouse == 1:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]*100} order by code,date,num"
                else:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]} order by code,date,num"
                if show_time:
                    df = self.chc.get_data_show_time(sql_order)
                else:
                    df = self.chc.get_data(sql_order)
                if self.clickhouse == 1:
                    df = ((df.set_index("code")) / 100).reset_index()
                df = the_func(df)
                if isinstance(df, pd.Series):
                    df = df.reset_index()
                df.columns = ["date", "code", "fac"]
                df = df.pivot(columns="code", index="date", values="fac")
                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                self.factor_new.append(df)
            self.factor_new = df
            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
            self.factor = (
                pd.concat([self.factor_old, self.factor_new])
                .sort_index()
                .drop_duplicates()
            )
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            # å­˜å…¥æœ¬åœ°
            self.factor.reset_index().to_feather(self.factor_file)
            logger.info(f"è¡¥å……{self.dates_new[0]}æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
        else:
            self.factor = self.factor_old
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            logger.info(f"å½“å‰æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼å·²ç»æ˜¯æœ€æ–°çš„äº†")


class pure_coldwinter(object):
    # DONE: å¯ä»¥è‡ªç”±æ·»åŠ å…¶ä»–è¦å‰”é™¤çš„å› å­ï¼Œæˆ–è€…æ›¿æ¢æŸäº›è¦å‰”é™¤çš„å› å­
    def __init__(
        self,
        facs_dict: dict = None,
        momentum: bool = 1,
        earningsyield: bool = 1,
        growth: bool = 1,
        liquidity: bool = 1,
        size: bool = 1,
        leverage: bool = 1,
        beta: bool = 1,
        nonlinearsize: bool = 1,
        residualvolatility: bool = 1,
        booktoprice: bool = 1,
    ) -> None:
        """è¯»å…¥10ç§å¸¸ç”¨é£æ ¼å› å­ï¼Œå¹¶å¯ä»¥é¢å¤–åŠ å…¥å…¶ä»–å› å­

        Parameters
        ----------
        facs_dict : dict, optional
            é¢å¤–åŠ å…¥çš„å› å­ï¼Œåå­—ä¸ºkeyï¼Œå› å­çŸ©é˜µä¸ºvalueï¼Œå½¢å¦‚`{'åè½¬': ret20, 'æ¢æ‰‹': tr20}`, by default None
        momentum : bool, optional
            æ˜¯å¦åˆ å»åŠ¨é‡å› å­, by default 1
        earningsyield : bool, optional
            æ˜¯å¦åˆ å»ç›ˆåˆ©å› å­, by default 1
        growth : bool, optional
            æ˜¯å¦åˆ å»æˆé•¿å› å­, by default 1
        liquidity : bool, optional
            æ˜¯å¦åˆ å»æµåŠ¨æ€§å› å­, by default 1
        size : bool, optional
            æ˜¯å¦åˆ å»è§„æ¨¡å› å­, by default 1
        leverage : bool, optional
            æ˜¯å¦åˆ å»æ æ†å› å­, by default 1
        beta : bool, optional
            æ˜¯å¦åˆ å»è´å¡”å› å­, by default 1
        nonlinearsize : bool, optional
            æ˜¯å¦åˆ å»éçº¿æ€§å¸‚å€¼å› å­, by default 1
        residualvolatility : bool, optional
            æ˜¯å¦åˆ å»æ®‹å·®æ³¢åŠ¨ç‡å› å­, by default 1
        booktoprice : bool, optional
            æ˜¯å¦åˆ å»è´¦é¢å¸‚å€¼æ¯”å› å­, by default 1
        """
        self.homeplace = HomePlace()
        # barraå› å­æ•°æ®
        styles = os.listdir(self.homeplace.barra_data_file)
        styles = [i for i in styles if i.endswith(".feather")]
        barras = {}
        for s in styles:
            k = s.split(".")[0]
            v = pd.read_feather(self.homeplace.barra_data_file + s)
            v.columns = ["date"] + list(v.columns)[1:]
            v = v.set_index("date")
            barras[k] = v
        rename_dict = {
            "fac": "å› å­è‡ªèº«",
            "size": "å¸‚å€¼",
            "nonlinearsize": "éçº¿æ€§å¸‚å€¼",
            "booktoprice": "ä¼°å€¼",
            "earningsyield": "ç›ˆåˆ©",
            "growth": "æˆé•¿",
            "leverage": "æ æ†",
            "liquidity": "æµåŠ¨æ€§",
            "momentum": "åŠ¨é‡",
            "residualvolatility": "æ³¢åŠ¨ç‡",
            "beta": "è´å¡”",
        }
        if momentum == 0:
            barras = {k: v for k, v in barras.items() if k != "momentum"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "momentum"}
        if earningsyield == 0:
            barras = {k: v for k, v in barras.items() if k != "earningsyield"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "earningsyield"}
        if growth == 0:
            barras = {k: v for k, v in barras.items() if k != "growth"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "growth"}
        if liquidity == 0:
            barras = {k: v for k, v in barras.items() if k != "liquidity"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "liquidity"}
        if size == 0:
            barras = {k: v for k, v in barras.items() if k != "size"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "size"}
        if leverage == 0:
            barras = {k: v for k, v in barras.items() if k != "leverage"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "leverage"}
        if beta == 0:
            barras = {k: v for k, v in barras.items() if k != "beta"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "beta"}
        if nonlinearsize == 0:
            barras = {k: v for k, v in barras.items() if k != "nonlinearsize"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "nonlinearsize"}
        if residualvolatility == 0:
            barras = {k: v for k, v in barras.items() if k != "residualvolatility"}
            rename_dict = {
                k: v for k, v in rename_dict.items() if k != "residualvolatility"
            }
        if booktoprice == 0:
            barras = {k: v for k, v in barras.items() if k != "booktoprice"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "booktoprice"}
        if facs_dict is not None:
            barras.update(facs_dict)
        self.barras = barras
        self.rename_dict = rename_dict
        sort_names = list(rename_dict.values())
        if facs_dict is not None:
            sort_names = sort_names + list(facs_dict.keys())
        sort_names = [i for i in sort_names if i != "å› å­è‡ªèº«"]
        self.sort_names = sort_names

    def __call__(self):
        """è¿”å›çº¯å‡€å› å­å€¼"""
        return self.snow_fac

    def set_factors_df_wide(self, df):
        """ä¼ å…¥å› å­æ•°æ®ï¼Œæ—¶é—´ä¸ºç´¢å¼•ï¼Œä»£ç ä¸ºåˆ—å"""
        df1 = df.copy()
        # df1.index=df1.index-pd.DateOffset(months=1)
        df1 = df1.resample("M").last()
        df1 = df1.stack().reset_index()
        df1.columns = ["date", "code", "fac"]
        self.factors = df1.copy()

    def daily_to_monthly(self, df):
        """å°†æ—¥åº¦çš„barraå› å­æœˆåº¦åŒ–"""
        df = df.resample("M").last()
        return df

    def get_monthly_barras_industrys(self):
        """å°†barraå› å­å’Œè¡Œä¸šå“‘å˜é‡å˜æˆæœˆåº¦æ•°æ®"""
        for key, value in self.barras.items():
            self.barras[key] = self.daily_to_monthly(value)

    def wide_to_long(self, df, name):
        """å°†å®½æ•°æ®å˜æˆé•¿æ•°æ®ï¼Œä¾¿äºåç»­æ‹¼æ¥"""
        df = df.stack().reset_index()
        df.columns = ["date", "code", name]
        df = df.set_index(["date", "code"])
        return df

    def get_wide_barras_industrys(self):
        """å°†barraå› å­å’Œè¡Œä¸šå“‘å˜é‡éƒ½å˜æˆé•¿æ•°æ®"""
        for key, value in self.barras.items():
            self.barras[key] = self.wide_to_long(value, key)

    def get_corr_pri_ols_pri(self):
        """æ‹¼æ¥barraå› å­å’Œè¡Œä¸šå“‘å˜é‡ï¼Œç”Ÿæˆç”¨äºæ±‚ç›¸å…³ç³»æ•°å’Œçº¯å‡€å› å­çš„æ•°æ®è¡¨"""
        if self.factors.shape[0] > 1:
            self.factors = self.factors.set_index(["date", "code"])
        self.corr_pri = pd.concat(
            [self.factors] + list(self.barras.values()), axis=1
        ).dropna()

    # DONE: ä¿®æ”¹é£æ ¼å› å­å±•ç¤ºé¡ºåºè‡³æŠ¥å‘Šçš„é¡ºåº
    def get_corr(self):
        """è®¡ç®—æ¯ä¸€æœŸçš„ç›¸å…³ç³»æ•°ï¼Œå†æ±‚å¹³å‡å€¼"""
        self.corr_by_step = self.corr_pri.groupby(["date"]).apply(
            lambda x: x.corr().head(1)
        )
        self.__corr = self.corr_by_step.mean()
        self.__corr = self.__corr.rename(index=self.rename_dict)
        self.__corr = self.__corr.to_frame("ç›¸å…³ç³»æ•°").T

        self.__corr = self.__corr[self.sort_names]
        self.__corr = self.__corr.T

    @property
    def corr(self) -> pd.DataFrame:
        """å› å­å’Œ10ä¸ªå¸¸ç”¨é£æ ¼å› å­çš„ç›¸å…³ç³»æ•°

        Returns
        -------
        pd.DataFrame
            å› å­å’Œ10ä¸ªå¸¸ç”¨é£æ ¼å› å­çš„ç›¸å…³ç³»æ•°
        """
        return self.__corr.copy()

    def ols_in_group(self, df):
        """å¯¹æ¯ä¸ªæ—¶é—´æ®µè¿›è¡Œå›å½’ï¼Œå¹¶è®¡ç®—æ®‹å·®"""
        xs = list(df.columns)
        xs = [i for i in xs if i != "fac"]
        xs_join = "+".join(xs)
        ols_formula = "fac~" + xs_join
        ols_result = smf.ols(ols_formula, data=df).fit()
        ols_ws = {i: ols_result.params[i] for i in xs}
        ols_b = ols_result.params["Intercept"]
        to_minus = [ols_ws[i] * df[i] for i in xs]
        to_minus = reduce(lambda x, y: x + y, to_minus)
        df = df.assign(snow_fac=df.fac - to_minus - ols_b)
        df = df[["snow_fac"]]
        df = df.rename(columns={"snow_fac": "fac"})
        return df

    def get_snow_fac(self):
        """è·å¾—çº¯å‡€å› å­"""
        self.snow_fac = self.corr_pri.groupby(["date"]).apply(self.ols_in_group)
        self.snow_fac = self.snow_fac.unstack()
        self.snow_fac.columns = list(map(lambda x: x[1], list(self.snow_fac.columns)))

    def run(self):
        """è¿è¡Œä¸€äº›å¿…è¦çš„å‡½æ•°"""
        self.get_monthly_barras_industrys()
        self.get_wide_barras_industrys()
        self.get_corr_pri_ols_pri()
        self.get_corr()
        self.get_snow_fac()


class pure_snowtrain(object):
    """ç›´æ¥è¿”å›çº¯å‡€å› å­"""

    def __init__(
        self,
        factors: pd.DataFrame,
        facs_dict: dict = None,
        momentum: bool = 1,
        earningsyield: bool = 1,
        growth: bool = 1,
        liquidity: bool = 1,
        size: bool = 1,
        leverage: bool = 1,
        beta: bool = 1,
        nonlinearsize: bool = 1,
        residualvolatility: bool = 1,
        booktoprice: bool = 1,
    ) -> None:
        """è®¡ç®—å› å­å€¼ä¸10ç§å¸¸ç”¨é£æ ¼å› å­ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶è¿›è¡Œçº¯å‡€åŒ–ï¼Œå¯ä»¥é¢å¤–åŠ å…¥å…¶ä»–å› å­

        Parameters
        ----------
        factors : pd.DataFrame
            è¦è€ƒå¯Ÿçš„å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        facs_dict : dict, optional
            _description_, by default None
        momentum : bool, optional
            _description_, by default 1
        earningsyield : bool, optional
            _description_, by default 1
        growth : bool, optional
            _description_, by default 1
        liquidity : bool, optional
            _description_, by default 1
        size : bool, optional
            _description_, by default 1
        leverage : bool, optional
            _description_, by default 1
        beta : bool, optional
            _description_, by default 1
        nonlinearsize : bool, optional
            _description_, by default 1
        residualvolatility : bool, optional
            _description_, by default 1
        booktoprice : bool, optional
            _description_, by default 1
        """
        self.winter = pure_coldwinter(
            facs_dict=facs_dict,
            momentum=momentum,
            earningsyield=earningsyield,
            growth=growth,
            liquidity=liquidity,
            size=size,
            leverage=leverage,
            beta=beta,
            nonlinearsize=nonlinearsize,
            residualvolatility=residualvolatility,
            booktoprice=booktoprice,
        )
        self.winter.set_factors_df_wide(factors.copy())
        self.winter.run()
        self.corr = self.winter.corr

    def __call__(self) -> pd.DataFrame:
        """è·å¾—çº¯å‡€åŒ–ä¹‹åçš„å› å­å€¼

        Returns
        -------
        pd.DataFrame
            çº¯å‡€åŒ–ä¹‹åçš„å› å­å€¼
        """
        return self.winter.snow_fac.copy()


class pure_newyear(object):
    """è½¬ä¸ºç”Ÿæˆ25åˆ†ç»„å’Œç™¾åˆ†ç»„çš„æ”¶ç›ŠçŸ©é˜µè€Œå°è£…"""

    def __init__(
        self,
        facx: pd.DataFrame,
        facy: pd.DataFrame,
        group_num_single: int,
        namex: str = "ä¸»",
        namey: str = "æ¬¡",
    ) -> None:
        """æ¡ä»¶åŒå˜é‡æ’åºæ³•ï¼Œå…ˆå¯¹æ‰€æœ‰è‚¡ç¥¨ï¼Œä¾ç…§å› å­facxè¿›è¡Œæ’åº
        ç„¶ååœ¨æ¯ä¸ªç»„å†…ï¼Œä¾ç…§facyè¿›è¡Œæ’åºï¼Œæœ€åç»Ÿè®¡å„ä¸ªç»„å†…çš„å¹³å‡æ”¶ç›Šç‡

        Parameters
        ----------
        facx : pd.DataFrame
            é¦–å…ˆè¿›è¡Œæ’åºçš„å› å­ï¼Œé€šå¸¸ä¸ºæ§åˆ¶å˜é‡ï¼Œç›¸å½“äºæ­£äº¤åŒ–ä¸­çš„è‡ªå˜é‡
            indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        facy : pd.DataFrame
            åœ¨facxçš„å„ä¸ªç»„å†…ï¼Œä¾ç…§facyè¿›è¡Œæ’åºï¼Œä¸ºä¸»è¦è¦ç ”ç©¶çš„å› å­
            indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        group_num_single : int
            å•ä¸ªå› å­åˆ†æˆå‡ ç»„ï¼Œé€šå¸¸ä¸º5æˆ–10
        namex : str, optional
            facxè¿™ä¸€å› å­çš„åå­—, by default "ä¸»"
        namey : str, optional
            facyè¿™ä¸€å› å­çš„åå­—, by default "æ¬¡"
        """
        homex = pure_fallmount(facx)
        homey = pure_fallmount(facy)
        if group_num_single == 5:
            homexy = homex > homey
        elif group_num_single == 10:
            homexy = homex >> homey
        shen = pure_moonnight(
            homexy(), group_num_single**2, plt_plot=False, print_comments=False
        )
        sq = shen.shen.square_rets.copy()
        sq.index = [namex + str(i) for i in list(sq.index)]
        sq.columns = [namey + str(i) for i in list(sq.columns)]
        self.square_rets = sq

    def __call__(self) -> pd.DataFrame:
        """è°ƒç”¨å¯¹è±¡æ—¶ï¼Œè¿”å›æœ€ç»ˆç»“æœï¼Œæ­£æ–¹å½¢çš„åˆ†ç»„å¹´åŒ–æ”¶ç›Šç‡è¡¨

        Returns
        -------
        `pd.DataFrame`
            æ¯ä¸ªç»„çš„å¹´åŒ–æ”¶ç›Šç‡
        """
        return self.square_rets.copy()


class pure_dawn(object):
    """
    å› å­åˆ‡å‰²è®ºçš„æ¯æ¡†æ¶ï¼Œå¯ä»¥å¯¹ä¸¤ä¸ªå› å­è¿›è¡Œç±»ä¼¼äºå› å­åˆ‡å‰²çš„æ“ä½œ
    å¯ç”¨äºæ´¾ç”Ÿä»»ä½•"ä»¥ä¸¤ä¸ªå› å­ç”Ÿæˆä¸€ä¸ªå› å­"çš„å­ç±»
    ä½¿ç”¨ä¸¾ä¾‹
    cutå‡½æ•°é‡Œï¼Œå¿…é¡»å¸¦æœ‰è¾“å…¥å˜é‡df,dfæœ‰ä¸¤ä¸ªcolumnsï¼Œä¸€ä¸ªåä¸º'fac1'ï¼Œä¸€ä¸ªåä¸º'fac2'ï¼Œdfæ˜¯æœ€è¿‘ä¸€ä¸ªå›çœ‹æœŸå†…çš„æ•°æ®
    ```python
    class Cut(pure_dawn):

    def cut(self,df):
        df=df.sort_values('fac1')
        df=df.assign(fac3=df.fac1*df.fac2)
        ret0=df.fac2.iloc[:4].mean()
        ret1=df.fac2.iloc[4:8].mean()
        ret2=df.fac2.iloc[8:12].mean()
        ret3=df.fac2.iloc[12:16].mean()
        ret4=df.fac2.iloc[16:].mean()
        aret0=df.fac3.iloc[:4].mean()
        aret1=df.fac3.iloc[4:8].mean()
        aret2=df.fac3.iloc[8:12].mean()
        aret3=df.fac3.iloc[12:16].mean()
        aret4=df.fac3.iloc[16:].mean()
        return ret0,ret1,ret2,ret3,ret4,aret0,aret1,aret2,aret3,aret4

    cut=Cut(ct,ret_inday)
    cut.run(cut.cut)

    cut0=get_value(cut(),0)
    cut1=get_value(cut(),1)
    cut2=get_value(cut(),2)
    cut3=get_value(cut(),3)
    cut4=get_value(cut(),4)
    ```
    """

    def __init__(self, fac1: pd.DataFrame, fac2: pd.DataFrame, *args: list) -> None:
        """å‡ ä¸ªå› å­çš„æ“ä½œï¼Œæ¯ä¸ªæœˆæ“ä½œä¸€æ¬¡

        Parameters
        ----------
        fac1 : pd.DataFrame
            å› å­å€¼1ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        fac2 : pd.DataFrame
            å› å­2ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        """
        self.fac1 = fac1
        self.fac1 = self.fac1.stack().reset_index()
        self.fac1.columns = ["date", "code", "fac1"]
        self.fac2 = fac2
        self.fac2 = self.fac2.stack().reset_index()
        self.fac2.columns = ["date", "code", "fac2"]
        fac_all = pd.merge(self.fac1, self.fac2, on=["date", "code"])
        for i, fac in enumerate(args):
            fac = fac.stack().reset_index()
            fac.columns = ["date", "code", f"fac{i+3}"]
            fac_all = pd.merge(fac_all, fac, on=["date", "code"])
        fac_all = fac_all.sort_values(["date", "code"])
        self.fac = fac_all.copy()

    def __call__(self) -> pd.DataFrame:
        """è¿”å›æœ€ç»ˆæœˆåº¦å› å­å€¼

        Returns
        -------
        `pd.DataFrame`
            æœ€ç»ˆå› å­å€¼
        """
        return self.fac.copy()

    def get_fac_long_and_tradedays(self):
        """å°†ä¸¤ä¸ªå› å­çš„çŸ©é˜µè½¬åŒ–ä¸ºé•¿åˆ—è¡¨"""
        self.tradedays = sorted(list(set(self.fac.date)))

    def get_month_starts_and_ends(self, backsee=20):
        """è®¡ç®—å‡ºæ¯ä¸ªæœˆå›çœ‹æœŸé—´çš„èµ·ç‚¹æ—¥å’Œç»ˆç‚¹æ—¥"""
        self.month_ends = [
            i
            for i, j in zip(self.tradedays[:-1], self.tradedays[1:])
            if i.month != j.month
        ]
        self.month_ends.append(self.tradedays[-1])
        self.month_starts = [
            self.find_begin(self.tradedays, i, backsee=backsee) for i in self.month_ends
        ]
        self.month_starts[0] = self.tradedays[0]

    def find_begin(self, tradedays, end_day, backsee=20):
        """æ‰¾å‡ºå›çœ‹è‹¥å¹²å¤©çš„å¼€å§‹æ—¥ï¼Œé»˜è®¤ä¸º20"""
        end_day_index = tradedays.index(end_day)
        start_day_index = end_day_index - backsee + 1
        start_day = tradedays[start_day_index]
        return start_day

    def make_monthly_factors_single_code(self, df, func):
        """
        å¯¹å•ä¸€è‚¡ç¥¨æ¥è®¡ç®—æœˆåº¦å› å­
        funcä¸ºå•æœˆæ‰§è¡Œçš„å‡½æ•°ï¼Œè¿”å›å€¼åº”ä¸ºæœˆåº¦å› å­ï¼Œå¦‚ä¸€ä¸ªfloatæˆ–ä¸€ä¸ªlist
        dfä¸ºä¸€ä¸ªè‚¡ç¥¨çš„å››åˆ—è¡¨ï¼ŒåŒ…å«æ—¶é—´ã€ä»£ç ã€å› å­1å’Œå› å­2
        """
        res = {}
        for start, end in zip(self.month_starts, self.month_ends):
            this_month = df[(df.date >= start) & (df.date <= end)]
            res[end] = func(this_month)
        dates = list(res.keys())
        corrs = list(res.values())
        part = pd.DataFrame({"date": dates, "corr": corrs})
        return part

    def get_monthly_factor(self, func):
        """è¿è¡Œè‡ªå·±å†™çš„å‡½æ•°ï¼Œè·å¾—æœˆåº¦å› å­"""
        tqdm.tqdm.pandas(desc="when the dawn comes, tonight will be a memory too.")
        self.fac = self.fac.groupby(["code"]).progress_apply(
            lambda x: self.make_monthly_factors_single_code(x, func)
        )
        self.fac = (
            self.fac.reset_index(level=1, drop=True)
            .reset_index()
            .set_index(["date", "code"])
            .unstack()
        )
        self.fac.columns = [i[1] for i in list(self.fac.columns)]
        self.fac = self.fac.resample("M").last()

    @kk.desktop_sender(title="å˜¿ï¼Œåˆ‡å‰²å®Œæˆå•¦ğŸ›")
    def run(self, func: Callable, backsee: int = 20) -> None:
        """æ‰§è¡Œè®¡ç®—çš„æ¡†æ¶ï¼Œäº§ç”Ÿå› å­å€¼

        Parameters
        ----------
        func : Callable
            æ¯ä¸ªæœˆè¦è¿›è¡Œçš„æ“ä½œ
        backsee : int, optional
            å›çœ‹æœŸï¼Œå³æ¯ä¸ªæœˆæœˆåº•å¯¹è¿‡å»å¤šå°‘å¤©è¿›è¡Œè®¡ç®—, by default 20
        """
        self.get_fac_long_and_tradedays()
        self.get_month_starts_and_ends(backsee=backsee)
        self.get_monthly_factor(func)
