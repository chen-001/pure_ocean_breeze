import copy
import warnings
warnings.filterwarnings('ignore')
import os
import tqdm
import numpy as np
import pandas as pd
import scipy.io as scio
import statsmodels.formula.api as smf
from functools import partial, reduce
from collections import Iterable
import datetime
from dateutil.relativedelta import relativedelta
import matplotlib.pyplot as plt
import plotly.express as pe
import plotly.io as pio
import scipy.stats as ss
from loguru import logger
import time
from functools import lru_cache,wraps
plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
import h5py
from cachier import cachier
import pickle
import redis
import swifter
import knockknock as kk
import alphalens as al

NO_LOG = False
NO_COMMENT = False
NO_SAVE = False
NO_PLOT = False
NO_SUMMER = False

if NO_SUMMER:
    ...
else:
    print('æ¬¢è¿ä½¿ç”¨èŠ·ç¦å“¥çš„å›æµ‹æ¡†æ¶ğŸ‘‹')


class HomePlace(object):
    def __init__(self):
        user_file=os.path.expanduser('~')+'/'
        path_file=open(user_file+'paths.settings','rb')
        paths=pickle.load(path_file)
        self.__dict__=paths

class params_setter(object):
    '''ç”¨äºæ ‡æ³¨è®¾ç½®å‚æ•°éƒ¨åˆ†çš„è£…é¥°å™¨'''
    def __init__(self,slogan=None):
        if not slogan:
            slogan='è¿™æ˜¯è®¾ç½®å‚æ•°ç±»å‹çš„å‡½æ•°\n'
        self.slogan=slogan
        self.box={}

    def __call__(self,func):
        # func.__doc__=self.slogan+func.__doc__
        self.box[func.__name__]=func
        self.func=func

        def wrapper(*args,**kwargs):
            func(*args,**kwargs)
            # if not NO_LOG:
            #     logger.info(f'{func.__name__} has been called $ kind of params_setter')
        return wrapper

class main_process(object):
    '''ç”¨äºæ ‡è®°ä¸»é€»è¾‘è¿‡ç¨‹çš„è£…é¥°å™¨'''
    def __init__(self,slogan=None):
        if not slogan:
            slogan='è¿™æ˜¯ä¸»é€»è¾‘è¿‡ç¨‹çš„å‡½æ•°\n'
        self.slogan=slogan
        self.box={}

    def __call__(self,func):
        # func.__doc__=self.slogan+func.__doc__
        self.box[func.__name__]=func

        def wrapper(*args,**kwargs):
            func(*args,**kwargs)
            # if NO_LOG:
            #     logger.success(f'{func.__name__} has been called $ kind of main_process')
        return wrapper

class tool_box(object):
    '''ç”¨äºæ ‡æ³¨å·¥å…·ç®±éƒ¨åˆ†çš„è£…é¥°å™¨'''
    def __init__(self,slogan=None):
        if not slogan:
            slogan='è¿™æ˜¯å·¥å…·ç®±çš„å‡½æ•°\n'
        self.slogan=slogan
        self.box={}

    def __call__(self,func):
        # func.__doc__=self.slogan+func.__doc__
        self.box[func.__name__]=func

        def wrapper(*args,**kwargs):
            res=func(*args,**kwargs)
            # logger.success(f'{func.__name__} has been called $ kind of tool_box')
            return res
        return wrapper

class history_remain(object):
    '''ç”¨äºå†å²é—ç•™éƒ¨åˆ†çš„è£…é¥°å™¨'''
    def __init__(self,slogan=None):
        if not slogan:
            slogan='è¿™æ˜¯å†å²é—ç•™çš„å‡½æ•°\n'
        self.slogan=slogan
        self.box={}

    def __call__(self,func):
        # func.__doc__=self.slogan+func.__doc__
        self.box[func.__name__]=func

        def wrapper(*args,**kwargs):
            func(*args,**kwargs)
            # logger.success(f'{func.__name__} has been called $ kind of history_remain')
        return wrapper



@cachier()
def read_daily(path=None,open=0,close=0,high=0,low=0,tr=0,sharenum=0,volume=0,unadjust=0):
    '''è¯»å–æ—¥é¢‘æ•°æ®,ä½¿ç”¨read_daily.clear_cache()æ¥æ¸…ç©ºç¼“å­˜'''
    def read_mat(path):
        homeplace=HomePlace()
        col=list(scio.loadmat(homeplace.daily_data_file+'AllStockCode.mat').values())[3]
        index=list(scio.loadmat(homeplace.daily_data_file+'TradingDate_Daily.mat').values())[3]
        col=[i[0] for i in col[0]]
        index=index[0].tolist()
        path=homeplace.daily_data_file+path
        data=list(scio.loadmat(path).values())[3]
        data=pd.DataFrame(data,index=index,columns=col)
        data.index=pd.to_datetime(data.index,format='%Y%m%d')
        data=data.replace(0,np.nan)
        return data
    if not unadjust:
        if path:
            return read_mat(path)
        elif open:
            trs=read_mat('AllStock_DailyTR.mat')
            opens=read_mat('AllStock_DailyOpen_dividend.mat')
            return np.sign(trs)*opens
        elif close:
            trs=read_mat('AllStock_DailyTR.mat')
            closes=read_mat('AllStock_DailyClose_dividend.mat')
            return np.sign(trs)*closes
        elif high:
            trs=read_mat('AllStock_DailyTR.mat')
            highs=read_mat('AllStock_DailyHigh_dividend.mat')
            return np.sign(trs)*highs
        elif low:
            trs=read_mat('AllStock_DailyTR.mat')
            lows=read_mat('AllStock_DailyLow_dividend.mat')
            return np.sign(trs)*lows
        elif tr:
            trs=read_mat('AllStock_DailyTR.mat')
            return trs
        elif sharenum:
            sharenums=read_mat('AllStock_DailyAShareNum.mat')
            return sharenums
        elif volume:
            volumes=read_mat('AllStock_DailyVolume.mat')
            return volumes
        else:
            raise IOError('é˜ä¸‹æ€»å¾—è¯»ç‚¹ä»€ä¹ˆå§ï¼ŸğŸ¤’')
    else:
        if path:
            return read_mat(path)
        elif open:
            trs=read_mat('AllStock_DailyTR.mat')
            opens=read_mat('AllStock_DailyOpen.mat')
            return np.sign(trs)*opens
        elif close:
            trs=read_mat('AllStock_DailyTR.mat')
            closes=read_mat('AllStock_DailyClose.mat')
            return np.sign(trs)*closes
        elif high:
            trs=read_mat('AllStock_DailyTR.mat')
            highs=read_mat('AllStock_DailyHigh.mat')
            return np.sign(trs)*highs
        elif low:
            trs=read_mat('AllStock_DailyTR.mat')
            lows=read_mat('AllStock_DailyLow.mat')
            return np.sign(trs)*lows
        elif tr:
            trs=read_mat('AllStock_DailyTR.mat')
            return trs
        elif sharenum:
            sharenums=read_mat('AllStock_DailyAShareNum.mat')
            return sharenums
        elif volume:
            volumes=read_mat('AllStock_DailyVolume.mat')
            return volumes
        else:
            raise IOError('é˜ä¸‹æ€»å¾—è¯»ç‚¹ä»€ä¹ˆå§ï¼ŸğŸ¤’')

def read_market(full=False,wide=True,open=0,high=0,low=0,close=0,amount=0,money=0):
    '''è¯»å–windå…¨Aæ—¥è¡Œæƒ…ï¼Œå¦‚æœä¸ºfullï¼Œåˆ™ç›´æ¥è¿”å›åŸå§‹è¡¨æ ¼ï¼Œå¦‚æœfullä¸ºFalseï¼Œåˆ™è¿”å›éƒ¨åˆ†æ•°æ®
    å¦‚æœwideä¸ºTrueï¼Œåˆ™è¿”å›æ–¹é˜µå½¢å¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œæ¯ä¸€åˆ—çš„æ•°æ®éƒ½ä¸€æ ·ï¼Œè¿™ä¹ˆåšæ˜¯æŒ‡ä¸ºäº†ä¾¿äºä¸ä¸ªè‚¡è¿ç®—'''
    market=pd.read_excel(homeplace.daily_data_file+'windå…¨Aæ—¥è¡Œæƒ….xlsx')
    market=market.drop(columns=['ä»£ç ','åç§°'])
    market.columns=['date','open','high','low','close','amount','money']
    market.money=market.money*1000000
    if full:
        return market
    else:
        if wide:
            tr=read_daily(tr=1)
            tr=np.abs(np.sign(tr)).replace(1,0).fillna(0)
            if open:
                market=market[['date','open']]
                market.date=pd.to_datetime(market.date)
                market=market[market.date.isin(list(tr.index))]
                market=market.set_index('date')
                market=market['open']
                tr_one=tr.iloc[:,0]
                market=market+tr_one
                market=market.fillna(method='ffill')
                market=pd.DataFrame({k:list(market) for k in list(tr.columns)},index=tr.index)
            elif high:
                market=market[['date','high']]
                market.date=pd.to_datetime(market.date)
                market=market[market.date.isin(list(tr.index))]
                market=market.set_index('date')
                market=market['high']
                tr_one=tr.iloc[:,0]
                market=market+tr_one
                market=market.fillna(method='ffill')
                market=pd.DataFrame({k:list(market) for k in list(tr.columns)},index=tr.index)
            elif low:
                market=market[['date','low']]
                market.date=pd.to_datetime(market.date)
                market=market[market.date.isin(list(tr.index))]
                market=market.set_index('date')
                market=market['date','low']
                tr_one=tr.iloc[:,0]
                market=market+tr_one
                market=market.fillna(method='ffill')
                market=pd.DataFrame({k:list(market) for k in list(tr.columns)},index=tr.index)
            elif close:
                market=market[['date','close']]
                market.date=pd.to_datetime(market.date)
                market=market[market.date.isin(list(tr.index))]
                market=market.set_index('date')
                market=market['close']
                tr_one=tr.iloc[:,0]
                market=market+tr_one
                market=market.fillna(method='ffill')
                market=pd.DataFrame({k:list(market) for k in list(tr.columns)},index=tr.index)
            elif amount:
                market=market[['date','amount']]
                market.date=pd.to_datetime(market.date)
                market=market[market.date.isin(list(tr.index))]
                market=market.set_index('date')
                market=market['amount']
                tr_one=tr.iloc[:,0]
                market=market+tr_one
                market=market.fillna(method='ffill')
                market=pd.DataFrame({k:list(market) for k in list(tr.columns)},index=tr.index)
            elif money:
                market=market[['date','money']]
                market.date=pd.to_datetime(market.date)
                market=market[market.date.isin(list(tr.index))]
                market=market.set_index('date')
                market=market['money']
                tr_one=tr.iloc[:,0]
                market=market+tr_one
                market=market.fillna(method='ffill')
                market=pd.DataFrame({k:list(market) for k in list(tr.columns)},index=tr.index)
            else:
                raise IOError('æ‚¨æ€»å¾—è¯»ç‚¹ä»€ä¹ˆå§ï¼ŸğŸ¤’')
            return market
        else:
            cols=[varname.nameof(i) for i in [open,high,low,close,amount,money] if i==1]
            market=market[['date']+cols]
            return market

def read_h5(path):
    '''è¯»å–h5æ–‡ä»¶ä¸­çš„æ‰€æœ‰å­—å…¸'''
    import tqdm
    import h5py
    import pandas as pd
    res={}
    a=h5py.File(path)
    for k,v in tqdm.tqdm(list(a.items()),desc='æ•°æ®åŠ è½½ä¸­â€¦â€¦'):
        value=list(v.values())[-1]
        col=[i.decode('utf-8') for i in list(list(v.values())[0])]
        ind=[i.decode('utf-8') for i in list(list(v.values())[1])]
        res[k]=pd.DataFrame(value,columns=col,index=ind)
    return res

def get_value(df,n):
    '''å¾ˆå¤šå› å­è®¡ç®—æ—¶ï¼Œä¼šä¸€æ¬¡æ€§ç”Ÿæˆå¾ˆå¤šå€¼ï¼Œä½¿ç”¨æ—¶åªå–å‡ºä¸€ä¸ªå€¼'''
    def get_value_single(x,n):
        try:
            return x[n]
        except Exception:
            return np.nan
    df=df.applymap(lambda x:get_value_single(x,n))
    return df

def comment_on_rets_and_nets(rets,nets,name):
    '''
    è¾“å…¥æ”¶ç›Šç‡åºåˆ—å’Œå‡€å€¼åºåˆ—ï¼Œè¾“å‡ºå¹´åŒ–æ”¶ç›Šã€å¹´åŒ–æ³¢åŠ¨ã€ä¿¡æ¯æ¯”ç‡ã€æœˆåº¦èƒœç‡å’Œæœ€å¤§å›æ’¤ç‡
    è¾“å…¥2ä¸ªpd.Seriesï¼Œæ—¶é—´æ˜¯ç´¢å¼•
    '''
    duration_nets=(nets.index[-1]-nets.index[0]).days
    year_nets=duration_nets/365
    ret_yearly=(nets.iloc[-1]/nets.iloc[0])**(1/year_nets)-1
    max_draw=((nets.cummax()-nets)/nets.cummax()).max()
    vol=np.std(rets)*(12**0.5)
    info_rate=ret_yearly/vol
    win_rate=len(rets[rets>0])/len(rets)
    comments=pd.DataFrame({
        'å¹´åŒ–æ”¶ç›Šç‡':ret_yearly,'å¹´åŒ–æ³¢åŠ¨ç‡':vol,'ä¿¡æ¯æ¯”ç‡':info_rate,'æœˆåº¦èƒœç‡':win_rate,'æœ€å¤§å›æ’¤ç‡':max_draw
    },index=[name]).T
    return comments

def comments_on_twins(series,series1):
    '''å¯¹twinsä¸­çš„ç»“æœç»™å‡ºè¯„ä»·
    è¯„ä»·æŒ‡æ ‡åŒ…æ‹¬å¹´åŒ–æ”¶ç›Šç‡ã€æ€»æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€å¹´åŒ–å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤ç‡ã€èƒœç‡'''
    ret=(series.iloc[-1]-series.iloc[0])/series.iloc[0]
    duration=(series.index[-1]-series.index[0]).days
    year=duration/365
    ret_yearly=(series.iloc[-1]/series.iloc[0])**(1/year)-1
    max_draw=-(series/series.expanding(1).max()-1).min()
    vol=np.std(series1)*(12**0.5)
    sharpe=ret_yearly/vol
    wins=series1[series1>0]
    win_rate=len(wins)/len(series1)
    return pd.Series([ret,ret_yearly,vol,sharpe,win_rate,max_draw],
                     index=['æ€»æ”¶ç›Šç‡','å¹´åŒ–æ”¶ç›Šç‡','å¹´åŒ–æ³¢åŠ¨ç‡','ä¿¡æ¯æ¯”ç‡','èƒœç‡','æœ€å¤§å›æ’¤ç‡'])

def comments_on_twins_periods(series,series1,periods=None):
    '''å¯¹twinsä¸­çš„ç»“æœç»™å‡ºè¯„ä»·
    è¯„ä»·æŒ‡æ ‡åŒ…æ‹¬å¹´åŒ–æ”¶ç›Šç‡ã€æ€»æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€å¹´åŒ–å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤ç‡ã€èƒœç‡'''
    ret=(series.iloc[-1]-series.iloc[0])/series.iloc[0]
    duration=(series.index[-1]-series.index[0]).days
    year=duration/365
    ret_yearly=(series.iloc[-1]/series.iloc[0])**(1/year)-1
    max_draw=-(series/series.expanding(1).max()-1).min()
    vol=np.std(series1)*(252**0.5)*(periods**0.5)
    sharpe=ret_yearly/vol
    wins=series1[series1>0]
    win_rate=len(wins)/len(series1)
    return pd.Series([ret,ret_yearly,vol,sharpe,win_rate,max_draw],
                     index=['æ€»æ”¶ç›Šç‡','å¹´åŒ–æ”¶ç›Šç‡','å¹´åŒ–æ³¢åŠ¨ç‡','ä¿¡æ¯æ¯”ç‡','èƒœç‡','æœ€å¤§å›æ’¤ç‡'])

def daily_factor_on300500(fac,hs300=False,zz500=False,zz800=False,zz1000=False,gz2000=False,other=False):
    '''è¾“å…¥æ—¥é¢‘å› å­ï¼ŒæŠŠæ—¥é¢‘å› å­å˜ä¸ºä»…åœ¨300æˆ–è€…500ä¸Šçš„è‚¡ç¥¨æ± '''
    last=fac.resample('M').last()
    homeplace=HomePlace()
    if fac.shape[0]/last.shape[0]>2:
        if hs300:
            df=pd.read_feather(homeplace.daily_data_file+'æ²ªæ·±300æˆåˆ†è‚¡.feather').set_index('index').replace(0,np.nan)
            df=df*fac
            df=df.dropna(how='all')
        elif zz500:
            df=pd.read_feather(homeplace.daily_data_file+'ä¸­è¯500æˆåˆ†è‚¡.feather').set_index('index').replace(0,np.nan)
            df=df*fac
            df=df.dropna(how='all')
        elif zz800:
            df1=pd.read_feather(homeplace.daily_data_file+'æ²ªæ·±300æˆåˆ†è‚¡.feather').set_index('index')
            df2=pd.read_feather(homeplace.daily_data_file+'ä¸­è¯500æˆåˆ†è‚¡.feather').set_index('index')
            df=df1+df2
            df=df.replace(0,np.nan)
            df=df*fac
            df=df.dropna(how='all')
        elif zz1000:
            df=pd.read_feather(homeplace.daily_data_file+'ä¸­è¯1000æˆåˆ†è‚¡.feather').set_index('index').replace(0,np.nan)
            df=df*fac
            df=df.dropna(how='all')
        elif gz2000:
            df=pd.read_feather(homeplace.daily_data_file+'å›½è¯2000æˆåˆ†è‚¡.feather').set_index('index').replace(0,np.nan)
            df=df*fac
            df=df.dropna(how='all')
        elif other:
            tr=read_daily(tr=1).fillna(0).replace(0,1)
            tr=np.sign(tr)
            df1=(tr*pd.read_feather(homeplace.daily_data_file+'æ²ªæ·±300æˆåˆ†è‚¡.feather').set_index('index')).fillna(0)
            df2=(tr*pd.read_feather(homeplace.daily_data_file+'ä¸­è¯500æˆåˆ†è‚¡.feather').set_index('index')).fillna(0)
            df3=(tr*pd.read_feather(homeplace.daily_data_file+'ä¸­è¯1000æˆåˆ†è‚¡.feather').set_index('index')).fillna(0)
            df=(1-df1)*(1-df2)*(1-df3)*tr
            df=df.replace(0,np.nan)*fac
            df=df.dropna(how='all')
        else:
            raise ValueError('æ€»å¾—æŒ‡å®šä¸€ä¸‹æ˜¯å“ªä¸ªæˆåˆ†è‚¡å§ğŸ¤’')
    else:
        if hs300:
            df=pd.read_feather(homeplace.daily_data_file+'æ²ªæ·±300æˆåˆ†è‚¡.feather').set_index('index').replace(0,np.nan).resample('M').last()
            df=df*fac
            df=df.dropna(how='all')
        elif zz500:
            df=pd.read_feather(homeplace.daily_data_file+'ä¸­è¯500æˆåˆ†è‚¡.feather').set_index('index').replace(0,np.nan).resample('M').last()
            df=df*fac
            df=df.dropna(how='all')
        elif zz800:
            df1=pd.read_feather(homeplace.daily_data_file+'æ²ªæ·±300æˆåˆ†è‚¡.feather').set_index('index').resample('M').last()
            df2=pd.read_feather(homeplace.daily_data_file+'ä¸­è¯500æˆåˆ†è‚¡.feather').set_index('index').resample('M').last()
            df=df1+df2
            df=df.replace(0,np.nan)
            df=df*fac
            df=df.dropna(how='all')
        elif zz1000:
            df=pd.read_feather(homeplace.daily_data_file+'ä¸­è¯1000æˆåˆ†è‚¡.feather').set_index('index').replace(0,np.nan).resample('M').last()
            df=df*fac
            df=df.dropna(how='all')
        elif gz2000:
            df=pd.read_feather(homeplace.daily_data_file+'å›½è¯2000æˆåˆ†è‚¡.feather').set_index('index').replace(0,np.nan).resample('M').last()
            df=df*fac
            df=df.dropna(how='all')
        elif other:
            tr=read_daily(tr=1).fillna(0).replace(0,1).resample('M').last()
            tr=np.sign(tr)
            df1=(tr*pd.read_feather(homeplace.daily_data_file+'æ²ªæ·±300æˆåˆ†è‚¡.feather').set_index('index').resample('M').last()).fillna(0)
            df2=(tr*pd.read_feather(homeplace.daily_data_file+'ä¸­è¯500æˆåˆ†è‚¡.feather').set_index('index').resample('M').last()).fillna(0)
            df3=(tr*pd.read_feather(homeplace.daily_data_file+'ä¸­è¯1000æˆåˆ†è‚¡.feather').set_index('index').resample('M').last()).fillna(0)
            df=(1-df1)*(1-df2)*(1-df3)
            df=df.replace(0,np.nan)*fac
            df=df.dropna(how='all')
        else:
            raise ValueError('æ€»å¾—æŒ‡å®šä¸€ä¸‹æ˜¯å“ªä¸ªæˆåˆ†è‚¡å§ğŸ¤’')
    return df

def select_max(df1,df2):
    '''ä¸¤ä¸ªcolumnsä¸indexå®Œå…¨ç›¸åŒçš„dfï¼Œæ¯ä¸ªå€¼éƒ½æŒ‘å‡ºè¾ƒå¤§å€¼'''
    return (df1+df2+np.abs(df1-df2))/2

def select_min(df1,df2):
    '''ä¸¤ä¸ªcolumnsä¸indexå®Œå…¨ç›¸åŒçš„dfï¼Œæ¯ä¸ªå€¼éƒ½æŒ‘å‡ºè¾ƒå°å€¼'''
    return (df1+df2-np.abs(df1-df2))/2

@kk.desktop_sender(title='å˜¿ï¼Œè¡Œä¸šä¸­æ€§åŒ–åšå®Œå•¦ï½ğŸ›')
def decap(df,daily=False,monthly=False):
    '''åšå¸‚å€¼ä¸­æ€§åŒ–'''
    tqdm.tqdm.pandas()
    share=read_daily('AllStock_DailyAShareNum.mat')
    undi_close=read_daily('AllStock_DailyClose.mat')
    cap=(share*undi_close).stack().reset_index()
    cap.columns=['date','code','cap']
    cap.cap=ss.boxcox(cap.cap)[0]
    def single(x):
        x.cap=ss.boxcox(x.cap)[0]
        return x
    cap=cap.groupby(['date']).apply(single)
    cap=cap.set_index(['date','code']).unstack()
    cap.columns=[i[1] for i in list(cap.columns)]
    cap_monthly=cap.resample('M').last()
    last=df.resample('M').last()
    if df.shape[0]/last.shape[0]<2:
        monthly=True
    else:
        daily=True
    if daily:
        df=(pure_fallmount(df)-(pure_fallmount(cap),))()
    elif monthly:
        df=(pure_fallmount(df)-(pure_fallmount(cap_monthly),))()
    else:
        raise NotImplementedError('å¿…é¡»æŒ‡å®šé¢‘ç‡')
    return df

@kk.desktop_sender(title='å˜¿ï¼Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–åšå®Œå•¦ï½ğŸ›')
def decap_industry(df,daily=False,monthly=False):
    last=df.resample('M').last()
    homeplace=HomePlace()
    share=read_daily('AllStock_DailyAShareNum.mat')
    undi_close=read_daily('AllStock_DailyClose.mat')
    cap=(share*undi_close).stack().reset_index()
    cap.columns=['date','code','cap']
    cap.cap=ss.boxcox(cap.cap)[0]
    def single(x):
        x.cap=ss.boxcox(x.cap)[0]
        return x
    cap=cap.groupby(['date']).apply(single)
    df=df.stack().reset_index()
    df.columns=['date','code','fac']
    df=pd.merge(df,cap,on=['date','code'])
    if df.shape[0]/last.shape[0]<2:
        monthly=True
    else:
        daily=True
    def neutralize_factors(df):
        '''ç»„å†…å¯¹å› å­è¿›è¡Œå¸‚å€¼ä¸­æ€§åŒ–'''
        industry_codes=list(df.columns)
        industry_codes=[i for i in industry_codes if i.startswith('w')]
        industry_codes_str='+'.join(industry_codes)
        ols_result = smf.ols('fac~cap+'+industry_codes_str, data=df).fit()
        ols_w = ols_result.params['cap']
        ols_b = ols_result.params['Intercept']
        ols_bs={}
        for ind in industry_codes:
            ols_bs[ind]=ols_result.params[ind]
        df.fac = df.fac - ols_w * df.cap - ols_b
        for k,v in ols_bs.items():
            df.fac=df.fac-v*df[k]
        df = df[['fac']]
        return df
    if monthly:
        industry_dummy=pd.read_feather(homeplace.daily_data_file+'ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather').set_index('date').groupby('code').resample('M').last()
        industry_dummy=industry_dummy.fillna(0).drop(columns=['code']).reset_index()
        industry_ws=[f'w{i}' for i in range(1,industry_dummy.shape[1]-1)]
        col=['code','date']+industry_ws
    elif daily:
        industry_dummy=pd.read_feather(homeplace.daily_data_file+'ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather').fillna(0)
        industry_ws=[f'w{i}' for i in range(1,industry_dummy.shape[1]-1)]
        col=['date','code']+industry_ws
    industry_dummy.columns=col
    df=pd.merge(df,industry_dummy,on=['date','code'])
    df=df.set_index(['date','code'])
    tqdm.tqdm.pandas()
    df=df.groupby(['date']).progress_apply(neutralize_factors)
    df=df.unstack()
    df.columns=[i[1] for i in list(df.columns)]
    return df

def detect_nan(df):
    x=np.sum(df.to_numpy().flatten())
    if np.isnan(x):
        print(df)
        return np.nan
    else:
        return x

def deboth(df):
    shen=pure_moonnight(df,boxcox=1)
    return shen()

def boom_four(df,minus=None,backsee=20,daily=False,min_periods=None):
    '''ä½¿ç”¨20å¤©å‡å€¼å’Œæ ‡å‡†å·®ï¼Œç”Ÿæˆ4ä¸ªå› å­'''
    if min_periods is None:
        min_periods=int(backsee*0.5)
    if not daily:
        df_mean=df.rolling(backsee,min_periods=min_periods).mean().resample('M').last()
        df_std=df.rolling(backsee,min_periods=min_periods).std().resample('M').last()
        twins_add=(pure_fallmount(df_mean)+(pure_fallmount(df_std),))()
        rtwins_add=df_mean.rank(axis=1)+df_std.rank(axis=1)
        twins_minus=(pure_fallmount(df_mean)+(pure_fallmount(-df_std),))()
        rtwins_minus=df_mean.rank(axis=1)-df_std.rank(axis=1)
    else:
        df_mean=df.rolling(backsee,min_periods=min_periods).mean()
        df_std=df.rolling(backsee,min_periods=min_periods).std()
        twins_add=(pure_fallmount(df_mean)+(pure_fallmount(df_std),))()
        rtwins_add=df_mean.rank(axis=1)+df_std.rank(axis=1)
        twins_minus=(pure_fallmount(df_mean)+(pure_fallmount(-df_std),))()
        rtwins_minus=df_mean.rank(axis=1)-df_std.rank(axis=1)
    return df_mean,df_std,twins_add,rtwins_add,twins_minus,rtwins_minus

def get_abs(df,median=False,square=False):
    '''ç”Ÿäº§å› å­æˆªé¢ä¸Šè·ç¦»å‡å€¼çš„è·ç¦»'''
    if not square:
        if median:
            return np.abs((df.T-df.T.median()).T)
        else:
            return np.abs((df.T-df.T.mean()).T)
    else:
        if median:
            return ((df.T-df.T.median()).T)**2
        else:
            return ((df.T-df.T.mean()).T)**2

def add_cross_standardlize(*args):
    '''å°†ä¼—å¤šå› å­æ¨ªæˆªé¢æ ‡å‡†åŒ–ä¹‹åç›¸åŠ '''
    fms=[pure_fallmount(i) for i in args]
    one=fms[0]
    others=fms[1:]
    final=one+others
    return final()

def get_normal(df):
    '''å°†å› å­æ¨ªæˆªé¢æ­£æ€åŒ–'''
    df=df.replace(0,np.nan)
    df=df.T.apply(lambda x:ss.boxcox(x)[0]).T
    return df


def read_index_three(day=False):
    '''è¯»å–ä¸‰å¤§æŒ‡æ•°çš„åŸå§‹è¡Œæƒ…æ•°æ®ï¼Œè¿”å›å¹¶ä¿å­˜åœ¨æœ¬åœ°'''
    def read(file):
        homeplace=HomePlace()
        file1=homeplace.daily_data_file+f'{file}æ—¥è¡Œæƒ….xlsx'
        df=pd.read_excel(file1)
        df.columns=['code','name','date','open','high','low','close','amount','money']
        df=df[['date','close']]
        df.date=pd.to_datetime(df.date)
        df=df.set_index('date')
        df=df.resample('M').last()
        df.columns=[file]
        if not day:
            df=df[df.index>=pd.Timestamp('2013-04-01')]
        else:
            df=df[df.index>=pd.Timestamp(day)]
        df=df/df[file].iloc[0]
        return df
    hs300=read('æ²ªæ·±300')
    zz500=read('ä¸­è¯500')
    zz1000=read('ä¸­è¯1000')
    w=pd.ExcelWriter('3510åŸå§‹è¡Œæƒ….xlsx')
    hs300.to_excel(w,sheet_name='300')
    zz500.to_excel(w,sheet_name='500')
    zz1000.to_excel(w,sheet_name='1000')
    w.save()
    w.close()
    return hs300,zz500,zz1000

def make_relative_comments(ret_fac,hs300=0,zz500=0,zz1000=0,day=False):
    if hs300:
        net_index=read_index_three(day=day)[0].iloc[:,0]
    elif zz500:
        net_index=read_index_three(day=day)[1].iloc[:,0]
    elif zz1000:
        net_index=read_index_three(day=day)[2].iloc[:,0]
    else:
        raise IOError('ä½ æ€»å¾—æŒ‡å®šä¸€ä¸ªè‚¡ç¥¨æ± å§ï¼Ÿ')
    ret_index=net_index.pct_change()
    ret=ret_fac-ret_index
    ret=ret.dropna()
    net=(1+ret).cumprod()
    net=net/net.iloc[0]
    com=comments_on_twins(net,ret)
    return com

def make_relative_comments_plot(ret_fac,hs300=0,zz500=0,zz1000=0,day=False):
    if hs300:
        net_index=read_index_three(day=day)[0].iloc[:,0]
    elif zz500:
        net_index=read_index_three(day=day)[1].iloc[:,0]
    elif zz1000:
        net_index=read_index_three(day=day)[2].iloc[:,0]
    else:
        raise IOError('ä½ æ€»å¾—æŒ‡å®šä¸€ä¸ªè‚¡ç¥¨æ± å§ï¼Ÿ')
    ret_index=net_index.pct_change()
    ret=ret_fac-ret_index
    ret=ret.dropna()
    net=(1+ret).cumprod()
    net=net/net.iloc[0]
    com=comments_on_twins(net,ret)
    net.plot()
    plt.show()
    return net


def comments_ten(shen):
    rets_cols=list(shen.shen.group_rets.columns)
    rets_cols=rets_cols[:-1]
    coms=[]
    for i in rets_cols:
        ret=shen.shen.group_rets[i]
        net=shen.shen.group_net_values[i]
        com=comments_on_twins(net,ret)
        com=com.to_frame(i)
        coms.append(com)
    df=pd.concat(coms,axis=1)
    return df.T

def coin_reverse(ret20,vol20,mean=1,positive_negtive=0):
    '''æ ¹æ®vol20çš„å¤§å°ï¼Œç¿»è½¬ä¸€åŠret20ï¼ŒæŠŠvol20è¾ƒå¤§çš„éƒ¨åˆ†ï¼Œç»™ret20æ·»åŠ è´Ÿå·'''
    if positive_negtive:
        if not mean:
            down20=np.sign(ret20)
            down20=down20.replace(1,np.nan)
            down20=down20.replace(-1,1)

            vol20_down=down20*vol20
            vol20_down=(vol20_down.T-vol20_down.T.median()).T
            vol20_down=np.sign(vol20_down)
            ret20_down=ret20[ret20<0]
            ret20_down=vol20_down*ret20_down

            up20=np.sign(ret20)
            up20=up20.replace(-1,np.nan)

            vol20_up=up20*vol20
            vol20_up=(vol20_up.T-vol20_up.T.median()).T
            vol20_up=np.sign(vol20_up)
            ret20_up=ret20[ret20>0]
            ret20_up=vol20_up*ret20_up

            ret20_up=ret20_up.replace(np.nan,0)
            ret20_down=ret20_down.replace(np.nan,0)
            new_ret20=ret20_up+ret20_down
            new_ret20_tr=new_ret20.replace(0,np.nan)
            return new_ret20_tr
        else:
            down20=np.sign(ret20)
            down20=down20.replace(1,np.nan)
            down20=down20.replace(-1,1)

            vol20_down=down20*vol20
            vol20_down=(vol20_down.T-vol20_down.T.mean()).T
            vol20_down=np.sign(vol20_down)
            ret20_down=ret20[ret20<0]
            ret20_down=vol20_down*ret20_down

            up20=np.sign(ret20)
            up20=up20.replace(-1,np.nan)

            vol20_up=up20*vol20
            vol20_up=(vol20_up.T-vol20_up.T.mean()).T
            vol20_up=np.sign(vol20_up)
            ret20_up=ret20[ret20>0]
            ret20_up=vol20_up*ret20_up

            ret20_up=ret20_up.replace(np.nan,0)
            ret20_down=ret20_down.replace(np.nan,0)
            new_ret20=ret20_up+ret20_down
            new_ret20_tr=new_ret20.replace(0,np.nan)
            return new_ret20_tr
    else:
        if not mean:
            vol20_dummy=np.sign((vol20.T-vol20.T.median()).T)
            ret20=ret20*vol20_dummy
            return ret20
        else:
            vol20_dummy=np.sign((vol20.T-vol20.T.mean()).T)
            ret20=ret20*vol20_dummy
            return ret20


def indus_name(df,col_name=None):
    '''å°†2021ç‰ˆç”³ä¸‡è¡Œä¸šçš„ä»£ç ï¼Œè½¬åŒ–ä¸ºå¯¹åº”è¡Œä¸šçš„åå­—'''
    names=pd.DataFrame({
        'indus_we_cant_same':
        ['801170.SI','801010.SI','801140.SI','801080.SI','801780.SI','801110.SI','801230.SI','801950.SI',
        '801180.SI','801040.SI','801740.SI','801890.SI','801770.SI','801960.SI','801200.SI','801120.SI','801710.SI',
        '801720.SI','801880.SI','801750.SI','801050.SI','801790.SI','801150.SI','801980.SI','801030.SI','801730.SI',
        '801160.SI','801130.SI','801210.SI','801970.SI','801760.SI'],
        'è¡Œä¸šåç§°':
        ['äº¤é€šè¿è¾“','å†œæ—ç‰§æ¸”','è½»å·¥åˆ¶é€ ','ç”µå­','é“¶è¡Œ','å®¶ç”¨ç”µå™¨','ç»¼åˆ','ç…¤ç‚­','æˆ¿åœ°äº§','é’¢é“','å›½é˜²å†›å·¥','æœºæ¢°è®¾å¤‡',
        'é€šä¿¡','çŸ³æ²¹çŸ³åŒ–','å•†è´¸é›¶å”®','é£Ÿå“é¥®æ–™','å»ºç­‘ææ–™','å»ºç­‘è£…é¥°','æ±½è½¦','è®¡ç®—æœº','æœ‰è‰²é‡‘å±','éé“¶é‡‘è','åŒ»è¯ç”Ÿç‰©','ç¾å®¹æŠ¤ç†',
        'åŸºç¡€åŒ–å·¥','ç”µåŠ›è®¾å¤‡','å…¬ç”¨äº‹ä¸š','çººç»‡æœé¥°','ç¤¾ä¼šæœåŠ¡','ç¯ä¿','ä¼ åª’']
    }).sort_values(['indus_we_cant_same'])
    if col_name:
        names=names.rename(columns={'indus_we_cant_same':col_name})
        df=pd.merge(df,names,on=[col_name])
    else:
        df=df.reset_index()
        df=df.rename(columns={list(df.columns)[0]:'indus_we_cant_same'})
        df=pd.merge(df,names,on=['indus_we_cant_same']).set_index('è¡Œä¸šåç§°').drop(columns=['indus_we_cant_same'])
    return df

INDUS_DICT={k:v for k,v in zip(['801170.SI','801010.SI','801140.SI','801080.SI','801780.SI','801110.SI','801230.SI','801950.SI',
        '801180.SI','801040.SI','801740.SI','801890.SI','801770.SI','801960.SI','801200.SI','801120.SI','801710.SI',
        '801720.SI','801880.SI','801750.SI','801050.SI','801790.SI','801150.SI','801980.SI','801030.SI','801730.SI',
        '801160.SI','801130.SI','801210.SI','801970.SI','801760.SI'],['äº¤é€šè¿è¾“','å†œæ—ç‰§æ¸”','è½»å·¥åˆ¶é€ ','ç”µå­','é“¶è¡Œ','å®¶ç”¨ç”µå™¨','ç»¼åˆ','ç…¤ç‚­','æˆ¿åœ°äº§',
                                                                      'é’¢é“','å›½é˜²å†›å·¥','æœºæ¢°è®¾å¤‡','é€šä¿¡','çŸ³æ²¹çŸ³åŒ–','å•†è´¸é›¶å”®','é£Ÿå“é¥®æ–™','å»ºç­‘ææ–™',
                                                                      'å»ºç­‘è£…é¥°','æ±½è½¦','è®¡ç®—æœº','æœ‰è‰²é‡‘å±','éé“¶é‡‘è','åŒ»è¯ç”Ÿç‰©','ç¾å®¹æŠ¤ç†','åŸºç¡€åŒ–å·¥',
                                                                      'ç”µåŠ›è®¾å¤‡','å…¬ç”¨äº‹ä¸š','çººç»‡æœé¥°','ç¤¾ä¼šæœåŠ¡','ç¯ä¿','ä¼ åª’'])}

def multidfs_to_one(*args):
    '''å¾ˆå¤šä¸ªdfï¼Œå„æœ‰ä¸€éƒ¨åˆ†ï¼Œå…¶ä½™ä½ç½®éƒ½æ˜¯ç©ºï¼Œ
    æƒ³æŠŠå„è‡ªdfæœ‰å€¼çš„éƒ¨åˆ†ä¿ç•™ï¼Œéƒ½æ²¡æœ‰å€¼çš„éƒ¨åˆ†ç»§ç»­è®¾ä¸ºç©º'''
    dfs=[i.fillna(0) for i in args]
    background=np.sign(np.abs(np.sign(sum(dfs)))+1).replace(1,0)
    dfs=[(i+background).fillna(0) for i in dfs]
    df_nans=[i.isna() for i in dfs]
    nan=reduce(lambda x,y:x*y,df_nans)
    nan=nan.replace(1,np.nan)
    nan=nan.replace(0,1)
    df_final=sum(dfs)*nan
    return df_final


def to_tradeends(df):
    '''å°†æœ€åä¸€ä¸ªè‡ªç„¶æ—¥æ”¹å˜ä¸ºæœ€åä¸€ä¸ªäº¤æ˜“æ—¥'''
    trs=read_daily(tr=1)
    trs=trs.assign(tradeends=list(trs.index))
    trs=trs[['tradeends']]
    trs=trs.resample('M').last()
    df=pd.concat([trs,df],axis=1)
    df=df.set_index(['tradeends'])
    return df


def market_kind(df,zhuban=False,chuangye=False,kechuang=False,beijing=False):
    '''ä¸å®½åŸºæŒ‡æ•°æˆåˆ†è‚¡çš„å‡½æ•°ç±»ä¼¼ï¼Œé™å®šè‚¡ç¥¨åœ¨æŸä¸ªå…·ä½“æ¿å—ä¸Š'''
    trs=read_daily(tr=1)
    codes=list(trs.columns)
    dates=list(trs.index)
    if chuangye and kechuang:
        dummys=[1 if code[:2] in ['30','68'] else np.nan for code in codes]
    else:
        if zhuban:
            dummys=[1 if code[:2] in ['00','60'] else np.nan for code in codes]
        elif chuangye:
            dummys=[1 if code.startswith('3') else np.nan for code in codes]
        elif kechuang:
            dummys=[1 if code.startswith('68') else np.nan for code in codes]
        elif beijing:
            dummys=[1 if code.startswith('8') else np.nan for code in codes]
        else:
            raise ValueError('ä½ æ€»å¾—é€‰ä¸€ä¸ªè‚¡ç¥¨æ± å§ï¼ŸğŸ¤’')
    dummy_dict={k:v for k,v in zip(codes,dummys)}
    dummy_df=pd.DataFrame(dummy_dict,index=dates)
    df=df*dummy_df
    return df


def show_corr(fac1,fac2,method='spearman'):
    both1=fac1.stack().reset_index()
    befo1=fac2.stack().reset_index()
    both1.columns=['date','code','both']
    befo1.columns=['date','code','befo']
    twins=pd.merge(both1,befo1,on=['date','code']).set_index(['date','code'])
    corr=twins.groupby('date').apply(lambda x:x.corr(method=method).iloc[0,1])
    corr.plot()
    plt.show()
    return corr.mean()


# åŠè¡°æœŸåºåˆ—
def calc_exp_list(window,half_life):
    exp_wt = np.asarray([0.5 ** (1 / half_life)] * window) ** np.arange(window)
    return exp_wt[::-1] / np.sum(exp_wt)

#weighted_std
def calcWeightedStd(series, weights):
    '''
       åŠ æƒå¹³å‡std
    '''
    weights /= np.sum(weights)
    return np.sqrt(np.sum((series-np.mean(series)) ** 2 * weights))


def other_periods_comments_nets(fac,period=None,way=None,comments_writer=None,nets_writer=None,sheetname=None,group_num=10):
    '''ä¸åŒé¢‘ç‡ä¸‹çš„è¯„ä»·æŒ‡æ ‡ï¼Œè¯·è¾“å…¥è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–åçš„å› å­å€¼'''
    closes=read_daily(open=1).shift(-1)
    fac1=fac.stack()
    df=al.utils.get_clean_factor_and_forward_returns(fac1,closes[closes.index.isin(fac.index)],quantiles=group_num,periods=(period,))
    df=df.reset_index()
    ics=df.groupby(['date'])[[f'{period}D','factor']].apply(lambda x:x.corr(method='spearman').iloc[0,1])
    ic=ics.mean()
    ir=ics.std()
    icir=ic/ir*(252**0.5)/(period**0.5)
    df=df.groupby(['date','factor_quantile'])[f'{period}D'].mean()/period
    df=df.unstack()
    df.columns=[f'åˆ†ç»„{i}' for i in list(df.columns)]
    if way=='pos':
        df=df.assign(å¤šç©ºå¯¹å†²=df[f'åˆ†ç»„{group_num}']-df.åˆ†ç»„1)
    elif way=='neg':
        df=df.assign(å¤šç©ºå¯¹å†²=df.åˆ†ç»„1-df[f'åˆ†ç»„{group_num}'])
    nets=(df+1).cumprod()
    nets=nets.apply(lambda x:x/x.iloc[0])
    nets.plot()
    plt.show()
    comments=comments_on_twins_periods(nets.å¤šç©ºå¯¹å†²,df.å¤šç©ºå¯¹å†²,period)
    comments=pd.concat([pd.Series([ic,icir],index=['Rank IC','Rank ICIR']),comments])
    print(comments)
    if sheetname is None:
        ...
    else:
        if comments_writer is None:
            ...
        else:
            comments.to_excel(comments_writer,sheetname)
        if nets_writer is None:
            ...
        else:
            nets.to_excel(nets_writer,sheetname)
    return comments,nets


def get_list_std(delta_sts):
    '''åŒä¸€å¤©å¤šä¸ªå› å­ï¼Œè®¡ç®—è¿™äº›å› å­åœ¨å½“å¤©çš„æ ‡å‡†å·®'''
    delta_sts_mean=sum(delta_sts)/len(delta_sts)
    delta_sts_std=[(i-delta_sts_mean)**2 for i in delta_sts]
    delta_sts_std=sum(delta_sts_std)
    delta_sts_std=delta_sts_std**0.5/len(delta_sts)
    return delta_sts_std


def get_industry_dummies(daily=False,monthly=False):
    '''ç”Ÿæˆ31ä¸ªè¡Œä¸šçš„å“‘å˜é‡çŸ©é˜µï¼Œè¿”å›ä¸€ä¸ªå­—å…¸'''
    homeplace=HomePlace()
    if monthly:
        industry_dummy=pd.read_feather(homeplace.daily_data_file+'ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather')
        industry_dummy=industry_dummy.set_index('date').groupby('code').resample('M').last().fillna(0).drop(columns=['code']).reset_index()
    elif daily:
        industry_dummy=pd.read_feather(homeplace.daily_data_file+'ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather').fillna(0)
    else:
        raise ValueError('æ‚¨æ€»å¾—æŒ‡å®šä¸€ä¸ªé¢‘ç‡å§ï¼ŸğŸ¤’')
    ws=list(industry_dummy.columns)[2:]
    ress={}
    for w in ws:
        df=industry_dummy[['date', 'code', w]]
        df=df.pivot(index='date',columns='code',values=w)
        df=df.replace(0,np.nan)
        ress[w]=df
    return ress

class pure_moon():
    __slots__=[
        'homeplace'
        'path_prefix',
        'codes_path',
        'tradedays_path',
        'ages_path',
        'sts_path',
        'states_path',
        'opens_path',
        'closes_path',
        'highs_path',
        'lows_path',
        'pricloses_path',
        'flowshares_path',
        'amounts_path',
        'turnovers_path',
        'factors_file',
        'sts_monthly_file',
        'states_monthly_file',
        'sts_monthly_by10_file',
        'states_monthly_by10_file',
        'factors',
        'codes',
        'tradedays',
        'ages',
        'amounts',
        'closes',
        'flowshares',
        'highs',
        'lows',
        'opens',
        'pricloses',
        'states',
        'sts',
        'turnovers',
        'sts_monthly',
        'states_monthly',
        'ages_monthly',
        'tris_monthly',
        'opens_monthly',
        'closes_monthly',
        'rets_monthly',
        'opens_monthly_shift',
        'rets_monthly_begin',
        'limit_ups',
        'limit_downs',
        'data',
        'ic_icir_and_rank',
        'rets_monthly_limit_downs',
        'group_rets',
        'long_short_rets',
        'long_short_net_values',
        'group_net_values',
        'long_short_ret_yearly',
        'long_short_vol_yearly',
        'long_short_info_ratio',
        'long_short_win_times',
        'long_short_win_ratio',
        'retreats',
        'max_retreat',
        'long_short_comments',
        'total_comments',
        'square_rets',
        'cap',
        'cap_value',
        'industry_dummy',
        'industry_codes',
        'industry_codes_str',
        'industry_ws',
        'factors_out',
        'pricloses_copy',
        'flowshares_copy'
    ]

    @classmethod
    @lru_cache(maxsize=None)
    def __init__(cls):
        now=datetime.datetime.now()
        now=datetime.datetime.strftime(now,format='%Y-%m-%d %H:%M:%S')
        cls.homeplace=HomePlace()
        # logger.add('pure_moon'+now+'.log')
        # ç»å¯¹è·¯å¾„å‰ç¼€
        cls.path_prefix = cls.homeplace.daily_data_file
        # è‚¡ç¥¨ä»£ç æ–‡ä»¶
        cls.codes_path = 'AllStockCode.mat'
        # äº¤æ˜“æ—¥æœŸæ–‡ä»¶
        cls.tradedays_path = 'TradingDate_Daily.mat'
        # ä¸Šå¸‚å¤©æ•°æ–‡ä»¶
        cls.ages_path = 'AllStock_DailyListedDate.mat'
        # stæ—¥å­æ ‡å¿—æ–‡ä»¶
        cls.sts_path = 'AllStock_DailyST.mat'
        # äº¤æ˜“çŠ¶æ€æ–‡ä»¶
        cls.states_path = 'AllStock_DailyStatus.mat'
        # å¤æƒå¼€ç›˜ä»·æ•°æ®æ–‡ä»¶
        cls.opens_path = 'AllStock_DailyOpen_dividend.mat'
        # å¤æƒæ”¶ç›˜ä»·æ•°æ®æ–‡ä»¶
        cls.closes_path = 'AllStock_DailyClose_dividend.mat'
        #å¤æƒæœ€é«˜ä»·æ•°æ®æ–‡ä»¶
        cls.highs_path = 'Allstock_DailyHigh_dividend.mat'
        #å¤æƒæœ€ä½ä»·æ•°æ®æ–‡ä»¶
        cls.lows_path = 'Allstock_DailyLow_dividend.mat'
        # ä¸å¤æƒæ”¶ç›˜ä»·æ•°æ®æ–‡ä»¶
        cls.pricloses_path = 'AllStock_DailyClose.mat'
        # æµé€šè‚¡æœ¬æ•°æ®æ–‡ä»¶
        cls.flowshares_path = 'AllStock_DailyAShareNum.mat'
        # æˆäº¤é‡æ•°æ®æ–‡ä»¶
        cls.amounts_path = 'AllStock_DailyVolume.mat'
        # æ¢æ‰‹ç‡æ•°æ®æ–‡ä»¶
        cls.turnovers_path = 'AllStock_DailyTR.mat'
        # å› å­æ•°æ®æ–‡ä»¶
        cls.factors_file = ''
        # å·²ç»ç®—å¥½çš„æœˆåº¦stçŠ¶æ€æ–‡ä»¶
        cls.sts_monthly_file = 'sts_monthly.feather'
        # å·²ç»ç®—å¥½çš„æœˆåº¦äº¤æ˜“çŠ¶æ€æ–‡ä»¶
        cls.states_monthly_file = 'states_monthly.feather'
        # å·²ç»ç®—å¥½çš„æœˆåº¦st_by10çŠ¶æ€æ–‡ä»¶
        cls.sts_monthly_by10_file = 'sts_monthly_by10.feather'
        # å·²ç»ç®—å¥½çš„æœˆåº¦äº¤æ˜“çŠ¶æ€æ–‡ä»¶
        cls.states_monthly_by10_file = 'states_monthly_by10.feather'
        # æ‹¼æ¥ç»å¯¹è·¯å¾„å‰ç¼€å’Œç›¸å¯¹è·¯å¾„
        dirs = dir(cls)
        dirs.remove('new_path')
        dirs.remove('set_factor_file')
        dirs = [i for i in dirs if i.endswith('path')] + [i for i in dirs if i.endswith('file')]
        dirs_values = list(map(lambda x, y: getattr(x, y), [cls] * len(dirs), dirs))
        dirs_values = list(map(lambda x, y: x + y, [cls.path_prefix] * len(dirs), dirs_values))
        for attr, value in zip(dirs, dirs_values):
            setattr(cls, attr, value)

    def __call__(self, fallmount=0):
        '''è°ƒç”¨å¯¹è±¡åˆ™è¿”å›å› å­å€¼'''
        df=self.factors_out.copy()
        # df=df.set_index(['date', 'code']).unstack()
        df.columns=list(map(lambda x:x[1],list(df.columns)))
        if fallmount == 0:
            return df
        else:
            return pure_fallmount(df)

    @params_setter(slogan=None)
    # @lru_cache(maxsize=None)
    def set_factor_file(self, factors_file):
        '''è®¾ç½®å› å­æ–‡ä»¶çš„è·¯å¾„ï¼Œå› å­æ–‡ä»¶åˆ—ååº”ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•ä¸ºæ—¶é—´'''
        self.factors_file = factors_file
        self.factors = pd.read_feather(self.factors_file)
        self.factors = self.factors.set_index('date')
        self.factors = self.factors.resample('M').last()
        self.factors = self.factors.reset_index()

    @params_setter(slogan=None)
    # @lru_cache(maxsize=None)
    def set_factor_df_date_as_index(self, df):
        '''è®¾ç½®å› å­æ•°æ®çš„dataframeï¼Œå› å­è¡¨åˆ—ååº”ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•åº”ä¸ºæ—¶é—´'''
        df = df.reset_index()
        df.columns = ['date'] + list(df.columns)[1:]
        # df.date=df.date.apply(self.next_month_end)
        # df=df.set_index('date')
        self.factors = df
        self.factors = self.factors.set_index('date')
        self.factors = self.factors.resample('M').last()
        self.factors = self.factors.reset_index()

    @params_setter(slogan=None)
    # @lru_cache(maxsize=None)
    def set_factor_df_wide(self, df):
        '''ä»dataframeè¯»å…¥å› å­å®½æ•°æ®'''
        if isinstance(df,pure_fallmount):
            df=df()
        self.factors = df.copy()
        # self.factors.date=self.factors.date.apply(self.next_month_end)
        # self.factors=self.factors.set_index('date')
        self.factors = self.factors.set_index('date')
        self.factors = self.factors.resample('M').last()
        self.factors = self.factors.reset_index()

    # def set_factor_df_long(self,df):
    #     '''ä»dataframeè¯»å…¥å› å­é•¿æ•°æ®'''
    #     self.factors=df
    #     self.factors.columns=['date','code','fac']

    @classmethod
    @lru_cache(maxsize=None)
    @history_remain(slogan=None)
    def new_path(cls, **kwargs):
        '''ä¿®æ”¹æ—¥é¢‘æ•°æ®æ–‡ä»¶çš„è·¯å¾„ï¼Œä¾¿äºæ›´æ–°æ•°æ®
        è¦ä¿®æ”¹çš„è·¯å¾„ä»¥å­—å…¸å½¢å¼ä¼ å…¥ï¼Œé”®ä¸ºå±æ€§åï¼Œå€¼ä¸ºè¦è®¾ç½®çš„æ–°è·¯å¾„'''
        for key, value in kwargs.items():
            setattr(cls, key, value)

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def col_and_index(cls):
        '''è¯»å–è‚¡ç¥¨ä»£ç ï¼Œä½œä¸ºæœªæ¥è¡¨æ ¼çš„è¡Œå
        è¯»å–äº¤æ˜“æ—¥å†ï¼Œä½œä¸ºæœªæ¥è¡¨æ ¼çš„ç´¢å¼•'''
        cls.codes = list(scio.loadmat(cls.codes_path).values())[3]
        cls.tradedays = list(scio.loadmat(cls.tradedays_path).values())[3].astype(str)
        cls.codes = cls.codes.flatten().tolist()
        # cls.tradedays = cls.tradedays.flatten().tolist()
        cls.codes = list(map(lambda x: x[0], cls.codes))
        cls.tradedays=cls.tradedays[0].tolist()

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def loadmat(cls, path):
        '''é‡å†™ä¸€ä¸ªåŠ è½½matæ–‡ä»¶çš„å‡½æ•°ï¼Œä»¥ä½¿ä»£ç æ›´ç®€æ´'''
        return list(scio.loadmat(path).values())[3]

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def make_df(cls, data):
        '''å°†è¯»å…¥çš„æ•°æ®ï¼Œå’Œè‚¡ç¥¨ä»£ç ä¸æ—¶é—´æ‹¼æ¥ï¼Œåšæˆdataframe'''
        data = pd.DataFrame(data, columns=cls.codes, index=cls.tradedays)
        data.index = pd.to_datetime(data.index, format='%Y%m%d')
        return data

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def load_all_files(cls):
        '''åŠ å…¨éƒ¨çš„matæ–‡ä»¶'''
        attrs = dir(cls)
        attrs = [i for i in attrs if i.endswith('path')]
        attrs.remove('codes_path')
        attrs.remove('tradedays_path')
        attrs.remove('new_path')
        for attr in attrs:
            new_attr = attr[:-5]
            setattr(cls, new_attr, cls.make_df(cls.loadmat(getattr(cls, attr))))
        cls.opens = cls.opens.replace(0, np.nan)
        cls.closes = cls.closes.replace(0, np.nan)
        cls.pricloses_copy=cls.pricloses.copy()
        cls.flowshares_copy=cls.flowshares.copy()

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def judge_month_st(cls, df):
        '''æ¯”è¾ƒä¸€ä¸ªæœˆå†…stçš„å¤©æ•°ï¼Œå¦‚æœstå¤©æ•°å¤šï¼Œå°±åˆ é™¤æœ¬æœˆï¼Œå¦‚æœæ­£å¸¸å¤šï¼Œå°±ä¿ç•™æœ¬æœˆ'''
        st_count = len(df[df == 1])
        normal_count = len(df[df != 1])
        if st_count >= normal_count:
            return 0
        else:
            return 1

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def judge_month_st_by10(cls, df):
        '''æ¯”è¾ƒä¸€ä¸ªæœˆå†…æ­£å¸¸äº¤æ˜“çš„å¤©æ•°ï¼Œå¦‚æœå°‘äº10å¤©ï¼Œå°±åˆ é™¤æœ¬æœˆ'''
        normal_count = len(df[df != 1])
        if normal_count < 10:
            return 0
        else:
            return 1

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def judge_month_state(cls, df):
        '''æ¯”è¾ƒä¸€ä¸ªæœˆå†…éæ­£å¸¸äº¤æ˜“çš„å¤©æ•°ï¼Œå¦‚æœéæ­£å¸¸äº¤æ˜“å¤©æ•°å¤šï¼Œå°±åˆ é™¤æœ¬æœˆï¼Œå¦åˆ™ä¿ç•™æœ¬æœˆ'''
        abnormal_count = len(df[df == 0])
        normal_count = len(df[df == 1])
        if abnormal_count >= normal_count:
            return 0
        else:
            return 1

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def judge_month_state_by10(cls, df):
        '''æ¯”è¾ƒä¸€ä¸ªæœˆå†…æ­£å¸¸äº¤æ˜“å¤©æ•°ï¼Œå¦‚æœå°‘äº10å¤©ï¼Œå°±åˆ é™¤æœ¬æœˆ'''
        normal_count = len(df[df == 1])
        if normal_count < 10:
            return 0
        else:
            return 1

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def read_add(cls, pridf, df, func):
        '''ç”±äºæ•°æ®æ›´æ–°ï¼Œè¿‡å»è®¡ç®—çš„æœˆåº¦çŠ¶æ€å¯èƒ½éœ€è¦è¿½åŠ '''
        if not NO_LOG:
            logger.info(f'this is max_index of pridf{pridf.index.max()}')
            logger.info(f'this is max_index of df{df.index.max()}')
        if pridf.index.max() > df.index.max():
            df_add = pridf[pridf.index > df.index.max()]
            df_add = df_add.resample('M').apply(func)
            df = pd.concat([df, df_add])
            return df
        else:
            return df

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def write_feather(cls, df, path):
        '''å°†ç®—å‡ºæ¥çš„æ•°æ®å­˜å…¥æœ¬åœ°ï¼Œä»¥å…é€ æˆé‡å¤è¿ç®—'''
        df1 = df.copy()
        df1 = df1.reset_index()
        df1.to_feather(path)

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def daily_to_monthly(cls, pridf, path, func):
        '''æŠŠæ—¥åº¦çš„äº¤æ˜“çŠ¶æ€ã€stã€ä¸Šå¸‚å¤©æ•°ï¼Œè½¬åŒ–ä¸ºæœˆåº¦çš„ï¼Œå¹¶ç”Ÿæˆèƒ½å¦äº¤æ˜“çš„åˆ¤æ–­
        è¯»å–æœ¬åœ°å·²ç»ç®—å¥½çš„æ–‡ä»¶ï¼Œå¹¶è¿½åŠ æ–°çš„æ—¶é—´æ®µéƒ¨åˆ†ï¼Œå¦‚æœæœ¬åœ°æ²¡æœ‰å°±ç›´æ¥å…¨éƒ¨é‡æ–°ç®—'''
        try:
            if not NO_LOG:
                logger.info('try to read the prepared state file')
            month_df = pd.read_feather(path).set_index('index')
            if not NO_LOG:
                logger.info('state file load success')
            month_df = cls.read_add(pridf, month_df, func)
            if not NO_LOG:
                logger.info('adding after state file has finish')
            cls.write_feather(month_df, path)
            if not NO_LOG:
                logger.info('the feather is new now')
        except Exception as e:
            if not NO_LOG:
                logger.error('error occurs when read state files')
                logger.error(e)
            print('state file rewritingâ€¦â€¦')
            month_df = pridf.resample('M').apply(func)
            cls.write_feather(month_df, path)
        return month_df

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def daily_to_monthly_by10(cls, pridf, path, func):
        '''æŠŠæ—¥åº¦çš„äº¤æ˜“çŠ¶æ€ã€stã€ä¸Šå¸‚å¤©æ•°ï¼Œè½¬åŒ–ä¸ºæœˆåº¦çš„ï¼Œå¹¶ç”Ÿæˆèƒ½å¦äº¤æ˜“çš„åˆ¤æ–­
        è¯»å–æœ¬åœ°å·²ç»ç®—å¥½çš„æ–‡ä»¶ï¼Œå¹¶è¿½åŠ æ–°çš„æ—¶é—´æ®µéƒ¨åˆ†ï¼Œå¦‚æœæœ¬åœ°æ²¡æœ‰å°±ç›´æ¥å…¨éƒ¨é‡æ–°ç®—'''
        try:
            month_df = pd.read_feather(path).set_index('date')
            month_df = cls.read_add(pridf, month_df, func)
            cls.write_feather(month_df, path)
        except Exception:
            print('rewriting')
            month_df = pridf.resample('M').apply(func)
            cls.write_feather(month_df, path)
        return month_df

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def judge_month(cls):
        '''ç”Ÿæˆä¸€ä¸ªæœˆç»¼åˆåˆ¤æ–­çš„è¡¨æ ¼'''
        cls.sts_monthly = cls.daily_to_monthly(cls.sts, cls.sts_monthly_file, cls.judge_month_st)
        cls.states_monthly = cls.daily_to_monthly(cls.states, cls.states_monthly_file, cls.judge_month_state)
        cls.ages_monthly = cls.ages.resample('M').last()
        cls.ages_monthly = np.sign(cls.ages_monthly.applymap(lambda x: x - 60)).replace(-1, 0)
        cls.tris_monthly = cls.sts_monthly * cls.states_monthly * cls.ages_monthly
        cls.tris_monthly = cls.tris_monthly.replace(0, np.nan)

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def judge_month_by10(cls):
        '''ç”Ÿæˆä¸€ä¸ªæœˆç»¼åˆåˆ¤æ–­çš„è¡¨æ ¼'''
        cls.sts_monthly = cls.daily_to_monthly(cls.sts, cls.sts_monthly_by10_file, cls.judge_month_st_by10)
        cls.states_monthly = cls.daily_to_monthly(cls.states, cls.states_monthly_by10_file,
                                                    cls.judge_month_state_by10)
        cls.ages_monthly = cls.ages.resample('M').last()
        cls.ages_monthly = np.sign(cls.ages_monthly.applymap(lambda x: x - 60)).replace(-1, 0)
        cls.tris_monthly = cls.sts_monthly * cls.states_monthly * cls.ages_monthly
        cls.tris_monthly = cls.tris_monthly.replace(0, np.nan)

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def get_rets_month(cls):
        '''è®¡ç®—æ¯æœˆçš„æ”¶ç›Šç‡ï¼Œå¹¶æ ¹æ®æ¯æœˆåšå‡ºäº¤æ˜“çŠ¶æ€ï¼Œåšå‡ºåˆ å‡'''
        cls.opens_monthly = cls.opens.resample('M').first()
        cls.closes_monthly = cls.closes.resample('M').last()
        cls.rets_monthly = (cls.closes_monthly - cls.opens_monthly) / cls.opens_monthly
        cls.rets_monthly = cls.rets_monthly * cls.tris_monthly
        cls.rets_monthly = cls.rets_monthly.stack().reset_index()
        cls.rets_monthly.columns = ['date', 'code', 'ret']

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def neutralize_factors(cls, df):
        '''ç»„å†…å¯¹å› å­è¿›è¡Œå¸‚å€¼ä¸­æ€§åŒ–'''
        industry_codes=list(df.columns)
        industry_codes=[i for i in industry_codes if i.startswith('w')]
        industry_codes_str='+'.join(industry_codes)
        ols_result = smf.ols('fac~cap_size+'+industry_codes_str, data=df).fit()
        ols_w = ols_result.params['cap_size']
        ols_b = ols_result.params['Intercept']
        ols_bs={}
        for ind in industry_codes:
            ols_bs[ind]=ols_result.params[ind]
        df.fac = df.fac - ols_w * df.cap_size - ols_b
        for k,v in ols_bs.items():
            df.fac=df.fac-v*df[k]
        df = df[['fac']]
        return df

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def get_log_cap(cls,boxcox=False):
        '''è·å¾—å¯¹æ•°å¸‚å€¼'''
        try:
            cls.pricloses = cls.pricloses.replace(0, np.nan)
            cls.flowshares = cls.flowshares.replace(0, np.nan)
            cls.pricloses = cls.pricloses.resample('M').last()
        except Exception:
            cls.pricloses=cls.pricloses_copy.copy()
            cls.pricloses = cls.pricloses.replace(0, np.nan)
            cls.flowshares = cls.flowshares.replace(0, np.nan)
            cls.pricloses = cls.pricloses.resample('M').last()
        cls.pricloses = cls.pricloses.stack().reset_index()
        cls.pricloses.columns = ['date', 'code', 'priclose']
        try:
            cls.flowshares = cls.flowshares.resample('M').last()
        except Exception:
            cls.flowshares=cls.flowshares_copy.copy()
            cls.flowshares = cls.flowshares.resample('M').last()
        cls.flowshares = cls.flowshares.stack().reset_index()
        cls.flowshares.columns = ['date', 'code', 'flowshare']
        cls.flowshares = pd.merge(cls.flowshares, cls.pricloses, on=['date', 'code'])
        cls.cap = cls.flowshares.assign(cap_size=cls.flowshares.flowshare * cls.flowshares.priclose)
        if boxcox:
            def single(x):
                x.cap_size=ss.boxcox(x.cap_size)[0]
                return x
            cls.cap=cls.cap.groupby(['date']).apply(single)
        else:
            cls.cap['cap_size'] = np.log(cls.cap['cap_size'])

    # @lru_cache(maxsize=None)
    @main_process(slogan=None)
    def get_neutral_factors(self):
        '''å¯¹å› å­è¿›è¡Œå¸‚å€¼ä¸­æ€§åŒ–'''
        self.factors = self.factors.set_index('date')
        self.factors.index=self.factors.index+pd.DateOffset(months=1)
        self.factors = self.factors.resample('M').last()
        last_date=self.tris_monthly.index.max()+pd.DateOffset(months=1)
        last_date=last_date+pd.tseries.offsets.MonthEnd()
        add_tail=pd.DataFrame(1,index=[last_date],columns=self.tris_monthly.columns)
        tris_monthly=pd.concat([self.tris_monthly,add_tail])
        self.factors = self.factors * tris_monthly
        self.factors.index=self.factors.index-pd.DateOffset(months=1)
        self.factors=self.factors.resample('M').last()
        self.factors = self.factors.stack().reset_index()
        self.factors.columns = ['date', 'code', 'fac']
        self.factors = pd.merge(self.factors, self.cap, how='inner', on=['date', 'code'])
        self.industry_dummy=pd.read_feather(self.homeplace.daily_data_file+'ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather').set_index('date').groupby('code').resample('M').last()
        self.industry_dummy=self.industry_dummy.drop(columns=['code']).reset_index()
        self.industry_ws=[f'w{i}' for i in range(1,self.industry_dummy.shape[1]-1)]
        col=['code','date']+self.industry_ws
        self.industry_dummy.columns=col
        self.factors=pd.merge(self.factors,self.industry_dummy,on=['date','code'])
        self.factors = self.factors.set_index(['date', 'code'])
        self.factors = self.factors.groupby(['date']).apply(self.neutralize_factors)
        self.factors = self.factors.reset_index()

    # @lru_cache(maxsize=None)
    @main_process(slogan=None)
    def deal_with_factors(self):
        '''åˆ é™¤ä¸ç¬¦åˆäº¤æ˜“æ¡ä»¶çš„å› å­æ•°æ®'''
        self.factors = self.factors.set_index('date')
        self.factors_out=self.factors.copy()
        self.factors.index=self.factors.index+pd.DateOffset(months=1)
        self.factors = self.factors.resample('M').last()
        self.factors = self.factors * self.tris_monthly
        self.factors = self.factors.stack().reset_index()
        self.factors.columns = ['date', 'code', 'fac']

    # @lru_cache(maxsize=None)
    @main_process(slogan=None)
    def deal_with_factors_after_neutralize(self):
        '''ä¸­æ€§åŒ–ä¹‹åçš„å› å­å¤„ç†æ–¹æ³•'''
        self.factors = self.factors.set_index(['date', 'code'])
        self.factors = self.factors.unstack()
        self.factors_out=self.factors.copy()
        self.factors.index=self.factors.index+pd.DateOffset(months=1)
        self.factors = self.factors.resample('M').last()
        self.factors.columns = list(map(lambda x: x[1], list(self.factors.columns)))
        self.factors = self.factors.stack().reset_index()
        self.factors.columns = ['date', 'code', 'fac']

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def find_limit(cls, df, up=1):
        '''è®¡ç®—æ¶¨è·Œå¹…è¶…è¿‡9.8%çš„è‚¡ç¥¨ï¼Œå¹¶å°†å…¶å­˜å‚¨è¿›ä¸€ä¸ªé•¿åˆ—è¡¨é‡Œ
        å…¶ä¸­æ—¶é—´åˆ—ï¼Œä¸ºæŸæœˆçš„æœ€åä¸€å¤©ï¼›æ¶¨åœæ—¥è™½ç„¶ä¸ºä¸‹æœˆåˆç¬¬ä¸€å¤©ï¼Œä½†è¿™é‡Œæ ‡æ³¨çš„æ—¶é—´ç»Ÿä¸€ä¸ºä¸Šæœˆæœ€åä¸€å¤©'''
        limit_df = np.sign(df.applymap(lambda x: x - up * 0.098)).replace(-1 * up, np.nan)
        limit_df = limit_df.stack().reset_index()
        limit_df.columns = ['date', 'code', 'limit_up_signal']
        limit_df = limit_df[['date', 'code']]
        return limit_df

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def get_limit_ups_downs(cls):
        '''æ‰¾æœˆåˆç¬¬ä¸€å¤©å°±æ¶¨åœ'''
        '''æˆ–è€…æ˜¯æœˆæœ«è·Œåœçš„è‚¡ç¥¨'''
        cls.opens_monthly_shift = cls.opens_monthly.copy()
        cls.opens_monthly_shift = cls.opens_monthly_shift.shift(-1)
        cls.rets_monthly_begin = (cls.opens_monthly_shift - cls.closes_monthly) / cls.closes_monthly
        cls.closes2_monthly = cls.closes.shift(1).resample('M').last()
        cls.rets_monthly_last = (cls.closes_monthly - cls.closes2_monthly) / cls.closes2_monthly
        cls.limit_ups = cls.find_limit(cls.rets_monthly_begin, up=1)
        cls.limit_downs = cls.find_limit(cls.rets_monthly_last, up=-1)

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def get_ic_rankic(cls, df):
        '''è®¡ç®—ICå’ŒRankIC'''
        df1 = df[['ret', 'fac']]
        ic = df1.corr(method='pearson').iloc[0, 1]
        rankic = df1.corr(method='spearman').iloc[0, 1]
        df2 = pd.DataFrame({'ic': [ic], 'rankic': [rankic]})
        return df2

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def get_icir_rankicir(cls, df):
        '''è®¡ç®—ICIRå’ŒRankICIR'''
        ic = df.ic.mean()
        rankic = df.rankic.mean()
        icir = ic / np.std(df.ic) * (12 ** (0.5))
        rankicir = rankic / np.std(df.rankic) * (12 ** (0.5))
        return pd.DataFrame({'IC': [ic], 'ICIR': [icir], 'RankIC': [rankic], 'RankICIR': [rankicir]}, index=['è¯„ä»·æŒ‡æ ‡'])

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def get_ic_icir_and_rank(cls, df):
        '''è®¡ç®—ICã€ICIRã€RankICã€RankICIR'''
        df1 = df.groupby('date').apply(cls.get_ic_rankic)
        df2 = cls.get_icir_rankicir(df1)
        df2 = df2.T
        dura=(df.date.max()-df.date.min()).days/365
        t_value=df2.iloc[3,0]*(dura**(1/2))
        df3=pd.DataFrame({'è¯„ä»·æŒ‡æ ‡':[t_value]},index=['RankICå‡å€¼tå€¼'])
        df4=pd.concat([df2,df3])
        return df4

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def get_groups(cls, df, groups_num):
        '''ä¾æ®å› å­å€¼ï¼Œåˆ¤æ–­æ˜¯åœ¨ç¬¬å‡ ç»„'''
        if 'group' in list(df.columns):
            df = df.drop(columns=['group'])
        df = df.sort_values(['fac'], ascending=True)
        each_group = round(df.shape[0] / groups_num)
        l = list(map(lambda x, y: [x] * y, list(range(1, groups_num + 1)), [each_group] * groups_num))
        l = reduce(lambda x, y: x + y, l)
        if len(l) < df.shape[0]:
            l = l + [groups_num] * (df.shape[0] - len(l))
        l = l[:df.shape[0]]
        df.insert(0, 'group', l)
        return df

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    # @history_remain(slogan='abandoned')
    def next_month_end(cls, x):
        '''æ‰¾åˆ°ä¸‹ä¸ªæœˆæœ€åä¸€å¤©'''
        x1 = x = x + relativedelta(months=1)
        while x1.month == x.month:
            x1 = x1 + relativedelta(days=1)
        return x1 - relativedelta(days=1)

    @classmethod
    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def limit_old_to_new(cls, limit, data):
        '''è·å–è·Œåœè‚¡åœ¨æ—§æœˆçš„ç»„å·ï¼Œç„¶åå°†æ—¥æœŸè°ƒæ•´åˆ°æ–°æœˆé‡Œ
        æ¶¨åœè‚¡åˆ™è·å¾—æ–°æœˆé‡Œæ¶¨åœè‚¡çš„ä»£ç å’Œæ—¶é—´ï¼Œç„¶åç›´æ¥åˆ å»'''
        data1 = data.copy()
        data1 = data1.reset_index()
        data1.columns = ['data_index'] + list(data1.columns)[1:]
        old = pd.merge(limit, data1, how='inner', on=['date', 'code'])
        old = old.set_index('data_index')
        old = old[['group', 'date', 'code']]
        old.date = list(map(cls.next_month_end, list(old.date)))
        return old

    # @lru_cache(maxsize=None)
    @main_process(slogan=None)
    def get_data(self, groups_num):
        '''æ‹¼æ¥å› å­æ•°æ®å’Œæ¯æœˆæ”¶ç›Šç‡æ•°æ®ï¼Œå¹¶å¯¹æ¶¨åœå’Œè·Œåœè‚¡åŠ ä»¥å¤„ç†'''
        self.data = pd.merge(self.rets_monthly, self.factors, how='inner', on=['date', 'code'])
        self.ic_icir_and_rank = self.get_ic_icir_and_rank(self.data)
        self.data = self.data.groupby('date').apply(lambda x: self.get_groups(x, groups_num))
        self.data = self.data.reset_index(drop=True)
        limit_ups_object = self.limit_old_to_new(self.limit_ups, self.data)
        limit_downs_object = self.limit_old_to_new(self.limit_downs, self.data)
        self.data = self.data.drop(limit_ups_object.index)
        rets_monthly_limit_downs = pd.merge(self.rets_monthly, limit_downs_object, how='inner', on=['date', 'code'])
        self.data = pd.concat([self.data, rets_monthly_limit_downs])

    # @lru_cache(maxsize=None)
    @main_process(slogan=None)
    def select_data_time(self, time_start, time_end):
        '''ç­›é€‰ç‰¹å®šçš„æ—¶é—´æ®µ'''
        if time_start:
            self.data = self.data[self.data.date >= time_start]
        if time_end:
            self.data = self.data[self.data.date <= time_end]

    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def make_start_to_one(self, l):
        '''è®©å‡€å€¼åºåˆ—çš„ç¬¬ä¸€ä¸ªæ•°å˜æˆ1'''
        min_date = self.factors.date.min()
        add_date = min_date - relativedelta(days=min_date.day)
        add_l = pd.Series([1], index=[add_date])
        l = pd.concat([add_l, l])
        return l

    # @lru_cache(maxsize=None)
    @tool_box(slogan=None)
    def to_group_ret(self,l):
        '''æ¯ä¸€ç»„çš„å¹´åŒ–æ”¶ç›Šç‡'''
        ret=l[-1]**(12/len(l))-1
        return ret

    # @lru_cache(maxsize=None)
    @main_process(slogan=None)
    def get_group_rets_net_values(self, groups_num=10,value_weighted=False):
        '''è®¡ç®—ç»„å†…æ¯ä¸€æœŸçš„å¹³å‡æ”¶ç›Šï¼Œç”Ÿæˆæ¯æ—¥æ”¶ç›Šç‡åºåˆ—å’Œå‡€å€¼åºåˆ—'''
        if value_weighted:
            cap_value=self.pricloses_copy*self.flowshares_copy
            cap_value=cap_value.resample('M').last().shift(1)
            cap_value=cap_value*self.tris_monthly
            # cap_value=np.log(cap_value)
            cap_value=cap_value.stack().reset_index()
            cap_value.columns=['date','code','cap_value']
            self.data=pd.merge(self.data,cap_value,on=['date','code'])
            def in_g(df):
                df.cap_value=df.cap_value/df.cap_value.sum()
                df.ret=df.ret*df.cap_value
                return df.ret.sum()
            self.group_rets=self.data.groupby(['date','group']).apply(in_g)
        else:
            self.group_rets = self.data.groupby(['date', 'group']).apply(lambda x: x.ret.mean())
        # dropnaæ˜¯å› ä¸ºå¦‚æœè‚¡ç¥¨è¡Œæƒ…æ•°æ®æ¯”å› å­æ•°æ®çš„æˆªæ­¢æ—¥æœŸæ™šï¼Œè€Œæœ€åä¸€ä¸ªæœˆå‘ç”Ÿæœˆåˆè·Œåœæ—¶ï¼Œä¼šé€ æˆæœ€åæŸç»„å¤šå‡ºä¸€ä¸ªæœˆçš„æ•°æ®
        self.group_rets = self.group_rets.unstack()
        self.group_rets = self.group_rets[self.group_rets.index <= self.factors.date.max()]
        self.group_rets.columns = list(map(str, list(self.group_rets.columns)))
        self.group_rets = self.group_rets.add_prefix('group')
        self.long_short_rets = self.group_rets['group1'] - self.group_rets['group' + str(groups_num)]
        self.long_short_net_values = (self.long_short_rets + 1).cumprod()
        if self.long_short_net_values[-1] <= self.long_short_net_values[0]:
            self.long_short_rets = self.group_rets['group' + str(groups_num)] - self.group_rets['group1']
            self.long_short_net_values = (self.long_short_rets + 1).cumprod()
        self.long_short_net_values = self.make_start_to_one(self.long_short_net_values)
        self.group_rets = self.group_rets.assign(long_short=self.long_short_rets)
        self.group_net_values = self.group_rets.applymap(lambda x: x + 1)
        self.group_net_values = self.group_net_values.cumprod()
        self.group_net_values = self.group_net_values.apply(self.make_start_to_one)
        a=groups_num**(0.5)
        #åˆ¤æ–­æ˜¯å¦è¦ä¸¤ä¸ªå› å­ç”»è¡¨æ ¼
        if a==int(a):
            self.square_rets=self.group_net_values.iloc[:,:-1].apply(self.to_group_ret).to_numpy()
            self.square_rets=self.square_rets.reshape((int(a),int(a)))
            self.square_rets=pd.DataFrame(self.square_rets,columns=list(range(1,int(a)+1)),index=list(range(1,int(a)+1)))
            print('è¿™æ˜¯self.square_rets',self.square_rets)

    # @lru_cache(maxsize=None)
    @main_process(slogan=None)
    def get_long_short_comments(self,on_paper=False):
        '''è®¡ç®—å¤šç©ºå¯¹å†²çš„ç›¸å…³è¯„ä»·æŒ‡æ ‡
        åŒ…æ‹¬å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€æœˆåº¦èƒœç‡ã€æœ€å¤§å›æ’¤ç‡'''
        self.long_short_ret_yearly = self.long_short_net_values[-1] ** (12 / len(self.long_short_net_values)) - 1
        self.long_short_vol_yearly = np.std(self.long_short_rets) * (12 ** 0.5)
        self.long_short_info_ratio = self.long_short_ret_yearly / self.long_short_vol_yearly
        self.long_short_win_times = len(self.long_short_rets[self.long_short_rets > 0])
        self.long_short_win_ratio = self.long_short_win_times / len(self.long_short_rets)
        self.max_retreat = -(self.long_short_net_values/self.long_short_net_values.expanding(1).max()-1).min()
        if on_paper:
            self.long_short_comments=pd.DataFrame({
                'è¯„ä»·æŒ‡æ ‡':[
                    self.long_short_ret_yearly,
                    self.long_short_vol_yearly,
                    self.long_short_info_ratio,
                    self.long_short_win_ratio,
                    self.max_retreat
                ]
            },index=['å¹´åŒ–æ”¶ç›Šç‡','å¹´åŒ–æ³¢åŠ¨ç‡','æ”¶ç›Šæ³¢åŠ¨æ¯”','æœˆåº¦èƒœç‡','æœ€å¤§å›æ’¤ç‡'])
        else:
            self.long_short_comments = pd.DataFrame({
                'è¯„ä»·æŒ‡æ ‡': [
                    self.long_short_ret_yearly,
                    self.long_short_vol_yearly,
                    self.long_short_info_ratio,
                    self.long_short_win_ratio,
                    self.max_retreat
                ]
            }, index=['å¹´åŒ–æ”¶ç›Šç‡', 'å¹´åŒ–æ³¢åŠ¨ç‡', 'ä¿¡æ¯æ¯”ç‡', 'æœˆåº¦èƒœç‡', 'æœ€å¤§å›æ’¤ç‡'])

    # @lru_cache(maxsize=None)
    @main_process(slogan=None)
    def get_total_comments(self):
        '''ç»¼åˆICã€ICIRã€RankICã€RankICIR,å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€æœˆåº¦èƒœç‡ã€æœ€å¤§å›æ’¤ç‡'''
        self.total_comments = pd.concat([self.ic_icir_and_rank, self.long_short_comments])

    # @lru_cache(maxsize=None)
    @main_process(slogan=None)
    def plot_net_values(self, y2, filename):
        '''ä½¿ç”¨matplotlibæ¥ç”»å›¾ï¼Œy2ä¸ºæ˜¯å¦å¯¹å¤šç©ºç»„åˆé‡‡ç”¨åŒyè½´'''
        self.group_net_values.plot(secondary_y=y2)
        filename_path = filename + '.png'
        if not NO_SAVE:
            plt.savefig(filename_path)

    # @lru_cache(maxsize=None)
    @main_process(slogan=None)
    def plotly_net_values(self, filename):
        '''ä½¿ç”¨plotly.expressç”»å›¾'''
        fig = pe.line(self.group_net_values)
        filename_path = filename + '.html'
        pio.write_html(fig, filename_path, auto_open=True)

    @classmethod
    @main_process(slogan=None)
    @lru_cache(maxsize=None)
    def prerpare(cls):
        '''é€šç”¨æ•°æ®å‡†å¤‡'''
        cls.col_and_index()
        cls.load_all_files()
        cls.judge_month()
        cls.get_rets_month()

    # @lru_cache(maxsize=None)
    @kk.desktop_sender(title='å˜¿ï¼Œå›æµ‹ç»“æŸå•¦ï½ğŸ—“')
    def run(self, groups_num=10, neutralize=False, boxcox=False, value_weighted=False,y2=False, plt_plot=True,
            plotly_plot=False, filename='åˆ†ç»„å‡€å€¼å›¾',
            time_start=None, time_end=None, print_comments=True,comments_writer=None,net_values_writer=None,rets_writer=None,
            comments_sheetname=None,net_values_sheetname=None,rets_sheetname=None,on_paper=False,sheetname=None):
        '''è¿è¡Œå›æµ‹éƒ¨åˆ†'''
        if comments_writer and not (comments_sheetname or sheetname):
            raise IOError('æŠŠtotal_commentsè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’')
        if net_values_writer and not (net_values_sheetname or sheetname):
            raise IOError('æŠŠgroup_net_valuesè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’')
        if rets_writer and not (rets_sheetname or sheetname):
            raise IOError('æŠŠgroup_retsè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’')
        if neutralize:
            self.get_log_cap()
            self.get_neutral_factors()
            self.deal_with_factors_after_neutralize()
        elif boxcox:
            self.get_log_cap(boxcox=True)
            self.get_neutral_factors()
            self.deal_with_factors_after_neutralize()
        else:
            self.deal_with_factors()
        self.get_limit_ups_downs()
        self.get_data(groups_num)
        self.select_data_time(time_start, time_end)
        self.get_group_rets_net_values(groups_num=groups_num,value_weighted=value_weighted)
        self.get_long_short_comments(on_paper=on_paper)
        self.get_total_comments()
        if plt_plot:
            if not NO_PLOT:
                if filename:
                    self.plot_net_values(y2=y2, filename=filename)
                else:
                    self.plot_net_values(y2=y2,
                                         filename=self.factors_file.split('.')[-2].split('/')[-1] + str(groups_num) + 'åˆ†ç»„')
                plt.show()
        if plotly_plot:
            if not NO_PLOT:
                if filename:
                    self.plotly_net_values(filename=filename)
                else:
                    self.plotly_net_values(
                        filename=self.factors_file.split('.')[-2].split('/')[-1] + str(groups_num) + 'åˆ†ç»„')
        if print_comments:
            if not NO_COMMENT:
                print(self.total_comments)
        if sheetname:
            if comments_writer:
                total_comments=self.total_comments.copy()
                tc=list(total_comments.è¯„ä»·æŒ‡æ ‡)
                tc[0]=str(round(tc[0]*100,2))+'%'
                tc[1]=str(round(tc[1],2))
                tc[2]=str(round(tc[2]*100,2))+'%'
                tc[3]=str(round(tc[3],2))
                tc[4]=str(round(tc[4],2))
                tc[5]=str(round(tc[5]*100,2))+'%'
                tc[6]=str(round(tc[6]*100,2))+'%'
                tc[7]=str(round(tc[7],2))
                tc[8]=str(round(tc[8]*100,2))+'%'
                tc[9]=str(round(tc[9]*100,2))+'%'
                new_total_comments=pd.DataFrame({sheetname:tc},index=total_comments.index)
                new_total_comments.T.to_excel(comments_writer,sheet_name=sheetname)
            if net_values_writer:
                groups_net_values=self.group_net_values.copy()
                groups_net_values.index=groups_net_values.index.strftime('%Y/%m/%d')
                groups_net_values.columns=[f'åˆ†ç»„{i}' for i in range(1,len(list(groups_net_values.columns)))]+['å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰']
                groups_net_values.to_excel(net_values_writer,sheet_name=sheetname)
            if rets_writer:
                group_rets=self.group_rets.copy()
                group_rets.index=group_rets.index.strftime('%Y/%m/%d')
                group_rets.columns=[f'åˆ†ç»„{i}' for i in range(1,len(list(group_rets.columns)))]+['å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰']
                group_rets.to_excel(rets_writer,sheet_name=sheetname)
        else:
            if comments_writer and comments_sheetname:
                total_comments=self.total_comments.copy()
                tc=list(total_comments.è¯„ä»·æŒ‡æ ‡)
                tc[0]=str(round(tc[0]*100,2))+'%'
                tc[1]=str(round(tc[1],2))
                tc[2]=str(round(tc[2]*100,2))+'%'
                tc[3]=str(round(tc[3],2))
                tc[4]=str(round(tc[4],2))
                tc[5]=str(round(tc[5]*100,2))+'%'
                tc[6]=str(round(tc[6]*100,2))+'%'
                tc[7]=str(round(tc[7],2))
                tc[8]=str(round(tc[8]*100,2))+'%'
                tc[9]=str(round(tc[9]*100,2))+'%'
                new_total_comments=pd.DataFrame({comments_sheetname:tc},index=total_comments.index)
                new_total_comments.T.to_excel(comments_writer,sheet_name=comments_sheetname)
            if net_values_writer and net_values_sheetname:
                groups_net_values=self.group_net_values.copy()
                groups_net_values.index=groups_net_values.index.strftime('%Y/%m/%d')
                groups_net_values.columns=[f'åˆ†ç»„{i}' for i in range(1,len(list(groups_net_values.columns)))]+['å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰']
                groups_net_values.to_excel(net_values_writer,sheet_name=net_values_sheetname)
            if rets_writer and rets_sheetname:
                group_rets=self.group_rets.copy()
                group_rets.index=group_rets.index.strftime('%Y/%m/%d')
                group_rets.columns=[f'åˆ†ç»„{i}' for i in range(1,len(list(group_rets.columns)))]+['å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰']
                group_rets.to_excel(rets_writer,sheet_name=rets_sheetname)


class pure_fall():
    def __init__(
            self,
            minute_files_path=None,
            minute_columns=['date', 'open', 'high', 'low', 'close', 'amount', 'money'],
            daily_factors_path=None,
            monthly_factors_path=None
    ):
        self.homeplace=HomePlace()
        if monthly_factors_path:
            # åˆ†é’Ÿæ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
            self.minute_files_path = minute_files_path
        else:
            self.minute_files_path=self.homeplace.minute_data_file[:-1]
        # åˆ†é’Ÿæ•°æ®æ–‡ä»¶å¤¹
        self.minute_files = os.listdir(self.minute_files_path)
        self.minute_files = [i for i in self.minute_files if i.endswith('.mat')]
        self.minute_files=sorted(self.minute_files)
        # åˆ†é’Ÿæ•°æ®çš„è¡¨å¤´
        self.minute_columns = minute_columns
        # åˆ†é’Ÿæ•°æ®æ—¥é¢‘åŒ–ä¹‹åçš„æ•°æ®è¡¨
        self.daily_factors_list = []
        #æ›´æ–°æ•°æ®ç”¨çš„åˆ—è¡¨
        self.daily_factors_list_update=[]
        # å°†åˆ†é’Ÿæ•°æ®æ‹¼æˆä¸€å¼ æ—¥é¢‘å› å­è¡¨
        self.daily_factors = None
        # æœ€ç»ˆæœˆåº¦å› å­è¡¨æ ¼
        self.monthly_factors = None
        if daily_factors_path:
            # æ—¥é¢‘å› å­æ–‡ä»¶ä¿å­˜è·¯å¾„
            self.daily_factors_path = daily_factors_path
        else:
            self.daily_factors_path=self.homeplace.factor_data_file+'æ—¥é¢‘_'
        if monthly_factors_path:
            # æœˆé¢‘å› å­æ–‡ä»¶ä¿å­˜è·¯å¾„
            self.monthly_factors_path = monthly_factors_path
        else:
            self.monthly_factors_path=self.homeplace.factor_data_file+'æœˆé¢‘_'

    def __call__(self,monthly=False):
        '''ä¸ºäº†é˜²æ­¢å±æ€§åå¤ªå¤šï¼Œå¿˜è®°äº†è¦è°ƒç”¨å“ªä¸ªæ‰æ˜¯ç»“æœï¼Œå› æ­¤å¯ä»¥ç›´æ¥è¾“å‡ºæœˆåº¦æ•°æ®è¡¨'''
        if monthly:
            return self.monthly_factors.copy()
        else:
            try:
                return self.daily_factors.copy()
            except Exception:
                return self.monthly_factors.copy()

    def __add__(self, selfas):
        '''å°†å‡ ä¸ªå› å­æˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œå› å­å€¼ç›¸åŠ '''
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac2s=[]
        if not isinstance(selfas,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfas} is changed into Iterable')
            selfas=(selfas,)
        for selfa in selfas:
            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2s.append(fac2)
        for i in fac2s:
            fac1=fac1+i
        new_pure=pure_fall()
        new_pure.monthly_factors=fac1
        return new_pure

    def __mul__(self,selfas):
        '''å°†å‡ ä¸ªå› å­æ¨ªæˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œä½¿å…¶éƒ½ä¸ºæ­£æ•°ï¼Œç„¶åå› å­å€¼ç›¸ä¹˜'''
        fac1=self.standardlize_in_cross_section(self.monthly_factors)
        fac1=fac1-fac1.min()
        fac2s=[]
        if not isinstance(selfas,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfas} is changed into Iterable')
            selfas=(selfas,)
        for selfa in selfas:
            fac2=self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2=fac2-fac2.min()
            fac2s.append(fac2)
        for i in fac2s:
            fac1=fac1*i
        new_pure=pure_fall()
        new_pure.monthly_factors=fac1
        return new_pure

    def __truediv__(self, selfa):
        '''ä¸¤ä¸ªä¸€æ­£ä¸€å‰¯çš„å› å­ï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸å‡'''
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
        fac=fac1-fac2
        new_pure=pure_fall()
        new_pure.monthly_factors=fac
        return new_pure

    def __floordiv__(self, selfa):
        '''ä¸¤ä¸ªå› å­ä¸€æ­£ä¸€è´Ÿï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸é™¤'''
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
        fac1=fac1-fac1.min()
        fac2=fac2-fac2.min()
        fac=fac1/fac2
        fac=fac.replace(np.inf,np.nan)
        new_pure=pure_fall()
        new_pure.monthly_factors=fac
        return new_pure

    @kk.desktop_sender(title='å˜¿ï¼Œæ­£äº¤åŒ–ç»“æŸå•¦ï½ğŸ¬')
    def __sub__(self, selfa):
        '''ç”¨ä¸»å› å­å‰”é™¤å…¶ä»–ç›¸å…³å› å­ã€ä¼ ç»Ÿå› å­ç­‰
        selfaå¯ä»¥ä¸ºå¤šä¸ªå› å­å¯¹è±¡ç»„æˆçš„å…ƒç»„æˆ–åˆ—è¡¨ï¼Œæ¯ä¸ªè¾…åŠ©å› å­åªéœ€è¦æœ‰æœˆåº¦å› å­æ–‡ä»¶è·¯å¾„å³å¯'''
        tqdm.tqdm.pandas()
        if not isinstance(selfa,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfa} is changed into Iterable')
            selfa=(selfa,)
        fac_main = self.wide_to_long(self.monthly_factors, 'fac')
        fac_helps = [i.monthly_factors for i in selfa]
        help_names = ['help' + str(i) for i in range(1, (len(fac_helps) + 1))]
        fac_helps = list(map(self.wide_to_long, fac_helps, help_names))
        fac_helps = pd.concat(fac_helps, axis=1)
        facs = pd.concat([fac_main, fac_helps], axis=1).dropna()
        facs = facs.groupby('date').progress_apply(lambda x: self.de_in_group(x, help_names))
        facs = facs.unstack()
        facs.columns = list(map(lambda x: x[1], list(facs.columns)))
        return facs

    def __gt__(self,selfa):
        '''ç”¨äºè¾“å‡º25åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­'''
        x=self.monthly_factors.copy()
        y=selfa.monthly_factors.copy()
        x=x.stack().reset_index()
        y=y.stack().reset_index()
        x.columns=['date','code','fac']
        y.columns=['date','code','fac']
        shen=pure_moon()
        x=x.groupby('date').apply(lambda df:shen.get_groups(df,5))
        x=x.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupx'})
        xy=pd.merge(x,y,on=['date','code'])
        xy=xy.groupby(['date','groupx']).apply(lambda df:shen.get_groups(df,5))
        xy=xy.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupy'})
        xy=xy.assign(fac=xy.groupx*5+xy.groupy)
        xy=xy[['date','code','fac']]
        xy=xy.set_index(['date','code']).unstack()
        xy.columns=[i[1] for i in list(xy.columns)]
        new_pure=pure_fall()
        new_pure.monthly_factors=xy
        return new_pure

    def __rshift__(self, selfa):
        '''ç”¨äºè¾“å‡º100åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­'''
        x=self.monthly_factors.copy()
        y=selfa.monthly_factors.copy()
        x=x.stack().reset_index()
        y=y.stack().reset_index()
        x.columns=['date','code','fac']
        y.columns=['date','code','fac']
        shen=pure_moon()
        x=x.groupby('date').apply(lambda df:shen.get_groups(df,10))
        x=x.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupx'})
        xy=pd.merge(x,y,on=['date','code'])
        xy=xy.groupby(['date','groupx']).apply(lambda df:shen.get_groups(df,10))
        xy=xy.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupy'})
        xy=xy.assign(fac=xy.groupx*10+xy.groupy)
        xy=xy[['date','code','fac']]
        xy=xy.set_index(['date','code']).unstack()
        xy.columns=[i[1] for i in list(xy.columns)]
        new_pure=pure_fall()
        new_pure.monthly_factors=xy
        return new_pure

    def wide_to_long(self, df, i):
        '''å°†å®½æ•°æ®è½¬åŒ–ä¸ºé•¿æ•°æ®ï¼Œç”¨äºå› å­è¡¨è½¬åŒ–å’Œæ‹¼æ¥'''
        df = df.stack().reset_index()
        df.columns = ['date', 'code', i]
        df = df.set_index(['date', 'code'])
        return df

    def de_in_group(self, df, help_names):
        '''å¯¹æ¯ä¸ªæ—¶é—´ï¼Œåˆ†åˆ«åšå›å½’ï¼Œå‰”é™¤ç›¸å…³å› å­'''
        ols_order = 'fac~' + '+'.join(help_names)
        ols_result = smf.ols(ols_order, data=df).fit()
        params = {i: ols_result.params[i] for i in help_names}
        predict = [params[i] * df[i] for i in help_names]
        predict = reduce(lambda x, y: x + y, predict)
        df.fac = df.fac - predict - ols_result.params['Intercept']
        df = df[['fac']]
        return df

    def mat_to_df(self, mat,use_datetime=True):
        '''å°†matæ–‡ä»¶å˜æˆ'''
        mat_path = '/'.join([self.minute_files_path, mat])
        df = list(scio.loadmat(mat_path).values())[3]
        df = pd.DataFrame(df, columns=self.minute_columns)
        if use_datetime:
            df.date = pd.to_datetime(df.date.apply(str), format='%Y%m%d')
            df = df.set_index('date')
        return df

    def add_suffix(self, code):
        '''ç»™è‚¡ç¥¨ä»£ç åŠ ä¸Šåç¼€'''
        if not isinstance(code, str):
            code = str(code)
        if len(code) < 6:
            code = '0' * (6 - len(code)) + code
        if code.startswith('0') or code.startswith('3'):
            code = '.'.join([code, 'SZ'])
        elif code.startswith('6'):
            code = '.'.join([code, 'SH'])
        elif code.startswith('8'):
            code = '.'.join([code, 'BJ'])
        return code

    def minute_to_daily(self, func,add_priclose=False,add_tr=False,start_date=10000000,end_date=30000000,update=0):
        '''
        å°†åˆ†é’Ÿæ•°æ®å˜æˆæ—¥é¢‘å› å­ï¼Œå¹¶ä¸”æ·»åŠ åˆ°æ—¥é¢‘å› å­è¡¨é‡Œ
        é€šå¸¸åº”è¯¥æ¯å¤©ç”Ÿæˆä¸€ä¸ªæŒ‡æ ‡ï¼Œæœ€åä¸€åªè‚¡ç¥¨ä¼šç”Ÿæˆä¸€ä¸ªseries
        '''
        
        if add_priclose:
            for mat in tqdm.tqdm(self.minute_files,desc='æ¥æ—¥çºµä½¿åƒåƒé˜™æ­Œï¼Œé£˜äºè¿œæ–¹æˆ‘è·¯ä¸Šï¼›æ¥æ—¥çºµä½¿åƒåƒæ™šæ˜Ÿï¼Œäº®è¿‡ä»Šæ™šæœˆäº®ã€‚éƒ½ä¸åŠä»Šå®µè¿™åˆ»ç¾ä¸½ğŸŒ™'):
                try:
                    code = self.add_suffix(mat[-10:-4])
                    self.code=code
                    df = self.mat_to_df(mat,use_datetime=True)
                    if add_tr:
                        share=read_daily('AllStock_DailyAShareNum.mat')
                        share_this=share[code].to_frame('sharenum').reset_index()
                        share_this.columns=['date','sharenum']
                        df=df.reset_index()
                        df.columns=['date']+list(df.columns)[1:]
                        df=pd.merge(df,share_this,on=['date'],how='left')
                        df=df.assign(tr=df.amount/df.sharenum)
                    df=df.reset_index()
                    df.columns=['date']+list(df.columns)[1:]
                    df.date=df.date.dt.strftime('%Y%m%d')
                    df.date=df.date.astype(int)
                    df=df[(df.date>=start_date)&(df.date<=end_date)]
                    # df.date=pd.to_datetime(df.date,format='%Y%m%d')
                    priclose=df.groupby('date').last()
                    priclose=priclose.shift(1).reset_index()
                    df=pd.concat([priclose,df])
                    the_func = partial(func)
                    df = df.groupby('date').apply(the_func)
                    df = df.to_frame(name=code)
                    if not update:
                        self.daily_factors_list.append(df)
                    else:
                        self.daily_factors_list_update.append(df)
                except Exception as e:
                    if not NO_LOG:
                        logger.warning(f'{code} ç¼ºå¤±')
                        logger.error(e)
        else:
            for mat in tqdm.tqdm(self.minute_files,desc='æ¥æ—¥çºµä½¿åƒåƒé˜™æ­Œï¼Œé£˜äºè¿œæ–¹æˆ‘è·¯ä¸Šï¼›æ¥æ—¥çºµä½¿åƒåƒæ™šæ˜Ÿï¼Œäº®è¿‡ä»Šæ™šæœˆäº®ã€‚éƒ½ä¸åŠä»Šå®µè¿™åˆ»ç¾ä¸½ğŸŒ™'):
                try:
                    code = self.add_suffix(mat[-10:-4])
                    self.code=code
                    df = self.mat_to_df(mat,use_datetime=True)
                    if add_tr:
                        share=read_daily('AllStock_DailyAShareNum.mat')
                        share_this=share[code].to_frame('sharenum').reset_index()
                        share_this.columns=['date','sharenum']
                        df=df.reset_index()
                        df.columns=['date']+list(df.columns)[1:]
                        df=pd.merge(df,share_this,on=['date'],how='left')
                        df=df.assign(tr=df.amount/df.sharenum)
                    the_func = partial(func)
                    df=df.reset_index()
                    df.columns=['date']+list(df.columns)[1:]
                    df.date=df.date.dt.strftime('%Y%m%d')
                    df.date=df.date.astype(int)
                    df=df[(df.date>=start_date)&(df.date<=end_date)]
                    # df.date=pd.to_datetime(df.date,format='%Y%m%d')
                    df = df.groupby('date').apply(the_func)
                    df = df.to_frame(name=code)
                    if not update:
                        self.daily_factors_list.append(df)
                    else:
                        self.daily_factors_list_update.append(df)
                except Exception as e:
                    if not NO_LOG:
                        logger.warning(f'{code} ç¼ºå¤±')
                        logger.error(e)
        if update:
            self.daily_factors_update=pd.concat(self.daily_factors_list_update,axis=1)
            self.daily_factors_update.index=pd.to_datetime(self.daily_factors_update.index.astype(int),format='%Y%m%d')
            self.daily_factors=pd.concat([self.daily_factors,self.daily_factors_update])
        else:
            self.daily_factors=pd.concat(self.daily_factors_list,axis=1)
            self.daily_factors.index=pd.to_datetime(self.daily_factors.index.astype(int),format='%Y%m%d')
        self.daily_factors=self.daily_factors.dropna(how='all')
        self.daily_factors=self.daily_factors[self.daily_factors.index>=pd.Timestamp('2013-03-26')]
        self.daily_factors.reset_index().to_feather(self.daily_factors_path)
        if not NO_LOG:
            logger.success('æ›´æ–°å·²å®Œæˆ')

    def minute_to_daily_whole(self, func,start_date=10000000,end_date=30000000,update=0):
        '''
        å°†åˆ†é’Ÿæ•°æ®å˜æˆæ—¥é¢‘å› å­ï¼Œå¹¶ä¸”æ·»åŠ åˆ°æ—¥é¢‘å› å­è¡¨é‡Œ
        é€šå¸¸åº”è¯¥æ¯å¤©ç”Ÿæˆä¸€ä¸ªæŒ‡æ ‡ï¼Œæœ€åä¸€åªè‚¡ç¥¨ä¼šç”Ÿæˆä¸€ä¸ªseries
        '''
        for mat in tqdm.tqdm(self.minute_files):
            self.code=self.add_suffix(mat[-10:-4])
            df = self.mat_to_df(mat)
            df.date=df.date.astype(int)
            df=df[(df.date>=start_date)&(df.date<=end_date)]
            # df.date=pd.to_datetime(df.date,format='%Y%m%d')
            the_func = partial(func)
            df = func(df)
            if isinstance(df,pd.DataFrame):
                df.columns = [self.code]
                if not update:
                    self.daily_factors_list.append(df)
                else:
                    self.daily_factors_list_update.append(df)
            elif isinstance(df,pd.Series):
                df=df.to_frame(name=self.code)
                if not update:
                    self.daily_factors_list.append(df)
                else:
                    self.daily_factors_list_update.append(df)
            else:
                if not NO_LOG:
                    logger.warning(f'df is {df}')
        if update:
            self.daily_factors_update=pd.concat(self.daily_factors_list_update,axis=1)
            self.daily_factors=pd.concat([self.daily_factors,self.daily_factors_update])
        else:
            self.daily_factors = pd.concat(self.daily_factors_list, axis=1)
        self.daily_factors=self.daily_factors.dropna(how='all')
        self.daily_factors=self.daily_factors[self.daily_factors.index>=pd.Timestamp('2013-03-26')]
        self.daily_factors.reset_index().to_feather(self.daily_factors_path)
        if not NO_LOG:
            logger.success('æ›´æ–°å·²å®Œæˆ')

    def standardlize_in_cross_section(self, df):
        '''
        åœ¨æ¨ªæˆªé¢ä¸Šåšæ ‡å‡†åŒ–
        è¾“å…¥çš„dfåº”ä¸ºï¼Œåˆ—åæ˜¯è‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•æ˜¯æ—¶é—´
        '''
        df = df.T
        df = (df - df.mean()) / df.std()
        df = df.T
        return df

    @kk.desktop_sender(title='å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ')
    def get_daily_factors(self, func, whole=False,add_priclose=False,add_tr=False,start_date=10000000,end_date=30000000):
        '''è°ƒç”¨åˆ†é’Ÿåˆ°æ—¥åº¦æ–¹æ³•ï¼Œç®—å‡ºæ—¥é¢‘æ•°æ®'''
        try:
            self.daily_factors = pd.read_feather(self.daily_factors_path)
            self.daily_factors = self.daily_factors.set_index('date')
            now_minute_data=self.mat_to_df(self.minute_files[0])
            if self.daily_factors.index.max()<now_minute_data.index.max():
                if not NO_LOG:
                    logger.info(f'ä¸Šæ¬¡å­˜å‚¨çš„å› å­å€¼åˆ°{self.daily_factors.index.max()}ï¼Œè€Œåˆ†é’Ÿæ•°æ®æœ€æ–°åˆ°{now_minute_data.index.max()}ï¼Œå¼€å§‹æ›´æ–°â€¦â€¦')
                start_date_update=int(datetime.datetime.strftime(self.daily_factors.index.max()+pd.Timedelta('1 day'),'%Y%m%d'))
                end_date_update=int(datetime.datetime.strftime(now_minute_data.index.max(),'%Y%m%d'))
                if whole:
                    self.minute_to_daily_whole(func,start_date=start_date_update,end_date=end_date_update,update=1)
                else:
                    self.minute_to_daily(func,start_date=start_date_update,end_date=end_date_update,update=1)
        except Exception:
            if whole:
                self.minute_to_daily_whole(func,start_date=start_date,end_date=end_date)
            else:
                self.minute_to_daily(func,add_priclose=add_priclose,add_tr=add_tr,start_date=start_date,end_date=end_date)

    def get_neutral_monthly_factors(self, df,boxcox=False):
        '''å¯¹æœˆåº¦å› å­åšå¸‚å€¼ä¸­æ€§åŒ–å¤„ç†'''
        shen = pure_moon()
        shen.set_factor_df_date_as_index(df)
        if boxcox:
            shen.run(5, boxcox=True, plt=False, print_comments=False)
        else:
            shen.run(5,neutralize=True,plt=False,print_comments=False)
        new_factors = shen.factors.copy()
        new_factors = new_factors.set_index(['date', 'code']).unstack()
        new_factors.columns = list(map(lambda x: x[1], list(new_factors.columns)))
        new_factors = new_factors.reset_index()
        add_start_point = new_factors.date.min()
        add_start_point = add_start_point - pd.Timedelta(days=add_start_point.day)
        new_factors.date = new_factors.date.shift(1)
        new_factors.date = new_factors.date.fillna(add_start_point)
        new_factors = new_factors.set_index('date')
        return new_factors

    def get_monthly_factors(self, func, neutralize,boxcox):
        '''å°†æ—¥é¢‘çš„å› å­è½¬åŒ–ä¸ºæœˆé¢‘å› å­'''
        two_parts=self.monthly_factors_path.split('.')
        try:
            self.monthly_factors = pd.read_feather(self.monthly_factors_path)
            self.monthly_factors = self.monthly_factors.set_index('date')
        except Exception:
            the_func = partial(func)
            self.monthly_factors = the_func(self.daily_factors)
            if neutralize:
                self.monthly_factors = self.get_neutral_monthly_factors(self.monthly_factors)
            elif boxcox:
                self.monthly_factors=self.get_neutral_monthly_factors(self.monthly_factors,boxcox=True)

            self.monthly_factors.reset_index().to_feather(self.monthly_factors_path)

    def run(self, whole=False,daily_func=None, monthly_func=None,
            neutralize=False,boxcox=False):
        '''æ‰§å¿…è¦çš„å‡½æ•°ï¼Œå°†åˆ†é’Ÿæ•°æ®å˜æˆæœˆåº¦å› å­'''
        self.get_daily_factors(daily_func,whole)
        self.get_monthly_factors(monthly_func, neutralize,boxcox)


class pure_sunbath():
    def __init__(
            self,
            minute_files_path=None,
            minute_columns=['date','open','high','low','close','amount','money'],
            daily_factors_path=None,
            monthly_factors_path=None
    ):
        self.homeplace=HomePlace()
        print('åœ¨æµ´å®¤çš„æ¸©æš–é‡Œï¼Œè¿é”™è¯¯ä¹Ÿä¸å¯æ€•ï¼›åœ¨é˜³å…‰çš„ç…§è€€ä¸‹ï¼Œé»‘æš—å°†æ— æ‰€éå½¢ã€‚')
        if monthly_factors_path:
            # åˆ†é’Ÿæ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
            self.minute_files_path=minute_files_path
        else:
            self.minute_files_path=self.homeplace.minute_data_file[:-1]
        # åˆ†é’Ÿæ•°æ®æ–‡ä»¶å¤¹
        self.minute_files=os.listdir(self.minute_files_path)
        self.minute_files=[i for i in self.minute_files if i.endswith('.mat')]
        self.minute_files=sorted(self.minute_files)
        # åˆ†é’Ÿæ•°æ®çš„è¡¨å¤´
        self.minute_columns=minute_columns
        # åˆ†é’Ÿæ•°æ®æ—¥é¢‘åŒ–ä¹‹åçš„æ•°æ®è¡¨
        self.daily_factors_list=[]
        #æ›´æ–°æ•°æ®ç”¨çš„åˆ—è¡¨
        self.daily_factors_list_update=[]
        # å°†åˆ†é’Ÿæ•°æ®æ‹¼æˆä¸€å¼ æ—¥é¢‘å› å­è¡¨
        self.daily_factors=None
        # æœ€ç»ˆæœˆåº¦å› å­è¡¨æ ¼
        self.monthly_factors=None
        if daily_factors_path:
            # æ—¥é¢‘å› å­æ–‡ä»¶ä¿å­˜è·¯å¾„
            self.daily_factors_path=daily_factors_path
        else:
            self.daily_factors_path=self.homeplace.factor_data_file+'æ—¥é¢‘_'
        if monthly_factors_path:
            # æœˆé¢‘å› å­æ–‡ä»¶ä¿å­˜è·¯å¾„
            self.monthly_factors_path=monthly_factors_path
        else:
            self.monthly_factors_path=self.homeplace.factor_data_file+'æœˆé¢‘_'

    def __call__(self,monthly=False):
        '''ä¸ºäº†é˜²æ­¢å±æ€§åå¤ªå¤šï¼Œå¿˜è®°äº†è¦è°ƒç”¨å“ªä¸ªæ‰æ˜¯ç»“æœï¼Œå› æ­¤å¯ä»¥ç›´æ¥è¾“å‡ºæœˆåº¦æ•°æ®è¡¨'''
        if monthly:
            return self.monthly_factors.copy()
        else:
            try:
                return self.daily_factors.copy()
            except Exception:
                return self.monthly_factors.copy()

    def __add__(self,selfas):
        '''å°†å‡ ä¸ªå› å­æˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œå› å­å€¼ç›¸åŠ '''
        fac1=self.standardlize_in_cross_section(self.monthly_factors)
        fac2s=[]
        if not isinstance(selfas,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfas} is changed into Iterable')
            selfas=(selfas,)
        for selfa in selfas:
            fac2=self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2s.append(fac2)
        for i in fac2s:
            fac1=fac1+i
        new_pure=pure_fall()
        new_pure.monthly_factors=fac1
        return new_pure

    def __mul__(self,selfas):
        '''å°†å‡ ä¸ªå› å­æ¨ªæˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œä½¿å…¶éƒ½ä¸ºæ­£æ•°ï¼Œç„¶åå› å­å€¼ç›¸ä¹˜'''
        fac1=self.standardlize_in_cross_section(self.monthly_factors)
        fac1=fac1-fac1.min()
        fac2s=[]
        if not isinstance(selfas,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfas} is changed into Iterable')
            selfas=(selfas,)
        for selfa in selfas:
            fac2=self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2=fac2-fac2.min()
            fac2s.append(fac2)
        for i in fac2s:
            fac1=fac1*i
        new_pure=pure_fall()
        new_pure.monthly_factors=fac1
        return new_pure

    def __truediv__(self,selfa):
        '''ä¸¤ä¸ªä¸€æ­£ä¸€å‰¯çš„å› å­ï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸å‡'''
        fac1=self.standardlize_in_cross_section(self.monthly_factors)
        fac2=self.standardlize_in_cross_section(selfa.monthly_factors)
        fac=fac1-fac2
        new_pure=pure_fall()
        new_pure.monthly_factors=fac
        return new_pure

    def __floordiv__(self,selfa):
        '''ä¸¤ä¸ªå› å­ä¸€æ­£ä¸€è´Ÿï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸é™¤'''
        fac1=self.standardlize_in_cross_section(self.monthly_factors)
        fac2=self.standardlize_in_cross_section(selfa.monthly_factors)
        fac1=fac1-fac1.min()
        fac2=fac2-fac2.min()
        fac=fac1/fac2
        fac=fac.replace(np.inf,np.nan)
        new_pure=pure_fall()
        new_pure.monthly_factors=fac
        return new_pure

    def __sub__(self,selfa):
        '''ç”¨ä¸»å› å­å‰”é™¤å…¶ä»–ç›¸å…³å› å­ã€ä¼ ç»Ÿå› å­ç­‰
        selfaå¯ä»¥ä¸ºå¤šä¸ªå› å­å¯¹è±¡ç»„æˆçš„å…ƒç»„æˆ–åˆ—è¡¨ï¼Œæ¯ä¸ªè¾…åŠ©å› å­åªéœ€è¦æœ‰æœˆåº¦å› å­æ–‡ä»¶è·¯å¾„å³å¯'''
        tqdm.tqdm.pandas()
        if not isinstance(selfa,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfa} is changed into Iterable')
            selfa=(selfa,)
        fac_main=self.wide_to_long(self.monthly_factors,'fac')
        fac_helps=[i.monthly_factors for i in selfa]
        help_names=['help'+str(i) for i in range(1,(len(fac_helps)+1))]
        fac_helps=list(map(self.wide_to_long,fac_helps,help_names))
        fac_helps=pd.concat(fac_helps,axis=1)
        facs=pd.concat([fac_main,fac_helps],axis=1).dropna()
        facs=facs.groupby('date').progress_apply(lambda x:self.de_in_group(x,help_names))
        facs=facs.unstack()
        facs.columns=list(map(lambda x:x[1],list(facs.columns)))
        return facs

    def __gt__(self,selfa):
        '''ç”¨äºè¾“å‡º25åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­'''
        x=self.monthly_factors.copy()
        y=selfa.monthly_factors.copy()
        x=x.stack().reset_index()
        y=y.stack().reset_index()
        x.columns=['date','code','fac']
        y.columns=['date','code','fac']
        shen=pure_moon()
        x=x.groupby('date').apply(lambda df:shen.get_groups(df,5))
        x=x.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupx'})
        xy=pd.merge(x,y,on=['date','code'])
        xy=xy.groupby(['date','groupx']).apply(lambda df:shen.get_groups(df,5))
        xy=xy.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupy'})
        xy=xy.assign(fac=xy.groupx*5+xy.groupy)
        xy=xy[['date','code','fac']]
        xy=xy.set_index(['date','code']).unstack()
        xy.columns=[i[1] for i in list(xy.columns)]
        new_pure=pure_fall()
        new_pure.monthly_factors=xy
        return new_pure

    def __rshift__(self,selfa):
        '''ç”¨äºè¾“å‡º100åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­'''
        x=self.monthly_factors.copy()
        y=selfa.monthly_factors.copy()
        x=x.stack().reset_index()
        y=y.stack().reset_index()
        x.columns=['date','code','fac']
        y.columns=['date','code','fac']
        shen=pure_moon()
        x=x.groupby('date').apply(lambda df:shen.get_groups(df,10))
        x=x.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupx'})
        xy=pd.merge(x,y,on=['date','code'])
        xy=xy.groupby(['date','groupx']).apply(lambda df:shen.get_groups(df,10))
        xy=xy.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupy'})
        xy=xy.assign(fac=xy.groupx*10+xy.groupy)
        xy=xy[['date','code','fac']]
        xy=xy.set_index(['date','code']).unstack()
        xy.columns=[i[1] for i in list(xy.columns)]
        new_pure=pure_fall()
        new_pure.monthly_factors=xy
        return new_pure

    def wide_to_long(self,df,i):
        '''å°†å®½æ•°æ®è½¬åŒ–ä¸ºé•¿æ•°æ®ï¼Œç”¨äºå› å­è¡¨è½¬åŒ–å’Œæ‹¼æ¥'''
        df=df.stack().reset_index()
        df.columns=['date','code',i]
        df=df.set_index(['date','code'])
        return df

    def de_in_group(self,df,help_names):
        '''å¯¹æ¯ä¸ªæ—¶é—´ï¼Œåˆ†åˆ«åšå›å½’ï¼Œå‰”é™¤ç›¸å…³å› å­'''
        ols_order='fac~'+'+'.join(help_names)
        ols_result=smf.ols(ols_order,data=df).fit()
        params={i:ols_result.params[i] for i in help_names}
        predict=[params[i]*df[i] for i in help_names]
        predict=reduce(lambda x,y:x+y,predict)
        df.fac=df.fac-predict-ols_result.params['Intercept']
        df=df[['fac']]
        return df

    def mat_to_df(self,mat,use_datetime=True):
        '''å°†matæ–‡ä»¶å˜æˆ'''
        mat_path='/'.join([self.minute_files_path,mat])
        df=list(scio.loadmat(mat_path).values())[3]
        df=pd.DataFrame(df,columns=self.minute_columns)
        if use_datetime:
            df.date=pd.to_datetime(df.date.apply(str),format='%Y%m%d')
            df=df.set_index('date')
        return df

    def add_suffix(self,code):
        '''ç»™è‚¡ç¥¨ä»£ç åŠ ä¸Šåç¼€'''
        if not isinstance(code,str):
            code=str(code)
        if len(code)<6:
            code='0'*(6-len(code))+code
        if code.startswith('0') or code.startswith('3'):
            code='.'.join([code,'SZ'])
        elif code.startswith('6'):
            code='.'.join([code,'SH'])
        elif code.startswith('8'):
            code='.'.join([code,'BJ'])
        return code

    def minute_to_daily(self,func,add_priclose=False,add_tr=False,start_date=10000000,end_date=30000000,update=0):
        '''
        å°†åˆ†é’Ÿæ•°æ®å˜æˆæ—¥é¢‘å› å­ï¼Œå¹¶ä¸”æ·»åŠ åˆ°æ—¥é¢‘å› å­è¡¨é‡Œ
        é€šå¸¸åº”è¯¥æ¯å¤©ç”Ÿæˆä¸€ä¸ªæŒ‡æ ‡ï¼Œæœ€åä¸€åªè‚¡ç¥¨ä¼šç”Ÿæˆä¸€ä¸ªseries
        '''

        if add_priclose:
            for mat in tqdm.tqdm(self.minute_files):
                # try:
                code=self.add_suffix(mat[-10:-4])
                self.code=code
                df=self.mat_to_df(mat,use_datetime=True)
                if add_tr:
                    share=read_daily('AllStock_DailyAShareNum.mat')
                    share_this=share[code].to_frame('sharenum').reset_index()
                    share_this.columns=['date','sharenum']
                    df=df.reset_index()
                    df.columns=['date']+list(df.columns)[1:]
                    df=pd.merge(df,share_this,on=['date'],how='left')
                    df=df.assign(tr=df.amount/df.sharenum)
                df=df.reset_index()
                df.columns=['date']+list(df.columns)[1:]
                df.date=df.date.dt.strftime('%Y%m%d')
                df.date=df.date.astype(int)
                df=df[(df.date>=start_date)&(df.date<=end_date)]
                # df.date=pd.to_datetime(df.date,format='%Y%m%d')
                priclose=df.groupby('date').last()
                priclose=priclose.shift(1).reset_index()
                df=pd.concat([priclose,df])
                the_func=partial(func)
                df=df.groupby('date').apply(the_func)
                df=df.to_frame(name=code)
                if not update:
                    self.daily_factors_list.append(df)
                else:
                    self.daily_factors_list_update.append(df)
                # except Exception as e:
                #     if not NO_LOG:
                #         logger.warning(f'{code} ç¼ºå¤±')
                #         logger.error(e)
        else:
            for mat in tqdm.tqdm(self.minute_files):
                # try:
                code=self.add_suffix(mat[-10:-4])
                self.code=code
                df=self.mat_to_df(mat,use_datetime=True)
                if add_tr:
                    share=read_daily('AllStock_DailyAShareNum.mat')
                    share_this=share[code].to_frame('sharenum').reset_index()
                    share_this.columns=['date','sharenum']
                    df=df.reset_index()
                    df.columns=['date']+list(df.columns)[1:]
                    df=pd.merge(df,share_this,on=['date'],how='left')
                    df=df.assign(tr=df.amount/df.sharenum)
                the_func=partial(func)
                df=df.reset_index()
                df.columns=['date']+list(df.columns)[1:]
                df.date=df.date.dt.strftime('%Y%m%d')
                df.date=df.date.astype(int)
                df=df[(df.date>=start_date)&(df.date<=end_date)]
                # df.date=pd.to_datetime(df.date,format='%Y%m%d')
                df=df.groupby('date').apply(the_func)
                df=df.to_frame(name=code)
                if not update:
                    self.daily_factors_list.append(df)
                else:
                    self.daily_factors_list_update.append(df)
                # except Exception as e:
                #     if not NO_LOG:
                #         logger.warning(f'{code} ç¼ºå¤±')
                #         logger.error(e)
        if update:
            self.daily_factors_update=pd.concat(self.daily_factors_list_update,axis=1)
            self.daily_factors_update.index=pd.to_datetime(self.daily_factors_update.index.astype(int),format='%Y%m%d')
            self.daily_factors=pd.concat([self.daily_factors,self.daily_factors_update])
        else:
            self.daily_factors=pd.concat(self.daily_factors_list,axis=1)
            self.daily_factors.index=pd.to_datetime(self.daily_factors.index.astype(int),format='%Y%m%d')
        self.daily_factors=self.daily_factors.dropna(how='all')
        self.daily_factors=self.daily_factors[self.daily_factors.index>=pd.Timestamp('2013-03-26')]
        self.daily_factors.reset_index().to_feather(self.daily_factors_path)
        if not NO_LOG:
            logger.success('æ›´æ–°å·²å®Œæˆ')

    def minute_to_daily_whole(self,func,start_date=10000000,end_date=30000000,update=0):
        '''
        å°†åˆ†é’Ÿæ•°æ®å˜æˆæ—¥é¢‘å› å­ï¼Œå¹¶ä¸”æ·»åŠ åˆ°æ—¥é¢‘å› å­è¡¨é‡Œ
        é€šå¸¸åº”è¯¥æ¯å¤©ç”Ÿæˆä¸€ä¸ªæŒ‡æ ‡ï¼Œæœ€åä¸€åªè‚¡ç¥¨ä¼šç”Ÿæˆä¸€ä¸ªseries
        '''
        for mat in tqdm.tqdm(self.minute_files):
            self.code=self.add_suffix(mat[-10:-4])
            df=self.mat_to_df(mat)
            df.date=df.date.astype(int)
            df=df[(df.date>=start_date)&(df.date<=end_date)]
            # df.date=pd.to_datetime(df.date,format='%Y%m%d')
            the_func=partial(func)
            df=func(df)
            if isinstance(df,pd.DataFrame):
                df.columns=[self.code]
                if not update:
                    self.daily_factors_list.append(df)
                else:
                    self.daily_factors_list_update.append(df)
            elif isinstance(df,pd.Series):
                df=df.to_frame(name=self.code)
                if not update:
                    self.daily_factors_list.append(df)
                else:
                    self.daily_factors_list_update.append(df)
            else:
                if not NO_LOG:
                    logger.warning(f'df is {df}')
        if update:
            self.daily_factors_update=pd.concat(self.daily_factors_list_update,axis=1)
            self.daily_factors=pd.concat([self.daily_factors,self.daily_factors_update])
        else:
            self.daily_factors=pd.concat(self.daily_factors_list,axis=1)
        self.daily_factors=self.daily_factors.dropna(how='all')
        self.daily_factors=self.daily_factors[self.daily_factors.index>=pd.Timestamp('2013-03-26')]
        self.daily_factors.reset_index().to_feather(self.daily_factors_path)
        if not NO_LOG:
            logger.success('æ›´æ–°å·²å®Œæˆ')

    def standardlize_in_cross_section(self,df):
        '''
        åœ¨æ¨ªæˆªé¢ä¸Šåšæ ‡å‡†åŒ–
        è¾“å…¥çš„dfåº”ä¸ºï¼Œåˆ—åæ˜¯è‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•æ˜¯æ—¶é—´
        '''
        df=df.T
        df=(df-df.mean())/df.std()
        df=df.T
        return df

    def get_daily_factors(self,func,whole=False,add_priclose=False,add_tr=False,start_date=10000000,end_date=30000000):
        '''è°ƒç”¨åˆ†é’Ÿåˆ°æ—¥åº¦æ–¹æ³•ï¼Œç®—å‡ºæ—¥é¢‘æ•°æ®'''
        try:
            self.daily_factors=pd.read_feather(self.daily_factors_path)
            self.daily_factors=self.daily_factors.set_index('date')
            now_minute_data=self.mat_to_df(self.minute_files[0])
            if self.daily_factors.index.max()<now_minute_data.index.max():
                if not NO_LOG:
                    logger.info(
                        f'ä¸Šæ¬¡å­˜å‚¨çš„å› å­å€¼åˆ°{self.daily_factors.index.max()}ï¼Œè€Œåˆ†é’Ÿæ•°æ®æœ€æ–°åˆ°{now_minute_data.index.max()}ï¼Œå¼€å§‹æ›´æ–°â€¦â€¦')
                start_date_update=int(
                    datetime.datetime.strftime(self.daily_factors.index.max()+pd.Timedelta('1 day'),'%Y%m%d'))
                end_date_update=int(datetime.datetime.strftime(now_minute_data.index.max(),'%Y%m%d'))
                if whole:
                    self.minute_to_daily_whole(func,start_date=start_date_update,end_date=end_date_update,update=1)
                else:
                    self.minute_to_daily(func,start_date=start_date_update,end_date=end_date_update,update=1)
        except Exception:
            if whole:
                self.minute_to_daily_whole(func,start_date=start_date,end_date=end_date)
            else:
                self.minute_to_daily(func,add_priclose=add_priclose,add_tr=add_tr,start_date=start_date,
                                     end_date=end_date)

    def get_neutral_monthly_factors(self,df,boxcox=False):
        '''å¯¹æœˆåº¦å› å­åšå¸‚å€¼ä¸­æ€§åŒ–å¤„ç†'''
        shen=pure_moon()
        shen.set_factor_df_date_as_index(df)
        if boxcox:
            shen.run(5,boxcox=True,plt=False,print_comments=False)
        else:
            shen.run(5,neutralize=True,plt=False,print_comments=False)
        new_factors=shen.factors.copy()
        new_factors=new_factors.set_index(['date','code']).unstack()
        new_factors.columns=list(map(lambda x:x[1],list(new_factors.columns)))
        new_factors=new_factors.reset_index()
        add_start_point=new_factors.date.min()
        add_start_point=add_start_point-pd.Timedelta(days=add_start_point.day)
        new_factors.date=new_factors.date.shift(1)
        new_factors.date=new_factors.date.fillna(add_start_point)
        new_factors=new_factors.set_index('date')
        return new_factors

    def get_monthly_factors(self,func,neutralize,boxcox):
        '''å°†æ—¥é¢‘çš„å› å­è½¬åŒ–ä¸ºæœˆé¢‘å› å­'''
        two_parts=self.monthly_factors_path.split('.')
        try:
            self.monthly_factors=pd.read_feather(self.monthly_factors_path)
            self.monthly_factors=self.monthly_factors.set_index('date')
        except Exception:
            the_func=partial(func)
            self.monthly_factors=the_func(self.daily_factors)
            if neutralize:
                self.monthly_factors=self.get_neutral_monthly_factors(self.monthly_factors)
            elif boxcox:
                self.monthly_factors=self.get_neutral_monthly_factors(self.monthly_factors,boxcox=True)

            self.monthly_factors.reset_index().to_feather(self.monthly_factors_path)

    def run(self,whole=False,daily_func=None,monthly_func=None,
            neutralize=False,boxcox=False):
        '''æ‰§å¿…è¦çš„å‡½æ•°ï¼Œå°†åˆ†é’Ÿæ•°æ®å˜æˆæœˆåº¦å› å­'''
        self.get_daily_factors(daily_func,whole)
        self.get_monthly_factors(monthly_func,neutralize,boxcox)


class run_away_with_me():
    def __init__(
            self,
            minute_files_path=None,
            minute_columns=['date','open','high','low','close','amount','money'],
            daily_factors_path=None,
            monthly_factors_path=None
    ):
        self.homeplace=HomePlace()
        if monthly_factors_path:
            # åˆ†é’Ÿæ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
            self.minute_files_path=minute_files_path
        else:
            self.minute_files_path=self.homeplace.minute_data_file[:-1]
        # åˆ†é’Ÿæ•°æ®æ–‡ä»¶å¤¹
        self.minute_files=os.listdir(self.minute_files_path)
        self.minute_files=[i for i in self.minute_files if i.endswith('.mat')]
        self.minute_files=sorted(self.minute_files)
        # åˆ†é’Ÿæ•°æ®çš„è¡¨å¤´
        self.minute_columns=minute_columns
        # åˆ†é’Ÿæ•°æ®æ—¥é¢‘åŒ–ä¹‹åçš„æ•°æ®è¡¨
        self.daily_factors_list=[]
        #æ›´æ–°æ•°æ®ç”¨çš„åˆ—è¡¨
        self.daily_factors_list_update=[]
        # å°†åˆ†é’Ÿæ•°æ®æ‹¼æˆä¸€å¼ æ—¥é¢‘å› å­è¡¨
        self.daily_factors=None
        # æœ€ç»ˆæœˆåº¦å› å­è¡¨æ ¼
        self.monthly_factors=None
        if daily_factors_path:
            # æ—¥é¢‘å› å­æ–‡ä»¶ä¿å­˜è·¯å¾„
            self.daily_factors_path=daily_factors_path
        else:
            self.daily_factors_path=self.homeplace.factor_data_file+'æ—¥é¢‘_'
        if monthly_factors_path:
            # æœˆé¢‘å› å­æ–‡ä»¶ä¿å­˜è·¯å¾„
            self.monthly_factors_path=monthly_factors_path
        else:
            self.monthly_factors_path=self.homeplace.factor_data_file+'æœˆé¢‘_'

    def __call__(self,monthly=False):
        '''ä¸ºäº†é˜²æ­¢å±æ€§åå¤ªå¤šï¼Œå¿˜è®°äº†è¦è°ƒç”¨å“ªä¸ªæ‰æ˜¯ç»“æœï¼Œå› æ­¤å¯ä»¥ç›´æ¥è¾“å‡ºæœˆåº¦æ•°æ®è¡¨'''
        if monthly:
            return self.monthly_factors.copy()
        else:
            try:
                return self.daily_factors.copy()
            except Exception:
                return self.monthly_factors.copy()

    def __add__(self,selfas):
        '''å°†å‡ ä¸ªå› å­æˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œå› å­å€¼ç›¸åŠ '''
        fac1=self.standardlize_in_cross_section(self.monthly_factors)
        fac2s=[]
        if not isinstance(selfas,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfas} is changed into Iterable')
            selfas=(selfas,)
        for selfa in selfas:
            fac2=self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2s.append(fac2)
        for i in fac2s:
            fac1=fac1+i
        new_pure=pure_fall()
        new_pure.monthly_factors=fac1
        return new_pure

    def __mul__(self,selfas):
        '''å°†å‡ ä¸ªå› å­æ¨ªæˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œä½¿å…¶éƒ½ä¸ºæ­£æ•°ï¼Œç„¶åå› å­å€¼ç›¸ä¹˜'''
        fac1=self.standardlize_in_cross_section(self.monthly_factors)
        fac1=fac1-fac1.min()
        fac2s=[]
        if not isinstance(selfas,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfas} is changed into Iterable')
            selfas=(selfas,)
        for selfa in selfas:
            fac2=self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2=fac2-fac2.min()
            fac2s.append(fac2)
        for i in fac2s:
            fac1=fac1*i
        new_pure=pure_fall()
        new_pure.monthly_factors=fac1
        return new_pure

    def __truediv__(self,selfa):
        '''ä¸¤ä¸ªä¸€æ­£ä¸€å‰¯çš„å› å­ï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸å‡'''
        fac1=self.standardlize_in_cross_section(self.monthly_factors)
        fac2=self.standardlize_in_cross_section(selfa.monthly_factors)
        fac=fac1-fac2
        new_pure=pure_fall()
        new_pure.monthly_factors=fac
        return new_pure

    def __floordiv__(self,selfa):
        '''ä¸¤ä¸ªå› å­ä¸€æ­£ä¸€è´Ÿï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸é™¤'''
        fac1=self.standardlize_in_cross_section(self.monthly_factors)
        fac2=self.standardlize_in_cross_section(selfa.monthly_factors)
        fac1=fac1-fac1.min()
        fac2=fac2-fac2.min()
        fac=fac1/fac2
        fac=fac.replace(np.inf,np.nan)
        new_pure=pure_fall()
        new_pure.monthly_factors=fac
        return new_pure

    def __sub__(self,selfa):
        '''ç”¨ä¸»å› å­å‰”é™¤å…¶ä»–ç›¸å…³å› å­ã€ä¼ ç»Ÿå› å­ç­‰
        selfaå¯ä»¥ä¸ºå¤šä¸ªå› å­å¯¹è±¡ç»„æˆçš„å…ƒç»„æˆ–åˆ—è¡¨ï¼Œæ¯ä¸ªè¾…åŠ©å› å­åªéœ€è¦æœ‰æœˆåº¦å› å­æ–‡ä»¶è·¯å¾„å³å¯'''
        tqdm.tqdm.pandas()
        if not isinstance(selfa,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfa} is changed into Iterable')
            selfa=(selfa,)
        fac_main=self.wide_to_long(self.monthly_factors,'fac')
        fac_helps=[i.monthly_factors for i in selfa]
        help_names=['help'+str(i) for i in range(1,(len(fac_helps)+1))]
        fac_helps=list(map(self.wide_to_long,fac_helps,help_names))
        fac_helps=pd.concat(fac_helps,axis=1)
        facs=pd.concat([fac_main,fac_helps],axis=1).dropna()
        facs=facs.groupby('date').progress_apply(lambda x:self.de_in_group(x,help_names))
        facs=facs.unstack()
        facs.columns=list(map(lambda x:x[1],list(facs.columns)))
        return facs

    def __gt__(self,selfa):
        '''ç”¨äºè¾“å‡º25åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­'''
        x=self.monthly_factors.copy()
        y=selfa.monthly_factors.copy()
        x=x.stack().reset_index()
        y=y.stack().reset_index()
        x.columns=['date','code','fac']
        y.columns=['date','code','fac']
        shen=pure_moon()
        x=x.groupby('date').apply(lambda df:shen.get_groups(df,5))
        x=x.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupx'})
        xy=pd.merge(x,y,on=['date','code'])
        xy=xy.groupby(['date','groupx']).apply(lambda df:shen.get_groups(df,5))
        xy=xy.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupy'})
        xy=xy.assign(fac=xy.groupx*5+xy.groupy)
        xy=xy[['date','code','fac']]
        xy=xy.set_index(['date','code']).unstack()
        xy.columns=[i[1] for i in list(xy.columns)]
        new_pure=pure_fall()
        new_pure.monthly_factors=xy
        return new_pure

    def __rshift__(self,selfa):
        '''ç”¨äºè¾“å‡º100åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­'''
        x=self.monthly_factors.copy()
        y=selfa.monthly_factors.copy()
        x=x.stack().reset_index()
        y=y.stack().reset_index()
        x.columns=['date','code','fac']
        y.columns=['date','code','fac']
        shen=pure_moon()
        x=x.groupby('date').apply(lambda df:shen.get_groups(df,10))
        x=x.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupx'})
        xy=pd.merge(x,y,on=['date','code'])
        xy=xy.groupby(['date','groupx']).apply(lambda df:shen.get_groups(df,10))
        xy=xy.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupy'})
        xy=xy.assign(fac=xy.groupx*10+xy.groupy)
        xy=xy[['date','code','fac']]
        xy=xy.set_index(['date','code']).unstack()
        xy.columns=[i[1] for i in list(xy.columns)]
        new_pure=pure_fall()
        new_pure.monthly_factors=xy
        return new_pure

    def wide_to_long(self,df,i):
        '''å°†å®½æ•°æ®è½¬åŒ–ä¸ºé•¿æ•°æ®ï¼Œç”¨äºå› å­è¡¨è½¬åŒ–å’Œæ‹¼æ¥'''
        df=df.stack().reset_index()
        df.columns=['date','code',i]
        df=df.set_index(['date','code'])
        return df

    def de_in_group(self,df,help_names):
        '''å¯¹æ¯ä¸ªæ—¶é—´ï¼Œåˆ†åˆ«åšå›å½’ï¼Œå‰”é™¤ç›¸å…³å› å­'''
        ols_order='fac~'+'+'.join(help_names)
        ols_result=smf.ols(ols_order,data=df).fit()
        params={i:ols_result.params[i] for i in help_names}
        predict=[params[i]*df[i] for i in help_names]
        predict=reduce(lambda x,y:x+y,predict)
        df.fac=df.fac-predict-ols_result.params['Intercept']
        df=df[['fac']]
        return df

    def mat_to_df(self,mat,use_datetime=True):
        '''å°†matæ–‡ä»¶å˜æˆ'''
        mat_path='/'.join([self.minute_files_path,mat])
        df=list(scio.loadmat(mat_path).values())[3]
        df=pd.DataFrame(df,columns=self.minute_columns)
        if use_datetime:
            df.date=pd.to_datetime(df.date.apply(str),format='%Y%m%d')
            df=df.set_index('date')
        return df

    def add_suffix(self,code):
        '''ç»™è‚¡ç¥¨ä»£ç åŠ ä¸Šåç¼€'''
        if not isinstance(code,str):
            code=str(code)
        if len(code)<6:
            code='0'*(6-len(code))+code
        if code.startswith('0') or code.startswith('3'):
            code='.'.join([code,'SZ'])
        elif code.startswith('6'):
            code='.'.join([code,'SH'])
        elif code.startswith('8'):
            code='.'.join([code,'BJ'])
        return code

    def minute_to_daily(self,func,add_priclose=False,add_tr=False,start_date=10000000,end_date=30000000,update=0):
        '''
        å°†åˆ†é’Ÿæ•°æ®å˜æˆæ—¥é¢‘å› å­ï¼Œå¹¶ä¸”æ·»åŠ åˆ°æ—¥é¢‘å› å­è¡¨é‡Œ
        é€šå¸¸åº”è¯¥æ¯å¤©ç”Ÿæˆä¸€ä¸ªæŒ‡æ ‡ï¼Œæœ€åä¸€åªè‚¡ç¥¨ä¼šç”Ÿæˆä¸€ä¸ªseries
        '''

        if add_priclose:
            for mat in tqdm.tqdm(self.minute_files):
                try:
                    code=self.add_suffix(mat[-10:-4])
                    self.code=code
                    df=self.mat_to_df(mat,use_datetime=True)
                    if add_tr:
                        share=read_daily('AllStock_DailyAShareNum.mat')
                        share_this=share[code].to_frame('sharenum').reset_index()
                        share_this.columns=['date','sharenum']
                        df=df.reset_index()
                        df.columns=['date']+list(df.columns)[1:]
                        df=pd.merge(df,share_this,on=['date'],how='left')
                        df=df.assign(tr=df.amount/df.sharenum)
                    df=df.reset_index()
                    df.columns=['date']+list(df.columns)[1:]
                    df.date=df.date.dt.strftime('%Y%m%d')
                    df.date=df.date.astype(int)
                    df=df[(df.date>=start_date)&(df.date<=end_date)]
                    # df.date=pd.to_datetime(df.date,format='%Y%m%d')
                    priclose=df.groupby('date').last()
                    priclose=priclose.shift(1).reset_index()
                    df=pd.concat([priclose,df])
                    the_func=partial(func)
                    date_sets=sorted(list(set(df.date)))
                    ress=[]
                    for i in range(len(date_sets)-19):
                        res=df[(df.date>=date_sets[i])&(df.date<=date_sets[i+19])]
                        res=the_func(res)
                        ress.append(res)
                    ress=pd.concat(ress)
                    if isinstance(ress,pd.DataFrame):
                        if 'date' in list(ress.columns):
                            ress=ress.set_index('date').iloc[:,0]
                        else:
                            ress=ress.iloc[:,0]
                    else:
                        ress=ress.to_frame(name=code)
                    if not update:
                        self.daily_factors_list.append(ress)
                    else:
                        self.daily_factors_list_update.append(ress)
                except Exception as e:
                    if not NO_LOG:
                        logger.warning(f'{code} ç¼ºå¤±')
                        logger.error(e)
        else:
            for mat in tqdm.tqdm(self.minute_files):
                try:
                    code=self.add_suffix(mat[-10:-4])
                    self.code=code
                    df=self.mat_to_df(mat,use_datetime=True)
                    if add_tr:
                        share=read_daily('AllStock_DailyAShareNum.mat')
                        share_this=share[code].to_frame('sharenum').reset_index()
                        share_this.columns=['date','sharenum']
                        df=df.reset_index()
                        df.columns=['date']+list(df.columns)[1:]
                        df=pd.merge(df,share_this,on=['date'],how='left')
                        df=df.assign(tr=df.amount/df.sharenum)
                    the_func=partial(func)
                    df=df.reset_index()
                    df.columns=['date']+list(df.columns)[1:]
                    df.date=df.date.dt.strftime('%Y%m%d')
                    df.date=df.date.astype(int)
                    df=df[(df.date>=start_date)&(df.date<=end_date)]
                    # df.date=pd.to_datetime(df.date,format='%Y%m%d')
                    date_sets=sorted(list(set(df.date)))
                    ress=[]
                    for i in range(len(date_sets)-19):
                        res=df[(df.date>=date_sets[i])&(df.date<=date_sets[i+19])]
                        res=the_func(res)
                        ress.append(res)
                    ress=pd.concat(ress)
                    if isinstance(ress,pd.DataFrame):
                        if 'date' in list(ress.columns):
                            ress=ress.set_index('date').iloc[:,0]
                        else:
                            ress=ress.iloc[:,0]
                    else:
                        ress=ress.to_frame(name=code)
                    if not update:
                        self.daily_factors_list.append(ress)
                    else:
                        self.daily_factors_list_update.append(ress)
                except Exception as e:
                    if not NO_LOG:
                        logger.warning(f'{code} ç¼ºå¤±')
                        logger.error(e)
        if update:
            self.daily_factors_update=pd.concat(self.daily_factors_list_update,axis=1)
            self.daily_factors_update.index=pd.to_datetime(self.daily_factors_update.index.astype(int),format='%Y%m%d')
            self.daily_factors=pd.concat([self.daily_factors,self.daily_factors_update])
        else:
            self.daily_factors=pd.concat(self.daily_factors_list,axis=1)
            self.daily_factors.index=pd.to_datetime(self.daily_factors.index.astype(int),format='%Y%m%d')
        self.daily_factors=self.daily_factors.dropna(how='all')
        self.daily_factors=self.daily_factors[self.daily_factors.index>=pd.Timestamp('2013-03-26')]
        self.daily_factors.reset_index().to_feather(self.daily_factors_path)
        if not NO_LOG:
            logger.success('æ›´æ–°å·²å®Œæˆ')

    def minute_to_daily_whole(self,func,start_date=10000000,end_date=30000000,update=0):
        '''
        å°†åˆ†é’Ÿæ•°æ®å˜æˆæ—¥é¢‘å› å­ï¼Œå¹¶ä¸”æ·»åŠ åˆ°æ—¥é¢‘å› å­è¡¨é‡Œ
        é€šå¸¸åº”è¯¥æ¯å¤©ç”Ÿæˆä¸€ä¸ªæŒ‡æ ‡ï¼Œæœ€åä¸€åªè‚¡ç¥¨ä¼šç”Ÿæˆä¸€ä¸ªseries
        '''
        for mat in tqdm.tqdm(self.minute_files):
            self.code=self.add_suffix(mat[-10:-4])
            df=self.mat_to_df(mat)
            df.date=df.date.astype(int)
            df=df[(df.date>=start_date)&(df.date<=end_date)]
            # df.date=pd.to_datetime(df.date,format='%Y%m%d')
            the_func=partial(func)
            df=func(df)
            if isinstance(df,pd.DataFrame):
                df.columns=[self.code]
                if not update:
                    self.daily_factors_list.append(df)
                else:
                    self.daily_factors_list_update.append(df)
            elif isinstance(df,pd.Series):
                df=df.to_frame(name=self.code)
                if not update:
                    self.daily_factors_list.append(df)
                else:
                    self.daily_factors_list_update.append(df)
            else:
                if not NO_LOG:
                    logger.warning(f'df is {df}')
        if update:
            self.daily_factors_update=pd.concat(self.daily_factors_list_update,axis=1)
            self.daily_factors=pd.concat([self.daily_factors,self.daily_factors_update])
        else:
            self.daily_factors=pd.concat(self.daily_factors_list,axis=1)
        self.daily_factors=self.daily_factors.dropna(how='all')
        self.daily_factors=self.daily_factors[self.daily_factors.index>=pd.Timestamp('2013-03-26')]
        self.daily_factors.reset_index().to_feather(self.daily_factors_path)
        if not NO_LOG:
            logger.success('æ›´æ–°å·²å®Œæˆ')

    def standardlize_in_cross_section(self,df):
        '''
        åœ¨æ¨ªæˆªé¢ä¸Šåšæ ‡å‡†åŒ–
        è¾“å…¥çš„dfåº”ä¸ºï¼Œåˆ—åæ˜¯è‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•æ˜¯æ—¶é—´
        '''
        df=df.T
        df=(df-df.mean())/df.std()
        df=df.T
        return df

    @kk.desktop_sender(title='å˜¿ï¼Œå¤šæ—¥åˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸ¬')
    def get_daily_factors(self,func,whole=False,add_priclose=False,add_tr=False,start_date=10000000,end_date=30000000):
        '''è°ƒç”¨åˆ†é’Ÿåˆ°æ—¥åº¦æ–¹æ³•ï¼Œç®—å‡ºæ—¥é¢‘æ•°æ®'''
        try:
            self.daily_factors=pd.read_feather(self.daily_factors_path)
            self.daily_factors=self.daily_factors.set_index('date')
            now_minute_data=self.mat_to_df(self.minute_files[0])
            if self.daily_factors.index.max()<now_minute_data.index.max():
                if not NO_LOG:
                    logger.info(
                        f'ä¸Šæ¬¡å­˜å‚¨çš„å› å­å€¼åˆ°{self.daily_factors.index.max()}ï¼Œè€Œåˆ†é’Ÿæ•°æ®æœ€æ–°åˆ°{now_minute_data.index.max()}ï¼Œå¼€å§‹æ›´æ–°â€¦â€¦')
                start_date_update=int(
                    datetime.datetime.strftime(self.daily_factors.index.max()+pd.Timedelta('1 day'),'%Y%m%d'))
                end_date_update=int(datetime.datetime.strftime(now_minute_data.index.max(),'%Y%m%d'))
                if whole:
                    self.minute_to_daily_whole(func,start_date=start_date_update,end_date=end_date_update,update=1)
                else:
                    self.minute_to_daily(func,start_date=start_date_update,end_date=end_date_update,update=1)
        except Exception:
            if whole:
                self.minute_to_daily_whole(func,start_date=start_date,end_date=end_date)
            else:
                self.minute_to_daily(func,add_priclose=add_priclose,add_tr=add_tr,start_date=start_date,
                                     end_date=end_date)

    def get_neutral_monthly_factors(self,df,boxcox=False):
        '''å¯¹æœˆåº¦å› å­åšå¸‚å€¼ä¸­æ€§åŒ–å¤„ç†'''
        shen=pure_moon()
        shen.set_factor_df_date_as_index(df)
        if boxcox:
            shen.run(5,boxcox=True,plt=False,print_comments=False)
        else:
            shen.run(5,neutralize=True,plt=False,print_comments=False)
        new_factors=shen.factors.copy()
        new_factors=new_factors.set_index(['date','code']).unstack()
        new_factors.columns=list(map(lambda x:x[1],list(new_factors.columns)))
        new_factors=new_factors.reset_index()
        add_start_point=new_factors.date.min()
        add_start_point=add_start_point-pd.Timedelta(days=add_start_point.day)
        new_factors.date=new_factors.date.shift(1)
        new_factors.date=new_factors.date.fillna(add_start_point)
        new_factors=new_factors.set_index('date')
        return new_factors

    def get_monthly_factors(self,func,neutralize,boxcox):
        '''å°†æ—¥é¢‘çš„å› å­è½¬åŒ–ä¸ºæœˆé¢‘å› å­'''
        two_parts=self.monthly_factors_path.split('.')
        try:
            self.monthly_factors=pd.read_feather(self.monthly_factors_path)
            self.monthly_factors=self.monthly_factors.set_index('date')
        except Exception:
            the_func=partial(func)
            self.monthly_factors=the_func(self.daily_factors)
            if neutralize:
                self.monthly_factors=self.get_neutral_monthly_factors(self.monthly_factors)
            elif boxcox:
                self.monthly_factors=self.get_neutral_monthly_factors(self.monthly_factors,boxcox=True)

            self.monthly_factors.reset_index().to_feather(self.monthly_factors_path)

    def run(self,whole=False,daily_func=None,monthly_func=None,
            neutralize=False,boxcox=False):
        '''æ‰§å¿…è¦çš„å‡½æ•°ï¼Œå°†åˆ†é’Ÿæ•°æ®å˜æˆæœˆåº¦å› å­'''
        self.get_daily_factors(daily_func,whole)
        self.get_monthly_factors(monthly_func,neutralize,boxcox)



class carry_me_to_bathroom():
    def __init__(
            self,
            minute_files_path=None,
            minute_columns=['date','open','high','low','close','amount','money'],
            daily_factors_path=None,
            monthly_factors_path=None
    ):
        self.homeplace=HomePlace()
        if monthly_factors_path:
            # åˆ†é’Ÿæ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
            self.minute_files_path=minute_files_path
        else:
            self.minute_files_path=self.homeplace.minute_data_file[:-1]
        # åˆ†é’Ÿæ•°æ®æ–‡ä»¶å¤¹
        self.minute_files=os.listdir(self.minute_files_path)
        self.minute_files=[i for i in self.minute_files if i.endswith('.mat')]
        self.minute_files=sorted(self.minute_files)
        # åˆ†é’Ÿæ•°æ®çš„è¡¨å¤´
        self.minute_columns=minute_columns
        # åˆ†é’Ÿæ•°æ®æ—¥é¢‘åŒ–ä¹‹åçš„æ•°æ®è¡¨
        self.daily_factors_list=[]
        #æ›´æ–°æ•°æ®ç”¨çš„åˆ—è¡¨
        self.daily_factors_list_update=[]
        # å°†åˆ†é’Ÿæ•°æ®æ‹¼æˆä¸€å¼ æ—¥é¢‘å› å­è¡¨
        self.daily_factors=None
        # æœ€ç»ˆæœˆåº¦å› å­è¡¨æ ¼
        self.monthly_factors=None
        if daily_factors_path:
            # æ—¥é¢‘å› å­æ–‡ä»¶ä¿å­˜è·¯å¾„
            self.daily_factors_path=daily_factors_path
        else:
            self.daily_factors_path=self.homeplace.factor_data_file+'æ—¥é¢‘_'
        if monthly_factors_path:
            # æœˆé¢‘å› å­æ–‡ä»¶ä¿å­˜è·¯å¾„
            self.monthly_factors_path=monthly_factors_path
        else:
            self.monthly_factors_path=self.homeplace.factor_data_file+'æœˆé¢‘_'

    def __call__(self,monthly=False):
        '''ä¸ºäº†é˜²æ­¢å±æ€§åå¤ªå¤šï¼Œå¿˜è®°äº†è¦è°ƒç”¨å“ªä¸ªæ‰æ˜¯ç»“æœï¼Œå› æ­¤å¯ä»¥ç›´æ¥è¾“å‡ºæœˆåº¦æ•°æ®è¡¨'''
        if monthly:
            return self.monthly_factors.copy()
        else:
            try:
                return self.daily_factors.copy()
            except Exception:
                return self.monthly_factors.copy()

    def __add__(self,selfas):
        '''å°†å‡ ä¸ªå› å­æˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œå› å­å€¼ç›¸åŠ '''
        fac1=self.standardlize_in_cross_section(self.monthly_factors)
        fac2s=[]
        if not isinstance(selfas,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfas} is changed into Iterable')
            selfas=(selfas,)
        for selfa in selfas:
            fac2=self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2s.append(fac2)
        for i in fac2s:
            fac1=fac1+i
        new_pure=pure_fall()
        new_pure.monthly_factors=fac1
        return new_pure

    def __mul__(self,selfas):
        '''å°†å‡ ä¸ªå› å­æ¨ªæˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œä½¿å…¶éƒ½ä¸ºæ­£æ•°ï¼Œç„¶åå› å­å€¼ç›¸ä¹˜'''
        fac1=self.standardlize_in_cross_section(self.monthly_factors)
        fac1=fac1-fac1.min()
        fac2s=[]
        if not isinstance(selfas,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfas} is changed into Iterable')
            selfas=(selfas,)
        for selfa in selfas:
            fac2=self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2=fac2-fac2.min()
            fac2s.append(fac2)
        for i in fac2s:
            fac1=fac1*i
        new_pure=pure_fall()
        new_pure.monthly_factors=fac1
        return new_pure

    def __truediv__(self,selfa):
        '''ä¸¤ä¸ªä¸€æ­£ä¸€å‰¯çš„å› å­ï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸å‡'''
        fac1=self.standardlize_in_cross_section(self.monthly_factors)
        fac2=self.standardlize_in_cross_section(selfa.monthly_factors)
        fac=fac1-fac2
        new_pure=pure_fall()
        new_pure.monthly_factors=fac
        return new_pure

    def __floordiv__(self,selfa):
        '''ä¸¤ä¸ªå› å­ä¸€æ­£ä¸€è´Ÿï¼Œå¯ä»¥ç”¨æ­¤æ–¹æ³•ç›¸é™¤'''
        fac1=self.standardlize_in_cross_section(self.monthly_factors)
        fac2=self.standardlize_in_cross_section(selfa.monthly_factors)
        fac1=fac1-fac1.min()
        fac2=fac2-fac2.min()
        fac=fac1/fac2
        fac=fac.replace(np.inf,np.nan)
        new_pure=pure_fall()
        new_pure.monthly_factors=fac
        return new_pure

    def __sub__(self,selfa):
        '''ç”¨ä¸»å› å­å‰”é™¤å…¶ä»–ç›¸å…³å› å­ã€ä¼ ç»Ÿå› å­ç­‰
        selfaå¯ä»¥ä¸ºå¤šä¸ªå› å­å¯¹è±¡ç»„æˆçš„å…ƒç»„æˆ–åˆ—è¡¨ï¼Œæ¯ä¸ªè¾…åŠ©å› å­åªéœ€è¦æœ‰æœˆåº¦å› å­æ–‡ä»¶è·¯å¾„å³å¯'''
        tqdm.tqdm.pandas()
        if not isinstance(selfa,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfa} is changed into Iterable')
            selfa=(selfa,)
        fac_main=self.wide_to_long(self.monthly_factors,'fac')
        fac_helps=[i.monthly_factors for i in selfa]
        help_names=['help'+str(i) for i in range(1,(len(fac_helps)+1))]
        fac_helps=list(map(self.wide_to_long,fac_helps,help_names))
        fac_helps=pd.concat(fac_helps,axis=1)
        facs=pd.concat([fac_main,fac_helps],axis=1).dropna()
        facs=facs.groupby('date').progress_apply(lambda x:self.de_in_group(x,help_names))
        facs=facs.unstack()
        facs.columns=list(map(lambda x:x[1],list(facs.columns)))
        return facs

    def __gt__(self,selfa):
        '''ç”¨äºè¾“å‡º25åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­'''
        x=self.monthly_factors.copy()
        y=selfa.monthly_factors.copy()
        x=x.stack().reset_index()
        y=y.stack().reset_index()
        x.columns=['date','code','fac']
        y.columns=['date','code','fac']
        shen=pure_moon()
        x=x.groupby('date').apply(lambda df:shen.get_groups(df,5))
        x=x.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupx'})
        xy=pd.merge(x,y,on=['date','code'])
        xy=xy.groupby(['date','groupx']).apply(lambda df:shen.get_groups(df,5))
        xy=xy.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupy'})
        xy=xy.assign(fac=xy.groupx*5+xy.groupy)
        xy=xy[['date','code','fac']]
        xy=xy.set_index(['date','code']).unstack()
        xy.columns=[i[1] for i in list(xy.columns)]
        new_pure=pure_fall()
        new_pure.monthly_factors=xy
        return new_pure

    def __rshift__(self,selfa):
        '''ç”¨äºè¾“å‡º100åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­'''
        x=self.monthly_factors.copy()
        y=selfa.monthly_factors.copy()
        x=x.stack().reset_index()
        y=y.stack().reset_index()
        x.columns=['date','code','fac']
        y.columns=['date','code','fac']
        shen=pure_moon()
        x=x.groupby('date').apply(lambda df:shen.get_groups(df,10))
        x=x.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupx'})
        xy=pd.merge(x,y,on=['date','code'])
        xy=xy.groupby(['date','groupx']).apply(lambda df:shen.get_groups(df,10))
        xy=xy.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupy'})
        xy=xy.assign(fac=xy.groupx*10+xy.groupy)
        xy=xy[['date','code','fac']]
        xy=xy.set_index(['date','code']).unstack()
        xy.columns=[i[1] for i in list(xy.columns)]
        new_pure=pure_fall()
        new_pure.monthly_factors=xy
        return new_pure

    def wide_to_long(self,df,i):
        '''å°†å®½æ•°æ®è½¬åŒ–ä¸ºé•¿æ•°æ®ï¼Œç”¨äºå› å­è¡¨è½¬åŒ–å’Œæ‹¼æ¥'''
        df=df.stack().reset_index()
        df.columns=['date','code',i]
        df=df.set_index(['date','code'])
        return df

    def de_in_group(self,df,help_names):
        '''å¯¹æ¯ä¸ªæ—¶é—´ï¼Œåˆ†åˆ«åšå›å½’ï¼Œå‰”é™¤ç›¸å…³å› å­'''
        ols_order='fac~'+'+'.join(help_names)
        ols_result=smf.ols(ols_order,data=df).fit()
        params={i:ols_result.params[i] for i in help_names}
        predict=[params[i]*df[i] for i in help_names]
        predict=reduce(lambda x,y:x+y,predict)
        df.fac=df.fac-predict-ols_result.params['Intercept']
        df=df[['fac']]
        return df

    def mat_to_df(self,mat,use_datetime=True):
        '''å°†matæ–‡ä»¶å˜æˆ'''
        mat_path='/'.join([self.minute_files_path,mat])
        df=list(scio.loadmat(mat_path).values())[3]
        df=pd.DataFrame(df,columns=self.minute_columns)
        if use_datetime:
            df.date=pd.to_datetime(df.date.apply(str),format='%Y%m%d')
            df=df.set_index('date')
        return df

    def add_suffix(self,code):
        '''ç»™è‚¡ç¥¨ä»£ç åŠ ä¸Šåç¼€'''
        if not isinstance(code,str):
            code=str(code)
        if len(code)<6:
            code='0'*(6-len(code))+code
        if code.startswith('0') or code.startswith('3'):
            code='.'.join([code,'SZ'])
        elif code.startswith('6'):
            code='.'.join([code,'SH'])
        elif code.startswith('8'):
            code='.'.join([code,'BJ'])
        return code

    def minute_to_daily(self,func,add_priclose=False,add_tr=False,start_date=10000000,end_date=30000000,update=0):
        '''
        å°†åˆ†é’Ÿæ•°æ®å˜æˆæ—¥é¢‘å› å­ï¼Œå¹¶ä¸”æ·»åŠ åˆ°æ—¥é¢‘å› å­è¡¨é‡Œ
        é€šå¸¸åº”è¯¥æ¯å¤©ç”Ÿæˆä¸€ä¸ªæŒ‡æ ‡ï¼Œæœ€åä¸€åªè‚¡ç¥¨ä¼šç”Ÿæˆä¸€ä¸ªseries
        '''

        if add_priclose:
            for mat in tqdm.tqdm(self.minute_files):
                code=self.add_suffix(mat[-10:-4])
                self.code=code
                df=self.mat_to_df(mat,use_datetime=True)
                if add_tr:
                    share=read_daily('AllStock_DailyAShareNum.mat')
                    share_this=share[code].to_frame('sharenum').reset_index()
                    share_this.columns=['date','sharenum']
                    df=df.reset_index()
                    df.columns=['date']+list(df.columns)[1:]
                    df=pd.merge(df,share_this,on=['date'],how='left')
                    df=df.assign(tr=df.amount/df.sharenum)
                df=df.reset_index()
                df.columns=['date']+list(df.columns)[1:]
                df.date=df.date.dt.strftime('%Y%m%d')
                df.date=df.date.astype(int)
                df=df[(df.date>=start_date)&(df.date<=end_date)]
                # df.date=pd.to_datetime(df.date,format='%Y%m%d')
                priclose=df.groupby('date').last()
                priclose=priclose.shift(1).reset_index()
                df=pd.concat([priclose,df])
                the_func=partial(func)
                date_sets=sorted(list(set(df.date)))
                ress=[]
                for i in range(len(date_sets)-19):
                    res=df[(df.date>=date_sets[i])&(df.date<=date_sets[i+19])]
                    res=the_func(res)
                    ress.append(res)
                ress=pd.concat(ress)
                if isinstance(ress,pd.DataFrame):
                    if 'date' in list(ress.columns):
                        ress=ress.set_index('date').iloc[:,0]
                    else:
                        ress=ress.iloc[:,0]
                else:
                    ress=ress.to_frame(name=code)
                if not update:
                    self.daily_factors_list.append(ress)
                else:
                    self.daily_factors_list_update.append(ress)
        else:
            for mat in tqdm.tqdm(self.minute_files):
                code=self.add_suffix(mat[-10:-4])
                self.code=code
                df=self.mat_to_df(mat,use_datetime=True)
                if add_tr:
                    share=read_daily('AllStock_DailyAShareNum.mat')
                    share_this=share[code].to_frame('sharenum').reset_index()
                    share_this.columns=['date','sharenum']
                    df=df.reset_index()
                    df.columns=['date']+list(df.columns)[1:]
                    df=pd.merge(df,share_this,on=['date'],how='left')
                    df=df.assign(tr=df.amount/df.sharenum)
                the_func=partial(func)
                df=df.reset_index()
                df.columns=['date']+list(df.columns)[1:]
                df.date=df.date.dt.strftime('%Y%m%d')
                df.date=df.date.astype(int)
                df=df[(df.date>=start_date)&(df.date<=end_date)]
                # df.date=pd.to_datetime(df.date,format='%Y%m%d')
                date_sets=sorted(list(set(df.date)))
                ress=[]
                for i in range(len(date_sets)-19):
                    res=df[(df.date>=date_sets[i])&(df.date<=date_sets[i+19])]
                    res=the_func(res)
                    ress.append(res)
                ress=pd.concat(ress)
                if isinstance(ress,pd.DataFrame):
                    if 'date' in list(ress.columns):
                        ress=ress.set_index('date').iloc[:,0]
                    else:
                        ress=ress.iloc[:,0]
                else:
                    ress=ress.to_frame(name=code)
                if not update:
                    self.daily_factors_list.append(ress)
                else:
                    self.daily_factors_list_update.append(ress)
        if update:
            self.daily_factors_update=pd.concat(self.daily_factors_list_update,axis=1)
            self.daily_factors_update.index=pd.to_datetime(self.daily_factors_update.index.astype(int),format='%Y%m%d')
            self.daily_factors=pd.concat([self.daily_factors,self.daily_factors_update])
        else:
            self.daily_factors=pd.concat(self.daily_factors_list,axis=1)
            self.daily_factors.index=pd.to_datetime(self.daily_factors.index.astype(int),format='%Y%m%d')
        self.daily_factors=self.daily_factors.dropna(how='all')
        self.daily_factors=self.daily_factors[self.daily_factors.index>=pd.Timestamp('2013-03-26')]
        self.daily_factors.reset_index().to_feather(self.daily_factors_path)
        if not NO_LOG:
            logger.success('æ›´æ–°å·²å®Œæˆ')

    def minute_to_daily_whole(self,func,start_date=10000000,end_date=30000000,update=0):
        '''
        å°†åˆ†é’Ÿæ•°æ®å˜æˆæ—¥é¢‘å› å­ï¼Œå¹¶ä¸”æ·»åŠ åˆ°æ—¥é¢‘å› å­è¡¨é‡Œ
        é€šå¸¸åº”è¯¥æ¯å¤©ç”Ÿæˆä¸€ä¸ªæŒ‡æ ‡ï¼Œæœ€åä¸€åªè‚¡ç¥¨ä¼šç”Ÿæˆä¸€ä¸ªseries
        '''
        for mat in tqdm.tqdm(self.minute_files):
            self.code=self.add_suffix(mat[-10:-4])
            df=self.mat_to_df(mat)
            df.date=df.date.astype(int)
            df=df[(df.date>=start_date)&(df.date<=end_date)]
            # df.date=pd.to_datetime(df.date,format='%Y%m%d')
            the_func=partial(func)
            df=func(df)
            if isinstance(df,pd.DataFrame):
                df.columns=[self.code]
                if not update:
                    self.daily_factors_list.append(df)
                else:
                    self.daily_factors_list_update.append(df)
            elif isinstance(df,pd.Series):
                df=df.to_frame(name=self.code)
                if not update:
                    self.daily_factors_list.append(df)
                else:
                    self.daily_factors_list_update.append(df)
            else:
                if not NO_LOG:
                    logger.warning(f'df is {df}')
        if update:
            self.daily_factors_update=pd.concat(self.daily_factors_list_update,axis=1)
            self.daily_factors=pd.concat([self.daily_factors,self.daily_factors_update])
        else:
            self.daily_factors=pd.concat(self.daily_factors_list,axis=1)
        self.daily_factors=self.daily_factors.dropna(how='all')
        self.daily_factors=self.daily_factors[self.daily_factors.index>=pd.Timestamp('2013-03-26')]
        self.daily_factors.reset_index().to_feather(self.daily_factors_path)
        if not NO_LOG:
            logger.success('æ›´æ–°å·²å®Œæˆ')

    def standardlize_in_cross_section(self,df):
        '''
        åœ¨æ¨ªæˆªé¢ä¸Šåšæ ‡å‡†åŒ–
        è¾“å…¥çš„dfåº”ä¸ºï¼Œåˆ—åæ˜¯è‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•æ˜¯æ—¶é—´
        '''
        df=df.T
        df=(df-df.mean())/df.std()
        df=df.T
        return df

    def get_daily_factors(self,func,whole=False,add_priclose=False,add_tr=False,start_date=10000000,end_date=30000000):
        '''è°ƒç”¨åˆ†é’Ÿåˆ°æ—¥åº¦æ–¹æ³•ï¼Œç®—å‡ºæ—¥é¢‘æ•°æ®'''
        try:
            self.daily_factors=pd.read_feather(self.daily_factors_path)
            self.daily_factors=self.daily_factors.set_index('date')
            now_minute_data=self.mat_to_df(self.minute_files[0])
            if self.daily_factors.index.max()<now_minute_data.index.max():
                if not NO_LOG:
                    logger.info(
                        f'ä¸Šæ¬¡å­˜å‚¨çš„å› å­å€¼åˆ°{self.daily_factors.index.max()}ï¼Œè€Œåˆ†é’Ÿæ•°æ®æœ€æ–°åˆ°{now_minute_data.index.max()}ï¼Œå¼€å§‹æ›´æ–°â€¦â€¦')
                start_date_update=int(
                    datetime.datetime.strftime(self.daily_factors.index.max()+pd.Timedelta('1 day'),'%Y%m%d'))
                end_date_update=int(datetime.datetime.strftime(now_minute_data.index.max(),'%Y%m%d'))
                if whole:
                    self.minute_to_daily_whole(func,start_date=start_date_update,end_date=end_date_update,update=1)
                else:
                    self.minute_to_daily(func,start_date=start_date_update,end_date=end_date_update,update=1)
        except Exception:
            if whole:
                self.minute_to_daily_whole(func,start_date=start_date,end_date=end_date)
            else:
                self.minute_to_daily(func,add_priclose=add_priclose,add_tr=add_tr,start_date=start_date,
                                     end_date=end_date)

    def get_neutral_monthly_factors(self,df,boxcox=False):
        '''å¯¹æœˆåº¦å› å­åšå¸‚å€¼ä¸­æ€§åŒ–å¤„ç†'''
        shen=pure_moon()
        shen.set_factor_df_date_as_index(df)
        if boxcox:
            shen.run(5,boxcox=True,plt=False,print_comments=False)
        else:
            shen.run(5,neutralize=True,plt=False,print_comments=False)
        new_factors=shen.factors.copy()
        new_factors=new_factors.set_index(['date','code']).unstack()
        new_factors.columns=list(map(lambda x:x[1],list(new_factors.columns)))
        new_factors=new_factors.reset_index()
        add_start_point=new_factors.date.min()
        add_start_point=add_start_point-pd.Timedelta(days=add_start_point.day)
        new_factors.date=new_factors.date.shift(1)
        new_factors.date=new_factors.date.fillna(add_start_point)
        new_factors=new_factors.set_index('date')
        return new_factors

    def get_monthly_factors(self,func,neutralize,boxcox):
        '''å°†æ—¥é¢‘çš„å› å­è½¬åŒ–ä¸ºæœˆé¢‘å› å­'''
        two_parts=self.monthly_factors_path.split('.')
        try:
            self.monthly_factors=pd.read_feather(self.monthly_factors_path)
            self.monthly_factors=self.monthly_factors.set_index('date')
        except Exception:
            the_func=partial(func)
            self.monthly_factors=the_func(self.daily_factors)
            if neutralize:
                self.monthly_factors=self.get_neutral_monthly_factors(self.monthly_factors)
            elif boxcox:
                self.monthly_factors=self.get_neutral_monthly_factors(self.monthly_factors,boxcox=True)

            self.monthly_factors.reset_index().to_feather(self.monthly_factors_path)

    def run(self,whole=False,daily_func=None,monthly_func=None,
            neutralize=False,boxcox=False):
        '''æ‰§å¿…è¦çš„å‡½æ•°ï¼Œå°†åˆ†é’Ÿæ•°æ®å˜æˆæœˆåº¦å› å­'''
        self.get_daily_factors(daily_func,whole)
        self.get_monthly_factors(monthly_func,neutralize,boxcox)


class pure_fallmount(pure_fall):
    '''ç»§æ‰¿è‡ªçˆ¶ç±»ï¼Œä¸“ä¸ºåšå› å­æˆªé¢æ ‡å‡†åŒ–ä¹‹åç›¸åŠ å’Œå› å­å‰”é™¤å…¶ä»–è¾…åŠ©å› å­çš„ä½œç”¨'''
    def __init__(self, monthly_factors):
        '''è¾“å…¥æœˆåº¦å› å­å€¼ï¼Œä»¥è®¾å®šæ–°çš„å¯¹è±¡'''
        super(pure_fall, self).__init__()
        self.monthly_factors = monthly_factors

    def __call__(self,monthly=False):
        '''ä¸ºäº†é˜²æ­¢å±æ€§åå¤ªå¤šï¼Œå¿˜è®°äº†è¦è°ƒç”¨å“ªä¸ªæ‰æ˜¯ç»“æœï¼Œå› æ­¤å¯ä»¥ç›´æ¥è¾“å‡ºæœˆåº¦æ•°æ®è¡¨'''
        if monthly:
            return self.monthly_factors.copy()
        else:
            try:
                return self.daily_factors.copy()
            except Exception:
                return self.monthly_factors.copy()

    def __add__(self, selfas):
        '''è¿”å›ä¸€ä¸ªå¯¹è±¡ï¼Œè€Œéä¸€ä¸ªè¡¨æ ¼ï¼Œå¦‚éœ€è¡¨æ ¼è¯·è°ƒç”¨å¯¹è±¡'''
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac2s = []
        if not isinstance(selfas,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfas} is changed into Iterable')
            selfas=(selfas,)
        for selfa in selfas:
            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2s.append(fac2)
        for i in fac2s:
            fac1 = fac1 + i
        new_pure = pure_fallmount(fac1)
        return new_pure

    def __mul__(self,selfas):
        '''å°†å‡ ä¸ªå› å­æ¨ªæˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œä½¿å…¶éƒ½ä¸ºæ­£æ•°ï¼Œç„¶åå› å­å€¼ç›¸ä¹˜'''
        fac1=self.standardlize_in_cross_section(self.monthly_factors)
        fac1=fac1-fac1.min()
        fac2s=[]
        if not isinstance(selfas,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfas} is changed into Iterable')
            selfas=(selfas,)
        for selfa in selfas:
            fac2=self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2=fac2-fac2.min()
            fac2s.append(fac2)
        for i in fac2s:
            fac1=fac1*i
        new_pure=pure_fall()
        new_pure.monthly_factors=fac1
        return new_pure

    def __sub__(self, selfa):
        '''è¿”å›å¯¹è±¡ï¼Œå¦‚éœ€è¡¨æ ¼ï¼Œè¯·è°ƒç”¨å¯¹è±¡'''
        tqdm.tqdm.pandas()
        if not isinstance(selfa,Iterable):
            if not NO_LOG:
                logger.warning(f'{selfa} is changed into Iterable')
            selfa=(selfa,)
        fac_main = self.wide_to_long(self.monthly_factors, 'fac')
        fac_helps = [i.monthly_factors for i in selfa]
        help_names = ['help' + str(i) for i in range(1, (len(fac_helps) + 1))]
        fac_helps = list(map(self.wide_to_long, fac_helps, help_names))
        fac_helps = pd.concat(fac_helps, axis=1)
        facs = pd.concat([fac_main, fac_helps], axis=1).dropna()
        facs = facs.groupby('date').progress_apply(lambda x: self.de_in_group(x, help_names))
        facs = facs.unstack()
        facs.columns = list(map(lambda x: x[1], list(facs.columns)))
        new_pure = pure_fallmount(facs)
        return new_pure

    def __gt__(self,selfa):
        '''ç”¨äºè¾“å‡º25åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­'''
        x=self.monthly_factors.copy()
        y=selfa.monthly_factors.copy()
        x=x.stack().reset_index()
        y=y.stack().reset_index()
        x.columns=['date','code','fac']
        y.columns=['date','code','fac']
        shen=pure_moon()
        x=x.groupby('date').apply(lambda df:shen.get_groups(df,5))
        x=x.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupx'})
        xy=pd.merge(x,y,on=['date','code'])
        xy=xy.groupby(['date','groupx']).apply(lambda df:shen.get_groups(df,5))
        xy=xy.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupy'})
        xy=xy.assign(fac=xy.groupx*5+xy.groupy)
        xy=xy[['date','code','fac']]
        xy=xy.set_index(['date','code']).unstack()
        xy.columns=[i[1] for i in list(xy.columns)]
        new_pure=pure_fallmount(xy)
        return new_pure

    def __rshift__(self, selfa):
        '''ç”¨äºè¾“å‡º100åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­'''
        x=self.monthly_factors.copy()
        y=selfa.monthly_factors.copy()
        x=x.stack().reset_index()
        y=y.stack().reset_index()
        x.columns=['date','code','fac']
        y.columns=['date','code','fac']
        shen=pure_moon()
        x=x.groupby('date').apply(lambda df:shen.get_groups(df,10))
        x=x.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupx'})
        xy=pd.merge(x,y,on=['date','code'])
        xy=xy.groupby(['date','groupx']).apply(lambda df:shen.get_groups(df,10))
        xy=xy.reset_index(drop=True).drop(columns=['fac']).rename(columns={'group':'groupy'})
        xy=xy.assign(fac=xy.groupx*10+xy.groupy)
        xy=xy[['date','code','fac']]
        xy=xy.set_index(['date','code']).unstack()
        xy.columns=[i[1] for i in list(xy.columns)]
        new_pure=pure_fallmount(xy)
        return new_pure

class pure_winter():
    def __init__(self):
        self.homeplace=HomePlace()
        # barraå› å­æ•°æ®
        self.barras = self.read_h5(self.homeplace.barra_data_file+'FactorLoading_Style.h5')

    def __call__(self,fallmount=0):
        '''è¿”å›çº¯å‡€å› å­å€¼'''
        if fallmount==0:
            return self.snow_fac
        else:
            return pure_fallmount(self.snow_fac)

    def read_h5(self,path):
        '''è¯»å…¥h5æ–‡ä»¶'''
        res={}
        a=h5py.File(path)
        for k,v in tqdm.tqdm(list(a.items()),desc='æ•°æ®åŠ è½½ä¸­â€¦â€¦'):
            value=list(v.values())[-1]
            col=[i.decode('utf-8') for i in list(list(v.values())[0])]
            ind=[i.decode('utf-8') for i in list(list(v.values())[1])]
            res[k]=pd.DataFrame(value,columns=col,index=ind)
        return res

    @history_remain(slogan='abandoned')
    def last_month_end(self, x):
        '''æ‰¾åˆ°ä¸‹ä¸ªæœˆæœ€åä¸€å¤©'''
        x1 = x = x - relativedelta(months=1)
        while x1.month == x.month:
            x1 = x1 + relativedelta(days=1)
        return x1 - relativedelta(days=1)

    @history_remain(slogan='abandoned')
    def set_factors_df(self, df):
        '''ä¼ å…¥å› å­dataframeï¼Œåº”ä¸ºä¸‰åˆ—ï¼Œç¬¬ä¸€åˆ—æ˜¯æ—¶é—´ï¼Œç¬¬äºŒåˆ—æ˜¯è‚¡ç¥¨ä»£ç ï¼Œç¬¬ä¸‰åˆ—æ˜¯å› å­å€¼'''
        df1=df.copy()
        df1.columns = ['date', 'code', 'fac']
        df1 = df1.set_index(['date', 'code'])
        df1 = df1.unstack().reset_index()
        df1.date = df1.date.apply(self.last_month_end)
        df1 = df1.set_index(['date']).stack()
        self.factors = df1.copy()

    def set_factors_df_wide(self, df):
        '''ä¼ å…¥å› å­æ•°æ®ï¼Œæ—¶é—´ä¸ºç´¢å¼•ï¼Œä»£ç ä¸ºåˆ—å'''
        df1=df.copy()
        # df1.index=df1.index-pd.DateOffset(months=1)
        df1=df1.resample('M').last()
        df1=df1.stack().reset_index()
        df1.columns = ['date', 'code','fac']
        self.factors=df1.copy()

    def daily_to_monthly(self, df):
        '''å°†æ—¥åº¦çš„barraå› å­æœˆåº¦åŒ–'''
        df.index = pd.to_datetime(df.index, format='%Y%m%d')
        df = df.resample('M').last()
        return df

    def get_monthly_barras_industrys(self):
        '''å°†barraå› å­å’Œè¡Œä¸šå“‘å˜é‡å˜æˆæœˆåº¦æ•°æ®'''
        for key, value in self.barras.items():
            self.barras[key] = self.daily_to_monthly(value)

    def wide_to_long(self, df, name):
        '''å°†å®½æ•°æ®å˜æˆé•¿æ•°æ®ï¼Œä¾¿äºåç»­æ‹¼æ¥'''
        df = df.stack().reset_index()
        df.columns = ['date', 'code', name]
        df = df.set_index(['date', 'code'])
        return df

    def get_wide_barras_industrys(self):
        '''å°†barraå› å­å’Œè¡Œä¸šå“‘å˜é‡éƒ½å˜æˆé•¿æ•°æ®'''
        for key, value in self.barras.items():
            self.barras[key] = self.wide_to_long(value, key)

    def get_corr_pri_ols_pri(self):
        '''æ‹¼æ¥barraå› å­å’Œè¡Œä¸šå“‘å˜é‡ï¼Œç”Ÿæˆç”¨äºæ±‚ç›¸å…³ç³»æ•°å’Œçº¯å‡€å› å­çš„æ•°æ®è¡¨'''
        if self.factors.shape[0]>1:
            self.factors=self.factors.set_index(['date', 'code'])
        self.corr_pri = pd.concat([self.factors] + list(self.barras.values()), axis=1).dropna()

    def get_corr(self):
        '''è®¡ç®—æ¯ä¸€æœŸçš„ç›¸å…³ç³»æ•°ï¼Œå†æ±‚å¹³å‡å€¼'''
        self.corr_by_step = self.corr_pri.groupby(['date']).apply(lambda x: x.corr().head(1))
        self.__corr = self.corr_by_step.mean()
        self.__corr.index=['å› å­è‡ªèº«','è´å¡”','ä¼°å€¼','æ æ†',
                         'ç›ˆåˆ©','æˆé•¿','æµåŠ¨æ€§','åè½¬','æ³¢åŠ¨ç‡',
                         'å¸‚å€¼','éçº¿æ€§å¸‚å€¼']

    @property
    def corr(self):
        return self.__corr.copy()

    def ols_in_group(self, df):
        '''å¯¹æ¯ä¸ªæ—¶é—´æ®µè¿›è¡Œå›å½’ï¼Œå¹¶è®¡ç®—æ®‹å·®'''
        xs = list(df.columns)
        xs = [i for i in xs if i != 'fac']
        xs_join = '+'.join(xs)
        ols_formula = 'fac~' + xs_join
        ols_result = smf.ols(ols_formula, data=df).fit()
        ols_ws = {i: ols_result.params[i] for i in xs}
        ols_b = ols_result.params['Intercept']
        to_minus = [ols_ws[i] * df[i] for i in xs]
        to_minus = reduce(lambda x, y: x + y, to_minus)
        df = df.assign(snow_fac=df.fac - to_minus - ols_b)
        df = df[['snow_fac']]
        df = df.rename(columns={'snow_fac': 'fac'})
        return df

    def get_snow_fac(self):
        '''è·å¾—çº¯å‡€å› å­'''
        self.snow_fac = self.corr_pri.groupby(['date']).apply(self.ols_in_group)
        self.snow_fac = self.snow_fac.unstack()
        self.snow_fac.columns = list(map(lambda x: x[1], list(self.snow_fac.columns)))

    def run(self):
        '''è¿è¡Œä¸€äº›å¿…è¦çš„å‡½æ•°'''
        self.get_monthly_barras_industrys()
        self.get_wide_barras_industrys()
        self.get_corr_pri_ols_pri()
        self.get_corr()
        self.get_snow_fac()


class pure_snowtrain(pure_winter):
    '''ç›´æ¥è¿”å›çº¯å‡€å› å­'''

    def __init__(self, factors):
        '''ç›´æ¥è¾“å…¥åŸå§‹å› å­æ•°æ®'''
        super(pure_snowtrain, self).__init__()
        self.set_factors_df_wide(factors.copy())
        self.run()

    def __call__(self, fallmount=0):
        '''å¯ä»¥ç›´æ¥è¿”å›pure_fallmountå¯¹è±¡ï¼Œæˆ–çº¯å‡€å› å­çŸ©é˜µ'''
        if fallmount == 0:
            return self.snow_fac
        else:
            return pure_fallmount(self.snow_fac)


class pure_moonlight(pure_moon):
    '''ç»§æ‰¿è‡ªpure_moonå›æµ‹æ¡†æ¶ï¼Œä½¿ç”¨å…¶ä¸­çš„æ—¥é¢‘å¤æƒæ”¶ç›˜ä»·æ•°æ®ï¼Œä»¥åŠæ¢æ‰‹ç‡æ•°æ®'''

    def __init__(self):
        '''åŠ è½½å…¨éƒ¨æ•°æ®'''
        super(pure_moonlight, self).__init__()
        self.homeplace=HomePlace()
        self.col_and_index()
        self.load_all_files()
        self.judge_month()
        self.get_log_cap()
        # å¯¹æ•°å¸‚å€¼
        self.cap_as_factor = self.cap[['date', 'code', 'cap_size']].set_index(['date', 'code']).unstack()
        self.cap_as_factor.columns = list(map(lambda x: x[1], list(self.cap_as_factor.columns)))
        # ä¼ ç»Ÿåè½¬å› å­ret20
        self.ret20_database = self.homeplace.factor_data_file+'æœˆé¢‘_åè½¬å› å­ret20.feather'
        # ä¼ ç»Ÿæ¢æ‰‹ç‡å› å­turn20
        self.turn20_database = self.homeplace.factor_data_file+'æœˆé¢‘_æ¢æ‰‹ç‡å› å­turn20.feather'
        # ä¼ ç»Ÿæ³¢åŠ¨ç‡å› å­vol20
        self.vol20_database = self.homeplace.factor_data_file+'æœˆé¢‘_æ³¢åŠ¨ç‡å› å­vol20.feather'
        # #è‡ªåŠ¨æ›´æ–°
        self.get_updated_factors()

    def __call__(self, name):
        '''å¯ä»¥é€šè¿‡callæ–¹å¼ï¼Œç›´æ¥è·å–å¯¹åº”å› å­æ•°æ®'''
        value = getattr(self, name)
        return value

    def get_ret20(self, pri):
        '''è®¡ç®—20æ—¥æ¶¨è·Œå¹…å› å­'''
        past = pri.iloc[:-20, :]
        future = pri.iloc[20:, :]
        ret20 = (future.to_numpy() - past.to_numpy()) / past.to_numpy()
        df = pd.DataFrame(ret20, columns=pri.columns, index=future.index)
        df = df.resample('M').last()
        return df

    def get_turn20(self, pri):
        '''è®¡ç®—20æ¢æ‰‹ç‡å› å­'''
        turns = pri.rolling(20).mean()
        turns = turns.resample('M').last().reset_index()
        turns.columns = ['date'] + list(turns.columns)[1:]
        self.factors = turns
        self.get_neutral_factors()
        df = self.factors.copy()
        df = df.set_index(['date', 'code'])
        df = df.unstack()
        df.columns = list(map(lambda x: x[1], list(df.columns)))
        return df

    def get_vol20(self, pri):
        '''è®¡ç®—20æ—¥æ³¢åŠ¨ç‡å› å­'''
        rets = pri.pct_change()
        vol = rets.rolling(20).apply(np.std)
        df = vol.resample('M').last()
        return df

    def update_single_factor_in_database(self, path, pri, func):
        '''
        ç”¨åŸºç¡€æ•°æ®åº“æ›´æ–°å› å­æ•°æ®åº“
        æ‰§è¡Œé¡ºåºä¸ºï¼Œå…ˆè¯»å–æ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰ï¼Œå°±ç›´æ¥å…¨éƒ¨è®¡ç®—ï¼Œç„¶åå­˜å‚¨
        å¦‚æœæœ‰ï¼Œå°±è¯»å‡ºæ¥çœ‹çœ‹ã€‚æ•°æ®æ˜¯ä¸æ˜¯æœ€æ–°çš„
        å¦‚æœä¸æ˜¯æœ€æ–°çš„ï¼Œå°±å°†åŸå§‹æ•°æ®åœ¨ä¸Šæ¬¡å› å­å­˜å‚¨çš„æ—¥æœŸå¤„æˆªæ–­
        è®¡ç®—å‡ºæ–°çš„ä¸€æ®µæ—¶é—´çš„å› å­å€¼ï¼Œç„¶åè¿½åŠ å†™å…¥å› å­æ–‡ä»¶ä¸­
        '''
        the_func = partial(func)
        try:
            df = pd.read_feather(path)
            df.columns = ['date'] + list(df.columns)[1:]
            df = df.set_index('date')
            if df.index.max() < pri.index.max():
                to_add = pri[(pri.index > df.index.max()) & (pri.index <= pri.index.max())]
                to_add = the_func(to_add)
                df = pd.concat([df, to_add])
                print('ä¹‹å‰æ•°æ®æœ‰ç‚¹æ—§äº†ï¼Œå·²ä¸ºæ‚¨å®Œæˆæ›´æ–°')
            else:
                print('æ•°æ®å¾ˆæ–°ï¼Œæ— éœ€æ›´æ–°')
            df1 = df.reset_index()
            df1.columns = ['date'] + list(df1.columns)[1:]
            df1.to_feather(path)
            return df
        except Exception:
            df = the_func(pri)
            df1 = df.reset_index()
            df1.columns = ['date'] + list(df1.columns)[1:]
            df1.to_feather(path)
            print('æ–°å› å­å»ºåº“å®Œæˆ')
            return df

    def get_updated_factors(self):
        '''æ›´æ–°å› å­æ•°æ®'''
        self.ret20 = self.update_single_factor_in_database(self.ret20_database, self.closes, self.get_ret20)
        self.turn20 = self.update_single_factor_in_database(self.turn20_database, self.turnovers, self.get_turn20)
        self.vol20 = self.update_single_factor_in_database(self.vol20_database, self.closes, self.get_vol20)


class pure_moonnight():
    '''å°è£…é€‰è‚¡æ¡†æ¶'''
    __slots__ = ['shen']
    def __init__(self, factors, groups_num=10, neutralize=False, boxcox=False,by10=False, value_weighted=False,y2=False, plt_plot=True, plotly_plot=False,
                 filename='åˆ†ç»„å‡€å€¼å›¾', time_start=None, time_end=None, print_comments=True,comments_writer=None,net_values_writer=None,rets_writer=None,
            comments_sheetname=None,net_values_sheetname=None,rets_sheetname=None,on_paper=False,sheetname=None):
        '''ç›´æ¥è¾“å…¥å› å­æ•°æ®'''
        if isinstance(factors,pure_fallmount):
            factors=factors().copy()
        self.shen=pure_moon()
        self.shen.set_factor_df_date_as_index(factors)
        self.shen.prerpare()
        self.shen.run(groups_num=groups_num,neutralize=neutralize,boxcox=boxcox,value_weighted=value_weighted,y2=y2,plt_plot=plt_plot,
                      plotly_plot=plotly_plot,filename=filename,time_start=time_start,time_end=time_end,print_comments=print_comments,
                      comments_writer=comments_writer,net_values_writer=net_values_writer,rets_writer=rets_writer,
                      comments_sheetname=comments_sheetname,net_values_sheetname=net_values_sheetname,
                      rets_sheetname=rets_sheetname,on_paper=on_paper,sheetname=sheetname)

    def __call__(self, fallmount=0):
        '''è°ƒç”¨åˆ™è¿”å›å› å­æ•°æ®'''
        if fallmount == 0:
            df=self.shen.factors_out.copy()
            # df=df.set_index(['date', 'code']).unstack()
            df.columns=list(map(lambda x:x[1],list(df.columns)))
            return df
        else:
            return pure_fallmount(df)

class pure_newyear():
    '''è½¬ä¸ºç”Ÿæˆ25åˆ†ç»„å’Œç™¾åˆ†ç»„çš„æ”¶ç›ŠçŸ©é˜µè€Œå°è£…'''

    def __init__(self,facx,facy,group_num_single,namex='ä¸»',namey='æ¬¡'):
        '''åˆå§‹åŒ–æ—¶å³è¿›è¡Œå›æµ‹'''
        homex=pure_fallmount(facx)
        homey=pure_fallmount(facy)
        if group_num_single==5:
            homexy=homex>homey
        elif group_num_single==10:
            homexy=homex>>homey
        shen=pure_moonnight(homexy(),group_num_single**2,plt_plot=False,print_comments=False)
        sq=shen.shen.square_rets.copy()
        sq.index=[namex+str(i) for i in list(sq.index)]
        sq.columns=[namey+str(i) for i in list(sq.columns)]
        self.square_rets=sq

    def __call__(self):
        '''è°ƒç”¨å¯¹è±¡æ—¶ï¼Œè¿”å›æœ€ç»ˆç»“æœï¼Œæ­£æ–¹å½¢çš„åˆ†ç»„å¹´åŒ–æ”¶ç›Šç‡è¡¨'''
        return self.square_rets

class pure_dawn():
    '''
    å› å­åˆ‡å‰²è®ºçš„æ¯æ¡†æ¶ï¼Œå¯ä»¥å¯¹ä¸¤ä¸ªå› å­è¿›è¡Œç±»ä¼¼äºå› å­åˆ‡å‰²çš„æ“ä½œ
    å¯ç”¨äºæ´¾ç”Ÿä»»ä½•"ä»¥ä¸¤ä¸ªå› å­ç”Ÿæˆä¸€ä¸ªå› å­"çš„å­ç±»
    ä½¿ç”¨ä¸¾ä¾‹
    cutå‡½æ•°é‡Œï¼Œå¿…é¡»å¸¦æœ‰è¾“å…¥å˜é‡df,dfæœ‰ä¸¤ä¸ªcolumnsï¼Œä¸€ä¸ªåä¸º'fac1'ï¼Œä¸€ä¸ªåä¸º'fac2'ï¼Œdfæ˜¯æœ€è¿‘ä¸€ä¸ªå›çœ‹æœŸå†…çš„æ•°æ®
    class Cut(pure_dawn):
        def __init__(self,fac1,fac2):
        self.fac1=fac1
        self.fac2=fac2
        super(Cut,self).__init__(fac1=fac1,fac2=fac2)

    def cut(self,df):
        df=df.sort_values('fac1')
        df=df.assign(fac3=df.fac1*df.fac2)
        ret0=df.fac2.iloc[:4].mean()
        ret1=df.fac2.iloc[4:8].mean()
        ret2=df.fac2.iloc[8:12].mean()
        ret3=df.fac2.iloc[12:16].mean()
        ret4=df.fac2.iloc[16:].mean()
        aret0=df.fac3.iloc[:4].mean()
        aret1=df.fac3.iloc[4:8].mean()
        aret2=df.fac3.iloc[8:12].mean()
        aret3=df.fac3.iloc[12:16].mean()
        aret4=df.fac3.iloc[16:].mean()
        return ret0,ret1,ret2,ret3,ret4,aret0,aret1,aret2,aret3,aret4

    cut=Cut(ct,ret_inday)
    cut.run(cut.cut)

    cut0=get_value(cut(),0)
    cut1=get_value(cut(),1)
    cut2=get_value(cut(),2)
    cut3=get_value(cut(),3)
    cut4=get_value(cut(),4)
    '''

    def __init__(self,fac1,fac2,*args):
        self.fac1=fac1
        self.fac1=self.fac1.stack().reset_index()
        self.fac1.columns=['date','code','fac1']
        self.fac2=fac2
        self.fac2=self.fac2.stack().reset_index()
        self.fac2.columns=['date','code','fac2']
        fac_all=pd.merge(self.fac1,self.fac2,on=['date','code'])
        for i,fac in enumerate(args):
            fac=fac.stack().reset_index()
            fac.columns=['date','code',f'fac{i+3}']
            fac_all=pd.merge(fac_all,fac,on=['date','code'])
        fac_all=fac_all.sort_values(['date','code'])
        self.fac=fac_all.copy()

    def __call__(self):
        '''è¿”å›æœ€ç»ˆæœˆåº¦å› å­å€¼'''
        return self.fac

    def get_fac_long_and_tradedays(self):
        '''å°†ä¸¤ä¸ªå› å­çš„çŸ©é˜µè½¬åŒ–ä¸ºé•¿åˆ—è¡¨'''
        self.tradedays=sorted(list(set(self.fac.date)))

    def get_month_starts_and_ends(self,backsee=20):
        '''è®¡ç®—å‡ºæ¯ä¸ªæœˆå›çœ‹æœŸé—´çš„èµ·ç‚¹æ—¥å’Œç»ˆç‚¹æ—¥'''
        self.month_ends=[i for i,j in zip(self.tradedays[:-1],self.tradedays[1:]) if i.month!=j.month]
        self.month_ends.append(self.tradedays[-1])
        self.month_starts=[self.find_begin(self.tradedays,i,backsee=backsee) for i in self.month_ends]
        self.month_starts[0]=self.tradedays[0]

    def find_begin(self,tradedays,end_day,backsee=20):
        '''æ‰¾å‡ºå›çœ‹è‹¥å¹²å¤©çš„å¼€å§‹æ—¥ï¼Œé»˜è®¤ä¸º20'''
        end_day_index=tradedays.index(end_day)
        start_day_index=end_day_index-backsee+1
        start_day=tradedays[start_day_index]
        return start_day

    def make_monthly_factors_single_code(self,df,func):
        '''
        å¯¹å•ä¸€è‚¡ç¥¨æ¥è®¡ç®—æœˆåº¦å› å­
        funcä¸ºå•æœˆæ‰§è¡Œçš„å‡½æ•°ï¼Œè¿”å›å€¼åº”ä¸ºæœˆåº¦å› å­ï¼Œå¦‚ä¸€ä¸ªfloatæˆ–ä¸€ä¸ªlist
        dfä¸ºä¸€ä¸ªè‚¡ç¥¨çš„å››åˆ—è¡¨ï¼ŒåŒ…å«æ—¶é—´ã€ä»£ç ã€å› å­1å’Œå› å­2
        '''
        res={}
        for start,end in zip(self.month_starts,self.month_ends):
            this_month=df[(df.date>=start)&(df.date<=end)]
            res[end]=func(this_month)
        dates=list(res.keys())
        corrs=list(res.values())
        part=pd.DataFrame({'date':dates,'corr':corrs})
        return part

    def get_monthly_factor(self,func):
        '''è¿è¡Œè‡ªå·±å†™çš„å‡½æ•°ï¼Œè·å¾—æœˆåº¦å› å­'''
        tqdm.tqdm.pandas(desc='when the dawn comes, tonight will be a memory too.')
        self.fac=self.fac.groupby(['code']).progress_apply(lambda x:self.make_monthly_factors_single_code(x,func))
        self.fac=self.fac.reset_index(level=1,drop=True).reset_index().set_index(['date','code']).unstack()
        self.fac.columns=[i[1] for i in list(self.fac.columns)]
        self.fac=self.fac.resample('M').last()

    @kk.desktop_sender(title='å˜¿ï¼Œåˆ‡å‰²å®Œæˆå•¦ğŸ›')
    def run(self,func,backsee=20):
        '''è¿è¡Œå¿…è¦çš„å‡½æ•°'''
        self.get_fac_long_and_tradedays()
        self.get_month_starts_and_ends(backsee=backsee)
        self.get_monthly_factor(func)


class pure_cloud(object):
    '''
    ä¸ºäº†æµ‹è¯•å…¶ä»–ä¸åŒçš„é¢‘ç‡è€Œè®¾è®¡çš„ç±»ï¼Œä»…è€ƒè™‘äº†ä¸Šå¸‚æ»¡60å¤©è¿™ä¸€è¦ç´ 
    è¿™ä¸€å›æµ‹é‡‡å–çš„æ–¹æ¡ˆæ˜¯ï¼Œå¯¹äºå›æµ‹é¢‘ç‡nå¤©ï¼Œå°†åˆå§‹èµ„é‡‘ç­‰åˆ†æˆnç¬”ï¼Œæ¯å¤©ä»¥1/nçš„èµ„é‡‘è°ƒä»“
    æ¯ç¬”èµ„é‡‘ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œæœ€ç»ˆæ±‡èšæˆä¸€ä¸ªæ”¶ç›Šç‡åºåˆ—
    '''
    def __init__(self,fac,freq,group=10,boxcox=1,trade_cost=0,print_comments=1,plt_plot=1,plotly_plot=0,
                 filename='å‡€å€¼èµ°åŠ¿å›¾',comments_writer=None,nets_writer=None,sheet_name=None):
        '''næ˜¯å›æµ‹çš„é¢‘ç‡ï¼Œç­‰åˆ†æˆnä»½ï¼Œgroupæ˜¯å›æµ‹çš„ç»„æ•°ï¼Œboxcoxæ˜¯æ˜¯å¦åšè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–'''
        self.fac=fac
        self.freq=freq
        self.group=group
        self.boxcox=boxcox
        self.trade_cost=trade_cost
        moon=pure_moon()
        moon.prerpare()
        ages=moon.ages.copy()
        ages=(ages>=60)+0
        self.ages=ages.replace(0,np.nan)
        self.closes=read_daily(close=1)
        self.rets=((self.closes.shift(-self.freq)/self.closes-1)*self.ages)/self.freq
        self.run(print_comments=print_comments,plt_plot=plt_plot,plotly_plot=plotly_plot,filename=filename)
        if comments_writer:
            if sheet_name:
                self.long_short_comments.to_excel(comments_writer,sheet_name=sheet_name)
            else:
                raise AttributeError('å¿…é¡»åˆ¶å®šsheet_nameå‚æ•°ğŸ¤’')
        if nets_writer:
            if sheet_name:
                self.group_nets.to_excel(nets_writer,sheet_name=sheet_name)
            else:
                raise AttributeError('å¿…é¡»åˆ¶å®šsheet_nameå‚æ•°ğŸ¤’')

    def comments(self,series,series1):
        '''å¯¹twinsä¸­çš„ç»“æœç»™å‡ºè¯„ä»·
        è¯„ä»·æŒ‡æ ‡åŒ…æ‹¬å¹´åŒ–æ”¶ç›Šç‡ã€æ€»æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€å¹´åŒ–å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤ç‡ã€èƒœç‡'''
        ret=(series.iloc[-1]-series.iloc[0])/series.iloc[0]
        duration=(series.index[-1]-series.index[0]).days
        year=duration/365
        ret_yearly=(series.iloc[-1]/series.iloc[0])**(1/year)-1
        max_draw=-(series/series.expanding(1).max()-1).min()
        vol=np.std(series1)*(250**0.5)
        sharpe=ret_yearly/vol
        wins=series1[series1>0]
        win_rate=len(wins)/len(series1)
        return pd.Series([ret,ret_yearly,vol,sharpe,max_draw,win_rate],
                         index=['æ€»æ”¶ç›Šç‡','å¹´åŒ–æ”¶ç›Šç‡','å¹´åŒ–æ³¢åŠ¨ç‡','ä¿¡æ¯æ¯”ç‡','æœ€å¤§å›æ’¤ç‡','èƒœç‡'])

    @kk.desktop_sender(title='å˜¿ï¼Œå˜é¢‘å›æµ‹ç»“æŸå•¦ï½ğŸ—“')
    def run(self,print_comments,plt_plot,plotly_plot,filename):
        '''å¯¹å› å­å€¼åˆ†ç»„å¹¶åŒ¹é…'''
        if self.boxcox:
            self.fac=decap_industry(self.fac)
        self.fac=self.fac.T.apply(lambda x:pd.qcut(x,self.group,labels=False,duplicates='drop')).T
        self.fac=self.fac.shift(1)
        self.vs=[(((self.fac==i)+0).replace(0,np.nan)*self.rets).mean(axis=1) for i in range(self.group)]
        self.group_rets=pd.DataFrame({f'group{k}':list(v) for k,v in zip(range(1,self.group+1),self.vs)},index=self.vs[0].index)
        self.group_rets=self.group_rets.dropna(how='all')
        self.group_rets=self.group_rets
        self.group_nets=(self.group_rets+1).cumprod()
        self.group_nets=self.group_nets.apply(lambda x:x/x.iloc[0])
        self.one=self.group_nets['group1']
        self.end=self.group_nets[f'group{self.group}']
        if self.one.iloc[-1]>self.end.iloc[-1]:
            self.long_name='group1'
            self.short_name=f'group{self.group}'
        else:
            self.long_name=f'group{self.group}'
            self.short_name='group1'
        self.long_short_ret=self.group_rets[self.long_name]-self.group_rets[self.short_name]
        self.long_short_net=(self.long_short_ret+1).cumprod()
        self.long_short_net=self.long_short_net/self.long_short_net.iloc[0]
        if self.long_short_net.iloc[-1]<1:
            self.long_short_ret=self.group_rets[self.short_name]-self.group_rets[self.long_name]
            self.long_short_net=(self.long_short_ret+1).cumprod()
            self.long_short_net=self.long_short_net/self.long_short_net.iloc[0]
            self.long_short_ret=self.group_rets[self.short_name]-self.group_rets[self.long_name]-2*self.trade_cost/self.freq
            self.long_short_net=(self.long_short_ret+1).cumprod()
            self.long_short_net=self.long_short_net/self.long_short_net.iloc[0]
        else:
            self.long_short_ret=self.group_rets[self.long_name]-self.group_rets[self.short_name]-2*self.trade_cost/self.freq
            self.long_short_net=(self.long_short_ret+1).cumprod()
            self.long_short_net=self.long_short_net/self.long_short_net.iloc[0]
        self.group_rets=pd.concat([self.group_rets,self.long_short_ret.to_frame('long_short')],axis=1)
        self.group_nets=pd.concat([self.group_nets,self.long_short_net.to_frame('long_short')],axis=1)
        self.long_short_comments=self.comments(self.long_short_net,self.long_short_ret)
        if print_comments:
            print(self.long_short_comments)
        if plt_plot:
            self.group_nets.plot()
            plt.savefig(filename+'.png')
            plt.show()
        if plotly_plot:
            fig=pe.line(self.group_nets)
            filename_path=filename+'.html'
            pio.write_html(fig,filename_path,auto_open=True)
#
#
# class pure_cloud(object):
#     '''
#     ä¸ºäº†æµ‹è¯•å…¶ä»–ä¸åŒçš„é¢‘ç‡è€Œè®¾è®¡çš„ç±»ï¼Œä»…è€ƒè™‘äº†ä¸Šå¸‚æ»¡60å¤©è¿™ä¸€è¦ç´ 
#     è¿™ä¸€å›æµ‹é‡‡å–çš„æ–¹æ¡ˆæ˜¯ï¼Œå¯¹äºå›æµ‹é¢‘ç‡nå¤©ï¼Œå°†åˆå§‹èµ„é‡‘ç­‰åˆ†æˆnç¬”ï¼Œæ¯å¤©ä»¥1/nçš„èµ„é‡‘è°ƒä»“
#     æ¯ç¬”èµ„é‡‘ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œæœ€ç»ˆæ±‡èšæˆä¸€ä¸ªæ”¶ç›Šç‡åºåˆ—
#     '''
#     def __init__(self,fac,freq,group=10,boxcox=1,trade_cost=0,print_comments=1,plt_plot=1,plotly_plot=0,
#                  filename='å‡€å€¼èµ°åŠ¿å›¾',comments_writer=None,nets_writer=None,sheet_name=None):
#         '''næ˜¯å›æµ‹çš„é¢‘ç‡ï¼Œç­‰åˆ†æˆnä»½ï¼Œgroupæ˜¯å›æµ‹çš„ç»„æ•°ï¼Œboxcoxæ˜¯æ˜¯å¦åšè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–'''
#         self.fac=fac
#         self.freq=freq
#         self.group=group
#         self.boxcox=boxcox
#         self.trade_cost=trade_cost
#         moon=pure_moon()
#         moon.prerpare()
#         ages=moon.ages.copy()
#         ages=(ages>=60)+0
#         self.ages=ages.replace(0,np.nan)
#         self.closes=read_daily(close=1)
#         self.rets=((self.closes.shift(-self.freq)/self.closes-1)*self.ages-trade_cost)/self.freq
#         self.run(print_comments=print_comments,plt_plot=plt_plot,plotly_plot=plotly_plot,filename=filename)
#         if comments_writer:
#             if sheet_name:
#                 self.long_short_comments.to_excel(comments_writer,sheet_name=sheet_name)
#             else:
#                 raise AttributeError('å¿…é¡»åˆ¶å®šsheet_nameå‚æ•°ğŸ¤’')
#         if nets_writer:
#             if sheet_name:
#                 self.group_nets.to_excel(nets_writer,sheet_name=sheet_name)
#             else:
#                 raise AttributeError('å¿…é¡»åˆ¶å®šsheet_nameå‚æ•°ğŸ¤’')
#
#     def comments(self,series,series1):
#         '''å¯¹twinsä¸­çš„ç»“æœç»™å‡ºè¯„ä»·
#         è¯„ä»·æŒ‡æ ‡åŒ…æ‹¬å¹´åŒ–æ”¶ç›Šç‡ã€æ€»æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€å¹´åŒ–å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤ç‡ã€èƒœç‡'''
#         ret=(series.iloc[-1]-series.iloc[0])/series.iloc[0]
#         duration=(series.index[-1]-series.index[0]).days
#         year=duration/365
#         ret_yearly=(series.iloc[-1]/series.iloc[0])**(1/year)-1
#         max_draw=-(series/series.expanding(1).max()-1).min()
#         vol=np.std(series1)*(250**0.5)
#         sharpe=ret_yearly/vol
#         wins=series1[series1>0]
#         win_rate=len(wins)/len(series1)
#         return pd.Series([ret,ret_yearly,vol,sharpe,max_draw,win_rate],
#                          index=['æ€»æ”¶ç›Šç‡','å¹´åŒ–æ”¶ç›Šç‡','å¹´åŒ–æ³¢åŠ¨ç‡','ä¿¡æ¯æ¯”ç‡','æœ€å¤§å›æ’¤ç‡','èƒœç‡'])
#
#     def run(self,print_comments,plt_plot,plotly_plot,filename):
#         '''å¯¹å› å­å€¼åˆ†ç»„å¹¶åŒ¹é…'''
#         if self.boxcox:
#             self.fac=decap_industry(self.fac)
#         self.fac=self.fac.T.apply(lambda x:pd.qcut(x,self.group,labels=False,duplicates='drop')).T
#         self.fac=self.fac.shift(1)
#         self.vs=[(((self.fac==i)+0).replace(0,np.nan)*self.rets).mean(axis=1) for i in range(self.group)]
#         self.group_rets=pd.DataFrame({f'group{k}':list(v) for k,v in zip(range(1,self.group+1),self.vs)},index=self.vs[0].index)
#         self.group_rets=self.group_rets.dropna(how='all')
#         self.group_nets=(self.group_rets+1).cumprod()
#         self.group_nets=self.group_nets.apply(lambda x:x/x.iloc[0])
#         self.one=self.group_nets['group1']
#         self.end=self.group_nets[f'group{self.group}']
#         if self.one.iloc[-1]>self.end.iloc[-1]:
#             self.long_name='group1'
#             self.short_name=f'group{self.group}'
#         else:
#             self.long_name=f'group{self.group}'
#             self.short_name='group1'
#         self.long_short_ret=self.group_rets[self.long_name]-self.group_rets[self.short_name]
#         self.long_short_net=(self.long_short_ret+1).cumprod()
#         self.long_short_net=self.long_short_net/self.long_short_net.iloc[0]
#         if self.long_short_net.iloc[-1]<1:
#             self.long_short_ret=self.group_rets[self.short_name]-self.group_rets[self.long_name]
#             self.long_short_net=(self.long_short_ret+1).cumprod()
#             self.long_short_net=self.long_short_net/self.long_short_net.iloc[0]
#         self.group_rets=pd.concat([self.group_rets,self.long_short_ret.to_frame('long_short')],axis=1)
#         self.group_nets=pd.concat([self.group_nets,self.long_short_net.to_frame('long_short')],axis=1)
#         self.long_short_comments=self.comments(self.long_short_net,self.long_short_ret)
#         if print_comments:
#             print(self.long_short_comments)
#         if plt_plot:
#             self.group_nets.plot()
#             plt.savefig(filename+'.png')
#             plt.show()
#         if plotly_plot:
#             fig=pe.line(self.group_nets)
#             filename_path=filename+'.html'
#             pio.write_html(fig,filename_path,auto_open=True)


class pure_wood(object):
    '''ä¸€ç§å› å­åˆæˆçš„æ–¹æ³•ï¼Œçµæ„Ÿæ¥æºäºadaboostç®—æ³•
    adaboostç®—æ³•çš„ç²¾ç¥æ˜¯ï¼Œæ‰¾åˆ°å‡ ä¸ªåˆ†ç±»æ•ˆæœè¾ƒå·®çš„å¼±å­¦ä¹ å™¨ï¼Œé€šè¿‡æ”¹å˜ä¸åŒåˆ†ç±»æœŸè®­ç»ƒæ—¶çš„æ ·æœ¬çš„æƒé‡ï¼Œ
    è®¡ç®—æ¯ä¸ªå­¦ä¹ å™¨çš„é”™è¯¯æ¦‚ç‡ï¼Œé€šè¿‡çº¿æ€§åŠ æƒç»„åˆçš„æ–¹å¼ï¼Œè®©å¼±åˆ†ç±»æœŸè¿›è¡ŒæŠ•ç¥¨ï¼Œå†³å®šæœ€ç»ˆåˆ†ç±»ç»“æœï¼Œ
    è¿™é‡Œå–é€šè¿‡è®¡ç®—å„ä¸ªå› å­å¤šå¤´æˆ–ç©ºå¤´çš„é”™è¯¯æ¦‚ç‡ï¼Œè¿›è€Œé€šè¿‡é”™è¯¯æ¦‚ç‡å¯¹å› å­è¿›è¡ŒåŠ æƒç»„åˆï¼Œ
    æ­¤å¤„æ–¹å¼å°†åˆ†ä¸ºä¸¤ç§ï¼Œä¸€ç§æ˜¯ä¸»å‰¯å› å­çš„æ–¹å¼ï¼Œå¦ä¸€ç§æ˜¯å…¨ç­‰ä»·çš„æ–¹å¼ï¼Œ
    ä¸»å‰¯å› å­å³å…ˆæŒ‡å®šä¸€ä¸ªä¸»å› å­ï¼ˆé€šå¸¸ä¸ºæ•ˆæœæ›´å¥½çš„é‚£ä¸ªå› å­ï¼‰ï¼Œç„¶åæŒ‡å®šè‹¥å¹²ä¸ªå‰¯å› å­ï¼Œå…ˆè®¡ç®—ä¸»å› å­çš„é”™è¯¯æ¦‚ç‡ï¼Œ
    æ‰¾åˆ°ä¸»å› å­å¤šå¤´é‡Œåˆ†ç±»é”™è¯¯çš„éƒ¨åˆ†ï¼Œç„¶åé€šè¿‡æé«˜æœŸåŠ æƒï¼Œä¾æ¬¡è®¡ç®—åç»­å‰¯å› å­çš„é”™è¯¯æ¦‚ç‡ï¼ˆé€šå¸¸æŒ‰ç…§å› å­æ•ˆæœä»å¥½åˆ°åæ’åºï¼‰ï¼Œ
    æœ€ç»ˆå¯¹ä¾æ¬¡å¾—åˆ°çš„é”™è¯¯æ¦‚ç‡åšè¿ç®—ï¼Œç„¶ååŠ æƒ
    å…¨ç­‰ä»·æ–¹å¼å³ä¸åŒºåˆ†ä¸»å‰¯å› å­ï¼Œåˆ†åˆ«ç‹¬ç«‹è®¡ç®—æ¯ä¸ªå› å­çš„é”™è¯¯æ¦‚ç‡ï¼Œç„¶åè¿›è¡ŒåŠ æƒ'''
    def __init__(self,domain_fac: pd.DataFrame,subdomain_facs: list,group_num=10):
        '''å£°æ˜ä¸»å‰¯å› å­å’Œåˆ†ç»„æ•°'''
        self.domain_fac=domain_fac
        self.subdomain_facs=subdomain_facs
        self.group_num=group_num
        opens=read_daily(open=1).resample('M').first()
        closes=read_daily(close=1).resample('M').last()
        self.ret_next=closes/opens-1
        self.ret_next=self.ret_next.shift(-1)
        self.domain_fac=self.domain_fac.T.apply(lambda x:pd.qcut(x,group_num,labels=False,duplicates='drop')).T+1
        self.ret_next=self.ret_next.T.apply(lambda x:pd.qcut(x,group_num,labels=False,duplicates='drop')).T+1
        self.subdomain_facs=[i.T.apply(lambda x:pd.qcut(x,group_num,labels=False,duplicates='drop')).T+1 for i in self.subdomain_facs]
        self.get_all_a()
        self.get_three_new_facs()

    def __call__(self, *args, **kwargs):
        return copy.copy(self.new_facs)

    def get_a_and_new_weight(self,n,fac,weight=None):
        '''è®¡ç®—ä¸»å› å­çš„æƒé‡å’Œæƒé‡çŸ©é˜µ'''
        fac_at_n=(fac==n)+0
        ret_at_n=(self.ret_next==n)+0
        not_nan=fac_at_n+ret_at_n
        not_nan=not_nan[not_nan.index.isin(fac_at_n.index)]
        not_nan=(not_nan>0)+0
        wrong=((ret_at_n-fac_at_n)>0)+0
        wrong=wrong[wrong.index.isin(fac_at_n.index)]
        right=((ret_at_n-fac_at_n)==0)+0
        right=right[right.index.isin(fac_at_n.index)]
        wrong=wrong*not_nan
        right=right*not_nan
        wrong=wrong.dropna(how='all')
        right=right.dropna(how='all')
        if isinstance(weight,pd.DataFrame):
            e_rate=(wrong*weight).sum(axis=1)
            a_rate=0.5*np.log((1-e_rate)/e_rate)
            wrong_here=-wrong
            g_df=multidfs_to_one(right,wrong_here)
            on_exp=(g_df.T*a_rate.to_numpy()).T
            with_exp=np.exp(on_exp)
            new_weight=weight*with_exp
            new_weight=(new_weight.T/new_weight.sum(axis=1).to_numpy()).T
        else:
            e_rate=(right.sum(axis=1))/(right.sum(axis=1)+wrong.sum(axis=1))
            a_rate=0.5*np.log((1-e_rate)/e_rate)
            wrong_here=-wrong
            g_df=multidfs_to_one(right,wrong_here)
            on_exp=(g_df.T*a_rate.to_numpy()).T
            with_exp=np.exp(on_exp)
            new_weight=with_exp.copy()
            new_weight=(new_weight.T/new_weight.sum(axis=1).to_numpy()).T
        return a_rate,new_weight

    def get_all_a(self):
        '''è®¡ç®—æ¯ä¸ªå› å­çš„aå€¼'''
        #ç¬¬ä¸€ç»„éƒ¨åˆ†
        one_a_domain,one_weight=self.get_a_and_new_weight(1,self.domain_fac)
        one_a_list=[one_a_domain]
        for fac in self.subdomain_facs:
            one_new_a,one_weight=self.get_a_and_new_weight(1,fac,one_weight)
            one_a_list.append(one_new_a)
        self.a_list_one=one_a_list
        #æœ€åä¸€ç»„éƒ¨åˆ†
        end_a_domain,end_weight=self.get_a_and_new_weight(self.group_num,self.domain_fac)
        end_a_list=[end_a_domain]
        for fac in self.subdomain_facs:
            end_new_a,end_weight=self.get_a_and_new_weight(self.group_num,fac,end_weight)
            end_a_list.append(end_new_a)
        self.a_list_end=end_a_list

    def get_three_new_facs(self):
        '''åˆ†åˆ«ä½¿ç”¨ç¬¬ä¸€ç»„åŠ å¼ºã€æœ€åä¸€ç»„åŠ å¼ºã€ä¸¤ç»„å¹³å‡çš„æ–¹å¼ç»“åˆ'''
        one_fac=sum([(i.iloc[1:,:].T*j.iloc[:-1].to_numpy()).T for i,j in zip([self.domain_fac]+self.subdomain_facs,self.a_list_one)])
        end_fac=sum([(i.iloc[1:,:].T*j.iloc[:-1].to_numpy()).T for i,j in zip([self.domain_fac]+self.subdomain_facs,self.a_list_end)])
        both_fac=one_fac+end_fac
        self.new_facs=[one_fac,end_fac,both_fac]


class pure_fire(object):
    '''ä¸€ç§å› å­åˆæˆçš„æ–¹æ³•ï¼Œçµæ„Ÿæ¥æºäºadaboostç®—æ³•
    adaboostç®—æ³•çš„ç²¾ç¥æ˜¯ï¼Œæ‰¾åˆ°å‡ ä¸ªåˆ†ç±»æ•ˆæœè¾ƒå·®çš„å¼±å­¦ä¹ å™¨ï¼Œé€šè¿‡æ”¹å˜ä¸åŒåˆ†ç±»æœŸè®­ç»ƒæ—¶çš„æ ·æœ¬çš„æƒé‡ï¼Œ
    è®¡ç®—æ¯ä¸ªå­¦ä¹ å™¨çš„é”™è¯¯æ¦‚ç‡ï¼Œé€šè¿‡çº¿æ€§åŠ æƒç»„åˆçš„æ–¹å¼ï¼Œè®©å¼±åˆ†ç±»æœŸè¿›è¡ŒæŠ•ç¥¨ï¼Œå†³å®šæœ€ç»ˆåˆ†ç±»ç»“æœï¼Œ
    è¿™é‡Œå–é€šè¿‡è®¡ç®—å„ä¸ªå› å­å¤šå¤´æˆ–ç©ºå¤´çš„é”™è¯¯æ¦‚ç‡ï¼Œè¿›è€Œé€šè¿‡é”™è¯¯æ¦‚ç‡å¯¹å› å­è¿›è¡ŒåŠ æƒç»„åˆï¼Œ
    æ­¤å¤„æ–¹å¼å°†åˆ†ä¸ºä¸¤ç§ï¼Œä¸€ç§æ˜¯ä¸»å‰¯å› å­çš„æ–¹å¼ï¼Œå¦ä¸€ç§æ˜¯å…¨ç­‰ä»·çš„æ–¹å¼ï¼Œ
    ä¸»å‰¯å› å­å³å…ˆæŒ‡å®šä¸€ä¸ªä¸»å› å­ï¼ˆé€šå¸¸ä¸ºæ•ˆæœæ›´å¥½çš„é‚£ä¸ªå› å­ï¼‰ï¼Œç„¶åæŒ‡å®šè‹¥å¹²ä¸ªå‰¯å› å­ï¼Œå…ˆè®¡ç®—ä¸»å› å­çš„é”™è¯¯æ¦‚ç‡ï¼Œ
    æ‰¾åˆ°ä¸»å› å­å¤šå¤´é‡Œåˆ†ç±»é”™è¯¯çš„éƒ¨åˆ†ï¼Œç„¶åé€šè¿‡æé«˜æœŸåŠ æƒï¼Œä¾æ¬¡è®¡ç®—åç»­å‰¯å› å­çš„é”™è¯¯æ¦‚ç‡ï¼ˆé€šå¸¸æŒ‰ç…§å› å­æ•ˆæœä»å¥½åˆ°åæ’åºï¼‰ï¼Œ
    æœ€ç»ˆå¯¹ä¾æ¬¡å¾—åˆ°çš„é”™è¯¯æ¦‚ç‡åšè¿ç®—ï¼Œç„¶ååŠ æƒ
    å…¨ç­‰ä»·æ–¹å¼å³ä¸åŒºåˆ†ä¸»å‰¯å› å­ï¼Œåˆ†åˆ«ç‹¬ç«‹è®¡ç®—æ¯ä¸ªå› å­çš„é”™è¯¯æ¦‚ç‡ï¼Œç„¶åè¿›è¡ŒåŠ æƒ'''
    def __init__(self,facs: list,group_num=10):
        '''å£°æ˜ä¸»å‰¯å› å­å’Œåˆ†ç»„æ•°'''
        self.facs=facs
        self.group_num=group_num
        opens=read_daily(open=1).resample('M').first()
        closes=read_daily(close=1).resample('M').last()
        self.ret_next=closes/opens-1
        self.ret_next=self.ret_next.shift(-1)
        self.ret_next=self.ret_next.T.apply(lambda x:pd.qcut(x,group_num,labels=False,duplicates='drop')).T+1
        self.facs=[i.T.apply(lambda x:pd.qcut(x,group_num,labels=False,duplicates='drop')).T+1 for i in self.facs]
        self.get_all_a()
        self.get_three_new_facs()

    def __call__(self, *args, **kwargs):
        return copy.copy(self.new_facs)

    def get_a(self,n,fac):
        '''è®¡ç®—ä¸»å› å­çš„æƒé‡å’Œæƒé‡çŸ©é˜µ'''
        fac_at_n=(fac==n)+0
        ret_at_n=(self.ret_next==n)+0
        not_nan=fac_at_n+ret_at_n
        not_nan=not_nan[not_nan.index.isin(fac_at_n.index)]
        not_nan=(not_nan>0)+0
        wrong=((ret_at_n-fac_at_n)>0)+0
        wrong=wrong[wrong.index.isin(fac_at_n.index)]
        right=((ret_at_n-fac_at_n)==0)+0
        right=right[right.index.isin(fac_at_n.index)]
        wrong=wrong*not_nan
        right=right*not_nan
        wrong=wrong.dropna(how='all')
        right=right.dropna(how='all')
        e_rate=(right.sum(axis=1))/(right.sum(axis=1)+wrong.sum(axis=1))
        a_rate=0.5*np.log((1-e_rate)/e_rate)
        return a_rate

    def get_all_a(self):
        '''è®¡ç®—æ¯ä¸ªå› å­çš„aå€¼'''
        #ç¬¬ä¸€ç»„éƒ¨åˆ†
        self.a_list_one=[self.get_a(1,i) for i in self.facs]
        #æœ€åä¸€ç»„éƒ¨åˆ†
        self.a_list_end=[self.get_a(self.group_num,i) for i in self.facs]

    def get_three_new_facs(self):
        '''åˆ†åˆ«ä½¿ç”¨ç¬¬ä¸€ç»„åŠ å¼ºã€æœ€åä¸€ç»„åŠ å¼ºã€ä¸¤ç»„å¹³å‡çš„æ–¹å¼ç»“åˆ'''
        one_fac=sum([(i.iloc[1:,:].T*j.iloc[:-1].to_numpy()).T for i,j in zip(self.facs,self.a_list_one)])
        end_fac=sum([(i.iloc[1:,:].T*j.iloc[:-1].to_numpy()).T for i,j in zip(self.facs,self.a_list_end)])
        both_fac=one_fac+end_fac
        self.new_facs=[one_fac,end_fac,both_fac]



