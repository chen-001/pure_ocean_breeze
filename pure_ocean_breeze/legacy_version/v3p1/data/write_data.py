__updated__ = "2022-09-13 16:43:45"

try:
    import rqdatac

    rqdatac.init()
except Exception:
    print("æš‚æ—¶æœªè¿æ¥ç±³ç­")
from loguru import logger
import os
import time
import datetime
import numpy as np
import pandas as pd
import scipy.io as scio
from sqlalchemy import FLOAT, INT, VARCHAR, BIGINT
from tenacity import retry
import pickledb
import tqdm
from functools import reduce
import dcube as dc
from pure_ocean_breeze.legacy_version.v3p1.state.homeplace import HomePlace

homeplace = HomePlace()
try:
    pro = dc.pro_api(homeplace.api_token)
except Exception:
    print("æš‚æ—¶æœªè¿æ¥æ•°ç«‹æ–¹")
from pure_ocean_breeze.data.database import (
    sqlConfig,
    ClickHouseClient,
    PostgreSQL,
    Questdb,
)
from pure_ocean_breeze.legacy_version.v3p1.data.read_data import read_daily, read_money_flow
from pure_ocean_breeze.legacy_version.v3p1.data.dicts import INDUS_DICT, INDEX_DICT, ZXINDUS_DICT
from pure_ocean_breeze.legacy_version.v3p1.data.tools import ç”Ÿæˆæ¯æ—¥åˆ†ç±»è¡¨, add_suffix, convert_code


def database_update_minute_data_to_clickhouse_and_questdb(kind: str) -> None:
    """ä½¿ç”¨ç±³ç­æ›´æ–°åˆ†é’Ÿæ•°æ®è‡³clickhouseå’Œquestdbä¸­

    Parameters
    ----------
    kind : str
        æ›´æ–°è‚¡ç¥¨åˆ†é’Ÿæ•°æ®æˆ–æŒ‡æ•°åˆ†é’Ÿæ•°æ®ï¼Œè‚¡ç¥¨åˆ™'stock'ï¼ŒæŒ‡æ•°åˆ™'index'

    Raises
    ------
    `IOError`
        å¦‚æœæœªæŒ‡å®šè‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼Œå°†æŠ¥é”™
    """
    if kind == "stock":
        code_type = "CS"
    elif kind == "index":
        code_type = "INDX"
    else:
        raise IOError("æ€»å¾—æŒ‡å®šä¸€ç§ç±»å‹å§ï¼Ÿè¯·ä»stockå’Œindexä¸­é€‰ä¸€ä¸ª")
    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user1 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user1}MB")
    # è·å–å…¨éƒ¨è‚¡ç¥¨/æŒ‡æ•°ä»£ç 
    cs = rqdatac.all_instruments(type=code_type, market="cn", date=None)
    codes = list(cs.order_book_id)
    # è·å–ä¸Šæ¬¡æ›´æ–°æˆªæ­¢æ—¶é—´
    chc = ClickHouseClient("minute_data")
    last_date = max(chc.show_all_dates(f"minute_data_{kind}"))
    # æœ¬æ¬¡æ›´æ–°èµ·å§‹æ—¥æœŸ
    start_date = pd.Timestamp(str(last_date)) + pd.Timedelta(days=1)
    start_date = datetime.datetime.strftime(start_date, "%Y-%m-%d")
    # æœ¬æ¬¡æ›´æ–°ç»ˆæ­¢æ—¥æœŸ
    end_date = datetime.datetime.now()
    if end_date.hour < 17:
        end_date = end_date - pd.Timedelta(days=1)
    end_date = datetime.datetime.strftime(end_date, "%Y-%m-%d")
    logger.info(f"æœ¬æ¬¡å°†ä¸‹è½½ä»{start_date}åˆ°{end_date}çš„æ•°æ®")
    # ä¸‹è½½æ•°æ®
    ts = rqdatac.get_price(
        codes,
        start_date=start_date,
        end_date=end_date,
        frequency="1m",
        fields=["volume", "total_turnover", "high", "low", "close", "open"],
        adjust_type="none",
        skip_suspended=False,
        market="cn",
        expect_df=True,
        time_slice=None,
    )
    # è°ƒæ•´æ•°æ®æ ¼å¼
    ts = ts.reset_index()
    ts = ts.rename(
        columns={
            "order_book_id": "code",
            "datetime": "date",
            "volume": "amount",
            "total_turnover": "money",
        }
    )
    ts = ts.sort_values(["code", "date"])
    ts.date = ts.date.dt.strftime("%Y%m%d").astype(int)
    ts = ts.groupby(["code", "date"]).apply(
        lambda x: x.assign(num=list(range(1, x.shape[0] + 1)))
    )
    ts = (np.around(ts.set_index("code"), 2) * 100).astype(int).reset_index()
    ts.code = ts.code.str.replace(".XSHE", ".SZ")
    ts.code = ts.code.str.replace(".XSHG", ".SH")
    # æ•°æ®å†™å…¥æ•°æ®åº“
    ts.to_sql(f"minute_data_{kind}", chc.engine, if_exists="append", index=False)
    ts = ts.set_index("code")
    ts = ts / 100
    ts = ts.reset_index()
    qdb = Questdb()
    ts.date = ts.date.astype(int).astype(str)
    ts.num = ts.num.astype(int).astype(str)
    qdb.write_via_csv(ts, f"minute_data_{kind}")
    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user2 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    user12 = round(user2 - user1, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user2}MBï¼Œæœ¬é¡¹æ›´æ–°æ¶ˆè€—æµé‡{user12}MB")


def database_update_minute_data_to_postgresql(kind: str) -> None:
    """ä½¿ç”¨ç±³ç­æ›´æ–°åˆ†é’Ÿæ•°æ®è‡³postgresqlä¸­

    Parameters
    ----------
    kind : str
        æ›´æ–°è‚¡ç¥¨åˆ†é’Ÿæ•°æ®æˆ–æŒ‡æ•°åˆ†é’Ÿæ•°æ®ï¼Œè‚¡ç¥¨åˆ™'stock'ï¼ŒæŒ‡æ•°åˆ™'index'

    Raises
    ------
    `IOError`
        å¦‚æœæœªæŒ‡å®šè‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼Œå°†æŠ¥é”™
    """
    if kind == "stock":
        code_type = "CS"
    elif kind == "index":
        code_type = "INDX"
    else:
        raise IOError("æ€»å¾—æŒ‡å®šä¸€ç§ç±»å‹å§ï¼Ÿè¯·ä»stockå’Œindexä¸­é€‰ä¸€ä¸ª")
    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user1 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user1}MB")
    # è·å–å…¨éƒ¨è‚¡ç¥¨/æŒ‡æ•°ä»£ç 
    cs = rqdatac.all_instruments(type=code_type, market="cn", date=None)
    codes = list(cs.order_book_id)
    # è·å–ä¸Šæ¬¡æ›´æ–°æˆªæ­¢æ—¶é—´
    qdb = Questdb()
    last_date = max(qdb.show_all_dates(f"minute_data_{kind}"))
    # æœ¬æ¬¡æ›´æ–°èµ·å§‹æ—¥æœŸ
    start_date = pd.Timestamp(str(last_date)) + pd.Timedelta(days=1)
    start_date = datetime.datetime.strftime(start_date, "%Y-%m-%d")
    # æœ¬æ¬¡æ›´æ–°ç»ˆæ­¢æ—¥æœŸ
    end_date = datetime.datetime.now()
    if end_date.hour < 17:
        end_date = end_date - pd.Timedelta(days=1)
    end_date = datetime.datetime.strftime(end_date, "%Y-%m-%d")
    logger.info(f"æœ¬æ¬¡å°†ä¸‹è½½ä»{start_date}åˆ°{end_date}çš„æ•°æ®")
    # ä¸‹è½½æ•°æ®
    ts = rqdatac.get_price(
        codes,
        start_date=start_date,
        end_date=end_date,
        frequency="1m",
        fields=["volume", "total_turnover", "high", "low", "close", "open"],
        adjust_type="none",
        skip_suspended=False,
        market="cn",
        expect_df=True,
        time_slice=None,
    )
    # è°ƒæ•´æ•°æ®æ ¼å¼
    ts = ts.reset_index()
    ts = ts.rename(
        columns={
            "order_book_id": "code",
            "datetime": "date",
            "volume": "amount",
            "total_turnover": "money",
        }
    )
    ts = ts.sort_values(["code", "date"])
    ts.date = ts.date.dt.strftime("%Y%m%d").astype(int)
    ts = ts.groupby(["code", "date"]).apply(
        lambda x: x.assign(num=list(range(1, x.shape[0] + 1)))
    )
    ts.code = ts.code.str.replace(".XSHE", ".SZ")
    ts.code = ts.code.str.replace(".XSHG", ".SH")
    # æ•°æ®å†™å…¥æ•°æ®åº“
    pgdb = PostgreSQL("minute_data")
    ts.to_sql(
        f"minute_data_{kind}",
        pgdb.engine,
        if_exists="append",
        index=False,
        dtype={
            "code": VARCHAR(9),
            "date": p.INT,
            "open": FLOAT,
            "high": FLOAT,
            "low": FLOAT,
            "close": FLOAT,
            "amount": FLOAT,
            "money": FLOAT,
            "num": INT,
        },
    )
    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user2 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    user12 = round(user2 - user1, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user2}MBï¼Œæœ¬é¡¹æ›´æ–°æ¶ˆè€—æµé‡{user12}MB")


def database_update_minute_data_to_questdb(kind: str) -> None:
    """ä½¿ç”¨ç±³ç­æ›´æ–°åˆ†é’Ÿæ•°æ®è‡³questdbä¸­

    Parameters
    ----------
    kind : str
        æ›´æ–°è‚¡ç¥¨åˆ†é’Ÿæ•°æ®æˆ–æŒ‡æ•°åˆ†é’Ÿæ•°æ®ï¼Œè‚¡ç¥¨åˆ™'stock'ï¼ŒæŒ‡æ•°åˆ™'index'

    Raises
    ------
    `IOError`
        å¦‚æœæœªæŒ‡å®šè‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼Œå°†æŠ¥é”™
    """
    if kind == "stock":
        code_type = "CS"
    elif kind == "index":
        code_type = "INDX"
    else:
        raise IOError("æ€»å¾—æŒ‡å®šä¸€ç§ç±»å‹å§ï¼Ÿè¯·ä»stockå’Œindexä¸­é€‰ä¸€ä¸ª")
    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user1 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user1}MB")
    # è·å–å…¨éƒ¨è‚¡ç¥¨/æŒ‡æ•°ä»£ç 
    cs = rqdatac.all_instruments(type=code_type, market="cn", date=None)
    codes = list(cs.order_book_id)
    # è·å–ä¸Šæ¬¡æ›´æ–°æˆªæ­¢æ—¶é—´
    qdb = Questdb()
    last_date = max(qdb.show_all_dates(f"minute_data_{kind}"))
    # æœ¬æ¬¡æ›´æ–°èµ·å§‹æ—¥æœŸ
    start_date = pd.Timestamp(str(last_date)) + pd.Timedelta(days=1)
    start_date = datetime.datetime.strftime(start_date, "%Y-%m-%d")
    # æœ¬æ¬¡æ›´æ–°ç»ˆæ­¢æ—¥æœŸ
    end_date = datetime.datetime.now()
    if end_date.hour < 17:
        end_date = end_date - pd.Timedelta(days=1)
    end_date = datetime.datetime.strftime(end_date, "%Y-%m-%d")
    logger.info(f"æœ¬æ¬¡å°†ä¸‹è½½ä»{start_date}åˆ°{end_date}çš„æ•°æ®")
    # ä¸‹è½½æ•°æ®
    ts = rqdatac.get_price(
        codes,
        start_date=start_date,
        end_date=end_date,
        frequency="1m",
        fields=["volume", "total_turnover", "high", "low", "close", "open"],
        adjust_type="none",
        skip_suspended=False,
        market="cn",
        expect_df=True,
        time_slice=None,
    )
    # è°ƒæ•´æ•°æ®æ ¼å¼
    ts = ts.reset_index()
    ts = ts.rename(
        columns={
            "order_book_id": "code",
            "datetime": "date",
            "volume": "amount",
            "total_turnover": "money",
        }
    )
    ts = ts.sort_values(["code", "date"])
    ts.date = ts.date.dt.strftime("%Y%m%d").astype(int)
    ts = ts.groupby(["code", "date"]).apply(
        lambda x: x.assign(num=list(range(1, x.shape[0] + 1)))
    )
    ts = ts.ffill().dropna()
    ts.code = ts.code.str.replace(".XSHE", ".SZ")
    ts.code = ts.code.str.replace(".XSHG", ".SH")
    ts.date = ts.date.astype(int).astype(str)
    ts.num = ts.num.astype(int).astype(str)
    # æ•°æ®å†™å…¥æ•°æ®åº“
    qdb = Questdb()
    qdb.write_via_csv(df, f"minute_data_{kind}")
    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user2 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    user12 = round(user2 - user1, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user2}MBï¼Œæœ¬é¡¹æ›´æ–°æ¶ˆè€—æµé‡{user12}MB")


def database_update_minute_data_to_mysql(kind: str) -> None:
    """ä½¿ç”¨ç±³ç­æ›´æ–°åˆ†é’Ÿæ•°æ®è‡³mmysqlä¸­

    Parameters
    ----------
    kind : str
        æ›´æ–°è‚¡ç¥¨åˆ†é’Ÿæ•°æ®æˆ–æŒ‡æ•°åˆ†é’Ÿæ•°æ®ï¼Œè‚¡ç¥¨åˆ™'stock'ï¼ŒæŒ‡æ•°åˆ™'index'

    Raises
    ------
    `IOError`
        å¦‚æœæœªæŒ‡å®šè‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼Œå°†æŠ¥é”™
    """
    if kind == "stock":
        code_type = "CS"
    elif kind == "index":
        code_type = "INDX"
    else:
        raise IOError("æ€»å¾—æŒ‡å®šä¸€ç§ç±»å‹å§ï¼Ÿè¯·ä»stockå’Œindexä¸­é€‰ä¸€ä¸ª")
    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user1 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user1}MB")
    # è·å–å…¨éƒ¨è‚¡ç¥¨/æŒ‡æ•°ä»£ç 
    cs = rqdatac.all_instruments(type=code_type, market="cn", date=None)
    codes = list(cs.order_book_id)
    # è·å–ä¸Šæ¬¡æ›´æ–°æˆªæ­¢æ—¶é—´
    # è¿æ¥2ä¸ªæ•°æ®åº“
    sqlsa = sqlConfig("minute_data_stock_alter")
    sqlia = sqlConfig("minute_data_index_alter")
    last_date = max(sqlsa.show_tables(full=False))
    # æœ¬æ¬¡æ›´æ–°èµ·å§‹æ—¥æœŸ
    start_date = pd.Timestamp(str(last_date)) + pd.Timedelta(days=1)
    start_date = datetime.datetime.strftime(start_date, "%Y-%m-%d")
    # æœ¬æ¬¡æ›´æ–°ç»ˆæ­¢æ—¥æœŸ
    end_date = datetime.datetime.now()
    if end_date.hour < 17:
        end_date = end_date - pd.Timedelta(days=1)
    end_date = datetime.datetime.strftime(end_date, "%Y-%m-%d")
    logger.info(f"æœ¬æ¬¡å°†ä¸‹è½½ä»{start_date}åˆ°{end_date}çš„æ•°æ®")
    # ä¸‹è½½æ•°æ®
    ts = rqdatac.get_price(
        codes,
        start_date=start_date,
        end_date=end_date,
        frequency="1m",
        fields=["volume", "total_turnover", "high", "low", "close", "open"],
        adjust_type="none",
        skip_suspended=False,
        market="cn",
        expect_df=True,
        time_slice=None,
    )
    # è°ƒæ•´æ•°æ®æ ¼å¼
    ts = ts.reset_index()
    ts = ts.rename(
        columns={
            "order_book_id": "code",
            "datetime": "date",
            "volume": "amount",
            "total_turnover": "money",
        }
    )
    ts = ts.sort_values(["code", "date"])
    ts.date = ts.date.dt.strftime("%Y%m%d").astype(int)
    ts = ts.groupby(["code", "date"]).apply(
        lambda x: x.assign(num=list(range(1, x.shape[0] + 1)))
    )
    ts.code = ts.code.str.replace(".XSHE", ".SZ")
    ts.code = ts.code.str.replace(".XSHG", ".SH")
    codes = list(set(ts.code))
    dates = list(set(ts.date))
    # æ•°æ®å†™å…¥æ•°æ®åº“
    fails = []
    # è‚¡ç¥¨
    if kind == "stock":
        # æŠŠæ¯å¤©å†™å…¥æ¯å¤©æ‰€æœ‰è‚¡ç¥¨ä¸€å¼ è¡¨
        for date in dates:
            dfi = ts[ts.date == date]
            try:
                dfi.drop(columns=["date"]).to_sql(
                    name=str(date),
                    con=sqlsa.engine,
                    if_exists="append",
                    index=False,
                    dtype={
                        "code": VARCHAR(9),
                        "open": FLOAT,
                        "high": FLOAT,
                        "low": FLOAT,
                        "close": FLOAT,
                        "amount": FLOAT,
                        "money": FLOAT,
                        "num": INT,
                    },
                )
            except Exception:
                try:
                    if sqlsa.get_data(date).shape[0] == 0:
                        dfi.drop(columns=["date"]).to_sql(
                            name=str(date),
                            con=sqlsa.engine,
                            if_exists="replace",
                            index=False,
                            dtype={
                                "code": VARCHAR(9),
                                "open": FLOAT,
                                "high": FLOAT,
                                "low": FLOAT,
                                "close": FLOAT,
                                "amount": FLOAT,
                                "money": FLOAT,
                                "num": INT,
                            },
                        )
                except Exception:
                    fails.append(date)
                    logger.warning(f"è‚¡ç¥¨{date}å†™å…¥å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥")
    # æŒ‡æ•°
    else:
        # æŠŠæ¯å¤©å†™å…¥æ¯å¤©æ‰€æœ‰æŒ‡æ•°ä¸€å¼ è¡¨
        for date in dates:
            dfi = ts[ts.date == date]
            try:
                dfi.drop(columns=["date"]).to_sql(
                    name=str(date),
                    con=sqlia.engine,
                    if_exists="append",
                    index=False,
                    dtype={
                        "code": VARCHAR(9),
                        "open": FLOAT,
                        "high": FLOAT,
                        "low": FLOAT,
                        "close": FLOAT,
                        "amount": FLOAT,
                        "money": FLOAT,
                        "num": INT,
                    },
                )
            except Exception:
                try:
                    if sqlia.get_data(date).shape[0] == 0:
                        dfi.drop(columns=["date"]).to_sql(
                            name=str(date),
                            con=sqlia.engine,
                            if_exists="replace",
                            index=False,
                            dtype={
                                "code": VARCHAR(9),
                                "open": FLOAT,
                                "high": FLOAT,
                                "low": FLOAT,
                                "close": FLOAT,
                                "amount": FLOAT,
                                "money": FLOAT,
                                "num": INT,
                            },
                        )
                except Exception:
                    fails.append(date)
                    logger.warning(f"æŒ‡æ•°{date}å†™å…¥å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥")

    # è·å–å‰©ä½™ä½¿ç”¨é¢
    user2 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    user12 = round(user2 - user1, 2)
    logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user2}MBï¼Œæœ¬é¡¹æ›´æ–°æ¶ˆè€—æµé‡{user12}MB")


@retry
def download_single_daily(day):
    """æ›´æ–°å•æ—¥çš„æ•°æ®"""
    try:
        # 8ä¸ªä»·æ ¼ï¼Œäº¤æ˜“çŠ¶æ€ï¼Œæˆäº¤é‡ï¼Œ
        df1 = pro.a_daily(
            trade_date=day,
            fields=[
                "code",
                "trade_date",
                "open",
                "high",
                "low",
                "close",
                "volume",
                "adjopen",
                "adjclose",
                "adjhigh",
                "adjlow",
                "tradestatus",
            ],
        )
        # æ¢æ‰‹ç‡ï¼Œæµé€šè‚¡æœ¬ï¼Œæ¢æ‰‹ç‡è¦é™¤ä»¥100ï¼Œæµé€šè‚¡æœ¬è¦ä¹˜ä»¥10000
        df2 = pro.daily_basic(
            trade_date=day,
            fields=["ts_code", "trade_date", "turnover_rate_f", "float_share"],
        )
        time.sleep(1)
        return df1, df2
    except Exception:
        time.sleep(60)
        # 8ä¸ªä»·æ ¼ï¼Œäº¤æ˜“çŠ¶æ€ï¼Œæˆäº¤é‡ï¼Œ
        df1 = pro.a_daily(
            trade_date=day,
            fields=[
                "code",
                "trade_date",
                "open",
                "high",
                "low",
                "close",
                "volume",
                "adjopen",
                "adjclose",
                "adjhigh",
                "adjlow",
                "tradestatus",
            ],
        )
        # æ¢æ‰‹ç‡ï¼Œæµé€šè‚¡æœ¬ï¼Œæ¢æ‰‹ç‡è¦é™¤ä»¥100ï¼Œæµé€šè‚¡æœ¬è¦ä¹˜ä»¥10000
        df2 = pro.daily_basic(
            trade_date=day,
            fields=["ts_code", "trade_date", "turnover_rate_f", "float_share"],
        )
        time.sleep(1)
        return df1, df2


@retry
def download_calendar(startdate, enddate):
    """æ›´æ–°å•æ—¥çš„æ•°æ®"""
    try:
        # äº¤æ˜“æ—¥å†
        df0 = pro.a_calendar(start_date=startdate, end_date=enddate)
        time.sleep(1)
        return df0
    except Exception:
        time.sleep(60)
        # äº¤æ˜“æ—¥å†
        df0 = pro.a_calendar(start_date=startdate, end_date=enddate)
        time.sleep(1)
        return df0


def database_update_daily_files(startdate: str = None, enddate: str = None) -> None:
    """æ›´æ–°æ•°æ®åº“ä¸­çš„æ—¥é¢‘æ•°æ®

    Parameters
    ----------
    startdate : str, optional
        å½¢å¦‚'20220501'ï¼Œä¸å¡«å†™å°†è‡ªåŠ¨è¯†åˆ«, by default None
    enddate : str, optional
        å½¢å¦‚'20220701'ï¼Œä¸å¡«å†™å°†è‡ªåŠ¨è¯†åˆ«, by default None

    Raises
    ------
    `ValueError`
        å¦‚æœä¸Šæ¬¡æ›´æ–°åˆ°æœ¬æ¬¡æ›´æ–°æ²¡æœ‰æ–°çš„äº¤æ˜“æ—¥ï¼Œå°†æŠ¥é”™
    """
    read_daily.clear_cache()
    homeplace = HomePlace()
    config = pickledb.load(homeplace.update_data_file + "database_config.db", False)
    if startdate:
        ...
    else:
        startdate = config.get("daily_enddate")
        logger.info(
            f"ä¸Šæ¬¡æ›´æ–°åˆ°{datetime.datetime.strftime(pd.Timestamp(startdate)-pd.Timedelta(days=1),format='%Y-%m-%d')}"
        )
    if enddate:
        ...
    else:
        enddate = datetime.datetime.now()
        if enddate.hour < 17:
            enddate = enddate - pd.Timedelta(days=1)
        else:
            ...
        enddate = datetime.datetime.strftime(enddate, "%Y%m%d")
        logger.info(
            f"æœ¬æ¬¡å°†æ›´æ–°åˆ°{datetime.datetime.strftime(pd.Timestamp(enddate),format='%Y-%m-%d')}"
        )
    # äº¤æ˜“æ—¥å†
    df0 = download_calendar(startdate, enddate)
    tradedates = sorted(list(set(df0.trade_date)))
    if len(tradedates) > 1:
        # å­˜å‚¨æ¯å¤©æ•°æ®
        df1s = []
        df2s = []
        for day in tqdm.tqdm(tradedates, desc="æ­£åœ¨ä¸‹è½½æ—¥é¢‘æ•°æ®"):
            df1, df2 = download_single_daily(day)
            df1s.append(df1)
            df2s.append(df2)
        # 8ä¸ªä»·æ ¼ï¼Œäº¤æ˜“çŠ¶æ€ï¼Œæˆäº¤é‡ï¼Œ
        df1s = pd.concat(df1s)
        # æ¢æ‰‹ç‡ï¼Œæµé€šè‚¡æœ¬ï¼Œæ¢æ‰‹ç‡è¦é™¤ä»¥100ï¼Œæµé€šè‚¡æœ¬è¦ä¹˜ä»¥10000
        df2s = pd.concat(df2s)
    elif len(tradedates) == 1:
        df1s, df2s = download_single_daily(tradedates[0])
    else:
        raise ValueError("ä»ä¸Šæ¬¡æ›´æ–°åˆ°è¿™æ¬¡æ›´æ–°ï¼Œè¿˜æ²¡æœ‰ç»è¿‡äº¤æ˜“æ—¥ã€‚æ”¾å‡å°±å¥½å¥½ä¼‘æ¯å§ï¼Œåˆ«è·‘ä»£ç äº†ğŸ¤’")
    df1s.tradestatus = (df1s.tradestatus == "äº¤æ˜“") + 0
    df2s = df2s.rename(columns={"ts_code": "code"})
    df1s.trade_date = df1s.trade_date.apply(int)
    df2s.trade_date = df2s.trade_date.apply(int)
    both_codes = list(set(df1s.code) & set(df2s.code))
    df1s = df1s[df1s.code.isin(both_codes)]
    df2s = df2s[df2s.code.isin(both_codes)]
    # stè‚¡
    df3 = pro.ashare_st()

    suffix_path = homeplace.update_data_file
    suffix_path_save = homeplace.daily_data_file
    codes = list(scio.loadmat(suffix_path_save + "AllStockCode.mat").values())[3]
    codes = [i[0] for i in codes[0]]
    days = list(scio.loadmat(suffix_path_save + "TradingDate_Daily.mat").values())[3]
    days = [i[0] for i in days]

    def read_mat(path):
        col = list(scio.loadmat(suffix_path_save + "AllStockCode.mat").values())[3]
        index = list(scio.loadmat(suffix_path_save + "TradingDate_Daily.mat").values())[
            3
        ]
        col = [i[0] for i in col[0]]
        if len(index) > 1000:
            index = [i[0] for i in index]
        else:
            index = index[0]
        path = suffix_path_save + path
        data = list(scio.loadmat(path).values())[3]
        data = pd.DataFrame(data, index=index, columns=col)
        return data

    def to_mat(df, row, filename=None, ind="trade_date", col="code"):
        df = df[[ind, col, row]].set_index([ind, col])
        df = df.unstack()
        df.columns = [i[1] for i in list(df.columns)]
        old = read_mat(filename)
        new = pd.concat([old, df])
        scio.savemat(
            suffix_path_save + filename, {"data": new.to_numpy()}, do_compression=True
        )
        logger.success(filename + "å·²æ›´æ–°")
        return new

    # è‚¡ç¥¨æ—¥è¡Œæƒ…ï¼ˆæœªå¤æƒé«˜å¼€ä½æ”¶ï¼Œå¤æƒé«˜å¼€ä½æ”¶ï¼Œäº¤æ˜“çŠ¶æ€ï¼Œæˆäº¤é‡ï¼‰
    part1 = df1s.copy()
    # æœªå¤æƒå¼€ç›˜ä»·
    opens = to_mat(part1, "open", "AllStock_DailyOpen.mat")
    # æœªå¤æƒæœ€é«˜ä»·
    highs = to_mat(part1, "high", "AllStock_DailyHigh.mat")
    # æœªå¤æƒæœ€ä½ä»·
    lows = to_mat(part1, "low", "AllStock_DailyLow.mat")
    # æœªå¤æƒæ”¶ç›˜ä»·
    closes = to_mat(part1, "close", "AllStock_DailyClose.mat")
    # æˆäº¤é‡
    volumes = to_mat(part1, "volume", "AllStock_DailyVolume.mat")
    # å¤æƒå¼€ç›˜ä»·
    diopens = to_mat(part1, "adjopen", "AllStock_DailyOpen_dividend.mat")
    # å¤æƒæœ€é«˜ä»·
    dihighs = to_mat(part1, "adjhigh", "AllStock_DailyHigh_dividend.mat")
    # å¤æƒæœ€ä½ä»·
    dilows = to_mat(part1, "adjlow", "AllStock_DailyLow_dividend.mat")
    # å¤æƒæ”¶ç›˜ä»·
    dicloses = to_mat(part1, "adjclose", "AllStock_DailyClose_dividend.mat")
    # äº¤æ˜“çŠ¶æ€
    status = to_mat(part1, "tradestatus", "AllStock_DailyStatus.mat")

    # æ¢æ‰‹ç‡
    part2 = df2s[["trade_date", "code", "turnover_rate_f"]]
    part2 = part2.set_index(["trade_date", "code"]).unstack()
    part2.columns = [i[1] for i in list(part2.columns)]
    part2 = part2 / 100
    part2_old = read_mat("AllStock_DailyTR.mat")
    part2_new = pd.concat([part2_old, part2])
    # part2_new=part2_new.dropna(how='all',axis=1)
    part2_new = part2_new[closes.columns]
    scio.savemat(
        suffix_path_save + "AllStock_DailyTR.mat",
        {"data": part2_new.to_numpy()},
        do_compression=True,
    )
    logger.success("æ¢æ‰‹ç‡æ›´æ–°å®Œæˆ")

    # #äº¤æ˜“æ—¥å†å’Œè‚¡ç¥¨ä»£ç 
    # part2_new=part2_new.reset_index()
    # part2_new.columns=['date']+list(part2_new.columns)[1:]
    # part2_new.to_feather('æ—¥å†ä¸ä»£ç æš‚å­˜.feather')

    # æµé€šè‚¡æ•°
    # è¯»å–æ–°çš„æµé€šè‚¡å˜åŠ¨æ•°
    part3 = df2s[["trade_date", "code", "float_share"]]
    part3 = part3.set_index(["trade_date", "code"]).unstack()
    part3.columns = [i[1] for i in list(part3.columns)]
    part3 = part3 * 10000
    part3_old = read_mat("AllStock_DailyAShareNum.mat")
    part3_new = pd.concat([part3_old, part3])
    # part2_new=part2_new.dropna(how='all',axis=1)
    part3_new = part3_new[closes.columns]
    scio.savemat(
        suffix_path_save + "AllStock_DailyAShareNum.mat",
        {"data": part3_new.to_numpy()},
        do_compression=True,
    )
    logger.success("æµé€šè‚¡æ•°æ›´æ–°å®Œæˆ")

    # st
    part4 = df3[["s_info_windcode", "entry_dt", "remove_dt"]]
    part4 = part4.sort_values("s_info_windcode")
    part4.remove_dt = part4.remove_dt.fillna(enddate).astype(int)
    part4 = part4.set_index("s_info_windcode").stack()
    part4 = part4.reset_index().assign(
        he=sorted(list(range(int(part4.shape[0] / 2))) * 2)
    )
    part4 = part4.drop(columns=["level_1"])
    part4.columns = ["code", "date", "he"]
    part4.date = pd.to_datetime(part4.date, format="%Y%m%d")

    def single(df):
        full = pd.DataFrame({"date": pd.date_range(df.date.min(), df.date.max())})
        df = pd.merge(full, df, on=["date"], how="left")
        df = df.fillna(method="ffill")
        return df

    tqdm.tqdm.pandas()
    part4 = part4.groupby(["code", "he"]).progress_apply(single)
    part4.date = part4.date.dt.strftime("%Y%m%d").astype(int)
    part4 = part4[part4.date.isin(list(part2_new.index))]
    part4 = part4.reset_index(drop=True)
    part4 = part4.assign(st=1)

    part4 = part4.drop_duplicates(subset=["date", "code"]).pivot(
        index="date", columns="code", values="st"
    )

    part4_0 = pd.DataFrame(0, columns=part2_new.columns, index=part2_new.index)
    part4_0 = part4_0 + part4
    # old=read_mat('AllStock_DailyST.mat')
    # part4_new=pd.concat([old,part4_0])
    part4_0 = part4_0.replace(np.nan, 0)
    part4_0 = part4_0[part4_0.index.isin(list(part2_new.index))]
    part4_0 = part4_0.T
    part4_0 = part4_0[part4_0.index.isin(list(part2_new.columns))]
    part4_0 = part4_0.T
    # part4_0=part4_0.dropna(how='all',axis=1)
    part4_0 = part4_0[closes.columns]
    scio.savemat(
        suffix_path_save + "AllStock_DailyST.mat",
        {"data": part4_0.to_numpy()},
        do_compression=True,
    )
    logger.success("stæ›´æ–°å®Œäº†")

    # ä¸Šå¸‚å¤©æ•°
    part5_close = pd.read_feather(suffix_path + "BasicFactor_Close.txt").set_index(
        "index"
    )
    part5_close = part5_close[part5_close.index < 20040101]
    part5_close = pd.concat([part5_close, closes])
    # part5_close.reset_index().to_feather(suffix_path+'BasicFactor_Close.txt')
    part5 = np.sign(part5_close).fillna(method="ffill").cumsum()
    part5 = part5[part5.index.isin(list(part2_new.index))]
    part5 = part5.T
    part5 = part5[part5.index.isin(list(part2_new.columns))]
    part5 = part5.T
    # part5=part5.dropna(how='all',axis=1)
    part5 = part5[closes.columns]
    scio.savemat(
        suffix_path_save + "AllStock_DailyListedDate.mat",
        {"data": part5.to_numpy()},
        do_compression=True,
    )
    logger.success("ä¸Šå¸‚å¤©æ•°æ›´æ–°å®Œäº†")

    # äº¤æ˜“æ—¥å†å’Œè‚¡ç¥¨ä»£ç 
    scio.savemat(
        suffix_path_save + "TradingDate_Daily.mat",
        {"data": part2_new.index.to_numpy()},
        do_compression=True,
    )
    scio.savemat(
        suffix_path_save + "AllStockCode.mat",
        {"data": part2_new.columns.to_numpy()},
        do_compression=True,
    )
    enddate = pd.Timestamp(enddate) + pd.Timedelta(days=1)
    enddate = datetime.datetime.strftime(enddate, "%Y%m%d")
    config.set("daily_enddate", enddate)
    config.dump()
    logger.success("äº¤æ˜“æ—¥å†å’Œè‚¡ç¥¨ä»£ç æ›´æ–°å®Œäº†")
    read_daily.clear_cache()
    logger.success(
        f"æ—¥é¢‘æ•°æ®å·²æ›´æ–°ï¼Œç°åœ¨æœ€æ–°çš„æ˜¯{datetime.datetime.strftime(pd.Timestamp(enddate)-pd.Timedelta(days=1),format='%Y-%m-%d')}"
    )


@retry
def download_single_day_style(day):
    """æ›´æ–°å•æ—¥çš„æ•°æ®"""
    try:
        style = pro.RMExposureDayGet(
            trade_date=str(day),
            fields="tradeDate,ticker,BETA,MOMENTUM,SIZE,EARNYILD,RESVOL,GROWTH,BTOP,LEVERAGE,LIQUIDTY,SIZENL",
        )
        time.sleep(1)
        return style
    except Exception:
        time.sleep(60)
        style = pro.RMExposureDayGet(
            trade_date=str(day),
            fields="tradeDate,ticker,BETA,MOMENTUM,SIZE,EARNYILD,RESVOL,GROWTH,BTOP,LEVERAGE,LIQUIDTY,SIZENL",
        )
        time.sleep(1)
        return style


def database_update_barra_files():
    fs = os.listdir(homeplace.barra_data_file)[0]
    fs = pd.read_feather(homeplace.barra_data_file + fs)
    fs.columns = ["date"] + list(fs.columns)[1:]
    fs = fs.set_index("date")
    last_date = fs.index.max()
    last_date = datetime.datetime.strftime(last_date, "%Y%m%d")
    now = datetime.datetime.now()
    if now.hour < 17:
        now = now - pd.Timedelta(days=1)
    now = datetime.datetime.strftime(now, "%Y%m%d")
    logger.info(f"é£æ ¼æš´éœ²æ•°æ®ä¸Šæ¬¡æ›´æ–°åˆ°{last_date}ï¼Œæœ¬æ¬¡å°†æ›´æ–°åˆ°{now}")
    df0 = download_calendar(last_date, now)
    tradedates = sorted(list(set(df0.trade_date)))
    style_names = [
        "beta",
        "momentum",
        "size",
        "residualvolatility",
        "earningsyield",
        "growth",
        "booktoprice",
        "leverage",
        "liquidity",
        "nonlinearsize",
    ]
    ds = {k: [] for k in style_names}
    for t in tqdm.tqdm(tradedates):
        style = download_single_day_style(t)
        style.columns = style.columns.str.lower()
        style = style.rename(
            columns={
                "earnyild": "earningsyield",
                "tradedate": "date",
                "ticker": "code",
                "resvol": "residualvolatility",
                "btop": "booktoprice",
                "sizenl": "nonlinearsize",
                "liquidty": "liquidity",
            }
        )
        style.date = pd.to_datetime(style.date, format="%Y%m%d")
        style.code = style.code.apply(add_suffix)
        sts = list(style.columns)[2:]
        for s in sts:
            ds[s].append(style.pivot(columns="code", index="date", values=s))
    for k, v in ds.items():
        old = pd.read_feather(homeplace.barra_data_file + k + ".feather")
        old.columns = ["date"] + list(old.columns)[1:]
        old = old.set_index("date")
        new = pd.concat(v)
        new = pd.concat([old, new])
        new.reset_index().to_feather(homeplace.barra_data_file + k + ".feather")
    logger.success(f"é£æ ¼æš´éœ²æ•°æ®å·²ç»æ›´æ–°åˆ°{now}")


"""æ›´æ–°300ã€500ã€1000è¡Œæƒ…æ•°æ®"""


@retry
def download_single_index(index_code: str):
    if index_code == "000300.SH":
        file = "æ²ªæ·±300"
    elif index_code == "000905.SH":
        file = "ä¸­è¯500"
    elif index_code == "000852.SH":
        file = "ä¸­è¯1000"
    else:
        file = index_code
    try:
        df = (
            pro.index_daily(ts_code=index_code)
            .sort_values("trade_date")
            .rename(columns={"trade_date": "date"})
        )
        df = df[["date", "close"]]
        df.date = pd.to_datetime(df.date, format="%Y%m%d")
        df = df.set_index("date")
        df = df.resample("M").last()
        df.columns = [file]
        return df
    except Exception:
        logger.warning("å°ipäº†ï¼Œè¯·ç­‰å¾…1åˆ†é’Ÿ")
        time.sleep(60)
        df = (
            pro.index_daily(ts_code=index_code)
            .sort_values("trade_date")
            .rename(columns={"trade_date": "date"})
        )
        df = df[["date", "close"]]
        df.date = pd.to_datetime(df.date, format="%Y%m%d")
        df = df.set_index("date")
        df = df.resample("M").last()
        df.columns = [file]
        return df


def database_update_index_three():
    """è¯»å–ä¸‰å¤§æŒ‡æ•°çš„åŸå§‹è¡Œæƒ…æ•°æ®ï¼Œè¿”å›å¹¶ä¿å­˜åœ¨æœ¬åœ°"""
    hs300 = download_single_index("000300.SH")
    zz500 = download_single_index("000905.SH")
    zz1000 = download_single_index("000852.SH")
    res = pd.concat([hs300, zz500, zz1000], axis=1)
    new_date = datetime.datetime.strftime(res.index.max(), "%Y%m%d")
    res.reset_index().to_feather(homeplace.daily_data_file + "3510è¡Œæƒ….feather")
    logger.success(f"3510è¡Œæƒ…æ•°æ®å·²ç»æ›´æ–°è‡³{new_date}")


"""æ›´æ–°ç”³ä¸‡ä¸€çº§è¡Œä¸šçš„è¡Œæƒ…"""


@retry
def download_single_industry_price(ind):
    try:
        df = pro.swindex_daily(code=ind)[["trade_date", "close"]].set_index(
            "trade_date"
        )
        df.columns = [ind]
        time.sleep(1)
        return df
    except Exception:
        time.sleep(60)
        df = pro.swindex_daily(code=ind)[["trade_date", "close"]].set_index(
            "trade_date"
        )
        df.columns = [ind]
        time.sleep(1)
        return df


def database_update_swindustry_prices():
    indus = []
    for ind in list(INDUS_DICT.keys()):
        df = download_single_industry_price(ind=ind)
        indus.append(df)
    indus = pd.concat(indus, axis=1).reset_index()
    indus.columns = ["date"] + list(indus.columns)[1:]
    indus = indus.set_index("date")
    indus = indus.dropna()
    indus.index = pd.to_datetime(indus.index, format="%Y%m%d")
    indus = indus.sort_index()
    indus.reset_index().to_feather(homeplace.daily_data_file + "ç”³ä¸‡å„è¡Œä¸šè¡Œæƒ…æ•°æ®.feather")
    new_date = datetime.datetime.strftime(indus.index.max(), "%Y%m%d")
    logger.success(f"ç”³ä¸‡ä¸€çº§è¡Œä¸šçš„è¡Œæƒ…æ•°æ®å·²ç»æ›´æ–°è‡³{new_date}")


def database_update_zxindustry_prices():
    zxinds = ZXINDUS_DICT
    now = datetime.datetime.now()
    zxprices = []
    for k in list(zxinds.keys()):
        ind = rqdatac.get_price(
            k, start_date="2010-01-01", end_date=now, fields="close"
        )
        ind = (
            ind.rename(columns={"close": zxinds[k]})
            .reset_index(level=1)
            .reset_index(drop=True)
        )
        zxprices.append(ind)
    zxprice = reduce(lambda x, y: pd.merge(x, y, on=["date"], how="outer"), zxprices)
    zxprice.reset_index(drop=True).to_feather(
        homeplace.daily_data_file + "ä¸­ä¿¡å„è¡Œä¸šè¡Œæƒ…æ•°æ®.feather"
    )
    new_date = datetime.datetime.strftime(zxprice.date.max(), "%Y%m%d")
    logger.success(f"ä¸­ä¿¡ä¸€çº§è¡Œä¸šçš„è¡Œæƒ…æ•°æ®å·²ç»æ›´æ–°è‡³{new_date}")


"""æ›´æ–°ç”³ä¸‡ä¸€çº§è¡Œä¸šå“‘å˜é‡"""


@retry
def download_single_industry_member(ind):
    try:
        df = pro.index_member(index_code=ind)
        # time.sleep(1)
        return df
    except Exception:
        time.sleep(60)
        df = pro.index_member(index_code=ind)
        time.sleep(1)
        return df


def database_update_industry_member():
    dfs = []
    for ind in tqdm.tqdm(INDUS_DICT.keys()):
        ff = download_single_industry_member(ind)
        ff = ç”Ÿæˆæ¯æ—¥åˆ†ç±»è¡¨(ff, "con_code", "in_date", "out_date", "index_code")
        khere = ff.index_code.iloc[0]
        ff = ff.assign(khere=1)
        ff.columns = ["code", "index_code", "date", khere]
        ff = ff.drop(columns=["index_code"])
        dfs.append(ff)
    res = pd.merge(dfs[0], dfs[1], on=["code", "date"])
    dfs = pd.concat(dfs)
    trs = read_daily(tr=1, start=20040101)
    dfs = dfs[dfs.date.isin(trs.index)]
    dfs = dfs.sort_values(["date", "code"])
    dfs = dfs[["date", "code"] + list(dfs.columns)[2:]]
    dfs = dfs.fillna(0)
    dfs.reset_index(drop=True).to_feather(
        homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.feather"
    )
    new_date = dfs.date.max()
    new_date = datetime.datetime.strftime(new_date, "%Y%m%d")
    logger.success(f"ç”³ä¸‡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡(å“‘å˜é‡)å·²ç»æ›´æ–°è‡³{new_date}")


@retry
def download_single_index_member_monthly(code):
    file = homeplace.daily_data_file + INDEX_DICT[code] + "æœˆæˆåˆ†è‚¡.feather"
    old = pd.read_feather(file).set_index("index")
    old_date = old.index.max()
    start_date = old_date + pd.Timedelta(days=1)
    end_date = datetime.datetime.now()
    if start_date >= end_date:
        logger.info(f"{INDEX_DICT[code]}æœˆæˆåˆ†è‚¡æ— éœ€æ›´æ–°ï¼Œä¸Šæ¬¡å·²ç»æ›´æ–°åˆ°äº†{start_date}")
    else:
        start_date, end_date = datetime.datetime.strftime(
            start_date, "%Y%m%d"
        ), datetime.datetime.strftime(end_date, "%Y%m%d")
        logger.info(f"{INDEX_DICT[code]}æœˆæˆåˆ†è‚¡ä¸Šæ¬¡æ›´æ–°åˆ°{start_date},æœ¬æ¬¡å°†æ›´æ–°åˆ°{end_date}")
        try:
            a = pro.index_weight(
                index_code=code, start_date=start_date, end_date=end_date
            )
            if a.shape[0] == 0:
                logger.info(f"{INDEX_DICT[code]}æœˆæˆåˆ†è‚¡æ— éœ€æ›´æ–°ï¼Œä¸Šæ¬¡å·²ç»æ›´æ–°åˆ°äº†{start_date}")
            else:
                time.sleep(1)
                a.trade_date = pd.to_datetime(a.trade_date, format="%Y%m%d")
                a = a.sort_values("trade_date").set_index("trade_date")
                a = (
                    a.groupby("con_code")
                    .resample("M")
                    .last()
                    .drop(columns=["con_code"])
                    .reset_index()
                )
                a = a.assign(num=1)
                a = (
                    a[["trade_date", "con_code", "num"]]
                    .rename(columns={"trade_date": "date", "con_code": "code"})
                    .pivot(columns="code", index="date", values="num")
                )
                a = pd.concat([old, a]).fillna(0)
                a.reset_index().to_feather(file)
                logger.success(f"å·²å°†{INDEX_DICT[code]}æœˆæˆåˆ†è‚¡æ›´æ–°è‡³{end_date}")
        except Exception:
            time.sleep(60)
            a = pro.index_weight(
                index_code=code, start_date=start_date, end_date=end_date
            )
            if a.shape[0] == 0:
                logger.info(f"{INDEX_DICT[code]}æœˆæˆåˆ†è‚¡æ— éœ€æ›´æ–°ï¼Œä¸Šæ¬¡å·²ç»æ›´æ–°åˆ°äº†{start_date}")
            else:
                time.sleep(1)
                a.trade_date = pd.to_datetime(a.trade_date, format="%Y%m%d")
                a = a.sort_values("trade_date").set_index("trade_date")
                a = (
                    a.groupby("con_code")
                    .resample("M")
                    .last()
                    .drop(columns=["con_code"])
                    .reset_index()
                )
                a = a.assign(num=1)
                a = (
                    a[["trade_date", "con_code", "num"]]
                    .rename(columns={"trade_date": "date", "con_code": "code"})
                    .pivot(columns="code", index="date", values="num")
                )
                a = pd.concat([old, a]).fillna(0)
                a.reset_index().to_feather(file)
                logger.success(f"å·²å°†{INDEX_DICT[code]}æœˆæˆåˆ†è‚¡æ›´æ–°è‡³{end_date}")


def database_update_index_members_monthly():
    for k in list(INDEX_DICT.keys()):
        download_single_index_member_monthly(k)


def download_single_index_member(code):
    file = homeplace.daily_data_file + INDEX_DICT[code] + "æ—¥æˆåˆ†è‚¡.feather"
    if code.endswith(".SH"):
        code = code[:6] + ".XSHG"
    elif code.endswith(".SZ"):
        code = code[:6] + ".XSHE"
    now = datetime.datetime.now()
    df = rqdatac.index_components(
        code, start_date="20100101", end_date=now, market="cn"
    )
    ress = []
    for k, v in df.items():
        res = pd.DataFrame(1, index=[pd.Timestamp(k)], columns=v)
        ress.append(res)
    ress = pd.concat(ress)
    ress.columns = [convert_code(i)[0] for i in list(ress.columns)]
    tr = np.sign(read_daily(tr=1, start=20100101))
    tr = np.sign(tr + ress)
    now_str = datetime.datetime.strftime(now, "%Y%m%d")
    tr.reset_index().to_feather(file)
    logger.success(f"å·²å°†{INDEX_DICT[convert_code(code)[0]]}æ—¥æˆåˆ†è‚¡æ›´æ–°è‡³{now_str}")


def database_update_index_members():
    for k in list(INDEX_DICT.keys()):
        download_single_index_member(k)


def database_save_final_factors(df: pd.DataFrame, name: str, order: int) -> None:
    """ä¿å­˜æœ€ç»ˆå› å­çš„å› å­å€¼

    Parameters
    ----------
    df : pd.DataFrame
        æœ€ç»ˆå› å­å€¼
    name : str
        å› å­çš„åå­—ï¼Œå¦‚â€œé€‚åº¦å†’é™©â€
    order : int
        å› å­çš„åºå·
    """
    homeplace = HomePlace()
    path = homeplace.final_factor_file + name + "_" + "å¤šå› å­" + str(order) + ".feather"
    df.reset_index().to_feather(path)
    final_date = df.index.max()
    final_date = datetime.datetime.strftime(final_date, "%Y%m%d")
    config = pickledb.load(homeplace.update_data_file + "database_config.db", False)
    config.set("data_refresh", "done")
    config.dump()
    logger.success(f"ä»Šæ—¥è®¡ç®—çš„å› å­å€¼ä¿å­˜ï¼Œæœ€æ–°ä¸€å¤©ä¸º{final_date}")


@retry
def download_single_day_money_flow(
    code: str, start_date: str, end_date: str
) -> pd.DataFrame:
    try:
        df = pro.ashare_moneyflow(
            code=code,
            start_date=start_date,
            end_date=end_date,
            fields="trade_dt,buy_value_exlarge_order,sell_value_exlarge_order,buy_value_large_order,sell_value_large_order,buy_value_med_order,sell_value_med_order,buy_value_small_order,sell_value_small_order",
        ).iloc[::-1, :]
        if df.shape[0] > 0:
            df = df.assign(code=code)
            return df
        time.sleep(0.1)
    except Exception:
        time.sleep(60)
        df = pro.ashare_moneyflow(
            code=code,
            start_date=start_date,
            end_date=end_date,
            fields="trade_dt,buy_value_exlarge_order,sell_value_exlarge_order,buy_value_large_order,sell_value_large_order,buy_value_med_order,sell_value_med_order,buy_value_small_order,sell_value_small_order",
        ).iloc[::-1, :]
        if df.shape[0] > 0:
            df = df.assign(code=code)
            return df
        time.sleep(0.1)


def database_update_money_flow():
    trs = read_daily(tr=1)
    codes = sorted(list(trs.columns))
    codes = [i for i in codes if i[0] != "8"]
    old = read_money_flow(buy=1, small=1)
    old_enddate = old.index.max()
    old_enddate_str = datetime.datetime.strftime(old_enddate, "%Y%m%d")
    new_trs = trs[trs.index > old_enddate]
    start_date = new_trs.index.min()
    start_date_str = datetime.datetime.strftime(start_date, "%Y%m%d")
    now = datetime.datetime.now()
    now_str = datetime.datetime.strftime(now, "%Y%m%d")
    logger.info(f"ä¸Šæ¬¡èµ„é‡‘æµæ•°æ®æ›´æ–°åˆ°{old_enddate_str}ï¼Œæœ¬æ¬¡å°†ä»{start_date_str}æ›´æ–°åˆ°{now_str}")
    dfs = []
    for code in tqdm.tqdm(codes):
        df = download_single_day_money_flow(
            code=code, start_date=start_date_str, end_date=now_str
        )
        dfs.append(df)
    dfs = pd.concat(dfs)
    dfs = dfs.rename(columns={"trade_dt": "date"})
    dfs.date = pd.to_datetime(dfs.date, format="%Y%m%d")
    ws = [i for i in list(dfs.columns) if i not in ["date", "code"]]
    for w in ws:
        old = pd.read_feather(
            homeplace.daily_data_file + w[:-6] + ".feather"
        ).set_index("date")
        new = dfs.pivot(index="date", columns="code", values=w)
        new = pd.concat([old, new])
        new = new[sorted(list(new.columns))]
        new.reset_index().to_feather(homeplace.daily_data_file + w[:-6] + ".feather")
    logger.success(f"å·²ç»å°†èµ„é‡‘æµæ•°æ®æ›´æ–°åˆ°{now_str}")
