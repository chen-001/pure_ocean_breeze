__updated__ = "2023-10-21 18:16:08"

import warnings

warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd
import knockknock as kk
import os
import tqdm.auto
import scipy.stats as ss
from scipy.optimize import linprog
import statsmodels.formula.api as smf
import matplotlib as mpl

mpl.rcParams.update(mpl.rcParamsDefault)
import matplotlib.pyplot as plt

plt.rcParams["axes.unicode_minus"] = False

from functools import reduce, lru_cache, partial
from dateutil.relativedelta import relativedelta
from loguru import logger
import datetime
from collections.abc import Iterable
import plotly.express as pe
import plotly.io as pio
from plotly.tools import FigureFactory as FF
import plotly.graph_objects as go
import pyfinance.ols as po
from texttable import Texttable
from xpinyin import Pinyin
import tradetime as tt
import cufflinks as cf
import deprecation
import duckdb
import concurrent
from mpire import WorkerPool
from scipy.optimize import minimize
from pure_ocean_breeze import __version__

cf.set_config_file(offline=True)
from typing import Callable, Union, Dict, List, Tuple
from pure_ocean_breeze.data.read_data import (
    read_daily,
    read_market,
    get_industry_dummies,
    read_swindustry_prices,
    read_zxindustry_prices,
    database_read_final_factors,
    read_index_single,
    FactorDone,
)
from pure_ocean_breeze.state.homeplace import HomePlace

try:
    homeplace = HomePlace()
except Exception:
    print("æ‚¨æš‚æœªåˆå§‹åŒ–ï¼ŒåŠŸèƒ½å°†å—é™")
from pure_ocean_breeze.state.states import STATES
from pure_ocean_breeze.state.decorators import do_on_dfs
from pure_ocean_breeze.data.database import *
from pure_ocean_breeze.data.dicts import INDUS_DICT
from pure_ocean_breeze.data.tools import (
    indus_name,
    drop_duplicates_index,
    to_percent,
    to_group,
    standardlize,
    select_max,
    select_min,
    merge_many,
)
from pure_ocean_breeze.labor.comment import (
    comments_on_twins,
    make_relative_comments,
    make_relative_comments_plot,
)


@do_on_dfs
def daily_factor_on300500(
    fac: pd.DataFrame,
    hs300: bool = 0,
    zz500: bool = 0,
    zz1000: bool = 0,
    gz2000: bool = 0,
    other: bool = 0,
) -> pd.DataFrame:
    """è¾“å…¥æ—¥é¢‘æˆ–æœˆé¢‘å› å­å€¼ï¼Œå°†å…¶é™å®šåœ¨æŸæŒ‡æ•°æˆåˆ†è‚¡çš„è‚¡ç¥¨æ± å†…ï¼Œ
    ç›®å‰ä»…æ”¯æŒæ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ã€å›½è¯2000æˆåˆ†è‚¡ï¼Œä»¥åŠè¿™å››ç§æŒ‡æ•°æˆåˆ†è‚¡çš„ç»„åˆå åŠ ï¼Œå’Œé™¤æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä»¥å¤–çš„è‚¡ç¥¨çš„æˆåˆ†è‚¡

    Parameters
    ----------
    fac : pd.DataFrame
        æœªé™å®šè‚¡ç¥¨æ± çš„å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    hs300 : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºæ²ªæ·±300, by default 0
    zz500 : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºä¸­è¯500, by default 0
    zz1000 : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºä¸­è¯1000, by default 0
    gz2000 : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºå›½è¯2000, by default 0
    other : bool, optional
        é™å®šè‚¡ç¥¨æ± ä¸ºé™¤æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä»¥å¤–çš„è‚¡ç¥¨çš„æˆåˆ†è‚¡, by default 0

    Returns
    -------
    `pd.DataFrame`
        ä»…åŒ…å«æˆåˆ†è‚¡åçš„å› å­å€¼ï¼Œéæˆåˆ†è‚¡çš„å› å­å€¼ä¸ºç©º

    Raises
    ------
    `ValueError`
        å¦‚æœæœªæŒ‡å®šä»»ä½•ä¸€ç§æŒ‡æ•°çš„æˆåˆ†è‚¡ï¼Œå°†æŠ¥é”™
    """
    last = fac.resample("M").last()
    homeplace = HomePlace()
    dummies = []
    if fac.shape[0] / last.shape[0] > 2:
        if hs300:
            df = pd.read_parquet(
                homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.parquet"
            ).fillna(0)
            dummies.append(df)
        if zz500:
            df = pd.read_parquet(
                homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.parquet"
            ).fillna(0)
            dummies.append(df)
        if zz1000:
            df = pd.read_parquet(
                homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.parquet"
            ).fillna(0)
            dummies.append(df)
        if gz2000:
            df = pd.read_parquet(
                homeplace.daily_data_file + "å›½è¯2000æ—¥æˆåˆ†è‚¡.parquet"
            ).fillna(0)
            dummies.append(df)
        if other:
            tr = read_daily(tr=1).fillna(0).replace(0, 1)
            tr = np.sign(tr)
            df1 = (
                tr * pd.read_parquet(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.parquet")
            ).fillna(0)
            df2 = (
                tr * pd.read_parquet(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.parquet")
            ).fillna(0)
            df3 = (
                tr * pd.read_parquet(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.parquet")
            ).fillna(0)
            df = (1 - df1) * (1 - df2) * (1 - df3) * tr
            df = df.replace(0, np.nan) * fac
            df = df.dropna(how="all")
        if (hs300 + zz500 + zz1000 + gz2000 + other) == 0:
            raise ValueError("æ€»å¾—æŒ‡å®šä¸€ä¸‹æ˜¯å“ªä¸ªæˆåˆ†è‚¡å§ğŸ¤’")
    else:
        if hs300:
            df = pd.read_parquet(
                homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.parquet"
            ).fillna(0)
            df = df.resample("M").last()
            dummies.append(df)
        if zz500:
            df = pd.read_parquet(
                homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.parquet"
            ).fillna(0)
            df = df.resample("M").last()
            dummies.append(df)
        if zz1000:
            df = pd.read_parquet(
                homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.parquet"
            ).fillna(0)
            df = df.resample("M").last()
            dummies.append(df)
        if gz2000:
            df = pd.read_parquet(
                homeplace.daily_data_file + "å›½è¯2000æ—¥æˆåˆ†è‚¡.parquet"
            ).fillna(0)
            df = df.resample("M").last()
            dummies.append(df)
        if other:
            tr = read_daily(tr=1).fillna(0).replace(0, 1).resample("M").last()
            tr = np.sign(tr)
            df1 = (
                tr * pd.read_parquet(homeplace.daily_data_file + "æ²ªæ·±300æ—¥æˆåˆ†è‚¡.parquet")
            ).fillna(0)
            df1 = df1.resample("M").last()
            df2 = (
                tr * pd.read_parquet(homeplace.daily_data_file + "ä¸­è¯500æ—¥æˆåˆ†è‚¡.parquet")
            ).fillna(0)
            df2 = df2.resample("M").last()
            df3 = (
                tr * pd.read_parquet(homeplace.daily_data_file + "ä¸­è¯1000æ—¥æˆåˆ†è‚¡.parquet")
            ).fillna(0)
            df3 = df3.resample("M").last()
            df = (1 - df1) * (1 - df2) * (1 - df3)
            df = df.replace(0, np.nan) * fac
            df = df.dropna(how="all")
        if (hs300 + zz500 + zz1000 + gz2000 + other) == 0:
            raise ValueError("æ€»å¾—æŒ‡å®šä¸€ä¸‹æ˜¯å“ªä¸ªæˆåˆ†è‚¡å§ğŸ¤’")
    if len(dummies) > 0:
        dummies = sum(dummies).replace(0, np.nan)
        df = (dummies * fac).dropna(how="all")
    return df


@do_on_dfs
def daily_factor_on_industry(
    df: pd.DataFrame, swindustry: bool = 0, zxindustry: bool = 0
) -> dict:
    """å°†ä¸€ä¸ªå› å­å˜ä¸ºä»…åœ¨æŸä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šçš„è‚¡ç¥¨

    Parameters
    ----------
    df : pd.DataFrame
        å…¨å¸‚åœºçš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    swindustry : bool, optional
        é€‰æ‹©ä½¿ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
    zxindustry : bool, optional
        é€‰æ‹©ä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0

    Returns
    -------
    dict
        keyä¸ºè¡Œä¸šä»£ç ï¼Œvalueä¸ºå¯¹åº”çš„è¡Œä¸šä¸Šçš„å› å­å€¼
    """
    df1 = df.resample("M").last()
    if df1.shape[0] * 2 > df.shape[0]:
        daily = 0
        monthly = 1
    else:
        daily = 1
        monthly = 0
    start = int(datetime.datetime.strftime(df.index.min(), "%Y%m%d"))
    ress = get_industry_dummies(
        daily=daily,
        monthly=monthly,
        start=start,
        swindustry=swindustry,
        zxindustry=zxindustry,
    )
    ress = {k: v * df for k, v in ress.items()}
    return ress


@do_on_dfs
def group_test_on_industry(
    df: pd.DataFrame,
    group_num: int = 10,
    trade_cost_double_side: float = 0,
    net_values_writer: pd.ExcelWriter = None,
    swindustry: bool = 0,
    zxindustry: bool = 0,
) -> pd.DataFrame:
    """åœ¨ç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šæµ‹è¯•æ¯ä¸ªè¡Œä¸šçš„åˆ†ç»„å›æµ‹

    Parameters
    ----------
    df : pd.DataFrame
        å…¨å¸‚åœºçš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    group_num : int, optional
        åˆ†ç»„æ•°é‡, by default 10
    trade_cost_double_side : float, optional
        äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
    net_values_writer : pd.ExcelWriter, optional
        ç”¨äºå­˜å‚¨å„ä¸ªè¡Œä¸šåˆ†ç»„åŠå¤šç©ºå¯¹å†²å‡€å€¼åºåˆ—çš„excelæ–‡ä»¶, by default None
    swindustry : bool, optional
        é€‰æ‹©ä½¿ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
    zxindustry : bool, optional
        é€‰æ‹©ä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0

    Returns
    -------
    pd.DataFrame
        å„ä¸ªè¡Œä¸šçš„ç»©æ•ˆè¯„ä»·æ±‡æ€»
    """
    dfs = daily_factor_on_industry(df, swindustry=swindustry, zxindustry=zxindustry)

    ks = []
    vs = []
    if swindustry:
        for k, v in dfs.items():
            shen = pure_moonnight(
                v,
                groups_num=group_num,
                trade_cost_double_side=trade_cost_double_side,
                net_values_writer=net_values_writer,
                sheetname=INDUS_DICT[k],
                plt_plot=0,
            )
            ks.append(k)
            vs.append(shen.shen.total_comments.T)
        vs = pd.concat(vs)
        vs.index = [INDUS_DICT[i] for i in ks]
    else:
        for k, v in dfs.items():
            shen = pure_moonnight(
                v,
                groups_num=group_num,
                trade_cost_double_side=trade_cost_double_side,
                net_values_writer=net_values_writer,
                sheetname=k,
                plt_plot=0,
            )
            ks.append(k)
            vs.append(shen.shen.total_comments.T)
        vs = pd.concat(vs)
        vs.index = ks
    return vs


@do_on_dfs
def rankic_test_on_industry(
    df: pd.DataFrame,
    excel_name: str = "è¡Œä¸šrankic.xlsx",
    png_name: str = "è¡Œä¸šrankicå›¾.png",
    swindustry: bool = 0,
    zxindustry: bool = 0,
) -> pd.DataFrame:
    """ä¸“é—¨è®¡ç®—å› å­å€¼åœ¨å„ä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šçš„Rank ICå€¼ï¼Œå¹¶ç»˜åˆ¶æŸ±çŠ¶å›¾

    Parameters
    ----------
    df : pd.DataFrame
        å…¨å¸‚åœºçš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    excel_name : str, optional
        ç”¨äºä¿å­˜å„ä¸ªè¡Œä¸šRank ICå€¼çš„excelæ–‡ä»¶çš„åå­—, by default 'è¡Œä¸šrankic.xlsx'
    png_name : str, optional
        ç”¨äºä¿å­˜å„ä¸ªè¡Œä¸šRank ICå€¼çš„æŸ±çŠ¶å›¾çš„åå­—, by default 'è¡Œä¸šrankicå›¾.png'
    swindustry : bool, optional
        é€‰æ‹©ä½¿ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
    zxindustry : bool, optional
        é€‰æ‹©ä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0

    Returns
    -------
    pd.DataFrame
        è¡Œä¸šåç§°ä¸å¯¹åº”çš„Rank IC
    """
    vs = group_test_on_industry(df, swindustry=swindustry, zxindustry=zxindustry)
    rankics = vs[["RankIC"]].T
    if excel_name is not None:
        rankics.to_excel(excel_name)
    rankics.plot(kind="bar")
    plt.show()
    plt.savefig(png_name)
    return rankics


@do_on_dfs
def long_test_on_industry(
    df: pd.DataFrame,
    nums: list,
    pos: bool = 0,
    neg: bool = 0,
    save_stock_list: bool = 0,
    swindustry: bool = 0,
    zxindustry: bool = 0,
) -> List[dict]:
    """å¯¹æ¯ä¸ªç”³ä¸‡/ä¸­ä¿¡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡ï¼Œä½¿ç”¨æŸå› å­æŒ‘é€‰å‡ºæœ€å¤šå¤´çš„nå€¼è‚¡ç¥¨ï¼Œè€ƒå¯Ÿå…¶è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•

    Parameters
    ----------
    df : pd.DataFrame
        ä½¿ç”¨çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    nums : list
        å¤šå¤´æƒ³é€‰å–çš„è‚¡ç¥¨çš„æ•°é‡ï¼Œä¾‹å¦‚[3,4,5]
    pos : bool, optional
        å› å­æ–¹å‘ä¸ºæ­£ï¼Œå³Rank ICä¸ºæ­£ï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºTrue, by default 0
    neg : bool, optional
        å› å­æ–¹å‘ä¸ºè´Ÿï¼Œå³Rank ICä¸ºè´Ÿï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºFalse, by default 0
    save_stock_list : bool, optional
        æ˜¯å¦ä¿å­˜æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•ï¼Œä¼šé™ä½è¿è¡Œé€Ÿåº¦, by default 0
    swindustry : bool, optional
        åœ¨ç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šæµ‹è¯•, by default 0
    zxindusrty : bool, optional
        åœ¨ä¸­ä¿¡ä¸€çº§è¡Œä¸šä¸Šæµ‹è¯•, by default 0
    Returns
    -------
    List[dict]
        è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•

    Raises
    ------
    IOError
        poså’Œnegå¿…é¡»æœ‰ä¸€ä¸ªä¸º1ï¼Œå¦åˆ™å°†æŠ¥é”™
    """
    fac = decap_industry(df, monthly=True)

    if swindustry:
        industry_dummy = pd.read_parquet(
            homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.parquet"
        ).fillna(0)
        indus = read_swindustry_prices()
    else:
        industry_dummy = pd.read_parquet(
            homeplace.daily_data_file + "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡åç§°ç‰ˆ.parquet"
        ).fillna(0)
        indus = read_zxindustry_prices()
    inds = list(industry_dummy.columns)
    ret_next = (
        read_daily(close=1).resample("M").last()
        / read_daily(open=1).resample("M").first()
        - 1
    )
    ages = read_daily(age=1).resample("M").last()
    ages = (ages >= 60) + 0
    ages = ages.replace(0, np.nan)
    ret_next = ret_next * ages
    ret_next_dummy = 1 - ret_next.isna()

    def save_ind(code, num):
        ind = industry_dummy[["date", "code", code]]
        ind = ind.pivot(index="date", columns="code", values=code)
        ind = ind.resample("M").last()
        ind = ind.replace(0, np.nan)
        fi = ind * fac
        fi = fi.dropna(how="all")
        fi = fi.shift(1)
        fi = fi * ret_next_dummy
        fi = fi.dropna(how="all")

        def sing(x):
            if neg:
                thr = x.nsmallest(num).iloc[-1]
            elif pos:
                thr = x.nlargest(num).iloc[-1]
            else:
                raise IOError("æ‚¨éœ€è¦æŒ‡å®šä¸€ä¸‹å› å­æ–¹å‘ğŸ¤’")
            x = (x <= thr) + 0
            return x

        fi = fi.T.apply(sing).T
        fi = fi.replace(0, np.nan)
        fi = fi * ret_next
        ret_long = fi.mean(axis=1)
        return ret_long

    ret_longs = {k: [] for k in nums}
    for num in tqdm.auto.tqdm(nums):
        for code in inds[2:]:
            df = save_ind(code, num).to_frame(code)
            ret_longs[num] = ret_longs[num] + [df]

    indus = indus.resample("M").last().pct_change()

    if swindustry:
        coms = {
            k: indus_name(pd.concat(v, axis=1).dropna(how="all").T).T
            for k, v in ret_longs.items()
        }
        rets = {
            k: (v - indus_name(indus.T).T).dropna(how="all") for k, v in coms.items()
        }
    else:
        coms = {k: pd.concat(v, axis=1).dropna(how="all") for k, v in ret_longs.items()}
        rets = {k: (v - indus).dropna(how="all") for k, v in coms.items()}

    nets = {k: (v + 1).cumprod() for k, v in rets.items()}
    nets = {
        k: v.apply(lambda x: x.dropna() / x.dropna().iloc[0]) for k, v in nets.items()
    }

    def comments_on_twins(nets: pd.Series, rets: pd.Series) -> pd.Series:
        series = nets.copy()
        series1 = rets.copy()
        ret = (series.iloc[-1] - series.iloc[0]) / series.iloc[0]
        duration = (series.index[-1] - series.index[0]).days
        year = duration / 365
        ret_yearly = (series.iloc[-1] / series.iloc[0]) ** (1 / year) - 1
        max_draw = -(series / series.expanding(1).max() - 1).min()
        vol = np.std(series1) * (12**0.5)
        sharpe = ret_yearly / vol
        wins = series1[series1 > 0]
        win_rate = len(wins) / len(series1)
        return pd.Series(
            [ret, ret_yearly, vol, sharpe, win_rate, max_draw],
            index=["æ€»æ”¶ç›Šç‡", "å¹´åŒ–æ”¶ç›Šç‡", "å¹´åŒ–æ³¢åŠ¨ç‡", "ä¿¡æ¯æ¯”ç‡", "èƒœç‡", "æœ€å¤§å›æ’¤ç‡"],
        )

    if swindustry:
        name = "ç”³ä¸‡"
    else:
        name = "ä¸­ä¿¡"
    w = pd.ExcelWriter(f"å„ä¸ª{name}ä¸€çº§è¡Œä¸šå¤šå¤´è¶…é¢ç»©æ•ˆ.xlsx")

    def com_all(df1, df2, num):
        cs = []
        for ind in list(df1.columns):
            c = comments_on_twins(df2[ind].dropna(), df1[ind].dropna()).to_frame(ind)
            cs.append(c)
        res = pd.concat(cs, axis=1).T
        res.to_excel(w, sheet_name=str(num))
        return res

    coms_finals = {k: com_all(rets[k], nets[k], k) for k in rets.keys()}
    w.save()
    w.close()

    rets_save = {k: v.dropna() for k, v in rets.items() if k in nums}
    u = pd.ExcelWriter(f"å„ä¸ª{name}ä¸€çº§è¡Œä¸šæ¯æœˆè¶…é¢æ”¶ç›Šç‡.xlsx")
    for k, v in rets_save.items():
        v.to_excel(u, sheet_name=str(k))
    u.save()
    u.close()

    if save_stock_list:

        def save_ind_stocks(code, num):
            ind = industry_dummy[["date", "code", code]]
            ind = ind.pivot(index="date", columns="code", values=code)
            ind = ind.replace(0, np.nan)
            fi = ind * fac
            fi = fi.dropna(how="all")
            fi = fi.shift(1)
            fi = fi * ret_next_dummy
            fi = fi.dropna(how="all")

            def sing(x):
                if neg:
                    thr = x.nsmallest(num)
                elif pos:
                    thr = x.nlargest(num)
                else:
                    raise IOError("æ‚¨éœ€è¦æŒ‡å®šä¸€ä¸‹å› å­æ–¹å‘ğŸ¤’")
                return tuple(thr.index)

            fi = fi.T.apply(sing)
            return fi

        stocks_longs = {k: {} for k in nums}

        for num in tqdm.auto.tqdm(nums):
            for code in inds[2:]:
                stocks_longs[num][code] = save_ind_stocks(code, num)

        for num in nums:
            w1 = pd.ExcelWriter(f"å„ä¸ª{name}ä¸€çº§è¡Œä¸šä¹°{num}åªçš„è‚¡ç¥¨åå•.xlsx")
            for k, v in stocks_longs[num].items():
                v = v.T
                v.index = v.index.strftime("%Y/%m/%d")
                v.to_excel(w1, sheet_name=INDUS_DICT[k])
            w1.save()
            w1.close()

        return [coms_finals, rets_save, stocks_longs]
    else:
        return [coms_finals, rets_save]


@do_on_dfs
def long_test_on_swindustry(
    df: pd.DataFrame,
    nums: list,
    pos: bool = 0,
    neg: bool = 0,
    save_stock_list: bool = 0,
) -> List[dict]:
    """å¯¹æ¯ä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡ï¼Œä½¿ç”¨æŸå› å­æŒ‘é€‰å‡ºæœ€å¤šå¤´çš„nå€¼è‚¡ç¥¨ï¼Œè€ƒå¯Ÿå…¶è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•

    Parameters
    ----------
    df : pd.DataFrame
        ä½¿ç”¨çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    nums : list
        å¤šå¤´æƒ³é€‰å–çš„è‚¡ç¥¨çš„æ•°é‡ï¼Œä¾‹å¦‚[3,4,5]
    pos : bool, optional
        å› å­æ–¹å‘ä¸ºæ­£ï¼Œå³Rank ICä¸ºæ­£ï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºTrue, by default 0
    neg : bool, optional
        å› å­æ–¹å‘ä¸ºè´Ÿï¼Œå³Rank ICä¸ºè´Ÿï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºFalse, by default 0
    save_stock_list : bool, optional
        æ˜¯å¦ä¿å­˜æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•ï¼Œä¼šé™ä½è¿è¡Œé€Ÿåº¦, by default 0
    Returns
    -------
    List[dict]
        è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•

    Raises
    ------
    IOError
        poså’Œnegå¿…é¡»æœ‰ä¸€ä¸ªä¸º1ï¼Œå¦åˆ™å°†æŠ¥é”™
    """
    res = long_test_on_industry(
        df=df,
        nums=nums,
        pos=pos,
        neg=neg,
        save_stock_list=save_stock_list,
        swindustry=1,
    )
    return res


@do_on_dfs
def long_test_on_zxindustry(
    df: pd.DataFrame,
    nums: list,
    pos: bool = 0,
    neg: bool = 0,
    save_stock_list: bool = 0,
) -> List[dict]:
    """å¯¹æ¯ä¸ªä¸­ä¿¡ä¸€çº§è¡Œä¸šæˆåˆ†è‚¡ï¼Œä½¿ç”¨æŸå› å­æŒ‘é€‰å‡ºæœ€å¤šå¤´çš„nå€¼è‚¡ç¥¨ï¼Œè€ƒå¯Ÿå…¶è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•

    Parameters
    ----------
    df : pd.DataFrame
        ä½¿ç”¨çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    nums : list
        å¤šå¤´æƒ³é€‰å–çš„è‚¡ç¥¨çš„æ•°é‡ï¼Œä¾‹å¦‚[3,4,5]
    pos : bool, optional
        å› å­æ–¹å‘ä¸ºæ­£ï¼Œå³Rank ICä¸ºæ­£ï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºTrue, by default 0
    neg : bool, optional
        å› å­æ–¹å‘ä¸ºè´Ÿï¼Œå³Rank ICä¸ºè´Ÿï¼Œåˆ™æŒ‡å®šæ­¤å¤„ä¸ºFalse, by default 0
    save_stock_list : bool, optional
        æ˜¯å¦ä¿å­˜æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•ï¼Œä¼šé™ä½è¿è¡Œé€Ÿåº¦, by default 0
    Returns
    -------
    List[dict]
        è¶…é¢æ”¶ç›Šç»©æ•ˆã€æ¯æœˆè¶…é¢æ”¶ç›Šã€æ¯æœˆæ¯ä¸ªè¡Œä¸šçš„å¤šå¤´åå•

    Raises
    ------
    IOError
        poså’Œnegå¿…é¡»æœ‰ä¸€ä¸ªä¸º1ï¼Œå¦åˆ™å°†æŠ¥é”™
    """
    res = long_test_on_industry(
        df=df,
        nums=nums,
        pos=pos,
        neg=neg,
        save_stock_list=save_stock_list,
        zxindustry=1,
    )
    return res


@do_on_dfs
@kk.desktop_sender(title="å˜¿ï¼Œè¡Œä¸šä¸­æ€§åŒ–åšå®Œå•¦ï½ğŸ›")
def decap(df: pd.DataFrame, daily: bool = 0, monthly: bool = 0) -> pd.DataFrame:
    """å¯¹å› å­åšå¸‚å€¼ä¸­æ€§åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        æœªä¸­æ€§åŒ–çš„å› å­ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    daily : bool, optional
        æœªä¸­æ€§åŒ–å› å­æ˜¯æ—¥é¢‘çš„åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0, by default 0
    monthly : bool, optional
        æœªä¸­æ€§åŒ–å› å­æ˜¯æœˆé¢‘çš„åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0, by default 0

    Returns
    -------
    `pd.DataFrame`
        å¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­

    Raises
    ------
    `NotImplementedError`
        å¦‚æœæœªæŒ‡å®šæ—¥é¢‘æˆ–æœˆé¢‘ï¼Œå°†æŠ¥é”™
    """
    tqdm.auto.tqdm.pandas()
    share = read_daily(sharenum=1)
    undi_close = read_daily(close=1, unadjust=1)
    cap = (share * undi_close).stack().reset_index()
    cap.columns = ["date", "code", "cap"]
    cap.cap = ss.boxcox(cap.cap)[0]

    def single(x):
        x.cap = ss.boxcox(x.cap)[0]
        return x

    cap = cap.groupby(["date"]).apply(single)
    cap = cap.set_index(["date", "code"]).unstack()
    cap.columns = [i[1] for i in list(cap.columns)]
    cap_monthly = cap.resample("M").last()
    last = df.resample("M").last()
    if df.shape[0] / last.shape[0] < 2:
        monthly = True
    else:
        daily = True
    if daily:
        df = (pure_fallmount(df) - (pure_fallmount(cap),))()
    elif monthly:
        df = (pure_fallmount(df) - (pure_fallmount(cap_monthly),))()
    else:
        raise NotImplementedError("å¿…é¡»æŒ‡å®šé¢‘ç‡")
    return df


@do_on_dfs
@kk.desktop_sender(title="å˜¿ï¼Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–åšå®Œå•¦ï½ğŸ›")
def decap_industry(
    df: pd.DataFrame,
    daily: bool = 0,
    monthly: bool = 0,
    swindustry: bool = 0,
    zxindustry: bool = 0,
) -> pd.DataFrame:
    """å¯¹å› å­åšè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        æœªä¸­æ€§åŒ–çš„å› å­ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    daily : bool, optional
        æœªä¸­æ€§åŒ–å› å­æ˜¯æ—¥é¢‘çš„åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0, by default 0
    monthly : bool, optional
        æœªä¸­æ€§åŒ–å› å­æ˜¯æœˆé¢‘çš„åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0, by default 0
    swindustry : bool, optional
        é€‰æ‹©ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
    zxindustry : bool, optional
        é€‰æ‹©ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0

    Returns
    -------
    `pd.DataFrame`
        è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­

    Raises
    ------
    `NotImplementedError`
        å¦‚æœæœªæŒ‡å®šæ—¥é¢‘æˆ–æœˆé¢‘ï¼Œå°†æŠ¥é”™
    """
    start_date = int(datetime.datetime.strftime(df.index.min(), "%Y%m%d"))
    last = df.resample("M").last()
    homeplace = HomePlace()
    if daily == 0 and monthly == 0:
        if df.shape[0] / last.shape[0] < 2:
            monthly = True
        else:
            daily = True
    if monthly:
        cap = read_daily(flow_cap=1, start=start_date).resample("M").last()
    else:
        cap = read_daily(flow_cap=1, start=start_date)
    cap = cap.stack().reset_index()
    cap.columns = ["date", "code", "cap"]
    cap.cap = ss.boxcox(cap.cap)[0]

    def single(x):
        x.cap = ss.boxcox(x.cap)[0]
        return x

    cap = cap.groupby(["date"]).apply(single)
    df = df.stack().reset_index()
    df.columns = ["date", "code", "fac"]
    df = pd.merge(df, cap, on=["date", "code"])

    def neutralize_factors(df):
        """ç»„å†…å¯¹å› å­è¿›è¡Œå¸‚å€¼ä¸­æ€§åŒ–"""
        industry_codes = list(df.columns)
        industry_codes = [i for i in industry_codes if i.startswith("w")]
        industry_codes_str = "+".join(industry_codes)
        ols_result = smf.ols("fac~cap+" + industry_codes_str, data=df).fit()
        ols_w = ols_result.params["cap"]
        ols_b = ols_result.params["Intercept"]
        ols_bs = {}
        for ind in industry_codes:
            ols_bs[ind] = ols_result.params[ind]
        df.fac = df.fac - ols_w * df.cap - ols_b
        for k, v in ols_bs.items():
            df.fac = df.fac - v * df[k]
        df = df[["fac"]]
        return df

    if swindustry:
        file_name = "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.parquet"
    else:
        file_name = "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ä»£ç ç‰ˆ.parquet"

    if monthly:
        industry_dummy = (
            pd.read_parquet(homeplace.daily_data_file + file_name)
            .fillna(0)
            .set_index("date")
            .groupby("code")
            .resample("M")
            .last()
        )
        industry_dummy = industry_dummy.fillna(0).drop(columns=["code"]).reset_index()
        industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
        col = ["code", "date"] + industry_ws
    elif daily:
        industry_dummy = pd.read_parquet(homeplace.daily_data_file + file_name).fillna(
            0
        )
        industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
        col = ["date", "code"] + industry_ws
    else:
        raise NotImplementedError("å¿…é¡»æŒ‡å®šé¢‘ç‡")
    industry_dummy.columns = col
    df = pd.merge(df, industry_dummy, on=["date", "code"])
    df = df.set_index(["date", "code"])
    tqdm.auto.tqdm.pandas()
    df = df.groupby(["date"]).progress_apply(neutralize_factors)
    df = df.unstack()
    df.columns = [i[1] for i in list(df.columns)]
    return df


@do_on_dfs
def deboth(df: pd.DataFrame) -> pd.DataFrame:
    """é€šè¿‡å›æµ‹çš„æ–¹å¼ï¼Œå¯¹æœˆé¢‘å› å­åšè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        æœªä¸­æ€§åŒ–çš„å› å­

    Returns
    -------
    `pd.DataFrame`
        è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­
    """
    shen = pure_moonnight(df, boxcox=1, plt_plot=0, print_comments=0)
    return shen()


@do_on_dfs
def boom_one(
    df: pd.DataFrame, backsee: int = 20, daily: bool = 0, min_periods: int = None
) -> pd.DataFrame:
    if min_periods is None:
        min_periods = int(backsee * 0.5)
    if not daily:
        df_mean = (
            df.rolling(backsee, min_periods=min_periods).mean().resample("M").last()
        )
    else:
        df_mean = df.rolling(backsee, min_periods=min_periods).mean()
    return df_mean


@do_on_dfs
def boom_four(
    df: pd.DataFrame, backsee: int = 20, daily: bool = 0, min_periods: int = None
) -> Tuple[pd.DataFrame]:
    """ç”Ÿæˆ20å¤©å‡å€¼ï¼Œ20å¤©æ ‡å‡†å·®ï¼ŒåŠäºŒè€…æ­£å‘z-scoreåˆæˆï¼Œæ­£å‘æ’åºåˆæˆï¼Œè´Ÿå‘z-scoreåˆæˆï¼Œè´Ÿå‘æ’åºåˆæˆè¿™6ä¸ªå› å­

    Parameters
    ----------
    df : pd.DataFrame
        åŸæ—¥é¢‘å› å­
    backsee : int, optional
        å›çœ‹å¤©æ•°, by default 20
    daily : bool, optional
        ä¸º1æ˜¯æ¯å¤©éƒ½æ»šåŠ¨ï¼Œä¸º0åˆ™ä»…ä¿ç•™æœˆåº•å€¼, by default 0
    min_periods : int, optional
        rollingæ—¶çš„æœ€å°æœŸ, by default backseeçš„ä¸€åŠ

    Returns
    -------
    `Tuple[pd.DataFrame]`
        6ä¸ªå› å­çš„å…ƒç»„
    """
    if min_periods is None:
        min_periods = int(backsee * 0.5)
    if not daily:
        df_mean = (
            df.rolling(backsee, min_periods=min_periods).mean().resample("M").last()
        )
        df_std = df.rolling(backsee, min_periods=min_periods).std().resample("M").last()
        twins_add = (pure_fallmount(df_mean) + (pure_fallmount(df_std),))()
        rtwins_add = df_mean.rank(axis=1) + df_std.rank(axis=1)
        twins_minus = (pure_fallmount(df_mean) + (pure_fallmount(-df_std),))()
        rtwins_minus = df_mean.rank(axis=1) - df_std.rank(axis=1)
    else:
        df_mean = df.rolling(backsee, min_periods=min_periods).mean()
        df_std = df.rolling(backsee, min_periods=min_periods).std()
        twins_add = (pure_fallmount(df_mean) + (pure_fallmount(df_std),))()
        rtwins_add = df_mean.rank(axis=1) + df_std.rank(axis=1)
        twins_minus = (pure_fallmount(df_mean) + (pure_fallmount(-df_std),))()
        rtwins_minus = df_mean.rank(axis=1) - df_std.rank(axis=1)
    return df_mean, df_std, twins_add, rtwins_add, twins_minus, rtwins_minus


def boom_fours(
    dfs: List[pd.DataFrame],
    backsee: Union[int, List[int]] = 20,
    daily: Union[bool, List[bool]] = 0,
    min_periods: Union[int, List[int]] = None,
) -> List[List[pd.DataFrame]]:
    """å¯¹å¤šä¸ªå› å­ï¼Œæ¯ä¸ªå› å­éƒ½è¿›è¡Œboom_fourçš„æ“ä½œ

    Parameters
    ----------
    dfs : List[pd.DataFrame]
        å¤šä¸ªå› å­çš„dataframeç»„æˆçš„list
    backsee : Union[int,List[int]], optional
        æ¯ä¸ªå› å­å›çœ‹æœŸæ•°, by default 20
    daily : Union[bool,List[bool]], optional
        æ¯ä¸ªå› å­æ˜¯å¦é€æ—¥è®¡ç®—, by default 0
    min_periods : Union[int,List[int]], optional
        æ¯ä¸ªå› å­è®¡ç®—çš„æœ€å°æœŸ, by default None

    Returns
    -------
    List[List[pd.DataFrame]]
        æ¯ä¸ªå› å­è¿›è¡Œboom_fouråçš„ç»“æœ
    """
    return boom_four(df=dfs, backsee=backsee, daily=daily, min_periods=min_periods)


@do_on_dfs
def add_cross_standardlize(*args: list) -> pd.DataFrame:
    """å°†ä¼—å¤šå› å­æ¨ªæˆªé¢åšz-scoreæ ‡å‡†åŒ–ä¹‹åç›¸åŠ 

    Returns
    -------
    `pd.DataFrame`
        åˆæˆåçš„å› å­
    """
    fms = [pure_fallmount(i) for i in args]
    one = fms[0]
    others = fms[1:]
    final = one + others
    return final()


@do_on_dfs
def to_tradeends(df: pd.DataFrame) -> pd.DataFrame:
    """å°†æœ€åä¸€ä¸ªè‡ªç„¶æ—¥æ”¹å˜ä¸ºæœ€åä¸€ä¸ªäº¤æ˜“æ—¥

    Parameters
    ----------
    df : pd.DataFrame
        indexä¸ºæ—¶é—´ï¼Œä¸ºæ¯ä¸ªæœˆçš„æœ€åä¸€å¤©

    Returns
    -------
    `pd.DataFrame`
        ä¿®æ”¹ä¸ºäº¤æ˜“æ—¥æ ‡æ³¨åçš„pd.DataFrame
    """
    """"""
    start = df.index.min()
    start = start - pd.tseries.offsets.MonthBegin()
    start = datetime.datetime.strftime(start, "%Y%m%d")
    trs = read_daily(tr=1, start=start)
    trs = trs.assign(tradeends=list(trs.index))
    trs = trs[["tradeends"]]
    trs = trs.resample("M").last()
    df = pd.concat([trs, df], axis=1)
    df = df.set_index(["tradeends"])
    return df


@do_on_dfs
def market_kind(
    df: pd.DataFrame,
    zhuban: bool = 0,
    chuangye: bool = 0,
    kechuang: bool = 0,
    beijing: bool = 0,
) -> pd.DataFrame:
    """ä¸å®½åŸºæŒ‡æ•°æˆåˆ†è‚¡çš„å‡½æ•°ç±»ä¼¼ï¼Œé™å®šè‚¡ç¥¨åœ¨æŸä¸ªå…·ä½“æ¿å—ä¸Š

    Parameters
    ----------
    df : pd.DataFrame
        åŸå§‹å…¨éƒ¨è‚¡ç¥¨çš„å› å­å€¼
    zhuban : bool, optional
        é™å®šåœ¨ä¸»æ¿èŒƒå›´å†…, by default 0
    chuangye : bool, optional
        é™å®šåœ¨åˆ›ä¸šæ¿èŒƒå›´å†…, by default 0
    kechuang : bool, optional
        é™å®šåœ¨ç§‘åˆ›æ¿èŒƒå›´å†…, by default 0
    beijing : bool, optional
        é™å®šåœ¨åŒ—äº¤æ‰€èŒƒå›´å†…, by default 0

    Returns
    -------
    `pd.DataFrame`
        é™åˆ¶èŒƒå›´åçš„å› å­å€¼ï¼Œå…¶ä½™ä¸ºç©º

    Raises
    ------
    `ValueError`
        å¦‚æœæœªæŒ‡å®šä»»ä½•è‚¡ç¥¨æ± ï¼Œå°†æŠ¥é”™
    """
    trs = read_daily(tr=1)
    codes = list(trs.columns)
    dates = list(trs.index)
    if chuangye and kechuang:
        dummys = [1 if code[:2] in ["30", "68"] else np.nan for code in codes]
    else:
        if zhuban:
            dummys = [1 if code[:2] in ["00", "60"] else np.nan for code in codes]
        elif chuangye:
            dummys = [1 if code.startswith("3") else np.nan for code in codes]
        elif kechuang:
            dummys = [1 if code.startswith("68") else np.nan for code in codes]
        elif beijing:
            dummys = [1 if code.startswith("8") else np.nan for code in codes]
        else:
            raise ValueError("ä½ æ€»å¾—é€‰ä¸€ä¸ªè‚¡ç¥¨æ± å§ï¼ŸğŸ¤’")
    dummy_dict = {k: v for k, v in zip(codes, dummys)}
    dummy_df = pd.DataFrame(dummy_dict, index=dates)
    df = df * dummy_df
    return df


def show_corr(
    fac1: pd.DataFrame,
    fac2: pd.DataFrame,
    method: str = "pearson",
    plt_plot: bool = 1,
    show_series: bool = 0,
    old_way: bool = 0,
) -> float:
    """å±•ç¤ºä¸¤ä¸ªå› å­çš„æˆªé¢ç›¸å…³æ€§

    Parameters
    ----------
    fac1 : pd.DataFrame
        å› å­1
    fac2 : pd.DataFrame
        å› å­2
    method : str, optional
        è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "pearson"
    plt_plot : bool, optional
        æ˜¯å¦ç”»å‡ºç›¸å…³ç³»æ•°çš„æ—¶åºå˜åŒ–å›¾, by default 1
    show_series : bool, optional
        è¿”å›ç›¸å…³æ€§çš„åºåˆ—ï¼Œè€Œéå‡å€¼
    old_way : bool, optional
        ä½¿ç”¨3.xç‰ˆæœ¬çš„æ–¹å¼æ±‚ç›¸å…³ç³»æ•°

    Returns
    -------
    `float`
        å¹³å‡æˆªé¢ç›¸å…³ç³»æ•°
    """
    if old_way:
        if method == "spearman":
            corr = show_x_with_func(fac1, fac2, lambda x: x.rank().corr().iloc[0, 1])
        else:
            corr = show_x_with_func(
                fac1, fac2, lambda x: x.corr(method=method).iloc[0, 1]
            )
    else:
        corr = fac1.corrwith(fac2, axis=1, method=method)
    if show_series:
        return corr
    else:
        if plt_plot:
            corr.plot(rot=60)
            plt.show()
        return corr.mean()


def show_corrs(
    factors: List[pd.DataFrame],
    factor_names: List[str] = None,
    print_bool: bool = True,
    show_percent: bool = True,
    method: str = "pearson",
) -> pd.DataFrame:
    """å±•ç¤ºå¾ˆå¤šå› å­ä¸¤ä¸¤ä¹‹é—´çš„æˆªé¢ç›¸å…³æ€§

    Parameters
    ----------
    factors : List[pd.DataFrame]
        æ‰€æœ‰å› å­æ„æˆçš„åˆ—è¡¨, by default None
    factor_names : List[str], optional
        ä¸Šè¿°å› å­ä¾æ¬¡çš„åå­—, by default None
    print_bool : bool, optional
        æ˜¯å¦æ‰“å°å‡ºä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼, by default True
    show_percent : bool, optional
        æ˜¯å¦ä»¥ç™¾åˆ†æ•°çš„å½¢å¼å±•ç¤º, by default True
    method : str, optional
        è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "pearson"

    Returns
    -------
    `pd.DataFrame`
        ä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼
    """
    corrs = []
    for i in range(len(factors)):
        main_i = factors[i]
        follows = factors[i + 1 :]
        corr = [show_corr(main_i, i, plt_plot=False, method=method) for i in follows]
        corr = [np.nan] * (i + 1) + corr
        corrs.append(corr)
    if factor_names is None:
        factor_names = [f"fac{i}" for i in list(range(1, len(factors) + 1))]
    corrs = pd.DataFrame(corrs, columns=factor_names, index=factor_names)
    np.fill_diagonal(corrs.to_numpy(), 1)
    if show_percent:
        pcorrs = corrs.applymap(to_percent)
    else:
        pcorrs = corrs.copy()
    if print_bool:
        return pcorrs
    else:
        return corrs


def show_cov(
    fac1: pd.DataFrame,
    fac2: pd.DataFrame,
    plt_plot: bool = 1,
    show_series: bool = 0,
) -> float:
    """å±•ç¤ºä¸¤ä¸ªå› å­çš„æˆªé¢ç›¸å…³æ€§

    Parameters
    ----------
    fac1 : pd.DataFrame
        å› å­1
    fac2 : pd.DataFrame
        å› å­2
    method : str, optional
        è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "spearman"
    plt_plot : bool, optional
        æ˜¯å¦ç”»å‡ºç›¸å…³ç³»æ•°çš„æ—¶åºå˜åŒ–å›¾, by default 1
    show_series : bool, optional
        è¿”å›ç›¸å…³æ€§çš„åºåˆ—ï¼Œè€Œéå‡å€¼

    Returns
    -------
    `float`
        å¹³å‡æˆªé¢ç›¸å…³ç³»æ•°
    """
    cov = show_x_with_func(fac1, fac2, lambda x: x.cov().iloc[0, 1])
    if show_series:
        return cov
    else:
        if plt_plot:
            cov.plot(rot=60)
            plt.show()
        return cov.mean()


def show_x_with_func(
    fac1: pd.DataFrame,
    fac2: pd.DataFrame,
    func: Callable,
) -> pd.Series:
    """å±•ç¤ºä¸¤ä¸ªå› å­çš„æŸç§æˆªé¢å…³ç³»

    Parameters
    ----------
    fac1 : pd.DataFrame
        å› å­1
    fac2 : pd.DataFrame
        å› å­2
    func : Callable
        è¦å¯¹ä¸¤ä¸ªå› å­åœ¨æˆªé¢ä¸Šçš„è¿›è¡Œçš„æ“ä½œ

    Returns
    -------
    `pd.Series`
        æˆªé¢å…³ç³»
    """
    the_func = partial(func)
    both1 = fac1.stack().reset_index()
    befo1 = fac2.stack().reset_index()
    both1.columns = ["date", "code", "both"]
    befo1.columns = ["date", "code", "befo"]
    twins = pd.merge(both1, befo1, on=["date", "code"]).set_index(["date", "code"])
    corr = twins.groupby("date").apply(the_func)
    return corr


def show_covs(
    factors: List[pd.DataFrame],
    factor_names: List[str] = None,
    print_bool: bool = True,
    show_percent: bool = True,
) -> pd.DataFrame:
    """å±•ç¤ºå¾ˆå¤šå› å­ä¸¤ä¸¤ä¹‹é—´çš„æˆªé¢ç›¸å…³æ€§

    Parameters
    ----------
    factors : List[pd.DataFrame]
        æ‰€æœ‰å› å­æ„æˆçš„åˆ—è¡¨, by default None
    factor_names : List[str], optional
        ä¸Šè¿°å› å­ä¾æ¬¡çš„åå­—, by default None
    print_bool : bool, optional
        æ˜¯å¦æ‰“å°å‡ºä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼, by default True
    show_percent : bool, optional
        æ˜¯å¦ä»¥ç™¾åˆ†æ•°çš„å½¢å¼å±•ç¤º, by default True

    Returns
    -------
    `pd.DataFrame`
        ä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼
    """
    corrs = []
    for i in range(len(factors)):
        main_i = factors[i]
        follows = factors[i + 1 :]
        corr = [show_cov(main_i, i, plt_plot=False) for i in follows]
        corr = [np.nan] * (i + 1) + corr
        corrs.append(corr)
    if factor_names is None:
        factor_names = [f"fac{i}" for i in list(range(1, len(factors) + 1))]
    corrs = pd.DataFrame(corrs, columns=factor_names, index=factor_names)
    np.fill_diagonal(corrs.to_numpy(), 1)
    if show_percent:
        pcorrs = corrs.applymap(to_percent)
    else:
        pcorrs = corrs.copy()
    if print_bool:
        print(pcorrs)
    return corrs


def de_cross(
    y: pd.DataFrame, xs: Union[List[pd.DataFrame], pd.DataFrame]
) -> pd.DataFrame:
    """ä½¿ç”¨è‹¥å¹²å› å­å¯¹æŸä¸ªå› å­è¿›è¡Œæ­£äº¤åŒ–å¤„ç†

    Parameters
    ----------
    y : pd.DataFrame
        ç ”ç©¶çš„ç›®æ ‡ï¼Œå›å½’ä¸­çš„y
    xs : Union[List[pd.DataFrame],pd.DataFrame]
        ç”¨äºæ­£äº¤åŒ–çš„è‹¥å¹²å› å­ï¼Œå›å½’ä¸­çš„x

    Returns
    -------
    pd.DataFrame
        æ­£äº¤åŒ–ä¹‹åçš„å› å­
    """
    if not isinstance(xs, list):
        xs = [xs]
    y = pure_fallmount(y)
    xs = [pure_fallmount(i) for i in xs]
    return (y - xs)()


@do_on_dfs
def show_corrs_with_old(
    df: pd.DataFrame = None,
    method: str = "pearson",
    only_new: bool = 1,
    with_son_factors: bool = 1,
    freq: str = "M",
    old_database: bool = 0,
) -> pd.DataFrame:
    """è®¡ç®—æ–°å› å­å’Œå·²æœ‰å› å­çš„ç›¸å…³ç³»æ•°

    Parameters
    ----------
    df : pd.DataFrame, optional
        æ–°å› å­, by default None
    method : str, optional
        æ±‚ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default 'pearson'
    only_new : bool, optional
        ä»…è®¡ç®—æ–°å› å­ä¸æ—§å› å­ä¹‹é—´çš„ç›¸å…³ç³»æ•°, by default 1
    with_son_factors : bool, optional
        è®¡ç®—æ–°å› å­ä¸æ•°æ®åº“ä¸­å„ä¸ªç»†åˆ†å› å­çš„ç›¸å…³ç³»æ•°, by default 1
    freq : str, optional
        è¯»å–å› å­æ•°æ®çš„é¢‘ç‡, by default 'M'
    old_database : bool, optional
        ä½¿ç”¨3.xç‰ˆæœ¬çš„æ•°æ®åº“, by default 0


    Returns
    -------
    pd.DataFrame
        ç›¸å…³ç³»æ•°çŸ©é˜µ
    """
    if df is not None:
        df0 = df.resample(freq).last()
        if df.shape[0] / df0.shape[0] > 2:
            daily = 1
        else:
            daily = 0
    if old_database:
        nums = os.listdir(homeplace.final_factor_file)
        nums = sorted(
            set(
                [
                    int(i.split("å¤šå› å­")[1].split("_æœˆ")[0])
                    for i in nums
                    if i.endswith("æœˆ.parquet")
                ]
            )
        )
        olds = []
        for i in nums:
            try:
                if daily:
                    old = database_read_final_factors(order=i)[0]
                else:
                    old = database_read_final_factors(order=i)[0].resample("M").last()
                olds.append(old)
            except Exception:
                break
        if df is not None:
            if only_new:
                corrs = [
                    to_percent(show_corr(df, i, plt_plot=0, method=method))
                    for i in olds
                ]
                corrs = pd.Series(corrs, index=[f"old{i}" for i in nums])
                corrs = corrs.to_frame(f"{method}ç›¸å…³ç³»æ•°").T
            else:
                olds = [df] + olds
                corrs = show_corrs(
                    olds, ["new"] + [f"old{i}" for i in nums], method=method
                )
        else:
            corrs = show_corrs(olds, [f"old{i}" for i in nums], method=method)
    else:
        qdb = Questdb()
        if freq == "M":
            factor_infos = qdb.get_data("select * from factor_infos where freq='æœˆ'")
        else:
            factor_infos = qdb.get_data("select * from factor_infos where freq='å‘¨'")
        if not with_son_factors:
            old_orders = list(set(factor_infos.order))
            if daily:
                olds = [FactorDone(order=i)() for i in old_orders]
            else:
                olds = [FactorDone(order=i)().resample(freq).last() for i in old_orders]
        else:
            old_orders = [
                i.order + i.son_name.replace("å› å­", "")
                for i in factor_infos.dropna().itertuples()
            ]
            if daily:
                olds = [
                    FactorDone(order=i.order)(i.son_name)
                    for i in factor_infos.dropna().itertuples()
                ]
            else:
                olds = [
                    FactorDone(order=i.order)(i.son_name).resample(freq).last()
                    for i in factor_infos.dropna().itertuples()
                ]
        if df is not None:
            if only_new:
                corrs = [
                    to_percent(show_corr(df, i, plt_plot=0, method=method))
                    for i in olds
                ]
                corrs = pd.Series(corrs, index=old_orders)
                corrs = corrs.to_frame(f"{method}ç›¸å…³ç³»æ•°")
                if corrs.shape[0] <= 30:
                    ...
                elif corrs.shape[0] <= 60:
                    corrs = corrs.reset_index()
                    corrs.columns = ["å› å­åç§°", "ç›¸å…³ç³»æ•°"]
                    corrs1 = corrs.iloc[:30, :]
                    corrs2 = corrs.iloc[30:, :].reset_index(drop=True)
                    corrs = pd.concat([corrs1, corrs2], axis=1).fillna("")
                elif corrs.shape[0] <= 90:
                    corrs = corrs.reset_index()
                    corrs.columns = ["å› å­åç§°", "ç›¸å…³ç³»æ•°"]
                    corrs1 = corrs.iloc[:30, :]
                    corrs2 = corrs.iloc[30:60, :].reset_index(drop=True)
                    corrs3 = corrs.iloc[60:90, :].reset_index(drop=True)
                    corrs = pd.concat([corrs1, corrs2, corrs3], axis=1).fillna("")
            else:
                olds = [df] + olds
                corrs = show_corrs(olds, old_orders, method=method)
        else:
            corrs = show_corrs(olds, old_orders, method=method)
    return corrs.sort_index()


@do_on_dfs
def remove_unavailable(df: pd.DataFrame) -> pd.DataFrame:
    """å¯¹æ—¥é¢‘æˆ–æœˆé¢‘å› å­å€¼ï¼Œå‰”é™¤stè‚¡ã€ä¸æ­£å¸¸äº¤æ˜“çš„è‚¡ç¥¨å’Œä¸Šå¸‚ä¸è¶³60å¤©çš„è‚¡ç¥¨

    Parameters
    ----------
    df : pd.DataFrame
        å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯å› å­å€¼

    Returns
    -------
    pd.DataFrame
        å‰”é™¤åçš„å› å­å€¼
    """
    df0 = df.resample("M").last()
    if df.shape[0] / df0.shape[0] > 2:
        daily = 1
    else:
        daily = 0
    if daily:
        state = read_daily(state=1).replace(0, np.nan)
        st = read_daily(st=1)
        age = read_daily(age=1)
        st = (1 - st).replace(0, np.nan)
        age = ((age >= 60) + 0).replace(0, np.nan)
        df = df * age * st * state
    else:
        moon = pure_moon(no_read_indu=1)
        moon.set_basic_data()
        moon.judge_month()
        df = moon.tris_monthly * df
    return df


class frequency_controller(object):
    def __init__(self, freq: str):
        self.homeplace = HomePlace()
        self.freq = freq

        if freq == "M":
            self.counts_one_year = 12
            self.time_shift = pd.DateOffset(months=1)
            self.states_files = (
                self.homeplace.daily_data_file + "states_monthly.parquet"
            )
            self.sts_files = self.homeplace.daily_data_file + "sts_monthly.parquet"
            self.comment_name = "æœˆ"
            self.days_in = 20
        elif freq == "W":
            self.counts_one_year = 52
            self.time_shift = pd.DateOffset(weeks=1)
            self.states_files = self.homeplace.daily_data_file + "states_weekly.parquet"
            self.sts_files = self.homeplace.daily_data_file + "sts_weekly.parquet"
            self.comment_name = "å‘¨"
            self.days_in = 5
        else:
            raise ValueError("'æš‚æ—¶ä¸æ”¯æŒæ­¤é¢‘ç‡å“ˆğŸ¤’ï¼Œç›®å‰ä»…æ”¯æŒæœˆé¢‘'M'ï¼Œå’Œå‘¨é¢‘'W'")

    def next_end(self, x):
        """æ‰¾åˆ°ä¸‹ä¸ªå‘¨æœŸçš„æœ€åä¸€å¤©"""
        if self.freq == "M":
            return x + pd.DateOffset(months=1) + pd.tseries.offsets.MonthEnd()
        elif self.freq == "W":
            return x + pd.DateOffset(weeks=1)
        else:
            raise ValueError("'æš‚æ—¶ä¸æ”¯æŒæ­¤é¢‘ç‡å“ˆğŸ¤’ï¼Œç›®å‰ä»…æ”¯æŒæœˆé¢‘'M'ï¼Œå’Œå‘¨é¢‘'W'")


class pure_moon(object):
    __slots__ = [
        "homeplace",
        "sts_monthly_file",
        "states_monthly_file",
        "factors",
        "codes",
        "tradedays",
        "ages",
        "amounts",
        "closes",
        "opens",
        "capital",
        "states",
        "sts",
        "turnovers",
        "sts_monthly",
        "states_monthly",
        "ages_monthly",
        "tris_monthly",
        "opens_monthly",
        "closes_monthly",
        "rets_monthly",
        "opens_monthly_shift",
        "rets_monthly_begin",
        "limit_ups",
        "limit_downs",
        "data",
        "ic_icir_and_rank",
        "rets_monthly_limit_downs",
        "group_rets",
        "long_short_rets",
        "long_short_net_values",
        "group_net_values",
        "long_short_ret_yearly",
        "long_short_vol_yearly",
        "long_short_info_ratio",
        "long_short_win_times",
        "long_short_win_ratio",
        "retreats",
        "max_retreat",
        "long_short_comments",
        "total_comments",
        "square_rets",
        "cap",
        "cap_value",
        "industry_dummy",
        "industry_codes",
        "industry_codes_str",
        "industry_ws",
        "__factors_out",
        "ics",
        "rankics",
        "factor_turnover_rates",
        "factor_turnover_rate",
        "group_rets_std",
        "group_rets_stds",
        "group_rets_skews",
        "group_rets_skew",
        "wind_out",
        "swindustry_dummy",
        "zxindustry_dummy",
        "closes2_monthly",
        "rets_monthly_last",
        "freq_ctrl",
        "freq",
        "factor_cover",
        "factor_cross_skew",
        "factor_cross_skew_after_neu",
        "pos_neg_rate",
        "corr_itself",
        "factor_cross_stds",
        "corr_itself_shift2",
        "rets_all",
        "inner_long_ret_yearly",
        "inner_short_ret_yearly",
        "inner_long_net_values",
        "inner_short_net_values",
        "group_mean_rets_monthly"
    ]

    @classmethod
    @lru_cache(maxsize=None)
    def __init__(
        cls,
        freq: str = "M",
        no_read_indu: bool = 0,
        swindustry_dummy: pd.DataFrame = None,
        zxindustry_dummy: pd.DataFrame = None,
        read_in_swindustry_dummy: bool = 0,
    ):
        cls.homeplace = HomePlace()
        cls.freq = freq
        cls.freq_ctrl = frequency_controller(freq)
        # å·²ç»ç®—å¥½çš„æœˆåº¦stçŠ¶æ€æ–‡ä»¶
        # week_here
        cls.sts_monthly_file = cls.freq_ctrl.sts_files
        # å·²ç»ç®—å¥½çš„æœˆåº¦äº¤æ˜“çŠ¶æ€æ–‡ä»¶
        # week_here
        cls.states_monthly_file = cls.freq_ctrl.states_files

        if swindustry_dummy is not None:
            cls.swindustry_dummy = swindustry_dummy
        if zxindustry_dummy is not None:
            cls.zxindustry_dummy = zxindustry_dummy

        def deal_dummy(industry_dummy):
            industry_dummy = industry_dummy.drop(columns=["code"]).reset_index()
            industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
            col = ["code", "date"] + industry_ws
            industry_dummy.columns = col
            industry_dummy = industry_dummy[
                industry_dummy.date >= pd.Timestamp("20100101")
            ]
            return industry_dummy

        if (swindustry_dummy is None) and (zxindustry_dummy is None):
            if not no_read_indu:
                if read_in_swindustry_dummy:
                    # week_here
                    cls.swindustry_dummy = (
                        pd.read_parquet(
                            cls.homeplace.daily_data_file + "ç”³ä¸‡è¡Œä¸š2021ç‰ˆå“‘å˜é‡.parquet"
                        )
                        .fillna(0)
                        .set_index("date")
                        .groupby("code")
                        .resample(freq)
                        .last()
                    )
                    cls.swindustry_dummy = deal_dummy(cls.swindustry_dummy)
                # week_here
                cls.zxindustry_dummy = (
                    pd.read_parquet(
                        cls.homeplace.daily_data_file + "ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ä»£ç ç‰ˆ.parquet"
                    )
                    .fillna(0)
                    .set_index("date")
                    .groupby("code")
                    .resample(freq)
                    .last()
                    .fillna(0)
                )

                cls.zxindustry_dummy = deal_dummy(cls.zxindustry_dummy)

    @property
    def factors_out(self):
        return self.__factors_out

    def __call__(self):
        """è°ƒç”¨å¯¹è±¡åˆ™è¿”å›å› å­å€¼"""
        return self.factors_out

    @classmethod
    @lru_cache(maxsize=None)
    def set_basic_data(
        cls,
        ages: pd.DataFrame = None,
        sts: pd.DataFrame = None,
        states: pd.DataFrame = None,
        opens: pd.DataFrame = None,
        closes: pd.DataFrame = None,
        capitals: pd.DataFrame = None,
        opens_average_first_day: bool = 0,
        total_cap: bool = 0,
    ):
        if ages is None:
            ages = read_daily(age=1, start=20100101)
        if sts is None:
            sts = read_daily(st=1, start=20100101)
        if states is None:
            states = read_daily(state=1, start=20100101)
        if opens is None:
            if opens_average_first_day:
                opens = read_daily(vwap=1, start=20100101)
            else:
                opens = read_daily(open=1, start=20100101)
        if closes is None:
            closes = read_daily(close=1, start=20100101)
        if capitals is None:
            if total_cap:
                capitals = (
                    read_daily(total_cap=1, start=20100101).resample(cls.freq).last()
                )
            else:
                capitals = (
                    read_daily(flow_cap=1, start=20100101).resample(cls.freq).last()
                )
        # ä¸Šå¸‚å¤©æ•°æ–‡ä»¶
        cls.ages = ages
        # stæ—¥å­æ ‡å¿—æ–‡ä»¶
        cls.sts = sts.fillna(0)
        # cls.sts = 1 - cls.sts.fillna(0)
        # äº¤æ˜“çŠ¶æ€æ–‡ä»¶
        cls.states = states
        # å¤æƒå¼€ç›˜ä»·æ•°æ®æ–‡ä»¶
        cls.opens = opens
        # å¤æƒæ”¶ç›˜ä»·æ•°æ®æ–‡ä»¶
        cls.closes = closes
        # æœˆåº•æµé€šå¸‚å€¼æ•°æ®
        cls.capital = capitals
        if cls.opens is not None:
            cls.opens = cls.opens.replace(0, np.nan)
        if cls.closes is not None:
            cls.closes = cls.closes.replace(0, np.nan)

    def set_factor_df_date_as_index(self, df: pd.DataFrame):
        """è®¾ç½®å› å­æ•°æ®çš„dataframeï¼Œå› å­è¡¨åˆ—ååº”ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•åº”ä¸ºæ—¶é—´"""
        # week_here
        self.factors = df.resample(self.freq).last().dropna(how="all")
        self.factor_cover = np.sign(self.factors.abs() + 1).sum().sum()
        opens = self.opens[self.opens.index >= self.factors.index.min()]
        total = np.sign(opens.resample(self.freq).last()).sum().sum()
        self.factor_cover = min(self.factor_cover / total, 1)
        self.factor_cross_skew = self.factors.skew(axis=1).mean()
        pos_num = ((self.factors > 0) + 0).sum().sum()
        neg_num = ((self.factors < 0) + 0).sum().sum()
        self.pos_neg_rate = pos_num / (neg_num + pos_num)
        self.corr_itself = show_corr(self.factors, self.factors.shift(1), plt_plot=0)
        self.corr_itself_shift2 = show_corr(
            self.factors, self.factors.shift(2), plt_plot=0
        )
        self.factor_cross_stds = self.factors.std(axis=1)

    @classmethod
    def judge_month_st(cls, df):
        """æ¯”è¾ƒä¸€ä¸ªæœˆå†…stçš„å¤©æ•°ï¼Œå¦‚æœstå¤©æ•°å¤šï¼Œå°±åˆ é™¤æœ¬æœˆï¼Œå¦‚æœæ­£å¸¸å¤šï¼Œå°±ä¿ç•™æœ¬æœˆ"""
        st_count = len(df[df == 1])
        normal_count = len(df[df != 1])
        if st_count >= normal_count:
            return 0
        else:
            return 1

    @classmethod
    def judge_month_state(cls, df):
        """æ¯”è¾ƒä¸€ä¸ªæœˆå†…éæ­£å¸¸äº¤æ˜“çš„å¤©æ•°ï¼Œå¦‚æœéæ­£å¸¸äº¤æ˜“å¤©æ•°å¤šï¼Œå°±åˆ é™¤æœ¬æœˆï¼Œå¦åˆ™ä¿ç•™æœ¬æœˆ"""
        abnormal_count = len(df[df == 0])
        normal_count = len(df[df == 1])
        if abnormal_count >= normal_count:
            return 0
        else:
            return 1

    @classmethod
    def read_add(cls, pridf, df, func):
        """ç”±äºæ•°æ®æ›´æ–°ï¼Œè¿‡å»è®¡ç®—çš„æœˆåº¦çŠ¶æ€å¯èƒ½éœ€è¦è¿½åŠ """
        if pridf.index.max() > df.index.max():
            df_add = pridf[pridf.index > df.index.max()]
            if df_add.shape[0] > int(cls.freq_ctrl.days_in / 2):
                df_1 = df_add.index.max()
                year = df_1.year
                month = df_1.month
                last = tt.date.get_close(year=year, m=month).pd_date()
                if (last == df_1)[0]:
                    # week_here
                    df_add = df_add.resample(cls.freq).apply(func)
                    df = pd.concat([df, df_add])
                    return df
                else:
                    df_add = df_add[
                        df_add.index < pd.Timestamp(year=year, month=month, day=1)
                    ]
                    if df_add.shape[0] > 0:
                        df_add = df_add.resample(cls.freq).apply(func)
                        df = pd.concat([df, df_add])
                        return df
                    else:
                        return df
            else:
                return df
        else:
            return df

    @classmethod
    def daily_to_monthly(cls, pridf, path, func):
        """æŠŠæ—¥åº¦çš„äº¤æ˜“çŠ¶æ€ã€stã€ä¸Šå¸‚å¤©æ•°ï¼Œè½¬åŒ–ä¸ºæœˆåº¦çš„ï¼Œå¹¶ç”Ÿæˆèƒ½å¦äº¤æ˜“çš„åˆ¤æ–­
        è¯»å–æœ¬åœ°å·²ç»ç®—å¥½çš„æ–‡ä»¶ï¼Œå¹¶è¿½åŠ æ–°çš„æ—¶é—´æ®µéƒ¨åˆ†ï¼Œå¦‚æœæœ¬åœ°æ²¡æœ‰å°±ç›´æ¥å…¨éƒ¨é‡æ–°ç®—"""
        try:
            month_df = pd.read_parquet(path)
            month_df = cls.read_add(pridf, month_df, func)
            month_df.to_parquet(path)
        except Exception as e:
            if not STATES["NO_LOG"]:
                logger.error("error occurs when read state files")
                logger.error(e)
            print("state file rewritingâ€¦â€¦")
            # week_here
            df_1 = pridf.index.max()
            year = df_1.year
            month = df_1.month
            last = tt.date.get_close(year=year, m=month).pd_date()
            if not (last == df_1)[0]:
                pridf = pridf[pridf.index < pd.Timestamp(year=year, month=month, day=1)]
            month_df = pridf.resample(cls.freq).apply(func)
            month_df.to_parquet(path)
        return month_df

    @classmethod
    @lru_cache(maxsize=None)
    def judge_month(cls):
        """ç”Ÿæˆä¸€ä¸ªæœˆç»¼åˆåˆ¤æ–­çš„è¡¨æ ¼"""
        if cls.freq == "M":
            cls.sts_monthly = cls.daily_to_monthly(
                cls.sts, cls.sts_monthly_file, cls.judge_month_st
            )
            cls.states_monthly = cls.daily_to_monthly(
                cls.states, cls.states_monthly_file, cls.judge_month_state
            )
            # week_here
            cls.ages_monthly = (cls.ages.resample(cls.freq).last() > 60) + 0
            cls.tris_monthly = cls.sts_monthly * cls.states_monthly * cls.ages_monthly
            cls.tris_monthly = cls.tris_monthly.replace(0, np.nan)
        else:
            cls.tris_monthly = (
                (1 - cls.sts).resample(cls.freq).last().ffill(limit=2)
                * cls.states.resample(cls.freq).last().ffill(limit=2)
                * ((cls.ages.resample(cls.freq).last() > 60) + 0)
            ).replace(0, np.nan)

    @classmethod
    @lru_cache(maxsize=None)
    def get_rets_month(cls):
        """è®¡ç®—æ¯æœˆçš„æ”¶ç›Šç‡ï¼Œå¹¶æ ¹æ®æ¯æœˆåšå‡ºäº¤æ˜“çŠ¶æ€ï¼Œåšå‡ºåˆ å‡"""
        # week_here
        cls.opens_monthly = cls.opens.resample(cls.freq).first()
        # week_here
        cls.closes_monthly = cls.closes.resample(cls.freq).last()
        cls.rets_monthly = (cls.closes_monthly - cls.opens_monthly) / cls.opens_monthly
        cls.rets_monthly = cls.rets_monthly * cls.tris_monthly
        cls.rets_monthly = cls.rets_monthly.stack().reset_index()
        cls.rets_monthly.columns = ["date", "code", "ret"]

    @classmethod
    def neutralize_factors(cls, df):
        """ç»„å†…å¯¹å› å­è¿›è¡Œå¸‚å€¼ä¸­æ€§åŒ–"""
        industry_codes = list(df.columns)
        industry_codes = [i for i in industry_codes if i.startswith("w")]
        industry_codes_str = "+".join(industry_codes)
        if len(industry_codes_str) > 0:
            ols_result = smf.ols("fac~cap_size+" + industry_codes_str, data=df).fit()
        else:
            ols_result = smf.ols("fac~cap_size", data=df).fit()
        ols_w = ols_result.params["cap_size"]
        ols_b = ols_result.params["Intercept"]
        ols_bs = {}
        for ind in industry_codes:
            ols_bs[ind] = ols_result.params[ind]
        df.fac = df.fac - ols_w * df.cap_size - ols_b
        for k, v in ols_bs.items():
            df.fac = df.fac - v * df[k]
        df = df[["fac"]]
        return df

    @classmethod
    @lru_cache(maxsize=None)
    def get_log_cap(cls, boxcox=True):
        """è·å¾—å¯¹æ•°å¸‚å€¼"""
        cls.cap = cls.capital.stack().reset_index()
        cls.cap.columns = ["date", "code", "cap_size"]
        if boxcox:

            def single(x):
                x.cap_size = ss.boxcox(x.cap_size)[0]
                return x

            cls.cap = cls.cap.groupby(["date"]).apply(single)
        else:
            cls.cap["cap_size"] = np.log(cls.cap["cap_size"])

    def get_neutral_factors(
        self, zxindustry_dummies=0, swindustry_dummies=0, only_cap=0
    ):
        """å¯¹å› å­è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–"""
        # week_here
        self.factors.index = self.factors.index + self.freq_ctrl.time_shift
        # week_here
        self.factors = self.factors.resample(self.freq).last()
        # week_here
        last_date = self.freq_ctrl.next_end(self.tris_monthly.index.max())
        add_tail = pd.DataFrame(1, index=[last_date], columns=self.tris_monthly.columns)
        tris_monthly = pd.concat([self.tris_monthly, add_tail])
        self.factors = self.factors * tris_monthly
        # week_here
        self.factors.index = self.factors.index - self.freq_ctrl.time_shift
        # week_here
        self.factors = self.factors.resample(self.freq).last()
        self.factors = self.factors.stack().reset_index()
        self.factors.columns = ["date", "code", "fac"]
        self.factors = pd.merge(
            self.factors, self.cap, how="inner", on=["date", "code"]
        )
        if not only_cap:
            if swindustry_dummies:
                self.factors = pd.merge(
                    self.factors, self.swindustry_dummy, on=["date", "code"]
                )
            else:
                self.factors = pd.merge(
                    self.factors, self.zxindustry_dummy, on=["date", "code"]
                )
        self.factors = self.factors.set_index(["date", "code"])
        self.factors = self.factors.groupby(["date"]).apply(self.neutralize_factors)
        self.factors = self.factors.reset_index()

    def deal_with_factors(self):
        """åˆ é™¤ä¸ç¬¦åˆäº¤æ˜“æ¡ä»¶çš„å› å­æ•°æ®"""
        self.__factors_out = self.factors.copy()
        # week_here
        self.factors.index = self.factors.index + self.freq_ctrl.time_shift
        # week_here
        self.factors = self.factors.resample(self.freq).last()
        self.factors = self.factors * self.tris_monthly
        self.factors = self.factors.stack().reset_index()
        self.factors.columns = ["date", "code", "fac"]

    def deal_with_factors_after_neutralize(self):
        """ä¸­æ€§åŒ–ä¹‹åçš„å› å­å¤„ç†æ–¹æ³•"""
        self.factors = self.factors.set_index(["date", "code"])
        self.factors = self.factors.unstack()
        self.__factors_out = self.factors.copy()
        self.__factors_out.columns = [i[1] for i in list(self.__factors_out.columns)]
        # week_here
        self.factors.index = self.factors.index + self.freq_ctrl.time_shift
        # week_here
        self.factors = self.factors.resample(self.freq).last()
        self.factors.columns = list(map(lambda x: x[1], list(self.factors.columns)))
        self.factors = self.factors.stack().reset_index()
        self.factors.columns = ["date", "code", "fac"]

    @classmethod
    def find_limit(cls, df, up=1):
        """è®¡ç®—æ¶¨è·Œå¹…è¶…è¿‡9.8%çš„è‚¡ç¥¨ï¼Œå¹¶å°†å…¶å­˜å‚¨è¿›ä¸€ä¸ªé•¿åˆ—è¡¨é‡Œ
        å…¶ä¸­æ—¶é—´åˆ—ï¼Œä¸ºæŸæœˆçš„æœ€åä¸€å¤©ï¼›æ¶¨åœæ—¥è™½ç„¶ä¸ºä¸‹æœˆåˆç¬¬ä¸€å¤©ï¼Œä½†è¿™é‡Œæ ‡æ³¨çš„æ—¶é—´ç»Ÿä¸€ä¸ºä¸Šæœˆæœ€åä¸€å¤©"""
        limit_df = np.sign(df.applymap(lambda x: x - up * 0.098)).replace(
            -1 * up, np.nan
        )
        limit_df = limit_df.stack().reset_index()
        limit_df.columns = ["date", "code", "limit_up_signal"]
        limit_df = limit_df[["date", "code"]]
        return limit_df

    @classmethod
    @lru_cache(maxsize=None)
    def get_limit_ups_downs(cls):
        """æ‰¾æœˆåˆç¬¬ä¸€å¤©å°±æ¶¨åœ"""
        """æˆ–è€…æ˜¯æœˆæœ«è·Œåœçš„è‚¡ç¥¨"""
        cls.opens_monthly_shift = cls.opens_monthly.copy()
        cls.opens_monthly_shift = cls.opens_monthly_shift.shift(-1)
        cls.rets_monthly_begin = (
            cls.opens_monthly_shift - cls.closes_monthly
        ) / cls.closes_monthly
        # week_here
        cls.closes2_monthly = cls.closes.shift(1).resample(cls.freq).last()
        cls.rets_monthly_last = (
            cls.closes_monthly - cls.closes2_monthly
        ) / cls.closes2_monthly
        cls.limit_ups = cls.find_limit(cls.rets_monthly_begin, up=1)
        cls.limit_downs = cls.find_limit(cls.rets_monthly_last, up=-1)

    def get_ic_rankic(cls, df):
        """è®¡ç®—ICå’ŒRankIC"""
        df1 = df[["ret", "fac"]]
        ic = df1.corr(method="pearson").iloc[0, 1]
        rankic = df1.rank().corr().iloc[0, 1]
        df2 = pd.DataFrame({"ic": [ic], "rankic": [rankic]})
        return df2

    def get_icir_rankicir(cls, df):
        """è®¡ç®—ICIRå’ŒRankICIR"""
        ic = df.ic.mean()
        rankic = df.rankic.mean()
        # week_here
        icir = ic / np.std(df.ic) * (cls.freq_ctrl.counts_one_year ** (0.5))
        # week_here
        rankicir = rankic / np.std(df.rankic) * (cls.freq_ctrl.counts_one_year ** (0.5))
        return pd.DataFrame(
            {"IC": [ic], "ICIR": [icir], "RankIC": [rankic], "RankICIR": [rankicir]},
            index=["è¯„ä»·æŒ‡æ ‡"],
        )

    def get_ic_icir_and_rank(cls, df):
        """è®¡ç®—ICã€ICIRã€RankICã€RankICIR"""
        df1 = df.groupby("date").apply(cls.get_ic_rankic)
        cls.ics = df1.ic
        cls.rankics = df1.rankic
        cls.ics = cls.ics.reset_index(drop=True, level=1).to_frame()
        cls.rankics = cls.rankics.reset_index(drop=True, level=1).to_frame()
        df2 = cls.get_icir_rankicir(df1)
        df2 = df2.T
        dura = (df.date.max() - df.date.min()).days / 365
        t_value = df2.iloc[3, 0] * (dura ** (1 / 2))
        df3 = pd.DataFrame({"è¯„ä»·æŒ‡æ ‡": [t_value]}, index=["RankIC.t"])
        df4 = pd.concat([df2, df3])
        return df4

    @classmethod
    def get_groups(cls, df, groups_num):
        """ä¾æ®å› å­å€¼ï¼Œåˆ¤æ–­æ˜¯åœ¨ç¬¬å‡ ç»„"""
        if "group" in list(df.columns):
            df = df.drop(columns=["group"])
        df = df.sort_values(["fac"], ascending=True)
        each_group = round(df.shape[0] / groups_num)
        l = list(
            map(
                lambda x, y: [x] * y,
                list(range(1, groups_num + 1)),
                [each_group] * groups_num,
            )
        )
        l = reduce(lambda x, y: x + y, l)
        if len(l) < df.shape[0]:
            l = l + [groups_num] * (df.shape[0] - len(l))
        l = l[: df.shape[0]]
        df.insert(0, "group", l)
        return df

    @classmethod
    def limit_old_to_new(cls, limit, data):
        """è·å–è·Œåœè‚¡åœ¨æ—§æœˆçš„ç»„å·ï¼Œç„¶åå°†æ—¥æœŸè°ƒæ•´åˆ°æ–°æœˆé‡Œ
        æ¶¨åœè‚¡åˆ™è·å¾—æ–°æœˆé‡Œæ¶¨åœè‚¡çš„ä»£ç å’Œæ—¶é—´ï¼Œç„¶åç›´æ¥åˆ å»"""
        data1 = data.copy()
        data1 = data1.reset_index()
        data1.columns = ["data_index"] + list(data1.columns)[1:]
        old = pd.merge(limit, data1, how="inner", on=["date", "code"])
        old = old.set_index("data_index")
        old = old[["group", "date", "code"]]
        # week_here
        old.date = list(map(cls.freq_ctrl.next_end, list(old.date)))
        return old

    def get_data(self, groups_num):
        """æ‹¼æ¥å› å­æ•°æ®å’Œæ¯æœˆæ”¶ç›Šç‡æ•°æ®ï¼Œå¹¶å¯¹æ¶¨åœå’Œè·Œåœè‚¡åŠ ä»¥å¤„ç†"""
        self.data = pd.merge(
            self.rets_monthly, self.factors, how="inner", on=["date", "code"]
        )
        self.ic_icir_and_rank = self.get_ic_icir_and_rank(self.data)
        self.data = self.data.groupby("date").apply(
            lambda x: self.get_groups(x, groups_num)
        )
        self.wind_out = self.data.copy()
        self.factor_turnover_rates = self.data.pivot(
            index="date", columns="code", values="group"
        )
        rates = []
        for i in range(1, groups_num + 1):
            son = (self.factor_turnover_rates == i) + 0
            son1 = son.diff()
            # self.factor_turnover_rates = self.factor_turnover_rates.diff()
            change = ((np.abs(np.sign(son1)) == 1) + 0).sum(axis=1)
            still = (((son1 == 0) + 0) * son).sum(axis=1)
            rate = change / (change + still)
            rates.append(rate.to_frame(f"group{i}"))
        rates = pd.concat(rates, axis=1).fillna(0)
        self.factor_turnover_rates = rates
        self.data = self.data.reset_index(drop=True)
        limit_ups_object = self.limit_old_to_new(self.limit_ups, self.data)
        limit_downs_object = self.limit_old_to_new(self.limit_downs, self.data)
        self.data = self.data.drop(limit_ups_object.index)
        rets_monthly_limit_downs = pd.merge(
            self.rets_monthly, limit_downs_object, how="inner", on=["date", "code"]
        )
        self.data = pd.concat([self.data, rets_monthly_limit_downs])

    def make_start_to_one(self, l):
        """è®©å‡€å€¼åºåˆ—çš„ç¬¬ä¸€ä¸ªæ•°å˜æˆ1"""
        min_date = self.factors.date.min()
        add_date = min_date - relativedelta(days=min_date.day)
        add_l = pd.Series([1], index=[add_date])
        l = pd.concat([add_l, l])
        return l

    def to_group_ret(self, l):
        """æ¯ä¸€ç»„çš„å¹´åŒ–æ”¶ç›Šç‡"""
        # week_here
        ret = l[-1] ** (self.freq_ctrl.counts_one_year / len(l)) - 1
        return ret

    def get_group_rets_net_values(
        self, groups_num=10, value_weighted=False, trade_cost_double_side=0
    ):
        """è®¡ç®—ç»„å†…æ¯ä¸€æœŸçš„å¹³å‡æ”¶ç›Šï¼Œç”Ÿæˆæ¯æ—¥æ”¶ç›Šç‡åºåˆ—å’Œå‡€å€¼åºåˆ—"""
        if value_weighted:
            cap_value = self.capital.copy()
            # week_here
            cap_value = cap_value.resample(self.freq).last().shift(1)
            cap_value = cap_value * self.tris_monthly
            # cap_value=np.log(cap_value)
            cap_value = cap_value.stack().reset_index()
            cap_value.columns = ["date", "code", "cap_value"]
            self.data = pd.merge(self.data, cap_value, on=["date", "code"])

            def in_g(df):
                df.cap_value = df.cap_value / df.cap_value.sum()
                df.ret = df.ret * df.cap_value
                return df.ret.sum()

            self.group_rets = self.data.groupby(["date", "group"]).apply(in_g)
            self.rets_all = self.data.groupby(["date"]).apply(in_g)
            self.group_rets_std = "å¸‚å€¼åŠ æƒæš‚æœªè®¾ç½®è¯¥åŠŸèƒ½ï¼Œæ•¬è¯·æœŸå¾…ğŸŒ™"
        else:
            self.group_rets = self.data.groupby(["date", "group"]).apply(
                lambda x: x.ret.mean()
            )
            self.rets_all = self.data.groupby(["date"]).apply(lambda x: x.ret.mean())
            self.group_rets_stds = self.data.groupby(["date", "group"]).ret.std()
            self.group_rets_std = (
                self.group_rets_stds.reset_index().groupby("group").mean()
            )
            self.group_rets_skews = self.data.groupby(["date", "group"]).ret.skew()
            self.group_rets_skew = (
                self.group_rets_skews.reset_index().groupby("group").mean()
            )
        # dropnaæ˜¯å› ä¸ºå¦‚æœè‚¡ç¥¨è¡Œæƒ…æ•°æ®æ¯”å› å­æ•°æ®çš„æˆªæ­¢æ—¥æœŸæ™šï¼Œè€Œæœ€åä¸€ä¸ªæœˆå‘ç”Ÿæœˆåˆè·Œåœæ—¶ï¼Œä¼šé€ æˆæœ€åæŸç»„å¤šå‡ºä¸€ä¸ªæœˆçš„æ•°æ®
        self.group_rets = self.group_rets.unstack()
        self.group_rets = self.group_rets[
            self.group_rets.index <= self.factors.date.max()
        ]
        self.group_rets.columns = list(map(str, list(self.group_rets.columns)))
        self.group_rets = self.group_rets.add_prefix("group")
        self.group_rets = (
            self.group_rets - self.factor_turnover_rates * trade_cost_double_side
        )
        self.rets_all = (
            self.rets_all
            - self.factor_turnover_rates.mean(axis=1) * trade_cost_double_side
        ).dropna()
        self.long_short_rets = (
            self.group_rets["group1"] - self.group_rets["group" + str(groups_num)]
        )
        self.inner_rets_long = self.group_rets.group1 - self.rets_all
        self.inner_rets_short = (
            self.rets_all - self.group_rets["group" + str(groups_num)]
        )
        self.long_short_net_values = self.make_start_to_one(
            (self.long_short_rets + 1).cumprod()
        )
        if self.long_short_net_values[-1] <= self.long_short_net_values[0]:
            self.long_short_rets = (
                self.group_rets["group" + str(groups_num)] - self.group_rets["group1"]
            )
            self.long_short_net_values = self.make_start_to_one(
                (self.long_short_rets + 1).cumprod()
            )
            self.inner_rets_long = (
                self.group_rets["group" + str(groups_num)] - self.rets_all
            )
            self.inner_rets_short = self.rets_all - self.group_rets.group1
        self.inner_long_net_values = self.make_start_to_one(
            (self.inner_rets_long + 1).cumprod()
        )
        self.inner_short_net_values = self.make_start_to_one(
            (self.inner_rets_short + 1).cumprod()
        )
        self.group_rets = self.group_rets.assign(long_short=self.long_short_rets)
        self.group_net_values = self.group_rets.applymap(lambda x: x + 1)
        self.group_net_values = self.group_net_values.cumprod()
        self.group_net_values = self.group_net_values.apply(self.make_start_to_one)
        a = groups_num ** (0.5)
        # åˆ¤æ–­æ˜¯å¦è¦ä¸¤ä¸ªå› å­ç”»è¡¨æ ¼
        if a == int(a):
            self.square_rets = (
                self.group_net_values.iloc[:, :-1].apply(self.to_group_ret).to_numpy()
            )
            self.square_rets = self.square_rets.reshape((int(a), int(a)))
            self.square_rets = pd.DataFrame(
                self.square_rets,
                columns=list(range(1, int(a) + 1)),
                index=list(range(1, int(a) + 1)),
            )
            print("è¿™æ˜¯self.square_rets", self.square_rets)

    def get_long_short_comments(self, on_paper=False):
        """è®¡ç®—å¤šç©ºå¯¹å†²çš„ç›¸å…³è¯„ä»·æŒ‡æ ‡
        åŒ…æ‹¬å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€æœˆåº¦èƒœç‡ã€æœ€å¤§å›æ’¤ç‡"""
        # week_here
        self.long_short_ret_yearly = (
            self.long_short_net_values[-1]
            ** (self.freq_ctrl.counts_one_year / len(self.long_short_net_values))
            - 1
        )
        self.inner_long_ret_yearly = (
            self.inner_long_net_values[-1]
            ** (self.freq_ctrl.counts_one_year / len(self.inner_long_net_values))
            - 1
        )
        self.inner_short_ret_yearly = (
            self.inner_short_net_values[-1]
            ** (self.freq_ctrl.counts_one_year / len(self.inner_short_net_values))
            - 1
        )
        # week_here
        self.long_short_vol_yearly = np.std(self.long_short_rets) * (
            self.freq_ctrl.counts_one_year**0.5
        )
        self.long_short_info_ratio = (
            self.long_short_ret_yearly / self.long_short_vol_yearly
        )
        self.long_short_win_times = len(self.long_short_rets[self.long_short_rets > 0])
        self.long_short_win_ratio = self.long_short_win_times / len(
            self.long_short_rets
        )
        self.max_retreat = -(
            self.long_short_net_values / self.long_short_net_values.expanding(1).max()
            - 1
        ).min()
        if on_paper:
            self.long_short_comments = pd.DataFrame(
                {
                    "è¯„ä»·æŒ‡æ ‡": [
                        self.long_short_ret_yearly,
                        self.long_short_vol_yearly,
                        self.long_short_info_ratio,
                        self.long_short_win_ratio,
                        self.max_retreat,
                    ]
                },
                # week_here
                index=[
                    "å¹´åŒ–æ”¶ç›Šç‡",
                    "å¹´åŒ–æ³¢åŠ¨ç‡",
                    "æ”¶ç›Šæ³¢åŠ¨æ¯”",
                    f"{self.freq_ctrl.comment_name}åº¦èƒœç‡",
                    "æœ€å¤§å›æ’¤ç‡",
                ],
            )
        else:
            self.long_short_comments = pd.DataFrame(
                {
                    "è¯„ä»·æŒ‡æ ‡": [
                        self.long_short_ret_yearly,
                        self.long_short_vol_yearly,
                        self.long_short_info_ratio,
                        self.long_short_win_ratio,
                        self.max_retreat,
                    ]
                },
                # week_here
                index=[
                    "å¹´åŒ–æ”¶ç›Šç‡",
                    "å¹´åŒ–æ³¢åŠ¨ç‡",
                    "ä¿¡æ¯æ¯”ç‡",
                    f"{self.freq_ctrl.comment_name}åº¦èƒœç‡",
                    "æœ€å¤§å›æ’¤ç‡",
                ],
            )

    def get_total_comments(self, groups_num):
        """ç»¼åˆICã€ICIRã€RankICã€RankICIR,å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€èƒœç‡ã€æœ€å¤§å›æ’¤ç‡"""
        rankic = self.rankics.mean()
        rankic_win = self.rankics[self.rankics * rankic > 0]
        rankic_win_ratio = rankic_win.dropna().shape[0] / self.rankics.dropna().shape[0]
        self.factor_cross_skew_after_neu = self.__factors_out.skew(axis=1).mean()
        if self.ic_icir_and_rank.iloc[2, 0] > 0:
            self.factor_turnover_rate = self.factor_turnover_rates[
                f"group{groups_num}"
            ].mean()
        else:
            self.factor_turnover_rate = self.factor_turnover_rates["group1"].mean()
        self.total_comments = pd.concat(
            [
                self.ic_icir_and_rank,
                pd.DataFrame(
                    {"è¯„ä»·æŒ‡æ ‡": [rankic_win_ratio]},
                    index=["RankICèƒœç‡"],
                ),
                self.long_short_comments,
                # week_here
                pd.DataFrame(
                    {
                        "è¯„ä»·æŒ‡æ ‡": [
                            self.factor_turnover_rate,
                            self.factor_cover,
                            self.pos_neg_rate,
                            self.factor_cross_skew,
                            self.inner_long_ret_yearly,
                            self.inner_long_ret_yearly
                            / (
                                self.inner_long_ret_yearly + self.inner_short_ret_yearly
                            ),
                            self.corr_itself,
                        ]
                    },
                    index=[
                        f"å¤šå¤´{self.freq_ctrl.comment_name}å‡æ¢æ‰‹",
                        "å› å­è¦†ç›–ç‡",
                        "å› å­æ­£å€¼å æ¯”",
                        "å› å­æˆªé¢ååº¦",
                        "å¤šå¤´è¶…å‡æ”¶ç›Š",
                        "å¤šå¤´æ”¶ç›Šå æ¯”",
                        "ä¸€é˜¶è‡ªç›¸å…³æ€§",
                    ],
                ),
            ]
        )
        self.group_mean_rets_monthly=self.group_rets.drop(columns=['long_short']).mean()
        self.group_mean_rets_monthly=self.group_mean_rets_monthly-self.group_mean_rets_monthly.mean()

    def plot_net_values(self, y2, filename, iplot=1, ilegend=1, without_breakpoint=0):
        """ä½¿ç”¨matplotlibæ¥ç”»å›¾ï¼Œy2ä¸ºæ˜¯å¦å¯¹å¤šç©ºç»„åˆé‡‡ç”¨åŒyè½´"""
        if not iplot:
            fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(33, 8))
            self.group_net_values.plot(secondary_y=y2, rot=60, ax=ax[0])
            self.group_net_values.plot(secondary_y=y2, ax=ax[0])
            b = self.rankics.copy()
            b.index = [int(i.year) if i.month == 1 else "" for i in list(b.index)]
            b.plot(kind="bar", rot=60, ax=ax[1])
            self.factor_cross_stds.plot(rot=60, ax=ax[2])

            filename_path = filename + ".png"
            if not STATES["NO_SAVE"]:
                plt.savefig(filename_path)
        else:
            
            tris = self.group_net_values
            if without_breakpoint:
                tris = tris.dropna()
            figs = cf.figures(
                tris,
                [
                    dict(kind="line", y=list(self.group_net_values.columns)),
                    # dict(kind="bar", y="å„ç»„æœˆå‡è¶…å‡æ”¶ç›Š"),
                    # dict(kind="bar", y="rankic"),
                ],
                asList=True,
            )
            comments = (
                self.total_comments.applymap(lambda x: round(x, 4))
                .rename(index={"RankICå‡å€¼tå€¼": "RankIC.t"})
                .reset_index()
            )
            here = pd.concat(
                [
                    comments.iloc[:6, :].reset_index(drop=True),
                    comments.iloc[6:12, :].reset_index(drop=True),
                    comments.iloc[12:, :].reset_index(drop=True),
                ],
                axis=1,
            )
            here.columns = ["ä¿¡æ¯ç³»æ•°", "ç»“æœ", "ç»©æ•ˆæŒ‡æ ‡", "ç»“æœ", "å…¶ä»–æŒ‡æ ‡", "ç»“æœ"]
            # here=here.to_numpy().tolist()+[['ä¿¡æ¯ç³»æ•°','ç»“æœ','ç»©æ•ˆæŒ‡æ ‡','ç»“æœ']]
            table = FF.create_table(here.iloc[::-1])
            table.update_yaxes(matches=None)
            pic2=go.Figure(go.Bar(y=list(self.group_mean_rets_monthly),x=[i.replace('roup','') for i in list(self.group_mean_rets_monthly.index)]))
            # table=go.Figure([go.Table(header=dict(values=list(here.columns)),cells=dict(values=here.to_numpy().tolist()))])
            pic3_data=go.Bar(y=list(self.rankics.rankic),x=list(self.rankics.index))
            pic3=go.Figure(data=[pic3_data])
            pic4_data=go.Line(y=list(self.rankics.rankic.cumsum()),x=list(self.rankics.index),name='y2',yaxis='y2')
            pic4_layout=go.Layout(yaxis2=dict(title='y2',side='right'))
            pic4=go.Figure(data=[pic4_data],layout=pic4_layout)
            figs.append(table)
            figs = [figs[-1]] + figs[:-1]
            figs.append(pic2)
            figs = [figs[0],figs[1],figs[-1],pic3]
            figs[1].update_layout(
                legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
            )
            figs[3].update_layout(yaxis2=dict(title='y2',side='right'))
            base_layout = cf.tools.get_base_layout(figs)

            sp = cf.subplots(
                figs,
                shape=(2, 11),
                base_layout=base_layout,
                vertical_spacing=0.15,
                horizontal_spacing=0.03,
                shared_yaxes=False,
                
                specs=[
                    [
                        {"rowspan": 2, "colspan": 4},
                        None,
                        None,
                        None,
                        {"rowspan": 2, "colspan": 4},
                        None,
                        None,
                        None,
                        {"colspan": 3},
                        None,
                        None,
                    ],
                    [
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        {"colspan": 3},
                        None,
                        None,
                    ],
                ],
                subplot_titles=["å‡€å€¼æ›²çº¿", "å„ç»„æœˆå‡è¶…å‡æ”¶ç›Š", "Rank ICæ—¶åºå›¾", "ç»©æ•ˆæŒ‡æ ‡"],
            )
            sp["layout"].update(showlegend=ilegend)
            # los=sp['layout']['annotations']
            # los[0]['font']['color']='#000000'
            # los[1]['font']['color']='#000000'
            # los[2]['font']['color']='#000000'
            # los[3]['font']['color']='#000000'
            # los[-1]['font']['color']='#ffffff'
            # los[-2]['font']['color']='#ffffff'
            # los[-3]['font']['color']='#ffffff'
            # los[-4]['font']['color']='#ffffff'
            # los[0]['text']=los[0]['text'][3:-4]
            # los[1]['text']=los[1]['text'][3:-4]
            # los[2]['text']=los[2]['text'][3:-4]
            # los[3]['text']=los[3]['text'][3:-4]
            # los[-1]['text']='<b>'+los[-1]['text']+'</b>'
            # los[-2]['text']='<b>'+los[-2]['text']+'</b>'
            # los[-3]['text']='<b>'+los[-3]['text']+'</b>'
            # los[-4]['text']='<b>'+los[-4]['text']+'</b>'
            # sp['layout']['annotations']=los
            # print(sp['layout']['annotations'])
            # sp['layout']['annotations'][0]['yanchor']='top'

            cf.iplot(sp)
            # tris=pd.concat([self.group_net_values,self.rankics,self.factor_turnover_rates],axis=1).rename(columns={0:'turnover_rate'})
            # sp=plyoo.make_subplots(rows=2,cols=8,vertical_spacing=.15,horizontal_spacing=.03,
            #                specs=[[{'rowspan':2,'colspan':2,'type':'domain'},None,{'rowspan':2,'colspan':4,'type':'xy'},None,None,None,{'colspan':2,'type':'xy'},None],
            #                       [None,None,None,None,None,None,{'colspan':2,'type':'xy'},None]],
            #                subplot_titles=['å‡€å€¼æ›²çº¿','Rank ICæ—¶åºå›¾','æœˆæ¢æ‰‹ç‡','ç»©æ•ˆæŒ‡æ ‡'])
            # comments=self.total_comments.applymap(lambda x:round(x,4)).rename(index={'RankICå‡å€¼tå€¼':'RankIC.t'}).reset_index()
            # here=pd.concat([comments.iloc[:5,:].reset_index(drop=True),comments.iloc[5:,:].reset_index(drop=True)],axis=1)
            # here.columns=['ä¿¡æ¯ç³»æ•°','ç»“æœ','ç»©æ•ˆæŒ‡æ ‡','ç»“æœ']
            # table=FF.create_table(here)
            # sp.add_trace(table)

    def plotly_net_values(self, filename):
        """ä½¿ç”¨plotly.expressç”»å›¾"""
        fig = pe.line(self.group_net_values)
        filename_path = filename + ".html"
        pio.write_html(fig, filename_path, auto_open=True)

    @classmethod
    @lru_cache(maxsize=None)
    def prerpare(cls):
        """é€šç”¨æ•°æ®å‡†å¤‡"""
        cls.judge_month()
        cls.get_rets_month()

    def run(
        self,
        groups_num=10,
        neutralize=False,
        boxcox=False,
        trade_cost_double_side=0,
        value_weighted=False,
        y2=False,
        plt_plot=True,
        plotly_plot=False,
        filename="åˆ†ç»„å‡€å€¼å›¾",
        print_comments=True,
        comments_writer=None,
        net_values_writer=None,
        rets_writer=None,
        comments_sheetname=None,
        net_values_sheetname=None,
        rets_sheetname=None,
        on_paper=False,
        sheetname=None,
        zxindustry_dummies=0,
        swindustry_dummies=0,
        only_cap=0,
        iplot=1,
        ilegend=0,
        without_breakpoint=0,
        beauty_comments=0,
    ):
        """è¿è¡Œå›æµ‹éƒ¨åˆ†"""
        if comments_writer and not (comments_sheetname or sheetname):
            raise IOError("æŠŠtotal_commentsè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
        if net_values_writer and not (net_values_sheetname or sheetname):
            raise IOError("æŠŠgroup_net_valuesè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
        if rets_writer and not (rets_sheetname or sheetname):
            raise IOError("æŠŠgroup_retsè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
        if neutralize:
            self.get_log_cap()
            self.get_neutral_factors(
                swindustry_dummies=swindustry_dummies,
                zxindustry_dummies=zxindustry_dummies,
            )
            self.deal_with_factors_after_neutralize()
        elif boxcox:
            self.get_log_cap(boxcox=True)
            self.get_neutral_factors(
                swindustry_dummies=swindustry_dummies,
                zxindustry_dummies=zxindustry_dummies,
                only_cap=only_cap,
            )
            self.deal_with_factors_after_neutralize()
        else:
            self.deal_with_factors()
        self.get_limit_ups_downs()
        self.get_data(groups_num)
        self.get_group_rets_net_values(
            groups_num=groups_num,
            value_weighted=value_weighted,
            trade_cost_double_side=trade_cost_double_side,
        )
        self.get_long_short_comments(on_paper=on_paper)
        self.get_total_comments(groups_num=groups_num)
        if on_paper:
            group1_ttest = ss.ttest_1samp(self.group_rets.group1, 0).pvalue
            group10_ttest = ss.ttest_1samp(
                self.group_rets[f"group{groups_num}"], 0
            ).pvalue
            group_long_short_ttest = ss.ttest_1samp(self.long_short_rets, 0).pvalue
            group1_ret = self.group_rets.group1.mean()
            group10_ret = self.group_rets[f"group{groups_num}"].mean()
            group_long_short_ret = self.long_short_rets.mean()
            papers = pd.DataFrame(
                {
                    "è¯„ä»·æŒ‡æ ‡": [
                        group1_ttest,
                        group10_ttest,
                        group_long_short_ttest,
                        group1_ret,
                        group10_ret,
                        group_long_short_ret,
                    ]
                },
                index=[
                    "åˆ†ç»„1på€¼",
                    f"åˆ†ç»„{groups_num}på€¼",
                    f"åˆ†ç»„1-åˆ†ç»„{groups_num}på€¼",
                    "åˆ†ç»„1æ”¶ç›Šç‡",
                    f"åˆ†ç»„{groups_num}æ”¶ç›Šç‡",
                    f"åˆ†ç»„1-åˆ†ç»„{groups_num}æ”¶ç›Šç‡",
                ],
            )
            self.total_comments = pd.concat([papers, self.total_comments])

        if plt_plot:
            if not STATES["NO_PLOT"]:
                if filename:
                    self.plot_net_values(
                        y2=y2,
                        filename=filename,
                        iplot=iplot,
                        ilegend=bool(ilegend),
                        without_breakpoint=without_breakpoint,
                    )
                else:
                    self.plot_net_values(
                        y2=y2,
                        filename=self.factors_file.split(".")[-2].split("/")[-1]
                        + str(groups_num)
                        + "åˆ†ç»„",
                        iplot=iplot,
                        ilegend=bool(ilegend),
                        without_breakpoint=without_breakpoint,
                    )
                plt.show()
        if plotly_plot:
            if not STATES["NO_PLOT"]:
                if filename:
                    self.plotly_net_values(filename=filename)
                else:
                    self.plotly_net_values(
                        filename=self.factors_file.split(".")[-2].split("/")[-1]
                        + str(groups_num)
                        + "åˆ†ç»„"
                    )
        if print_comments:
            if not STATES["NO_COMMENT"]:
                tb = Texttable()
                tb.set_cols_width(
                    [8] * 5 + [9] + [8] * 2 + [7] * 2 + [8] + [8] + [9] + [10] * 5
                )
                tb.set_cols_dtype(["f"] * 18)
                tb.header(list(self.total_comments.T.columns))
                tb.add_rows(self.total_comments.T.to_numpy(), header=False)
                print(tb.draw())
        if sheetname:
            if comments_writer:
                if not on_paper:
                    total_comments = self.total_comments.copy()
                    tc = list(total_comments.è¯„ä»·æŒ‡æ ‡)
                    if beauty_comments:
                        tc[0] = str(round(tc[0] * 100, 2)) + "%"
                        tc[1] = str(round(tc[1], 2))
                        tc[2] = str(round(tc[2] * 100, 2)) + "%"
                        tc[3] = str(round(tc[3], 2))
                        tc[4] = str(round(tc[4], 2))
                        tc[5] = str(round(tc[5] * 100, 2)) + "%"
                        tc[6] = str(round(tc[6] * 100, 2)) + "%"
                        tc[7] = str(round(tc[7] * 100, 2)) + "%"
                        tc[8] = str(round(tc[8], 2))
                        tc[9] = str(round(tc[9] * 100, 2)) + "%"
                        tc[10] = str(round(tc[10] * 100, 2)) + "%"
                        tc[11] = str(round(tc[11] * 100, 2)) + "%"
                        tc[12] = str(round(tc[12] * 100, 2)) + "%"
                        tc[13] = str(round(tc[13] * 100, 2)) + "%"
                        tc[14] = str(round(tc[14], 2))
                        tc[15] = str(round(tc[15], 2))
                        tc[16] = str(round(tc[16] * 100, 2)) + "%"
                        tc[17] = str(round(tc[17] * 100, 2)) + "%"
                    tc=tc+list(self.group_mean_rets_monthly)
                    new_total_comments = pd.DataFrame(
                        {sheetname: tc}, index=list(total_comments.index)+[f'ç¬¬{i}ç»„' for i in range(1,groups_num+1)]
                    )
                    new_total_comments.to_excel(comments_writer, sheet_name=sheetname)
                    rankic_twins=pd.concat([self.rankics.rankic,self.rankics.rankic.cumsum()],axis=1)
                    rankic_twins.columns=['RankIC','RankICç´¯ç§¯']
                    rankic_twins.to_excel(comments_writer,sheet_name=sheetname+'RankIC')
                else:
                    self.total_comments.rename(columns={"è¯„ä»·æŒ‡æ ‡": sheetname}).to_excel(
                        comments_writer, sheet_name=sheetname
                    )
            if net_values_writer:
                groups_net_values = self.group_net_values.copy()
                groups_net_values.index = groups_net_values.index.strftime("%Y/%m/%d")
                groups_net_values.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(groups_net_values.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                groups_net_values.to_excel(net_values_writer, sheet_name=sheetname)
            if rets_writer:
                group_rets = self.group_rets.copy()
                group_rets.index = group_rets.index.strftime("%Y/%m/%d")
                group_rets.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(group_rets.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                group_rets.to_excel(rets_writer, sheet_name=sheetname)
        else:
            if comments_writer and comments_sheetname:
                total_comments = self.total_comments.copy()
                tc = list(total_comments.è¯„ä»·æŒ‡æ ‡)
                tc[0] = str(round(tc[0] * 100, 2)) + "%"
                tc[1] = str(round(tc[1], 2))
                tc[2] = str(round(tc[2] * 100, 2)) + "%"
                tc[3] = str(round(tc[3], 2))
                tc[4] = str(round(tc[4], 2))
                tc[5] = str(round(tc[5] * 100, 2)) + "%"
                tc[6] = str(round(tc[6] * 100, 2)) + "%"
                tc[7] = str(round(tc[7] * 100, 2)) + "%"
                tc[8] = str(round(tc[8], 2))
                tc[9] = str(round(tc[9] * 100, 2)) + "%"
                tc[10] = str(round(tc[10] * 100, 2)) + "%"
                tc[11] = str(round(tc[11] * 100, 2)) + "%"
                tc[12] = str(round(tc[12] * 100, 2)) + "%"
                tc[13] = str(round(tc[13] * 100, 2)) + "%"
                tc[14] = str(round(tc[14], 2))
                tc[15] = str(round(tc[15], 2))
                tc[16] = str(round(tc[16] * 100, 2)) + "%"
                tc[17] = str(round(tc[17] * 100, 2)) + "%"
                new_total_comments = pd.DataFrame(
                    {comments_sheetname: tc}, index=total_comments.index
                )
                new_total_comments.T.to_excel(
                    comments_writer, sheet_name=comments_sheetname
                )
            if net_values_writer and net_values_sheetname:
                groups_net_values = self.group_net_values.copy()
                groups_net_values.index = groups_net_values.index.strftime("%Y/%m/%d")
                groups_net_values.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(groups_net_values.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                groups_net_values.to_excel(
                    net_values_writer, sheet_name=net_values_sheetname
                )
            if rets_writer and rets_sheetname:
                group_rets = self.group_rets.copy()
                group_rets.index = group_rets.index.strftime("%Y/%m/%d")
                group_rets.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(group_rets.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                group_rets.to_excel(rets_writer, sheet_name=rets_sheetname)


@do_on_dfs
class pure_moonnight(object):
    """å°è£…é€‰è‚¡æ¡†æ¶"""

    __slots__ = ["shen"]

    def __init__(
        self,
        factors: pd.DataFrame,
        groups_num: int = 10,
        freq: str = "M",
        neutralize: bool = 0,
        boxcox: bool = 1,
        trade_cost_double_side: float = 0,
        value_weighted: bool = 0,
        y2: bool = 0,
        plt_plot: bool = 1,
        plotly_plot: bool = 0,
        filename: str = "åˆ†ç»„å‡€å€¼å›¾",
        time_start: int = None,
        time_end: int = None,
        print_comments: bool = 1,
        comments_writer: pd.ExcelWriter = None,
        net_values_writer: pd.ExcelWriter = None,
        rets_writer: pd.ExcelWriter = None,
        comments_sheetname: str = None,
        net_values_sheetname: str = None,
        rets_sheetname: str = None,
        on_paper: bool = 0,
        sheetname: str = None,
        zxindustry_dummies: bool = 0,
        swindustry_dummies: bool = 0,
        ages: pd.DataFrame = None,
        sts: pd.DataFrame = None,
        states: pd.DataFrame = None,
        opens: pd.DataFrame = None,
        closes: pd.DataFrame = None,
        capitals: pd.DataFrame = None,
        swindustry_dummy: pd.DataFrame = None,
        zxindustry_dummy: pd.DataFrame = None,
        no_read_indu: bool = 0,
        only_cap: bool = 0,
        iplot: bool = 1,
        ilegend: bool = 0,
        without_breakpoint: bool = 0,
        opens_average_first_day: bool = 0,
        total_cap: bool = 0,
    ) -> None:
        """ä¸€é”®å›æµ‹æ¡†æ¶ï¼Œæµ‹è¯•å•å› å­çš„æœˆé¢‘è°ƒä»“çš„åˆ†ç»„è¡¨ç°
        æ¯æœˆæœˆåº•è®¡ç®—å› å­å€¼ï¼Œæœˆåˆç¬¬ä¸€å¤©å¼€ç›˜æ—¶ä¹°å…¥ï¼Œæœˆæœ«æ”¶ç›˜æœ€åä¸€å¤©æ”¶ç›˜æ—¶å–å‡º
        å‰”é™¤ä¸Šå¸‚ä¸è¶³60å¤©çš„ï¼Œåœç‰Œå¤©æ•°è¶…è¿‡ä¸€åŠçš„ï¼Œstå¤©æ•°è¶…è¿‡ä¸€åŠçš„
        æœˆæœ«æ”¶ç›˜è·Œåœçš„ä¸å–å‡ºï¼Œæœˆåˆå¼€ç›˜æ¶¨åœçš„ä¸ä¹°å…¥
        ç”±æœ€å¥½ç»„å’Œæœ€å·®ç»„çš„å¤šç©ºç»„åˆæ„æˆå¤šç©ºå¯¹å†²ç»„

        Parameters
        ----------
        factors : pd.DataFrame
            è¦ç”¨äºæ£€æµ‹çš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
        groups_num : int, optional
            åˆ†ç»„æ•°é‡, by default 10
        freq : str, optional
            å›æµ‹é¢‘ç‡, by default 'M'
        neutralize : bool, optional
            å¯¹æµé€šå¸‚å€¼å–è‡ªç„¶å¯¹æ•°ï¼Œä»¥å®Œæˆè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 0
        boxcox : bool, optional
            å¯¹æµé€šå¸‚å€¼åšæˆªé¢boxcoxå˜æ¢ï¼Œä»¥å®Œæˆè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 1
        trade_cost_double_side : float, optional
            äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
        value_weighted : bool, optional
            æ˜¯å¦ç”¨æµé€šå¸‚å€¼åŠ æƒ, by default 0
        y2 : bool, optional
            ç”»å›¾æ—¶æ˜¯å¦å¯ç”¨ç¬¬äºŒyè½´, by default 0
        plt_plot : bool, optional
            å°†åˆ†ç»„å‡€å€¼æ›²çº¿ç”¨matplotlibç”»å‡ºæ¥, by default 1
        plotly_plot : bool, optional
            å°†åˆ†ç»„å‡€å€¼æ›²çº¿ç”¨plotlyç”»å‡ºæ¥, by default 0
        filename : str, optional
            åˆ†ç»„å‡€å€¼æ›²çº¿çš„å›¾ä¿å­˜çš„åç§°, by default "åˆ†ç»„å‡€å€¼å›¾"
        time_start : int, optional
            å›æµ‹èµ·å§‹æ—¶é—´, by default None
        time_end : int, optional
            å›æµ‹ç»ˆæ­¢æ—¶é—´, by default None
        print_comments : bool, optional
            æ˜¯å¦æ‰“å°å‡ºè¯„ä»·æŒ‡æ ‡, by default 1
        comments_writer : pd.ExcelWriter, optional
            ç”¨äºè®°å½•è¯„ä»·æŒ‡æ ‡çš„xlsxæ–‡ä»¶, by default None
        net_values_writer : pd.ExcelWriter, optional
            ç”¨äºè®°å½•å‡€å€¼åºåˆ—çš„xlsxæ–‡ä»¶, by default None
        rets_writer : pd.ExcelWriter, optional
            ç”¨äºè®°å½•æ”¶ç›Šç‡åºåˆ—çš„xlsxæ–‡ä»¶, by default None
        comments_sheetname : str, optional
            åœ¨è®°å½•è¯„ä»·æŒ‡æ ‡çš„xlsxæ–‡ä»¶ä¸­ï¼Œè¯¥å·¥ä½œè¡¨çš„åç§°, by default None
        net_values_sheetname : str, optional
            åœ¨è®°å½•å‡€å€¼åºåˆ—çš„xlsxæ–‡ä»¶ä¸­ï¼Œè¯¥å·¥ä½œè¡¨çš„åç§°, by default None
        rets_sheetname : str, optional
            åœ¨è®°å½•æ”¶ç›Šç‡åºåˆ—çš„xlsxæ–‡ä»¶ä¸­ï¼Œè¯¥å·¥ä½œè¡¨çš„åç§°, by default None
        on_paper : bool, optional
            ä½¿ç”¨å­¦æœ¯åŒ–è¯„ä»·æŒ‡æ ‡, by default 0
        sheetname : str, optional
            å„ä¸ªpd.Excelwriterä¸­å·¥ä½œè¡¨çš„ç»Ÿä¸€åç§°, by default None
        zxindustry_dummies : bool, optional
            è¡Œä¸šä¸­æ€§åŒ–æ—¶ï¼Œé€‰ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0
        swindustry_dummies : bool, optional
            è¡Œä¸šä¸­æ€§åŒ–æ—¶ï¼Œé€‰ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
        ages : pd.DataFrame, optional
            è¾“å…¥è‚¡ç¥¨ä¸Šå¸‚å¤©æ•°çš„æ•°æ®ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯å¤©æ•°, by default None
        sts : pd.DataFrame, optional
            è¾“å…¥è‚¡ç¥¨æ¯å¤©æ˜¯å¦stçš„æ•°æ®ï¼Œæ˜¯stè‚¡å³ä¸º1ï¼Œå¦åˆ™ä¸º0ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯0æˆ–1, by default None
        states : pd.DataFrame, optional
            è¾“å…¥è‚¡ç¥¨æ¯å¤©äº¤æ˜“çŠ¶æ€çš„æ•°æ®ï¼Œæ­£å¸¸äº¤æ˜“ä¸º1ï¼Œå¦åˆ™ä¸º0ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯0æˆ–1, by default None
        opens : pd.DataFrame, optional
            è¾“å…¥è‚¡ç¥¨çš„å¤æƒå¼€ç›˜ä»·æ•°æ®ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯ä»·æ ¼, by default None
        closes : pd.DataFrame, optional
            è¾“å…¥è‚¡ç¥¨çš„å¤æƒæ”¶ç›˜ä»·æ•°æ®ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯ä»·æ ¼, by default None
        capitals : pd.DataFrame, optional
            è¾“å…¥è‚¡ç¥¨çš„æ¯æœˆæœˆæœ«æµé€šå¸‚å€¼æ•°æ®ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯æµé€šå¸‚å€¼, by default None
        swindustry_dummy : pd.DataFrame, optioanl
            ç†Ÿäººè‚¡ç¥¨çš„æ¯æœˆæœˆæœ«çš„ç”³ä¸‡ä¸€çº§è¡Œä¸šå“‘å˜é‡ï¼Œè¡¨åŒ…å«33åˆ—ï¼Œç¬¬ä¸€åˆ—ä¸ºè‚¡ç¥¨ä»£ç ï¼Œåä¸º`code`ï¼Œç¬¬äºŒåˆ—ä¸ºæœˆæœ«æœ€åä¸€å¤©çš„æ—¥æœŸï¼Œåä¸º`date`
            å…¶ä½™31åˆ—ï¼Œä¸ºå„ä¸ªè¡Œä¸šçš„å“‘å˜é‡ï¼Œåä¸º`w1`ã€`w2`ã€`w3`â€¦â€¦`w31`, by default None
        zxindustry_dummy : pd.DataFrame, optioanl
            ç†Ÿäººè‚¡ç¥¨çš„æ¯æœˆæœˆæœ«çš„ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ï¼Œè¡¨åŒ…å«32åˆ—ï¼Œç¬¬ä¸€åˆ—ä¸ºè‚¡ç¥¨ä»£ç ï¼Œåä¸º`code`ï¼Œç¬¬äºŒåˆ—ä¸ºæœˆæœ«æœ€åä¸€å¤©çš„æ—¥æœŸï¼Œåä¸º`date`
            å…¶ä½™30åˆ—ï¼Œä¸ºå„ä¸ªè¡Œä¸šçš„å“‘å˜é‡ï¼Œåä¸º`w1`ã€`w2`ã€`w3`â€¦â€¦`w30`, by default None
        no_read_indu : bool, optional
            ä¸è¯»å…¥è¡Œä¸šæ•°æ®, by default 0
        only_cap : bool, optional
            ä»…åšå¸‚å€¼ä¸­æ€§åŒ–, by default 0
        iplot : bool, optional
            ä½¿ç”¨cufflinkså‘ˆç°å›æµ‹ç»“æœ, by default 1
        ilegend : bool, optional
            ä½¿ç”¨cufflinksç»˜å›¾æ—¶ï¼Œæ˜¯å¦æ˜¾ç¤ºå›¾ä¾‹, by default 1
        without_breakpoint : bool, optional
            ç”»å›¾çš„æ—¶å€™æ˜¯å¦å»é™¤é—´æ–­ç‚¹, by default 0
        opens_average_first_day : bool, optional
            ä¹°å…¥æ—¶ä½¿ç”¨ç¬¬ä¸€å¤©çš„å¹³å‡ä»·æ ¼, by default 0
        total_cap : bool, optional
            åŠ æƒå’Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–æ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0
        """

        if not isinstance(factors, pd.DataFrame):
            factors = factors()
        if comments_writer is None and sheetname is not None:
            from pure_ocean_breeze.state.states import COMMENTS_WRITER

            comments_writer = COMMENTS_WRITER
        if net_values_writer is None and sheetname is not None:
            from pure_ocean_breeze.state.states import NET_VALUES_WRITER

            net_values_writer = NET_VALUES_WRITER
        if not on_paper:
            from pure_ocean_breeze.state.states import ON_PAPER

            on_paper = ON_PAPER
        if time_start is None:
            from pure_ocean_breeze.state.states import MOON_START

            if MOON_START is not None:
                factors = factors[factors.index >= pd.Timestamp(str(MOON_START))]
        else:
            factors = factors[factors.index >= pd.Timestamp(str(time_start))]
        if time_end is None:
            from pure_ocean_breeze.state.states import MOON_END

            if MOON_END is not None:
                factors = factors[factors.index <= pd.Timestamp(str(MOON_END))]
        else:
            factors = factors[factors.index <= pd.Timestamp(str(time_end))]
        if boxcox + neutralize == 0:
            no_read_indu = 1
        if only_cap + no_read_indu > 0:
            only_cap = no_read_indu = 1
        if iplot:
            print_comments = 0
        if total_cap:
            if opens_average_first_day:
                if freq == "M":
                    self.shen = pure_moon_b(
                        freq=freq,
                        no_read_indu=no_read_indu,
                        swindustry_dummy=swindustry_dummy,
                        zxindustry_dummy=zxindustry_dummy,
                        read_in_swindustry_dummy=swindustry_dummies,
                    )
                elif freq == "W":
                    self.shen = pure_week_b(
                        freq=freq,
                        no_read_indu=no_read_indu,
                        swindustry_dummy=swindustry_dummy,
                        zxindustry_dummy=zxindustry_dummy,
                        read_in_swindustry_dummy=swindustry_dummies,
                    )
            else:
                if freq == "M":
                    self.shen = pure_moon_c(
                        freq=freq,
                        no_read_indu=no_read_indu,
                        swindustry_dummy=swindustry_dummy,
                        zxindustry_dummy=zxindustry_dummy,
                        read_in_swindustry_dummy=swindustry_dummies,
                    )
                elif freq == "W":
                    self.shen = pure_week_c(
                        freq=freq,
                        no_read_indu=no_read_indu,
                        swindustry_dummy=swindustry_dummy,
                        zxindustry_dummy=zxindustry_dummy,
                        read_in_swindustry_dummy=swindustry_dummies,
                    )
        else:
            if opens_average_first_day:
                if freq == "M":
                    self.shen = pure_moon(
                        freq=freq,
                        no_read_indu=no_read_indu,
                        swindustry_dummy=swindustry_dummy,
                        zxindustry_dummy=zxindustry_dummy,
                        read_in_swindustry_dummy=swindustry_dummies,
                    )
                elif freq == "W":
                    self.shen = pure_week(
                        freq=freq,
                        no_read_indu=no_read_indu,
                        swindustry_dummy=swindustry_dummy,
                        zxindustry_dummy=zxindustry_dummy,
                        read_in_swindustry_dummy=swindustry_dummies,
                    )
            else:
                if freq == "M":
                    self.shen = pure_moon_a(
                        freq=freq,
                        no_read_indu=no_read_indu,
                        swindustry_dummy=swindustry_dummy,
                        zxindustry_dummy=zxindustry_dummy,
                        read_in_swindustry_dummy=swindustry_dummies,
                    )
                elif freq == "W":
                    self.shen = pure_week_a(
                        freq=freq,
                        no_read_indu=no_read_indu,
                        swindustry_dummy=swindustry_dummy,
                        zxindustry_dummy=zxindustry_dummy,
                        read_in_swindustry_dummy=swindustry_dummies,
                    )
        self.shen.set_basic_data(
            ages=ages,
            sts=sts,
            states=states,
            opens=opens,
            closes=closes,
            capitals=capitals,
            opens_average_first_day=opens_average_first_day,
            total_cap=total_cap,
        )
        self.shen.set_factor_df_date_as_index(factors)
        self.shen.prerpare()
        self.shen.run(
            groups_num=groups_num,
            neutralize=neutralize,
            boxcox=boxcox,
            trade_cost_double_side=trade_cost_double_side,
            value_weighted=value_weighted,
            y2=y2,
            plt_plot=plt_plot,
            plotly_plot=plotly_plot,
            filename=filename,
            print_comments=print_comments,
            comments_writer=comments_writer,
            net_values_writer=net_values_writer,
            rets_writer=rets_writer,
            comments_sheetname=comments_sheetname,
            net_values_sheetname=net_values_sheetname,
            rets_sheetname=rets_sheetname,
            on_paper=on_paper,
            sheetname=sheetname,
            swindustry_dummies=swindustry_dummies,
            zxindustry_dummies=zxindustry_dummies,
            only_cap=only_cap,
            iplot=iplot,
            ilegend=ilegend,
            without_breakpoint=without_breakpoint,
        )

    def __call__(self) -> pd.DataFrame:
        """å¦‚æœåšäº†è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ï¼Œåˆ™è¿”å›è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­æ•°æ®

        Returns
        -------
        `pd.DataFrame`
            å¦‚æœåšäº†è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ï¼Œåˆ™è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­æ•°æ®ï¼Œå¦åˆ™è¿”å›åŸå› å­æ•°æ®
        """
        return self.shen.factors_out

    def comments_ten(self) -> pd.DataFrame:
        """å¯¹å›æµ‹çš„ååˆ†ç»„ç»“æœåˆ†åˆ«ç»™å‡ºè¯„ä»·

        Returns
        -------
        `pd.DataFrame`
            è¯„ä»·æŒ‡æ ‡åŒ…æ‹¬å¹´åŒ–æ”¶ç›Šç‡ã€æ€»æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€å¹´åŒ–å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤ç‡ã€èƒœç‡
        """
        rets_cols = list(self.shen.group_rets.columns)
        rets_cols = rets_cols[:-1]
        coms = []
        for i in rets_cols:
            ret = self.shen.group_rets[i]
            net = self.shen.group_net_values[i]
            com = comments_on_twins(net, ret)
            com = com.to_frame(i)
            coms.append(com)
        df = pd.concat(coms, axis=1)
        return df.T

    def comment_yearly(self) -> pd.DataFrame:
        """å¯¹å›æµ‹çš„æ¯å¹´è¡¨ç°ç»™å‡ºè¯„ä»·

        Returns
        -------
        pd.DataFrame
            å„å¹´åº¦çš„æ”¶ç›Šç‡
        """
        df = self.shen.group_net_values.resample("Y").last().pct_change()
        df.index = df.index.year
        return df


class pure_week(pure_moon):
    ...


class pure_moon_a(pure_moon):
    ...


class pure_week_a(pure_moon):
    ...


class pure_moon_b(pure_moon):
    ...


class pure_week_b(pure_moon):
    ...


class pure_moon_c(pure_moon):
    ...


class pure_week_c(pure_moon):
    ...


class pure_fall(object):
    # DONEï¼šä¿®æ”¹ä¸ºå› å­æ–‡ä»¶åå¯ä»¥å¸¦â€œæ—¥é¢‘_â€œï¼Œä¹Ÿå¯ä»¥ä¸å¸¦â€œæ—¥é¢‘_â€œ
    def __init__(self, daily_path: str) -> None:
        """ä¸€ä¸ªä½¿ç”¨mysqlä¸­çš„åˆ†é’Ÿæ•°æ®ï¼Œæ¥æ›´æ–°å› å­å€¼çš„æ¡†æ¶

        Parameters
        ----------
        daily_path : str
            æ—¥é¢‘å› å­å€¼å­˜å‚¨æ–‡ä»¶çš„åå­—ï¼Œè¯·ä»¥'.parquet'ç»“å°¾
        """
        self.homeplace = HomePlace()
        # å°†åˆ†é’Ÿæ•°æ®æ‹¼æˆä¸€å¼ æ—¥é¢‘å› å­è¡¨
        self.daily_factors = None
        self.daily_factors_path = self.homeplace.factor_data_file + "æ—¥é¢‘_" + daily_path

    def __call__(self, monthly=False):
        """ä¸ºäº†é˜²æ­¢å±æ€§åå¤ªå¤šï¼Œå¿˜è®°äº†è¦è°ƒç”¨å“ªä¸ªæ‰æ˜¯ç»“æœï¼Œå› æ­¤å¯ä»¥ç›´æ¥è¾“å‡ºæœˆåº¦æ•°æ®è¡¨"""
        if monthly:
            return self.monthly_factors.copy()
        else:
            try:
                return self.daily_factors.copy()
            except Exception:
                return self.monthly_factors.copy()

    def wide_to_long(self, df, i):
        """å°†å®½æ•°æ®è½¬åŒ–ä¸ºé•¿æ•°æ®ï¼Œç”¨äºå› å­è¡¨è½¬åŒ–å’Œæ‹¼æ¥"""
        df = df.stack().reset_index()
        df.columns = ["date", "code", i]
        df = df.set_index(["date", "code"])
        return df

    def de_in_group(self, df, help_names):
        """å¯¹æ¯ä¸ªæ—¶é—´ï¼Œåˆ†åˆ«åšå›å½’ï¼Œå‰”é™¤ç›¸å…³å› å­"""
        ols_order = "fac~" + "+".join(help_names)
        ols_result = smf.ols(ols_order, data=df).fit()
        params = {i: ols_result.params[i] for i in help_names}
        predict = [params[i] * df[i] for i in help_names]
        predict = reduce(lambda x, y: x + y, predict)
        df.fac = df.fac - predict - ols_result.params["Intercept"]
        df = df[["fac"]]
        return df

    def standardlize_in_cross_section(self, df):
        """
        åœ¨æ¨ªæˆªé¢ä¸Šåšæ ‡å‡†åŒ–
        è¾“å…¥çš„dfåº”ä¸ºï¼Œåˆ—åæ˜¯è‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•æ˜¯æ—¶é—´
        """
        df = df.T
        df = (df - df.mean()) / df.std()
        df = df.T
        return df


class pure_fallmount(pure_fall):
    """ç»§æ‰¿è‡ªçˆ¶ç±»ï¼Œä¸“ä¸ºåšå› å­æˆªé¢æ ‡å‡†åŒ–ä¹‹åç›¸åŠ å’Œå› å­å‰”é™¤å…¶ä»–è¾…åŠ©å› å­çš„ä½œç”¨"""

    def __init__(self, monthly_factors):
        """è¾“å…¥æœˆåº¦å› å­å€¼ï¼Œä»¥è®¾å®šæ–°çš„å¯¹è±¡"""
        super(pure_fall, self).__init__()
        self.monthly_factors = monthly_factors

    def __call__(self, monthly=False):
        """ä¸ºäº†é˜²æ­¢å±æ€§åå¤ªå¤šï¼Œå¿˜è®°äº†è¦è°ƒç”¨å“ªä¸ªæ‰æ˜¯ç»“æœï¼Œå› æ­¤å¯ä»¥ç›´æ¥è¾“å‡ºæœˆåº¦æ•°æ®è¡¨"""
        if monthly:
            return self.monthly_factors.copy()
        else:
            try:
                return self.daily_factors.copy()
            except Exception:
                return self.monthly_factors.copy()

    def __add__(self, selfas):
        """è¿”å›ä¸€ä¸ªå¯¹è±¡ï¼Œè€Œéä¸€ä¸ªè¡¨æ ¼ï¼Œå¦‚éœ€è¡¨æ ¼è¯·è°ƒç”¨å¯¹è±¡"""
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac2s = []
        if not isinstance(selfas, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfas} is changed into Iterable")
            selfas = (selfas,)
        for selfa in selfas:
            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2s.append(fac2)
        for i in fac2s:
            fac1 = fac1 + i
        new_pure = pure_fallmount(fac1)
        return new_pure

    def __mul__(self, selfas):
        """å°†å‡ ä¸ªå› å­æ¨ªæˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œä½¿å…¶éƒ½ä¸ºæ­£æ•°ï¼Œç„¶åå› å­å€¼ç›¸ä¹˜"""
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac1 = fac1 - fac1.min()
        fac2s = []
        if not isinstance(selfas, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfas} is changed into Iterable")
            selfas = (selfas,)
        for selfa in selfas:
            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2 = fac2 - fac2.min()
            fac2s.append(fac2)
        for i in fac2s:
            fac1 = fac1 * i
        new_pure = pure_fall()
        new_pure.monthly_factors = fac1
        return new_pure

    def __sub__(self, selfa):
        """è¿”å›å¯¹è±¡ï¼Œå¦‚éœ€è¡¨æ ¼ï¼Œè¯·è°ƒç”¨å¯¹è±¡"""
        tqdm.auto.tqdm.pandas()
        if not isinstance(selfa, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfa} is changed into Iterable")
            selfa = (selfa,)
        fac_main = self.wide_to_long(self.monthly_factors, "fac")
        fac_helps = [i.monthly_factors for i in selfa]
        help_names = ["help" + str(i) for i in range(1, (len(fac_helps) + 1))]
        fac_helps = list(map(self.wide_to_long, fac_helps, help_names))
        fac_helps = pd.concat(fac_helps, axis=1)
        facs = pd.concat([fac_main, fac_helps], axis=1).dropna()
        facs = facs.groupby("date").progress_apply(
            lambda x: self.de_in_group(x, help_names)
        )
        facs = facs.unstack()
        facs.columns = list(map(lambda x: x[1], list(facs.columns)))
        new_pure = pure_fallmount(facs)
        return new_pure

    def __gt__(self, selfa):
        """ç”¨äºè¾“å‡º25åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
        x = self.monthly_factors.copy()
        y = selfa.monthly_factors.copy()
        x = x.stack().reset_index()
        y = y.stack().reset_index()
        x.columns = ["date", "code", "fac"]
        y.columns = ["date", "code", "fac"]
        shen = pure_moon()
        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 5))
        x = (
            x.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupx"})
        )
        xy = pd.merge(x, y, on=["date", "code"])
        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 5))
        xy = (
            xy.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupy"})
        )
        xy = xy.assign(fac=xy.groupx * 5 + xy.groupy)
        xy = xy[["date", "code", "fac"]]
        xy = xy.set_index(["date", "code"]).unstack()
        xy.columns = [i[1] for i in list(xy.columns)]
        new_pure = pure_fallmount(xy)
        return new_pure

    def __rshift__(self, selfa):
        """ç”¨äºè¾“å‡º100åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
        x = self.monthly_factors.copy()
        y = selfa.monthly_factors.copy()
        x = x.stack().reset_index()
        y = y.stack().reset_index()
        x.columns = ["date", "code", "fac"]
        y.columns = ["date", "code", "fac"]
        shen = pure_moon()
        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 10))
        x = (
            x.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupx"})
        )
        xy = pd.merge(x, y, on=["date", "code"])
        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 10))
        xy = (
            xy.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupy"})
        )
        xy = xy.assign(fac=xy.groupx * 10 + xy.groupy)
        xy = xy[["date", "code", "fac"]]
        xy = xy.set_index(["date", "code"]).unstack()
        xy.columns = [i[1] for i in list(xy.columns)]
        new_pure = pure_fallmount(xy)
        return new_pure


class pure_fall_frequent(object):
    """å¯¹å•åªè‚¡ç¥¨å•æ—¥è¿›è¡Œæ“ä½œ"""

    def __init__(
        self,
        factor_file: str,
        project: str = None,
        startdate: int = None,
        enddate: int = None,
        questdb_host: str = "127.0.0.1",
        kind: str = "stock",
        clickhouse: bool = 0,
        questdb: bool = 0,
        questdb_web_port: str = "9001",
        ignore_history_in_questdb: bool = 0,
        groupby_target: list = ["date", "code"],
    ) -> None:
        """åŸºäºclickhouseçš„åˆ†é’Ÿæ•°æ®ï¼Œè®¡ç®—å› å­å€¼ï¼Œæ¯å¤©çš„å› å­å€¼åªç”¨åˆ°å½“æ—¥çš„æ•°æ®

        Parameters
        ----------
        factor_file : str
            ç”¨äºä¿å­˜å› å­å€¼çš„æ–‡ä»¶åï¼Œéœ€ä¸ºparquetæ–‡ä»¶ï¼Œä»¥'.parquet'ç»“å°¾
        project : str, optional
            è¯¥å› å­æ‰€å±é¡¹ç›®ï¼Œå³å­æ–‡ä»¶å¤¹åç§°, by default None
        startdate : int, optional
            èµ·å§‹æ—¶é—´ï¼Œå½¢å¦‚20121231ï¼Œä¸ºå¼€åŒºé—´, by default None
        enddate : int, optional
            æˆªæ­¢æ—¶é—´ï¼Œå½¢å¦‚20220814ï¼Œä¸ºé—­åŒºé—´ï¼Œä¸ºç©ºåˆ™è®¡ç®—åˆ°æœ€è¿‘æ•°æ®, by default None
        questdb_host: str, optional
            questdbçš„hostï¼Œä½¿ç”¨NASæ—¶æ”¹ä¸º'192.168.1.3', by default '127.0.0.1'
        kind : str, optional
            ç±»å‹ä¸ºè‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼ŒæŒ‡æ•°ä¸º'index', by default "stock"
        clickhouse : bool, optional
            ä½¿ç”¨clickhouseä½œä¸ºæ•°æ®æºï¼Œå¦‚æœpostgresqlä¸æœ¬å‚æ•°éƒ½ä¸º0ï¼Œå°†ä¾ç„¶ä»clickhouseä¸­è¯»å–, by default 0
        questdb : bool, optional
            ä½¿ç”¨questdbä½œä¸ºæ•°æ®æº, by default 0
        questdb_web_port : str, optional
            questdbçš„web_port, by default '9001'
        ignore_history_in_questdb : bool, optional
            æ‰“æ–­åé‡æ–°ä»å¤´è®¡ç®—ï¼Œæ¸…é™¤åœ¨questdbä¸­çš„è®°å½•
        groupby_target: list, optional
            groupbyè®¡ç®—æ—¶ï¼Œåˆ†ç»„çš„ä¾æ®ï¼Œä½¿ç”¨æ­¤å‚æ•°æ—¶ï¼Œè‡ªå®šä¹‰å‡½æ•°çš„éƒ¨åˆ†ï¼Œå¦‚æœæŒ‡å®šæŒ‰ç…§['date']åˆ†ç»„groupbyè®¡ç®—ï¼Œ
            åˆ™è¿”å›æ—¶ï¼Œåº”å½“è¿”å›ä¸€ä¸ªä¸¤åˆ—çš„dataframeï¼Œç¬¬ä¸€åˆ—ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç¬¬äºŒåˆ—ä¸ºä¸ºå› å­å€¼, by default ['date','code']
        """
        homeplace = HomePlace()
        self.kind = kind
        self.groupby_target = groupby_target
        if clickhouse == 0 and questdb == 0:
            clickhouse = 1
        self.clickhouse = clickhouse
        self.questdb = questdb
        self.questdb_web_port = questdb_web_port
        if clickhouse == 1:
            # è¿æ¥clickhouse
            self.chc = ClickHouseClient("minute_data")
        elif questdb == 1:
            self.chc = Questdb(host=questdb_host, web_port=questdb_web_port)
        # å°†è®¡ç®—åˆ°ä¸€åŠçš„å› å­ï¼Œå­˜å…¥questdbä¸­ï¼Œé¿å…ä¸­é€”è¢«æ‰“æ–­åé‡æ–°è®¡ç®—ï¼Œè¡¨åå³ä¸ºå› å­æ–‡ä»¶åçš„æ±‰è¯­æ‹¼éŸ³
        pinyin = Pinyin()
        self.factor_file_pinyin = pinyin.get_pinyin(
            factor_file.replace(".parquet", ""), ""
        )
        self.factor_steps = Questdb(host=questdb_host, web_port=questdb_web_port)
        if project is not None:
            if not os.path.exists(homeplace.factor_data_file + project):
                os.makedirs(homeplace.factor_data_file + project)
            else:
                logger.info(f"å½“å‰æ­£åœ¨{project}é¡¹ç›®ä¸­â€¦â€¦")
        else:
            logger.warning("å½“å‰å› å­ä¸å±äºä»»ä½•é¡¹ç›®ï¼Œè¿™å°†é€ æˆå› å­æ•°æ®æ–‡ä»¶å¤¹çš„æ··ä¹±ï¼Œä¸ä¾¿äºç®¡ç†ï¼Œå»ºè®®æŒ‡å®šä¸€ä¸ªé¡¹ç›®åç§°")
        # å®Œæ•´çš„å› å­æ–‡ä»¶è·¯å¾„
        if project is not None:
            factor_file = homeplace.factor_data_file + project + "/" + factor_file
        else:
            factor_file = homeplace.factor_data_file + factor_file
        self.factor_file = factor_file
        # è¯»å…¥ä¹‹å‰çš„å› å­
        if os.path.exists(factor_file):
            factor_old = drop_duplicates_index(pd.read_parquet(self.factor_file))
            self.factor_old = factor_old
            # å·²ç»ç®—å¥½çš„æ—¥å­
            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
            self.dates_old = dates_old
        elif (not ignore_history_in_questdb) and self.factor_file_pinyin in list(
            self.factor_steps.get_data("show tables").table
        ):
            logger.info(
                f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œç°åœ¨å°†è¯»å–ä¸Šæ¬¡çš„æ•°æ®ï¼Œç»§ç»­è®¡ç®—"
            )
            factor_old = self.factor_steps.get_data_with_tuple(
                f"select * from '{self.factor_file_pinyin}'"
            ).drop_duplicates(subset=["date", "code"])
            factor_old = factor_old.pivot(index="date", columns="code", values="fac")
            factor_old = factor_old.sort_index()
            self.factor_old = factor_old
            # å·²ç»ç®—å¥½çš„æ—¥å­
            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
            self.dates_old = dates_old
        elif ignore_history_in_questdb and self.factor_file_pinyin in list(
            self.factor_steps.get_data("show tables").table
        ):
            logger.info(
                f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œä½†æ‚¨é€‰æ‹©é‡æ–°è®¡ç®—ï¼Œæ‰€ä»¥æ­£åœ¨åˆ é™¤åŸæ¥çš„æ•°æ®ï¼Œä»å¤´è®¡ç®—"
            )
            factor_old = self.factor_steps.do_order(
                f"drop table '{self.factor_file_pinyin}'"
            )
            self.factor_old = None
            self.dates_old = []
            logger.info("åˆ é™¤å®Œæ¯•ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
        else:
            self.factor_old = None
            self.dates_old = []
            logger.info("è¿™ä¸ªå› å­ä»¥å‰æ²¡æœ‰ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
        # è¯»å–å½“å‰æ‰€æœ‰çš„æ—¥å­
        dates_all = self.chc.show_all_dates(f"minute_data_{kind}")
        dates_all = [int(i) for i in dates_all]
        if startdate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i >= startdate]
        if enddate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i <= enddate]
        self.dates_all = dates_all
        # éœ€è¦æ–°è¡¥å……çš„æ—¥å­
        self.dates_new = sorted([i for i in dates_all if i not in self.dates_old])
        if len(self.dates_new) == 0:
            ...
        elif len(self.dates_new) == 1:
            self.dates_new_intervals = [[pd.Timestamp(str(self.dates_new[0]))]]
            print(f"åªç¼ºä¸€å¤©{self.dates_new[0]}")
        else:
            dates = [pd.Timestamp(str(i)) for i in self.dates_new]
            intervals = [[]] * len(dates)
            interbee = 0
            intervals[0] = intervals[0] + [dates[0]]
            for i in range(len(dates) - 1):
                val1 = dates[i]
                val2 = dates[i + 1]
                if val2 - val1 < pd.Timedelta(days=30):
                    ...
                else:
                    interbee = interbee + 1
                intervals[interbee] = intervals[interbee] + [val2]
            intervals = [i for i in intervals if len(i) > 0]
            print(f"å…±{len(intervals)}ä¸ªæ—¶é—´åŒºé—´ï¼Œåˆ†åˆ«æ˜¯")
            for date in intervals:
                print(f"ä»{date[0]}åˆ°{date[-1]}")
            self.dates_new_intervals = intervals
        self.factor_new = []

    def __call__(self) -> pd.DataFrame:
        """è·å¾—ç»è¿ç®—äº§ç”Ÿçš„å› å­

        Returns
        -------
        `pd.DataFrame`
            ç»è¿ç®—äº§ç”Ÿçš„å› å­å€¼
        """
        return self.factor.copy()

    def forward_dates(self, dates, many_days):
        dates_index = [self.dates_all.index(i) for i in dates]

        def value(x, a):
            if x >= 0:
                return a[x]
            else:
                return None

        return [value(i - many_days, self.dates_all) for i in dates_index]

    def select_one_calculate(
        self,
        date: pd.Timestamp,
        func: Callable,
        fields: str = "*",
        show_time: bool = 0,
    ) -> None:
        the_func = partial(func)
        if not isinstance(date, int):
            date = int(datetime.datetime.strftime(date, "%Y%m%d"))
        # å¼€å§‹è®¡ç®—å› å­å€¼
        if self.clickhouse == 1:
            sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={date * 100} order by code,date,num"
        else:
            sql_order = (
                f"select {fields} from minute_data_{self.kind} where date='{date}'"
            )
        if show_time:
            df = self.chc.get_data_show_time(sql_order)
        else:
            df = self.chc.get_data(sql_order)
        if self.clickhouse == 1:
            df = ((df.set_index("code")) / 100).reset_index()
        else:
            df.num = df.num.astype(int)
            df.date = df.date.astype(int)
            df = df.sort_values(["date", "num"])
        df = df.groupby(self.groupby_target).apply(the_func)
        if self.groupby_target == ["date", "code"]:
            df = df.to_frame("fac").reset_index()
            df.columns = ["date", "code", "fac"]
        else:
            df = df.reset_index()
        if (df is not None) and (df.shape[0] > 0):
            df = df.pivot(columns="code", index="date", values="fac")
            df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
            to_save = df.stack().reset_index()
            to_save.columns = ["date", "code", "fac"]
            self.factor_steps.write_via_df(
                to_save, self.factor_file_pinyin, tuple_col="fac"
            )
            return df

    def select_many_calculate(
        self,
        dates: List[pd.Timestamp],
        func: Callable,
        fields: str = "*",
        chunksize: int = 10,
        show_time: bool = 0,
        many_days: int = 1,
        n_jobs: int = 1,
        use_mpire: bool = 0,
    ) -> None:
        the_func = partial(func)
        factor_new = []
        dates = [int(datetime.datetime.strftime(i, "%Y%m%d")) for i in dates]
        if many_days == 1:
            # å°†éœ€è¦æ›´æ–°çš„æ—¥å­åˆ†å—ï¼Œæ¯200å¤©ä¸€ç»„ï¼Œä¸€èµ·è¿ç®—
            dates_new_len = len(dates)
            cut_points = list(range(0, dates_new_len, chunksize)) + [dates_new_len - 1]
            if cut_points[-1] == cut_points[-2]:
                cut_points = cut_points[:-1]
            cuts = tuple(zip(cut_points[:-many_days], cut_points[many_days:]))
            df_first = self.select_one_calculate(
                date=dates[0],
                func=func,
                fields=fields,
                show_time=show_time,
            )
            factor_new.append(df_first)

            def cal_one(date1, date2):
                if self.clickhouse == 1:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1] * 100} and date<={dates[date2] * 100} order by code,date,num"
                else:
                    sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{dates[date1]} and cast(date as int)<={dates[date2]} order by code,date,num"
                if show_time:
                    df = self.chc.get_data_show_time(sql_order)
                else:
                    df = self.chc.get_data(sql_order)
                if self.clickhouse == 1:
                    df = ((df.set_index("code")) / 100).reset_index()
                else:
                    df.num = df.num.astype(int)
                    df.date = df.date.astype(int)
                    df = df.sort_values(["date", "num"])
                df = df.groupby(self.groupby_target).apply(the_func)
                if self.groupby_target == ["date", "code"]:
                    df = df.to_frame("fac").reset_index()
                    df.columns = ["date", "code", "fac"]
                else:
                    df = df.reset_index()
                df = df.pivot(columns="code", index="date", values="fac")
                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                to_save = df.stack().reset_index()
                to_save.columns = ["date", "code", "fac"]
                self.factor_steps.write_via_df(
                    to_save, self.factor_file_pinyin, tuple_col="fac"
                )
                return df

            if n_jobs > 1:
                if use_mpire:
                    with WorkerPool(n_jobs=n_jobs) as pool:
                        factor_new_more = pool.map(
                            cal_one,
                            cut_points[:-many_days],
                            cut_points[many_days:],
                            progress_bar=True,
                        )
                else:
                    with concurrent.futures.ThreadPoolExecutor(
                        max_workers=n_jobs
                    ) as executor:
                        factor_new_more = list(
                            tqdm.auto.tqdm(
                                executor.map(
                                    cal_one,
                                    cut_points[:-many_days],
                                    cut_points[many_days:],
                                ),
                                total=len(cut_points[many_days:]),
                            )
                        )
                factor_new = factor_new + factor_new_more
            else:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                for date1, date2 in tqdm.auto.tqdm(cuts, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
                    df = cal_one(date1, date2)
                    factor_new.append(df)
        else:

            def cal_two(date1, date2):
                if date1 is not None:
                    if self.clickhouse == 1:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{date1*100} and date<={date2*100} order by code,date,num"
                    else:
                        sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{date1} and cast(date as int)<={date2} order by code,date,num"
                    if show_time:
                        df = self.chc.get_data_show_time(sql_order)
                    else:
                        df = self.chc.get_data(sql_order)
                    if self.clickhouse == 1:
                        df = ((df.set_index("code")) / 100).reset_index()
                    else:
                        df.num = df.num.astype(int)
                        df.date = df.date.astype(int)
                        df = df.sort_values(["date", "num"])
                    if self.groupby_target == [
                        "date",
                        "code",
                    ] or self.groupby_target == ["code"]:
                        df = df.groupby(["code"]).apply(the_func).reset_index()
                    else:
                        df = the_func(df)
                    df = df.assign(date=date2)
                    df.columns = ["code", "fac", "date"]
                    df = df.pivot(columns="code", index="date", values="fac")
                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                    to_save = df.stack().reset_index()
                    to_save.columns = ["date", "code", "fac"]
                    self.factor_steps.write_via_df(
                        to_save, self.factor_file_pinyin, tuple_col="fac"
                    )
                    return df

            pairs = self.forward_dates(dates, many_days=many_days)
            cuts2 = tuple(zip(pairs, dates))
            if n_jobs > 1:
                if use_mpire:
                    with WorkerPool(n_jobs=n_jobs) as pool:
                        factor_new_more = pool.map(
                            cal_two, pairs, dates, progress_bar=True
                        )
                else:
                    with concurrent.futures.ThreadPoolExecutor(
                        max_workers=n_jobs
                    ) as executor:
                        factor_new_more = list(
                            tqdm.auto.tqdm(
                                executor.map(cal_two, pairs, dates),
                                total=len(pairs),
                            )
                        )
                factor_new = factor_new + factor_new_more
            else:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                for date1, date2 in tqdm.auto.tqdm(cuts2, desc="çŸ¥ä¸å¯ä¹éª¤å¾—ï¼Œæ‰˜é—å“äºæ‚²é£ã€‚"):
                    df = cal_two(date1, date2)
                    factor_new.append(df)

        if len(factor_new) > 0:
            factor_new = pd.concat(factor_new)
            return factor_new
        else:
            return None

    def select_any_calculate(
        self,
        dates: List[pd.Timestamp],
        func: Callable,
        fields: str = "*",
        chunksize: int = 10,
        show_time: bool = 0,
        many_days: int = 1,
        n_jobs: int = 1,
        use_mpire: bool = 0,
    ) -> None:
        if len(dates) == 1 and many_days == 1:
            res = self.select_one_calculate(
                dates[0],
                func=func,
                fields=fields,
                show_time=show_time,
            )
        else:
            res = self.select_many_calculate(
                dates=dates,
                func=func,
                fields=fields,
                chunksize=chunksize,
                show_time=show_time,
                many_days=many_days,
                n_jobs=n_jobs,
                use_mpire=use_mpire,
            )
        if res is not None:
            self.factor_new.append(res)
        return res

    @staticmethod
    def for_cross_via_str(func):
        """è¿”å›å€¼ä¸ºä¸¤å±‚çš„listï¼Œæ¯ä¸€ä¸ªé‡Œå±‚çš„å°listä¸ºå•ä¸ªè‚¡ç¥¨åœ¨è¿™ä¸€å¤©çš„è¿”å›å€¼
        ä¾‹å¦‚
        ```python
        return [[0.11,0.24,0.55],[2.59,1.99,0.43],[1.32,8.88,7.77]â€¦â€¦]
        ```
        ä¸Šä¾‹ä¸­ï¼Œæ¯ä¸ªè‚¡ç¥¨ä¸€å¤©è¿”å›ä¸‰ä¸ªå› å­å€¼ï¼Œé‡Œå±‚çš„listæŒ‰ç…§è‚¡ç¥¨ä»£ç é¡ºåºæ’åˆ—"""

        def full_run(df, *args, **kwargs):
            codes = sorted(list(set(df.code)))
            res = func(df, *args, **kwargs)
            if isinstance(res[0], list):
                kind = 1
                res = [",".join(i) for i in res]
            else:
                kind = 0
            df = pd.DataFrame({"code": codes, "fac": res})
            if kind:
                df.fac = df.fac.apply(lambda x: [float(i) for i in x.split(",")])
            return df

        return full_run

    @staticmethod
    def for_cross_via_zip(func):
        """è¿”å›å€¼ä¸ºå¤šä¸ªpd.Seriesï¼Œæ¯ä¸ªpd.Seriesçš„indexä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå•ä¸ªå› å­å€¼
        ä¾‹å¦‚
        ```python
        return (
                    pd.Series([1.54,8.77,9.99â€¦â€¦],index=['000001.SZ','000002.SZ','000004.SZ'â€¦â€¦]),
                    pd.Series([3.54,6.98,9.01â€¦â€¦],index=['000001.SZ','000002.SZ','000004.SZ'â€¦â€¦]),
                )
        ```
        ä¸Šä¾‹ä¸­ï¼Œæ¯ä¸ªè‚¡ç¥¨ä¸€å¤©è¿”å›ä¸¤ä¸ªå› å­å€¼ï¼Œæ¯ä¸ªpd.Serieså¯¹åº”ä¸€ä¸ªå› å­å€¼
        """

        def full_run(df, *args, **kwargs):
            res = func(df, *args, **kwargs)
            if isinstance(res, pd.Series):
                res = res.reset_index()
                res.columns = ["code", "fac"]
                return res
            elif isinstance(res, pd.DataFrame):
                res.columns = [f"fac{i}" for i in range(len(res.columns))]
                res = res.assign(fac=list(zip(*[res[i] for i in list(res.columns)])))
                res = res[["fac"]].reset_index()
                res.columns = ["code", "fac"]
                return res
            elif res is None:
                ...
            else:
                res = pd.concat(res, axis=1)
                res.columns = [f"fac{i}" for i in range(len(res.columns))]
                res = res.assign(fac=list(zip(*[res[i] for i in list(res.columns)])))
                res = res[["fac"]].reset_index()
                res.columns = ["code", "fac"]
                return res

        return full_run

    def get_daily_factors_one(
        self,
        func: Callable,
        fields: str = "*",
        chunksize: int = 10,
        show_time: bool = 0,
        many_days: int = 1,
        n_jobs: int = 1,
        use_mpire: bool = 0,
    ):
        if len(self.dates_new) > 0:
            for interval in self.dates_new_intervals:
                df = self.select_any_calculate(
                    dates=interval,
                    func=func,
                    fields=fields,
                    chunksize=chunksize,
                    show_time=show_time,
                    many_days=many_days,
                    n_jobs=n_jobs,
                    use_mpire=use_mpire,
                )
            if len(self.factor_new) > 0:
                self.factor_new = pd.concat(self.factor_new)
                # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
                self.factor = pd.concat([self.factor_old, self.factor_new]).sort_index()
                self.factor = drop_duplicates_index(self.factor.dropna(how="all"))
                new_end_date = datetime.datetime.strftime(
                    self.factor.index.max(), "%Y%m%d"
                )
                # å­˜å…¥æœ¬åœ°
                self.factor.to_parquet(self.factor_file)
                logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
                # åˆ é™¤å­˜å‚¨åœ¨questdbçš„ä¸­é€”å¤‡ä»½æ•°æ®
                try:
                    self.factor_steps.do_order(
                        f"drop table '{self.factor_file_pinyin}'"
                    )
                    logger.info("å¤‡ä»½åœ¨questdbçš„è¡¨æ ¼å·²åˆ é™¤")
                except Exception:
                    logger.warning("åˆ é™¤questdbä¸­è¡¨æ ¼æ—¶ï¼Œå­˜åœ¨æŸä¸ªæœªçŸ¥é”™è¯¯ï¼Œè¯·å½“å¿ƒ")
            else:
                logger.warning("ç”±äºæŸç§åŸå› ï¼Œæ›´æ–°çš„å› å­å€¼è®¡ç®—å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥ğŸ¤’")
                # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
                self.factor = pd.concat([self.factor_old]).sort_index()
                self.factor = drop_duplicates_index(self.factor.dropna(how="all"))

        else:
            self.factor = drop_duplicates_index(self.factor_old)
            # å­˜å…¥æœ¬åœ°
            self.factor.to_parquet(self.factor_file)
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            logger.info(f"å½“å‰æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼å·²ç»æ˜¯æœ€æ–°çš„äº†")

    @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
    def get_daily_factors_two(
        self,
        func: Callable,
        fields: str = "*",
        chunksize: int = 10,
        show_time: bool = 0,
        many_days: int = 1,
        n_jobs: int = 1,
    ):
        self.get_daily_factors_one(
            func=func,
            fields=fields,
            chunksize=chunksize,
            show_time=show_time,
            many_days=many_days,
            n_jobs=n_jobs,
        )

    def get_daily_factors(
        self,
        func: Callable,
        fields: str = "*",
        chunksize: int = 10,
        show_time: bool = 0,
        many_days: int = 1,
        n_jobs: int = 1,
    ) -> None:
        """æ¯æ¬¡æŠ½å–chunksizeå¤©çš„æˆªé¢ä¸Šå…¨éƒ¨è‚¡ç¥¨çš„åˆ†é’Ÿæ•°æ®
        å¯¹æ¯å¤©çš„è‚¡ç¥¨çš„æ•°æ®è®¡ç®—å› å­å€¼

        Parameters
        ----------
        func : Callable
            ç”¨äºè®¡ç®—å› å­å€¼çš„å‡½æ•°
        fields : str, optional
            è‚¡ç¥¨æ•°æ®æ¶‰åŠåˆ°å“ªäº›å­—æ®µï¼Œæ’é™¤ä¸å¿…è¦çš„å­—æ®µï¼Œå¯ä»¥èŠ‚çº¦è¯»å–æ•°æ®çš„æ—¶é—´ï¼Œå½¢å¦‚'date,code,num,close,amount,open'
            æå–å‡ºçš„æ•°æ®ï¼Œè‡ªåŠ¨æŒ‰ç…§code,date,numæ’åºï¼Œå› æ­¤code,date,numæ˜¯å¿…ä¸å¯å°‘çš„å­—æ®µ, by default "*"
        chunksize : int, optional
            æ¯æ¬¡è¯»å–çš„æˆªé¢ä¸Šçš„å¤©æ•°, by default 10
        show_time : bool, optional
            å±•ç¤ºæ¯æ¬¡è¯»å–æ•°æ®æ‰€éœ€è¦çš„æ—¶é—´, by default 0
        many_days : int, optional
            è®¡ç®—æŸå¤©çš„å› å­å€¼æ—¶ï¼Œéœ€è¦ä½¿ç”¨ä¹‹å‰å¤šå°‘å¤©çš„æ•°æ®
        n_jobs : int, optional
            å¹¶è¡Œæ•°é‡, by default 1
        """
        try:
            self.get_daily_factors_two(
                func=func,
                fields=fields,
                chunksize=chunksize,
                show_time=show_time,
                many_days=many_days,
                n_jobs=n_jobs,
            )
        except Exception:
            self.get_daily_factors_one(
                func=func,
                fields=fields,
                chunksize=chunksize,
                show_time=show_time,
                many_days=many_days,
                n_jobs=n_jobs,
            )

    def drop_table(self):
        """ç›´æ¥åˆ é™¤å­˜å‚¨åœ¨questdbä¸­çš„æš‚å­˜æ•°æ®"""
        try:
            self.factor_steps.do_order(f"drop table '{self.factor_file_pinyin}'")
            logger.success(f"æš‚å­˜åœ¨questdbä¸­çš„æ•°æ®è¡¨æ ¼'{self.factor_file_pinyin}'å·²ç»åˆ é™¤")
        except Exception:
            logger.warning(f"æ‚¨è¦åˆ é™¤çš„è¡¨æ ¼'{self.factor_file_pinyin}'å·²ç»ä¸å­˜åœ¨äº†ï¼Œè¯·æ£€æŸ¥")


class pure_coldwinter(object):
    # DONE: å¯ä»¥è‡ªç”±æ·»åŠ å…¶ä»–è¦å‰”é™¤çš„å› å­ï¼Œæˆ–è€…æ›¿æ¢æŸäº›è¦å‰”é™¤çš„å› å­

    @classmethod
    @lru_cache(maxsize=None)
    def __init__(
        cls,
        momentum: bool = 1,
        earningsyield: bool = 1,
        growth: bool = 1,
        liquidity: bool = 1,
        size: bool = 1,
        leverage: bool = 1,
        beta: bool = 1,
        nonlinearsize: bool = 1,
        residualvolatility: bool = 1,
        booktoprice: bool = 1,
    ) -> None:
        """è¯»å…¥10ç§å¸¸ç”¨é£æ ¼å› å­ï¼Œå¹¶å¯ä»¥é¢å¤–åŠ å…¥å…¶ä»–å› å­

        Parameters
        ----------
        facs_dict : Dict, optional
            é¢å¤–åŠ å…¥çš„å› å­ï¼Œåå­—ä¸ºkeyï¼Œå› å­çŸ©é˜µä¸ºvalueï¼Œå½¢å¦‚`{'åè½¬': ret20, 'æ¢æ‰‹': tr20}`, by default None
        momentum : bool, optional
            æ˜¯å¦åˆ å»åŠ¨é‡å› å­, by default 1
        earningsyield : bool, optional
            æ˜¯å¦åˆ å»ç›ˆåˆ©å› å­, by default 1
        growth : bool, optional
            æ˜¯å¦åˆ å»æˆé•¿å› å­, by default 1
        liquidity : bool, optional
            æ˜¯å¦åˆ å»æµåŠ¨æ€§å› å­, by default 1
        size : bool, optional
            æ˜¯å¦åˆ å»è§„æ¨¡å› å­, by default 1
        leverage : bool, optional
            æ˜¯å¦åˆ å»æ æ†å› å­, by default 1
        beta : bool, optional
            æ˜¯å¦åˆ å»è´å¡”å› å­, by default 1
        nonlinearsize : bool, optional
            æ˜¯å¦åˆ å»éçº¿æ€§å¸‚å€¼å› å­, by default 1
        residualvolatility : bool, optional
            æ˜¯å¦åˆ å»æ®‹å·®æ³¢åŠ¨ç‡å› å­, by default 1
        booktoprice : bool, optional
            æ˜¯å¦åˆ å»è´¦é¢å¸‚å€¼æ¯”å› å­, by default 1
        """
        cls.homeplace = HomePlace()
        # barraå› å­æ•°æ®
        styles = os.listdir(cls.homeplace.barra_data_file)
        styles = [i for i in styles if (i.endswith(".parquet")) and (i[0] != ".")]
        barras = {}
        for s in styles:
            k = s.split(".")[0]
            v = pd.read_parquet(cls.homeplace.barra_data_file + s).resample("M").last()
            barras[k] = v
        rename_dict = {
            "size": "å¸‚å€¼",
            "nonlinearsize": "éçº¿æ€§å¸‚å€¼",
            "booktoprice": "ä¼°å€¼",
            "earningsyield": "ç›ˆåˆ©",
            "growth": "æˆé•¿",
            "leverage": "æ æ†",
            "liquidity": "æµåŠ¨æ€§",
            "momentum": "åŠ¨é‡",
            "residualvolatility": "æ³¢åŠ¨ç‡",
            "beta": "è´å¡”",
        }
        if momentum == 0:
            barras = {k: v for k, v in barras.items() if k != "momentum"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "momentum"}
        if earningsyield == 0:
            barras = {k: v for k, v in barras.items() if k != "earningsyield"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "earningsyield"}
        if growth == 0:
            barras = {k: v for k, v in barras.items() if k != "growth"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "growth"}
        if liquidity == 0:
            barras = {k: v for k, v in barras.items() if k != "liquidity"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "liquidity"}
        if size == 0:
            barras = {k: v for k, v in barras.items() if k != "size"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "size"}
        if leverage == 0:
            barras = {k: v for k, v in barras.items() if k != "leverage"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "leverage"}
        if beta == 0:
            barras = {k: v for k, v in barras.items() if k != "beta"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "beta"}
        if nonlinearsize == 0:
            barras = {k: v for k, v in barras.items() if k != "nonlinearsize"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "nonlinearsize"}
        if residualvolatility == 0:
            barras = {k: v for k, v in barras.items() if k != "residualvolatility"}
            rename_dict = {
                k: v for k, v in rename_dict.items() if k != "residualvolatility"
            }
        if booktoprice == 0:
            barras = {k: v for k, v in barras.items() if k != "booktoprice"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "booktoprice"}
        facs_dict = {
            "åè½¬_20å¤©æ”¶ç›Šç‡å‡å€¼": boom_one(read_daily(ret=1)),
            "æ³¢åŠ¨_20å¤©æ”¶ç›Šç‡æ ‡å‡†å·®": read_daily(ret=1)
            .rolling(20, min_periods=10)
            .std()
            .resample("M")
            .last(),
            "æ¢æ‰‹_20å¤©æ¢æ‰‹ç‡å‡å€¼": boom_one(read_daily(tr=1)),
        }
        barras.update(facs_dict)
        rename_dict.update({k: k for k in facs_dict.keys()})
        cls.barras = barras
        cls.rename_dict = rename_dict
        sort_names = list(rename_dict.values())
        cls.sort_names = sort_names
        cls.barras_together = merge_many(
            list(barras.values()), list(barras.keys()), how="inner"
        )

    def __call__(self):
        """è¿”å›çº¯å‡€å› å­å€¼"""
        return self.snow_fac

    def set_factors_df_wide(self, df: pd.DataFrame, other_factors: dict = None):
        """ä¼ å…¥å› å­æ•°æ®ï¼Œæ—¶é—´ä¸ºç´¢å¼•ï¼Œä»£ç ä¸ºåˆ—å"""
        df = df.resample("M").last()
        self.__corr = [
            df.corrwith(i, axis=1).mean() for i in list(self.barras.values())
        ]
        self.__corr = (
            pd.Series(
                self.__corr, index=[self.rename_dict[i] for i in self.barras.keys()]
            )
            .to_frame("ç›¸å…³ç³»æ•°")
            .T
        )
        self.__corr = self.__corr[self.sort_names]
        df = df.stack().reset_index()
        df.columns = ["date", "code", "fac"]
        self.factors = df
        self.corr_pri = pd.merge(df, self.barras_together, on=["date", "code"]).dropna()
        if other_factors is not None:
            other_factors = merge_many(
                list(other_factors.values()), list(other_factors.keys()), how="inner"
            )
            self.corr_pri = pd.merge(self.corr_pri, other_factors, on=["date", "code"])

    @property
    def corr(self) -> pd.DataFrame:
        """å› å­å’Œ10ä¸ªå¸¸ç”¨é£æ ¼å› å­çš„ç›¸å…³ç³»æ•°

        Returns
        -------
        pd.DataFrame
            å› å­å’Œ10ä¸ªå¸¸ç”¨é£æ ¼å› å­çš„ç›¸å…³ç³»æ•°
        """
        return self.__corr.copy()

    def ols_in_group(self, df):
        """å¯¹æ¯ä¸ªæ—¶é—´æ®µè¿›è¡Œå›å½’ï¼Œå¹¶è®¡ç®—æ®‹å·®"""
        xs = list(df.columns)
        xs = [i for i in xs if i != "fac"]
        xs_join = "+".join(xs)
        ols_formula = "fac~" + xs_join
        ols_result = smf.ols(ols_formula, data=df).fit()
        ols_ws = {i: ols_result.params[i] for i in xs}
        ols_b = ols_result.params["Intercept"]
        to_minus = [ols_ws[i] * df[i] for i in xs]
        to_minus = reduce(lambda x, y: x + y, to_minus)
        df = df.assign(snow_fac=df.fac - to_minus - ols_b)
        df = df[["snow_fac"]]
        df = df.rename(columns={"snow_fac": "fac"})
        return df

    def get_snow_fac(self):
        """è·å¾—çº¯å‡€å› å­"""
        self.snow_fac = (
            self.corr_pri.set_index(["date", "code"])
            .groupby(["date"])
            .apply(self.ols_in_group)
        )
        self.snow_fac = self.snow_fac.unstack()
        self.snow_fac.columns = list(map(lambda x: x[1], list(self.snow_fac.columns)))


@do_on_dfs
class pure_snowtrain(object):
    """ç›´æ¥è¿”å›çº¯å‡€å› å­"""

    def __init__(
        self,
        factors: pd.DataFrame,
        facs_dict: Dict = None,
        momentum: bool = 1,
        earningsyield: bool = 1,
        growth: bool = 1,
        liquidity: bool = 1,
        size: bool = 1,
        leverage: bool = 1,
        beta: bool = 1,
        nonlinearsize: bool = 1,
        residualvolatility: bool = 1,
        booktoprice: bool = 1,
    ) -> None:
        """è®¡ç®—å› å­å€¼ä¸10ç§å¸¸ç”¨é£æ ¼å› å­ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶è¿›è¡Œçº¯å‡€åŒ–ï¼Œå¯ä»¥é¢å¤–åŠ å…¥å…¶ä»–å› å­

        Parameters
        ----------
        factors : pd.DataFrame
            è¦è€ƒå¯Ÿçš„å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        facs_dict : Dict, optional
            é¢å¤–åŠ å…¥çš„å› å­ï¼Œåå­—ä¸ºkeyï¼Œå› å­çŸ©é˜µä¸ºvalueï¼Œå½¢å¦‚`{'åè½¬': ret20, 'æ¢æ‰‹': tr20}`, by default None
        momentum : bool, optional
            æ˜¯å¦åˆ å»åŠ¨é‡å› å­, by default 1
        earningsyield : bool, optional
            æ˜¯å¦åˆ å»ç›ˆåˆ©å› å­, by default 1
        growth : bool, optional
            æ˜¯å¦åˆ å»æˆé•¿å› å­, by default 1
        liquidity : bool, optional
            æ˜¯å¦åˆ å»æµåŠ¨æ€§å› å­, by default 1
        size : bool, optional
            æ˜¯å¦åˆ å»è§„æ¨¡å› å­, by default 1
        leverage : bool, optional
            æ˜¯å¦åˆ å»æ æ†å› å­, by default 1
        beta : bool, optional
            æ˜¯å¦åˆ å»è´å¡”å› å­, by default 1
        nonlinearsize : bool, optional
            æ˜¯å¦åˆ å»éçº¿æ€§å¸‚å€¼å› å­, by default 1
        residualvolatility : bool, optional
            æ˜¯å¦åˆ å»æ®‹å·®æ³¢åŠ¨ç‡å› å­, by default 1
        booktoprice : bool, optional
            æ˜¯å¦åˆ å»è´¦é¢å¸‚å€¼æ¯”å› å­, by default 1
        """
        self.winter = pure_coldwinter(
            momentum=momentum,
            earningsyield=earningsyield,
            growth=growth,
            liquidity=liquidity,
            size=size,
            leverage=leverage,
            beta=beta,
            nonlinearsize=nonlinearsize,
            residualvolatility=residualvolatility,
            booktoprice=booktoprice,
        )
        self.winter.set_factors_df_wide(factors, facs_dict)
        self.winter.get_snow_fac()
        self.corr = self.winter.corr

    def __call__(self) -> pd.DataFrame:
        """è·å¾—çº¯å‡€åŒ–ä¹‹åçš„å› å­å€¼

        Returns
        -------
        pd.DataFrame
            çº¯å‡€åŒ–ä¹‹åçš„å› å­å€¼
        """
        return self.winter.snow_fac.copy()

    def show_corr(self) -> pd.DataFrame:
        """å±•ç¤ºå› å­ä¸barraé£æ ¼å› å­çš„ç›¸å…³ç³»æ•°

        Returns
        -------
        pd.DataFrame
            ç›¸å…³ç³»æ•°è¡¨æ ¼
        """
        return self.corr.applymap(lambda x: to_percent(x))


class pure_newyear(object):
    """è½¬ä¸ºç”Ÿæˆ25åˆ†ç»„å’Œç™¾åˆ†ç»„çš„æ”¶ç›ŠçŸ©é˜µè€Œå°è£…"""

    def __init__(
        self,
        facx: pd.DataFrame,
        facy: pd.DataFrame,
        group_num_single: int,
        trade_cost_double_side: float = 0,
        namex: str = "ä¸»",
        namey: str = "æ¬¡",
    ) -> None:
        """æ¡ä»¶åŒå˜é‡æ’åºæ³•ï¼Œå…ˆå¯¹æ‰€æœ‰è‚¡ç¥¨ï¼Œä¾ç…§å› å­facxè¿›è¡Œæ’åº
        ç„¶ååœ¨æ¯ä¸ªç»„å†…ï¼Œä¾ç…§facyè¿›è¡Œæ’åºï¼Œæœ€åç»Ÿè®¡å„ä¸ªç»„å†…çš„å¹³å‡æ”¶ç›Šç‡

        Parameters
        ----------
        facx : pd.DataFrame
            é¦–å…ˆè¿›è¡Œæ’åºçš„å› å­ï¼Œé€šå¸¸ä¸ºæ§åˆ¶å˜é‡ï¼Œç›¸å½“äºæ­£äº¤åŒ–ä¸­çš„è‡ªå˜é‡
            indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        facy : pd.DataFrame
            åœ¨facxçš„å„ä¸ªç»„å†…ï¼Œä¾ç…§facyè¿›è¡Œæ’åºï¼Œä¸ºä¸»è¦è¦ç ”ç©¶çš„å› å­
            indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        group_num_single : int
            å•ä¸ªå› å­åˆ†æˆå‡ ç»„ï¼Œé€šå¸¸ä¸º5æˆ–10
        trade_cost_double_side : float, optional
            äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
        namex : str, optional
            facxè¿™ä¸€å› å­çš„åå­—, by default "ä¸»"
        namey : str, optional
            facyè¿™ä¸€å› å­çš„åå­—, by default "æ¬¡"
        """
        homex = pure_fallmount(facx)
        homey = pure_fallmount(facy)
        if group_num_single == 5:
            homexy = homex > homey
        elif group_num_single == 10:
            homexy = homex >> homey
        shen = pure_moonnight(
            homexy(),
            group_num_single**2,
            trade_cost_double_side=trade_cost_double_side,
            plt_plot=False,
            print_comments=False,
        )
        sq = shen.shen.square_rets.copy()
        sq.index = [namex + str(i) for i in list(sq.index)]
        sq.columns = [namey + str(i) for i in list(sq.columns)]
        self.square_rets = sq

    def __call__(self) -> pd.DataFrame:
        """è°ƒç”¨å¯¹è±¡æ—¶ï¼Œè¿”å›æœ€ç»ˆç»“æœï¼Œæ­£æ–¹å½¢çš„åˆ†ç»„å¹´åŒ–æ”¶ç›Šç‡è¡¨

        Returns
        -------
        `pd.DataFrame`
            æ¯ä¸ªç»„çš„å¹´åŒ–æ”¶ç›Šç‡
        """
        return self.square_rets.copy()


@do_on_dfs
def follow_tests(
    fac: pd.DataFrame,
    trade_cost_double_side_list: float = [0.001, 0.002, 0.003, 0.004, 0.005],
    groups_num: int = 10,
    index_member_value_weighted: bool = 0,
    comments_writer: pd.ExcelWriter = None,
    net_values_writer: pd.ExcelWriter = None,
    pos: bool = 0,
    neg: bool = 0,
    swindustry: bool = 0,
    zxindustry: bool = 0,
    nums: List[int] = [3],
    opens_average_first_day: bool = 0,
    total_cap: bool = 0,
    without_industry: bool = 1,
):
    """å› å­å®Œæˆå…¨Aæµ‹è¯•åï¼Œè¿›è¡Œçš„ä¸€äº›å¿…è¦çš„åç»­æµ‹è¯•ï¼ŒåŒ…æ‹¬å„ä¸ªåˆ†ç»„è¡¨ç°ã€ç›¸å…³ç³»æ•°ä¸çº¯å‡€åŒ–ã€3510çš„å¤šç©ºå’Œå¤šå¤´ã€å„ä¸ªè¡Œä¸šRank ICã€å„ä¸ªè¡Œä¸šä¹°3åªè¶…é¢è¡¨ç°

    Parameters
    ----------
    fac : pd.DataFrame
        è¦è¿›è¡Œåç»­æµ‹è¯•çš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯å› å­å€¼
    trade_cost_double_side : float, optional
        äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
    groups_num : int, optional
        åˆ†ç»„æ•°é‡, by default 10 
    index_member_value_weighted : bool, optional
        æˆåˆ†è‚¡å¤šå¤´é‡‡å–æµé€šå¸‚å€¼åŠ æƒ
    comments_writer : pd.ExcelWriter, optional
        å†™å…¥è¯„ä»·æŒ‡æ ‡çš„excel, by default None
    net_values_writer : pd.ExcelWriter, optional
        å†™å…¥å‡€å€¼åºåˆ—çš„excel, by default None
    pos : bool, optional
        å› å­çš„æ–¹å‘ä¸ºæ­£, by default 0
    neg : bool, optional
        å› å­çš„æ–¹å‘ä¸ºè´Ÿ, by default 0
    swindustry : bool, optional
        ä½¿ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸š, by default 0
    zxindustry : bool, optional
        ä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸š, by default 0
    nums : List[int], optional
        å„ä¸ªè¡Œä¸šä¹°å‡ åªè‚¡ç¥¨, by default [3]
    opens_average_first_day : bool, optional
        ä¹°å…¥æ—¶ä½¿ç”¨ç¬¬ä¸€å¤©çš„å¹³å‡ä»·æ ¼, by default 0
    total_cap : bool, optional
        åŠ æƒå’Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–æ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0
    without_industry : bool, optional
        æ˜¯å¦ä¸å¯¹è¡Œä¸šåšæµ‹è¯•, by default 1

    Raises
    ------
    IOError
        å¦‚æœæœªæŒ‡å®šå› å­æ­£è´Ÿæ–¹å‘ï¼Œå°†æŠ¥é”™
    """
    if comments_writer is None:
        from pure_ocean_breeze.state.states import COMMENTS_WRITER

        comments_writer = COMMENTS_WRITER
    if net_values_writer is None:
        from pure_ocean_breeze.state.states import NET_VALUES_WRITER

        net_values_writer = NET_VALUES_WRITER

    shen = pure_moonnight(
        fac,
        opens_average_first_day=opens_average_first_day,
        trade_cost_double_side=0.003,
    )
    if (
        shen.shen.group_net_values.group1.iloc[-1]
        > shen.shen.group_net_values.group10.iloc[-1]
    ):
        neg = 1
    else:
        pos = 1
    if comments_writer is not None:
        shen.comments_ten().to_excel(comments_writer, sheet_name="ååˆ†ç»„")
    print(shen.comments_ten())
    """ç›¸å…³ç³»æ•°ä¸çº¯å‡€åŒ–"""
    pure_fac = pure_snowtrain(fac)
    if comments_writer is not None:
        pure_fac.corr.to_excel(comments_writer, sheet_name="ç›¸å…³ç³»æ•°")
    print(pure_fac.corr)
    shen = pure_moonnight(
        pure_fac(),
        comments_writer=comments_writer,
        net_values_writer=net_values_writer,
        sheetname="çº¯å‡€",
        opens_average_first_day=opens_average_first_day,
        total_cap=total_cap,
    )
    """3510å¤šç©ºå’Œå¤šå¤´"""
    # 300
    fi300 = daily_factor_on300500(fac, hs300=1)
    shen = pure_moonnight(
        fi300,
        groups_num=groups_num,
        value_weighted=index_member_value_weighted,
        comments_writer=comments_writer,
        net_values_writer=net_values_writer,
        sheetname="300å¤šç©º",
        opens_average_first_day=opens_average_first_day,
        total_cap=total_cap,
        trade_cost_double_side=0.003,
    )
    if pos:
        if comments_writer is not None:
            make_relative_comments(shen.shen.group_rets[f'group{groups_num}'], hs300=1).to_excel(
                comments_writer, sheet_name="300è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments(
                    shen.shen.group_rets[f'group{groups_num}']
                    - shen.shen.factor_turnover_rates[f'group{groups_num}'] * i,
                    hs300=1,
                ).to_excel(comments_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments(shen.shen.group_rets[f'group{groups_num}'], hs300=1)
        if net_values_writer is not None:
            make_relative_comments_plot(shen.shen.group_rets[f'group{groups_num}'], hs300=1).to_excel(
                net_values_writer, sheet_name="300è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments_plot(
                    shen.shen.group_rets[f'group{groups_num}']
                    - shen.shen.factor_turnover_rates[f'group{groups_num}'] * i,
                    hs300=1,
                ).to_excel(net_values_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments_plot(shen.shen.group_rets[f'group{groups_num}'], hs300=1)
    elif neg:
        if comments_writer is not None:
            make_relative_comments(shen.shen.group_rets.group1, hs300=1).to_excel(
                comments_writer, sheet_name="300è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments(
                    shen.shen.group_rets.group1
                    - shen.shen.factor_turnover_rates.group1 * i,
                    hs300=1,
                ).to_excel(comments_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments(shen.shen.group_rets.group1, hs300=1)
        if net_values_writer is not None:
            make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1).to_excel(
                net_values_writer, sheet_name="300è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments_plot(
                    shen.shen.group_rets.group1
                    - shen.shen.factor_turnover_rates.group1 * i,
                    hs300=1,
                ).to_excel(net_values_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1)
    else:
        raise IOError("è¯·æŒ‡å®šå› å­çš„æ–¹å‘æ˜¯æ­£æ˜¯è´ŸğŸ¤’")
    # 500
    fi500 = daily_factor_on300500(fac, zz500=1)
    shen = pure_moonnight(
        fi500,
        groups_num=groups_num,
        value_weighted=index_member_value_weighted,
        comments_writer=comments_writer,
        net_values_writer=net_values_writer,
        sheetname="500å¤šç©º",
        opens_average_first_day=opens_average_first_day,
        total_cap=total_cap,
        trade_cost_double_side=0.003,
    )
    if pos:
        if comments_writer is not None:
            make_relative_comments(shen.shen.group_rets[f'group{groups_num}'], zz500=1).to_excel(
                comments_writer, sheet_name="500è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments(
                    shen.shen.group_rets[f'group{groups_num}']
                    - shen.shen.factor_turnover_rates[f'group{groups_num}'] * i,
                    zz500=1,
                ).to_excel(comments_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments(shen.shen.group_rets[f'group{groups_num}'], zz500=1)
        if net_values_writer is not None:
            make_relative_comments_plot(shen.shen.group_rets[f'group{groups_num}'], zz500=1).to_excel(
                net_values_writer, sheet_name="500è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments_plot(
                    shen.shen.group_rets[f'group{groups_num}']
                    - shen.shen.factor_turnover_rates[f'group{groups_num}'] * i,
                    zz500=1,
                ).to_excel(net_values_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments_plot(shen.shen.group_rets[f'group{groups_num}'], zz500=1)
    else:
        if comments_writer is not None:
            make_relative_comments(shen.shen.group_rets.group1, zz500=1).to_excel(
                comments_writer, sheet_name="500è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments(
                    shen.shen.group_rets.group1
                    - shen.shen.factor_turnover_rates.group1 * i,
                    zz500=1,
                ).to_excel(comments_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments(shen.shen.group_rets.group1, zz500=1)
        if net_values_writer is not None:
            make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1).to_excel(
                net_values_writer, sheet_name="500è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments_plot(
                    shen.shen.group_rets.group1
                    - shen.shen.factor_turnover_rates.group1 * i,
                    zz500=1,
                ).to_excel(net_values_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1)
    # 1000
    fi1000 = daily_factor_on300500(fac, zz1000=1)
    shen = pure_moonnight(
        fi1000,
        groups_num=groups_num,
        value_weighted=index_member_value_weighted,
        comments_writer=comments_writer,
        net_values_writer=net_values_writer,
        sheetname="1000å¤šç©º",
        opens_average_first_day=opens_average_first_day,
        total_cap=total_cap,
        trade_cost_double_side=0.003,
    )
    if pos:
        if comments_writer is not None:
            make_relative_comments(shen.shen.group_rets[f'group{groups_num}'], zz1000=1).to_excel(
                comments_writer, sheet_name="1000è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments(
                    shen.shen.group_rets[f'group{groups_num}']
                    - shen.shen.factor_turnover_rates[f'group{groups_num}'] * i,
                    zz1000=1,
                ).to_excel(comments_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments(shen.shen.group_rets[f'group{groups_num}'], zz1000=1)
        if net_values_writer is not None:
            make_relative_comments_plot(
                shen.shen.group_rets[f'group{groups_num}'], zz1000=1
            ).to_excel(net_values_writer, sheet_name="1000è¶…é¢")
            for i in trade_cost_double_side_list:
                make_relative_comments_plot(
                    shen.shen.group_rets[f'group{groups_num}']
                    - shen.shen.factor_turnover_rates[f'group{groups_num}'] * i,
                    zz1000=1,
                ).to_excel(net_values_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments_plot(shen.shen.group_rets[f'group{groups_num}'], zz1000=1)
    else:
        if comments_writer is not None:
            make_relative_comments(shen.shen.group_rets.group1, zz1000=1).to_excel(
                comments_writer, sheet_name="1000è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments(
                    shen.shen.group_rets.group1
                    - shen.shen.factor_turnover_rates.group1 * i,
                    zz1000=1,
                ).to_excel(comments_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments(shen.shen.group_rets.group1, zz1000=1)
        if net_values_writer is not None:
            make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1).to_excel(
                net_values_writer, sheet_name="1000è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments_plot(
                    shen.shen.group_rets.group1
                    - shen.shen.factor_turnover_rates.group1 * i,
                    zz1000=1,
                ).to_excel(net_values_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1)
    if not without_industry:
        # å„è¡Œä¸šRank IC
        rankics = rankic_test_on_industry(fac, comments_writer)
        # ä¹°3åªè¶…é¢è¡¨ç°
        rets = long_test_on_industry(
            fac, nums, pos=pos, neg=neg, swindustry=swindustry, zxindustry=zxindustry
        )
    logger.success("å› å­åç»­çš„å¿…è¦æµ‹è¯•å…¨éƒ¨å®Œæˆ")


def test_on_index_four_out(
    fac: pd.DataFrame,
    value_weighted: bool = 0,
    trade_cost_double_side_list: float = [0.001, 0.002, 0.003, 0.004, 0.005],
    group_num: int = 10,
    boxcox: bool = 1,
    comments_writer: pd.ExcelWriter = None,
    net_values_writer: pd.ExcelWriter = None,
    opens_average_first_day: bool = 0,
    total_cap: bool = 0,
):
    if comments_writer is None:
        from pure_ocean_breeze.state.states import COMMENTS_WRITER

        comments_writer = COMMENTS_WRITER
    if net_values_writer is None:
        from pure_ocean_breeze.state.states import NET_VALUES_WRITER

        net_values_writer = NET_VALUES_WRITER

    shen = pure_moonnight(
        fac,
        opens_average_first_day=opens_average_first_day,
    )
    if (
        shen.shen.group_net_values.group1.iloc[-1]
        > shen.shen.group_net_values10.iloc[-1]
    ):
        neg = 1
        pos = 0
    else:
        pos = 1
        neg = 0

    """3510å¤šç©ºå’Œå¤šå¤´"""
    # 300
    fi300 = daily_factor_on300500(fac, hs300=1)
    shen = pure_moonnight(
        fi300,
        value_weighted=value_weighted,
        groups_num=group_num,
        boxcox=boxcox,
        comments_writer=comments_writer,
        net_values_writer=net_values_writer,
        sheetname="300å¤šç©º",
        opens_average_first_day=opens_average_first_day,
        total_cap=total_cap,
    )
    if pos:
        if comments_writer is not None:
            make_relative_comments(
                shen.shen.group_rets[f"group{group_num}"], hs300=1
            ).to_excel(comments_writer, sheet_name="300è¶…é¢")
            for i in trade_cost_double_side_list:
                make_relative_comments(
                    shen.shen.group_rets[f"group{group_num}"]
                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
                    hs300=1,
                ).to_excel(comments_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments(shen.shen.group_rets[f"group{group_num}"], hs300=1)
        if net_values_writer is not None:
            make_relative_comments_plot(
                shen.shen.group_rets[f"group{group_num}"], hs300=1
            ).to_excel(net_values_writer, sheet_name="300è¶…é¢")
            for i in trade_cost_double_side_list:
                make_relative_comments_plot(
                    shen.shen.group_rets[f"group{group_num}"]
                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
                    hs300=1,
                ).to_excel(net_values_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments_plot(
                shen.shen.group_rets[f"group{group_num}"], hs300=1
            )
    elif neg:
        if comments_writer is not None:
            make_relative_comments(shen.shen.group_rets.group1, hs300=1).to_excel(
                comments_writer, sheet_name="300è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments(
                    shen.shen.group_rets.group1
                    - shen.shen.factor_turnover_rates.group1 * i,
                    hs300=1,
                ).to_excel(comments_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments(shen.shen.group_rets.group1, hs300=1)
        if net_values_writer is not None:
            make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1).to_excel(
                net_values_writer, sheet_name="300è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments_plot(
                    shen.shen.group_rets.group1
                    - shen.shen.factor_turnover_rates.group1 * i,
                    hs300=1,
                ).to_excel(net_values_writer, sheet_name=f"300è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments_plot(shen.shen.group_rets.group1, hs300=1)
    else:
        raise IOError("è¯·æŒ‡å®šå› å­çš„æ–¹å‘æ˜¯æ­£æ˜¯è´ŸğŸ¤’")
    # 500
    fi500 = daily_factor_on300500(fac, zz500=1)
    shen = pure_moonnight(
        fi500,
        value_weighted=value_weighted,
        groups_num=group_num,
        boxcox=boxcox,
        comments_writer=comments_writer,
        net_values_writer=net_values_writer,
        sheetname="500å¤šç©º",
        opens_average_first_day=opens_average_first_day,
        total_cap=total_cap,
    )
    if pos:
        if comments_writer is not None:
            make_relative_comments(
                shen.shen.group_rets[f"group{group_num}"], zz500=1
            ).to_excel(comments_writer, sheet_name="500è¶…é¢")
            for i in trade_cost_double_side_list:
                make_relative_comments(
                    shen.shen.group_rets[f"group{group_num}"]
                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
                    zz500=1,
                ).to_excel(comments_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments(shen.shen.group_rets[f"group{group_num}"], zz500=1)
        if net_values_writer is not None:
            make_relative_comments_plot(
                shen.shen.group_rets[f"group{group_num}"], zz500=1
            ).to_excel(net_values_writer, sheet_name="500è¶…é¢")
            for i in trade_cost_double_side_list:
                make_relative_comments_plot(
                    shen.shen.group_rets[f"group{group_num}"]
                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
                    zz500=1,
                ).to_excel(net_values_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments_plot(
                shen.shen.group_rets[f"group{group_num}"], zz500=1
            )
    else:
        if comments_writer is not None:
            make_relative_comments(shen.shen.group_rets.group1, zz500=1).to_excel(
                comments_writer, sheet_name="500è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments(
                    shen.shen.group_rets.group1
                    - shen.shen.factor_turnover_rates.group1 * i,
                    zz500=1,
                ).to_excel(comments_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments(shen.shen.group_rets.group1, zz500=1)
        if net_values_writer is not None:
            make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1).to_excel(
                net_values_writer, sheet_name="500è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments_plot(
                    shen.shen.group_rets.group1
                    - shen.shen.factor_turnover_rates.group1 * i,
                    zz500=1,
                ).to_excel(net_values_writer, sheet_name=f"500è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments_plot(shen.shen.group_rets.group1, zz500=1)
    # 1000
    fi1000 = daily_factor_on300500(fac, zz1000=1)
    shen = pure_moonnight(
        fi1000,
        value_weighted=value_weighted,
        groups_num=group_num,
        boxcox=boxcox,
        comments_writer=comments_writer,
        net_values_writer=net_values_writer,
        sheetname="1000å¤šç©º",
        opens_average_first_day=opens_average_first_day,
        total_cap=total_cap,
    )
    if pos:
        if comments_writer is not None:
            make_relative_comments(
                shen.shen.group_rets[f"group{group_num}"], zz1000=1
            ).to_excel(comments_writer, sheet_name="1000è¶…é¢")
            for i in trade_cost_double_side_list:
                make_relative_comments(
                    shen.shen.group_rets[f"group{group_num}"]
                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
                    zz1000=1,
                ).to_excel(comments_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments(shen.shen.group_rets[f"group{group_num}"], zz1000=1)
        if net_values_writer is not None:
            make_relative_comments_plot(
                shen.shen.group_rets[f"group{group_num}"], zz1000=1
            ).to_excel(net_values_writer, sheet_name="1000è¶…é¢")
            for i in trade_cost_double_side_list:
                make_relative_comments_plot(
                    shen.shen.group_rets[f"group{group_num}"]
                    - shen.shen.factor_turnover_rates[f"group{group_num}"] * i,
                    zz1000=1,
                ).to_excel(net_values_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments_plot(
                shen.shen.group_rets[f"group{group_num}"], zz1000=1
            )
    else:
        if comments_writer is not None:
            make_relative_comments(shen.shen.group_rets.group1, zz1000=1).to_excel(
                comments_writer, sheet_name="1000è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments(
                    shen.shen.group_rets.group1
                    - shen.shen.factor_turnover_rates.group1 * i,
                    zz1000=1,
                ).to_excel(comments_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments(shen.shen.group_rets.group1, zz1000=1)
        if net_values_writer is not None:
            make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1).to_excel(
                net_values_writer, sheet_name="1000è¶…é¢"
            )
            for i in trade_cost_double_side_list:
                make_relative_comments_plot(
                    shen.shen.group_rets.group1
                    - shen.shen.factor_turnover_rates.group1 * i,
                    zz1000=1,
                ).to_excel(net_values_writer, sheet_name=f"1000è¶…é¢åŒè¾¹è´¹ç‡{i}")
        else:
            make_relative_comments_plot(shen.shen.group_rets.group1, zz1000=1)


class pure_helper(object):
    def __init__(
        self,
        df_main: pd.DataFrame,
        df_helper: pd.DataFrame,
        func: Callable = None,
        group: int = 10,
    ) -> None:
        """ä½¿ç”¨å› å­bçš„å€¼å¤§å°ï¼Œå¯¹å› å­aè¿›è¡Œåˆ†ç»„ï¼Œå¹¶å¯ä»¥åœ¨ç»„å†…è¿›è¡ŒæŸç§æ“ä½œ

        Parameters
        ----------
        df_main : pd.DataFrame
            è¦è¢«åˆ†ç»„å¹¶è¿›è¡Œæ“ä½œçš„å› å­
        df_helper : pd.DataFrame
            ç”¨æ¥åšåˆ†ç»„çš„ä¾æ®
        func : Callable, optional
            åˆ†ç»„åï¼Œç»„å†…è¦è¿›è¡Œçš„æ“ä½œ, by default None
        group : int, optional
            è¦åˆ†çš„ç»„æ•°, by default 10
        """
        self.df_main = df_main
        self.df_helper = df_helper
        self.func = func
        self.group = group
        if self.func is None:
            self.__data = self.sort_a_with_b()
        else:
            self.__data = self.sort_a_with_b_func()

    @property
    def data(self):
        return self.__data

    def __call__(self) -> pd.DataFrame:
        return self.data

    def sort_a_with_b(self):
        dfb = to_group(self.df_helper, group=self.group)
        dfb = dfb.stack().reset_index()
        dfb.columns = ["date", "code", "group"]
        dfa = self.df_main.stack().reset_index()
        dfa.columns = ["date", "code", "target"]
        df = pd.merge(dfa, dfb, on=["date", "code"])
        return df

    def sort_a_with_b_func(self):
        the_func = partial(self.func)
        df = self.sort_a_with_b().drop(columns=["code"])
        df = (
            df.groupby(["date", "group"])
            .apply(the_func)
            .drop(columns=["group"])
            .reset_index()
        )
        df = df.pivot(index="date", columns="group", values="target")
        df.columns = [f"group{str(int(i+1))}" for i in list(df.columns)]
        return df


class pure_fama(object):
    # @lru_cache(maxsize=None)
    def __init__(
        self,
        factors: List[pd.DataFrame],
        minus_group: Union[list, float] = 3,
        backsee: int = 20,
        rets: pd.DataFrame = None,
        value_weighted: bool = 1,
        add_market: bool = 1,
        add_market_series: pd.Series = None,
        factors_names: list = None,
        betas_rets: bool = 0,
        total_cap: bool = 0,
    ) -> None:
        """ä½¿ç”¨famaä¸‰å› å­çš„æ–¹æ³•ï¼Œå°†ä¸ªè‚¡çš„æ”¶ç›Šç‡ï¼Œæ‹†åˆ†å‡ºå„ä¸ªå› å­å¸¦æ¥çš„æ”¶ç›Šç‡ä»¥åŠç‰¹è´¨çš„æ”¶ç›Šç‡
        åˆ†åˆ«è®¡ç®—æ¯ä¸€æœŸï¼Œå„ä¸ªå› å­æ”¶ç›Šç‡çš„å€¼ï¼Œè¶…é¢æ”¶ç›Šç‡ï¼Œå› å­çš„æš´éœ²ï¼Œä»¥åŠç‰¹è´¨æ”¶ç›Šç‡

        Parameters
        ----------
        factors : List[pd.DataFrame]
            ç”¨äºè§£é‡Šæ”¶ç›Šçš„å„ä¸ªå› å­å€¼ï¼Œæ¯ä¸€ä¸ªéƒ½æ˜¯indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼çš„dataframe
        minus_group : Union[list, float], optional
            æ¯ä¸€ä¸ªå› å­å°†æˆªé¢ä¸Šçš„è‚¡ç¥¨åˆ†ä¸ºå‡ ç»„, by default 3
        backsee : int, optional
            åšæ—¶åºå›å½’æ—¶ï¼Œå›çœ‹çš„å¤©æ•°, by default 20
        rets : pd.DataFrame, optional
            æ¯åªä¸ªè‚¡çš„æ”¶ç›Šç‡ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºæ”¶ç›Šç‡ï¼Œé»˜è®¤ä½¿ç”¨å½“æ—¥æ—¥é—´æ”¶ç›Šç‡, by default None
        value_weighted : bool, optional
            æ˜¯å¦ä½¿ç”¨æµé€šå¸‚å€¼åŠ æƒ, by default 1
        add_market : bool, optional
            æ˜¯å¦åŠ å…¥å¸‚åœºæ”¶ç›Šç‡å› å­ï¼Œé»˜è®¤ä½¿ç”¨ä¸­è¯å…¨æŒ‡çš„æ¯æ—¥æ—¥é—´æ”¶ç›Šç‡, by default 1
        add_market_series : bool, optional
            åŠ å…¥çš„å¸‚åœºæ”¶ç›Šç‡çš„æ•°æ®ï¼Œå¦‚æœæ²¡æŒ‡å®šï¼Œåˆ™ä½¿ç”¨ä¸­è¯å…¨æŒ‡çš„æ—¥é—´æ”¶ç›Šç‡, by default None
        factors_names : list, optional
            å„ä¸ªå› å­çš„åå­—ï¼Œé»˜è®¤ä¸ºfac0(å¸‚åœºæ”¶ç›Šç‡å› å­ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»fac1å¼€å§‹),fac1,fac2,fac3, by default None
        betas_rets : bool, optional
            æ˜¯å¦è®¡ç®—æ¯åªä¸ªè‚¡çš„ç”±äºæš´éœ²åœ¨æ¯ä¸ªå› å­ä¸Šæ‰€å¸¦æ¥çš„æ”¶ç›Šç‡, by default 0
        total_cap : bool, optional
            åŠ æƒæ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0
        """
        start = max(
            [int(datetime.datetime.strftime(i.index.min(), "%Y%m%d")) for i in factors]
        )
        self.backsee = backsee
        self.factors = factors
        self.factors_names = factors_names
        if isinstance(minus_group, int):
            minus_group = [minus_group] * len(factors)
        self.minus_group = minus_group
        if rets is None:
            closes = read_daily(close=1, start=start)
            rets = closes / closes.shift(1) - 1
        self.rets = rets
        self.factors_group = [
            to_group(i, group=j) for i, j in zip(self.factors, self.minus_group)
        ]
        self.factors_group_long = [(i == 0) + 0 for i in self.factors_group]
        self.factors_group_short = [
            (i == (j - 1)) + 0 for i, j in zip(self.factors_group, self.minus_group)
        ]
        self.value_weighted = value_weighted
        if value_weighted:
            if total_cap:
                self.cap = read_daily(total_cap=1, start=start)
            self.cap = read_daily(flow_cap=1, start=start)
            self.factors_group_long = [self.cap * i for i in self.factors_group_long]
            self.factors_group_short = [self.cap * i for i in self.factors_group_short]
            self.factors_group_long = [
                (i.T / i.T.sum()).T for i in self.factors_group_long
            ]
            self.factors_group_short = [
                (i.T / i.T.sum()).T for i in self.factors_group_short
            ]
            self.factors_rets_long = [
                (self.rets * i).sum(axis=1).to_frame(f"fac{num+1}")
                for num, i in enumerate(self.factors_group_long)
            ]
            self.factors_rets_short = [
                (self.rets * i).sum(axis=1).to_frame(f"fac{num+1}")
                for num, i in enumerate(self.factors_group_short)
            ]
        else:
            self.factors_rets_long = [
                (self.rets * i).mean(axis=1).to_frame(f"fac{num+1}")
                for num, i in enumerate(self.factors_group_long)
            ]
            self.factors_rets_short = [
                (self.rets * i).mean(axis=1).to_frame(f"fac{num+1}")
                for num, i in enumerate(self.factors_group_short)
            ]
        self.rets_long = pd.concat(self.factors_rets_long, axis=1)
        self.rets_short = pd.concat(self.factors_rets_short, axis=1)
        self.__factors_rets = self.rets_long - self.rets_short
        if add_market_series is not None:
            add_market = 1
        self.add_market = add_market
        if add_market:
            if add_market_series is None:
                closes = read_market(close=1, every_stock=0, start=start).to_frame(
                    "fac0"
                )
            else:
                closes = add_market_series.to_frame("fac0")
            rets = closes / closes.shift(1) - 1
            self.__factors_rets = pd.concat([rets, self.__factors_rets], axis=1)
            if factors_names is not None:
                factors_names = ["å¸‚åœº"] + factors_names
        self.__data = self.make_df(self.rets, self.__factors_rets)
        tqdm.auto.tqdm.pandas()
        self.__coefficients = (
            self.__data.groupby("code").progress_apply(self.ols_in).reset_index()
        )
        self.__coefficients = self.__coefficients.rename(
            columns={
                i: "co" + i for i in list(self.__coefficients.columns) if "fac" in i
            }
        )
        self.__data = pd.merge(
            self.__data.reset_index(), self.__coefficients, on=["date", "code"]
        )
        betas = [
            self.__data[i] * self.__data["co" + i]
            for i in list(self.__data.columns)
            if i.startswith("fac")
        ]
        betas = sum(betas)
        self.__data = self.__data.assign(
            idiosyncratic=self.__data.ret - self.__data.intercept - betas
        )
        self.__idiosyncratic = self.__data.pivot(
            index="date", columns="code", values="idiosyncratic"
        )
        self.__alphas = self.__data.pivot(
            index="date", columns="code", values="intercept"
        )
        if factors_names is None:
            self.__betas = {
                i: self.__data.pivot(index="date", columns="code", values=i)
                for i in list(self.__data.columns)
                if i.startswith("fac")
            }
        else:
            facs = [i for i in list(self.__data.columns) if i.startswith("fac")]
            self.__betas = {
                factors_names[num]: self.__data.pivot(
                    index="date", columns="code", values=i
                )
                for num, i in enumerate(facs)
            }
        if betas_rets:
            if add_market:
                if add_market_series is None:
                    factors = [read_market(close=1, start=start)] + factors
                else:
                    factors = [
                        pd.DataFrame(
                            {k: add_market_series for k in list(factors[0].columns)},
                            index=factors[0].index,
                        )
                    ] + factors
            self.__betas_rets = {
                d1[0]: d1[1] * d2 for d1, d2 in zip(self.__betas, factors)
            }
        else:
            self.__betas_rets = "æ‚¨å¦‚æœæƒ³è®¡ç®—å„ä¸ªè‚¡ç¥¨åœ¨å„ä¸ªå› å­çš„æ”¶ç›Šç‡ï¼Œè¯·å…ˆæŒ‡å®šbetas_retså‚æ•°ä¸ºTrue"

    @property
    def idiosyncratic(self):
        return self.__idiosyncratic

    @property
    def data(self):
        return self.__data

    @property
    def alphas(self):
        return self.__alphas

    @property
    def betas(self):
        return self.__betas

    @property
    def betas_rets(self):
        return self.__betas_rets

    @property
    def factors_rets(self):
        return self.__factors_rets

    @property
    def coefficients(self):
        return self.__coefficients

    def __call__(self):
        return self.idiosyncratic

    def make_df(self, rets, facs):
        rets = rets.stack().reset_index()
        rets.columns = ["date", "code", "ret"]
        facs = facs.reset_index()
        facs.columns = ["date"] + list(facs.columns)[1:]
        df = pd.merge(rets, facs, on=["date"])
        df = df.set_index("date")
        return df

    def ols_in(self, df):
        try:
            if self.add_market:
                x = df[["fac0"] + [f"fac{i+1}" for i in range(len(self.factors))]]
            else:
                x = df[[f"fac{i+1}" for i in range(len(self.factors))]]
            ols = po.PandasRollingOLS(
                y=df[["ret"]],
                x=x,
                window=self.backsee,
            )
            betas = ols.beta
            alpha = ols.alpha
            return pd.concat([alpha, betas], axis=1)
        except Exception:
            # æœ‰äº›æ•°æ®æ€»å…±ä¸è¶³ï¼Œé‚£å°±è·³è¿‡
            ...


class pure_rollingols(object):
    def __init__(
        self,
        y: pd.DataFrame,
        xs: Union[List[pd.DataFrame], pd.DataFrame],
        backsee: int = 20,
        factors_names: List[str] = None,
    ) -> None:
        """ä½¿ç”¨è‹¥å¹²ä¸ªdataframeï¼Œå¯¹åº”çš„è‚¡ç¥¨è¿›è¡ŒæŒ‡å®šçª—å£çš„æ—¶åºæ»šåŠ¨å›å½’

        Parameters
        ----------
        y : pd.DataFrame
            æ»šåŠ¨å›å½’ä¸­çš„å› å˜é‡yï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
        xs : Union[List[pd.DataFrame], pd.DataFrame]
            æ»šåŠ¨å›å½’ä¸­çš„è‡ªå˜é‡xiï¼Œæ¯ä¸€ä¸ªdataframeï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
        backsee : int, optional
            æ»šåŠ¨å›å½’çš„æ—¶é—´çª—å£, by default 20
        factors_names : List[str], optional
            xsä¸­ï¼Œæ¯ä¸ªå› å­çš„åå­—, by default None
        """
        self.backsee = backsee
        self.y = y
        if not isinstance(xs, list):
            xs = [xs]
        self.xs = xs
        y = y.stack().reset_index()
        xs = [i.stack().reset_index() for i in xs]
        y.columns = ["date", "code", "y"]
        xs = [
            i.rename(
                columns={list(i.columns)[1]: "code", list(i.columns)[2]: f"x{j+1}"}
            )
            for j, i in enumerate(xs)
        ]
        xs = [y] + xs
        xs = reduce(lambda x, y: pd.merge(x, y, on=["date", "code"]), xs)
        xs = xs.set_index("date")
        self.__data = xs
        self.haha = xs
        tqdm.auto.tqdm.pandas()
        self.__coefficients = (
            self.__data.groupby("code").progress_apply(self.ols_in).reset_index()
        )
        self.__coefficients = self.__coefficients.rename(
            columns={i: "co" + i for i in list(self.__coefficients.columns) if "x" in i}
        )
        self.__data = pd.merge(
            self.__data.reset_index(), self.__coefficients, on=["date", "code"]
        )
        betas = [
            self.__data[i] * self.__data["co" + i]
            for i in list(self.__data.columns)
            if i.startswith("x")
        ]
        betas = sum(betas)
        self.__data = self.__data.assign(
            residual=self.__data.y - self.__data.intercept - betas
        )
        self.__residual = self.__data.pivot(
            index="date", columns="code", values="residual"
        )
        self.__alphas = self.__data.pivot(
            index="date", columns="code", values="intercept"
        )
        if factors_names is None:
            self.__betas = {
                i: self.__data.pivot(index="date", columns="code", values=i)
                for i in list(self.__data.columns)
                if i.startswith("cox")
            }
        else:
            facs = [i for i in list(self.__data.columns) if i.startswith("cox")]
            self.__betas = {
                factors_names[num]: self.__data.pivot(
                    index="date", columns="code", values=i
                )
                for num, i in enumerate(facs)
            }
        if len(list(self.__betas)) == 1:
            self.__betas = list(self.__betas.values())[0]

    @property
    def residual(self):
        return self.__residual

    @property
    def data(self):
        return self.__data

    @property
    def alphas(self):
        return self.__alphas

    @property
    def betas(self):
        return self.__betas

    @property
    def coefficients(self):
        return self.__coefficients

    def ols_in(self, df):
        try:
            ols = po.PandasRollingOLS(
                y=df[["y"]],
                x=df[[f"x{i+1}" for i in range(len(self.xs))]],
                window=self.backsee,
            )
            betas = ols.beta
            alpha = ols.alpha
            return pd.concat([alpha, betas], axis=1)

        except Exception:
            # æœ‰äº›æ•°æ®æ€»å…±ä¸è¶³ï¼Œé‚£å°±è·³è¿‡
            ...


@do_on_dfs
def test_on_300500(
    df: pd.DataFrame,
    trade_cost_double_side: float = 0,
    group_num: int = 10,
    value_weighted: bool = 1,
    boxcox: bool = 0,
    hs300: bool = 0,
    zz500: bool = 0,
    zz1000: bool = 0,
    gz2000: bool = 0,
    iplot: bool = 1,
    opens_average_first_day: bool = 0,
    total_cap: bool = 0,
) -> pd.Series:
    """å¯¹å› å­åœ¨æŒ‡æ•°æˆåˆ†è‚¡å†…è¿›è¡Œå¤šç©ºå’Œå¤šå¤´æµ‹è¯•

    Parameters
    ----------
    df : pd.DataFrame
        å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    trade_cost_double_side : float, optional
        äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
    group_num : int
        åˆ†ç»„æ•°é‡, by default 10
    value_weighted : bool
        æ˜¯å¦è¿›è¡Œæµé€šå¸‚å€¼åŠ æƒ, by default 0
    hs300 : bool, optional
        åœ¨æ²ªæ·±300æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
    zz500 : bool, optional
        åœ¨ä¸­è¯500æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
    zz1000 : bool, optional
        åœ¨ä¸­è¯1000æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
    gz1000 : bool, optional
        åœ¨å›½è¯2000æˆåˆ†è‚¡å†…æµ‹è¯•, by default 0
    iplot : bo0l,optional
        å¤šç©ºå›æµ‹çš„æ—¶å€™ï¼Œæ˜¯å¦ä½¿ç”¨cufflinksç»˜ç”»
    opens_average_first_day : bool, optional
        ä¹°å…¥æ—¶ä½¿ç”¨ç¬¬ä¸€å¤©çš„å¹³å‡ä»·æ ¼, by default 0
    total_cap : bool, optional
        åŠ æƒå’Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–æ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0

    Returns
    -------
    pd.Series
        å¤šå¤´ç»„åœ¨è¯¥æŒ‡æ•°ä¸Šçš„è¶…é¢æ”¶ç›Šåºåˆ—
    """
    fi300 = daily_factor_on300500(
        df, hs300=hs300, zz500=zz500, zz1000=zz1000, gz2000=gz2000
    )
    shen = pure_moonnight(
        fi300,
        value_weighted=value_weighted,
        groups_num=group_num,
        trade_cost_double_side=trade_cost_double_side,
        boxcox=boxcox,
        iplot=iplot,
        opens_average_first_day=opens_average_first_day,
        total_cap=total_cap,
    )
    if (
        shen.shen.group_net_values.group1.iloc[-1]
        > shen.shen.group_net_values[f"group{group_num}"].iloc[-1]
    ):
        print(
            make_relative_comments(
                shen.shen.group_rets.group1,
                hs300=hs300,
                zz500=zz500,
                zz1000=zz1000,
                gz2000=gz2000,
            )
        )
        abrets = make_relative_comments_plot(
            shen.shen.group_rets.group1,
            hs300=hs300,
            zz500=zz500,
            zz1000=zz1000,
            gz2000=gz2000,
        )
        return abrets
    else:
        print(
            make_relative_comments(
                shen.shen.group_rets[f"group{group_num}"],
                hs300=hs300,
                zz500=zz500,
                zz1000=zz1000,
                gz2000=gz2000,
            )
        )
        abrets = make_relative_comments_plot(
            shen.shen.group_rets[f"group{group_num}"],
            hs300=hs300,
            zz500=zz500,
            zz1000=zz1000,
            gz2000=gz2000,
        )
        return abrets


@do_on_dfs
def test_on_index_four(
    df: pd.DataFrame,
    value_weighted: bool = 1,
    group_num: int = 10,
    trade_cost_double_side: float = 0,
    iplot: bool = 1,
    gz2000: bool = 0,
    boxcox: bool = 1,
    opens_average_first_day: bool = 0,
    total_cap: bool = 0,
) -> pd.DataFrame:
    """å¯¹å› å­åŒæ—¶åœ¨æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ã€å›½è¯2000è¿™4ä¸ªæŒ‡æ•°æˆåˆ†è‚¡å†…è¿›è¡Œå¤šç©ºå’Œå¤šå¤´è¶…é¢æµ‹è¯•

    Parameters
    ----------
    df : pd.DataFrame
        å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    value_weighted : bool
        æ˜¯å¦è¿›è¡Œæµé€šå¸‚å€¼åŠ æƒ, by default 0
    group_num : int
        åˆ†ç»„æ•°é‡, by default 10
    trade_cost_double_side : float, optional
        äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
    iplot : bol,optional
        å¤šç©ºå›æµ‹çš„æ—¶å€™ï¼Œæ˜¯å¦ä½¿ç”¨cufflinksç»˜ç”»
    gz2000 : bool, optional
        æ˜¯å¦è¿›è¡Œå›½è¯2000ä¸Šçš„æµ‹è¯•, by default 0
    boxcox : bool, optional
        æ˜¯å¦è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–å¤„ç†, by default 1
    opens_average_first_day : bool, optional
        ä¹°å…¥æ—¶ä½¿ç”¨ç¬¬ä¸€å¤©çš„å¹³å‡ä»·æ ¼, by default 0
    total_cap : bool, optional
        åŠ æƒå’Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–æ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0

    Returns
    -------
    pd.DataFrame
        å¤šå¤´ç»„åœ¨å„ä¸ªæŒ‡æ•°ä¸Šçš„è¶…é¢æ”¶ç›Šåºåˆ—
    """
    fi300 = daily_factor_on300500(df, hs300=1)
    shen = pure_moonnight(
        fi300,
        groups_num=group_num,
        value_weighted=value_weighted,
        trade_cost_double_side=trade_cost_double_side,
        iplot=iplot,
        boxcox=boxcox,
        opens_average_first_day=opens_average_first_day,
        total_cap=total_cap,
    )
    if (
        shen.shen.group_net_values.group1.iloc[-1]
        > shen.shen.group_net_values[f"group{group_num}"].iloc[-1]
    ):
        com300, net300 = make_relative_comments(
            shen.shen.group_rets.group1, hs300=1, show_nets=1
        )
        fi500 = daily_factor_on300500(df, zz500=1)
        shen = pure_moonnight(
            fi500,
            groups_num=group_num,
            value_weighted=value_weighted,
            trade_cost_double_side=trade_cost_double_side,
            iplot=iplot,
            boxcox=boxcox,
            opens_average_first_day=opens_average_first_day,
            total_cap=total_cap,
        )
        com500, net500 = make_relative_comments(
            shen.shen.group_rets.group1, zz500=1, show_nets=1
        )
        fi1000 = daily_factor_on300500(df, zz1000=1)
        shen = pure_moonnight(
            fi1000,
            groups_num=group_num,
            value_weighted=value_weighted,
            trade_cost_double_side=trade_cost_double_side,
            iplot=iplot,
            boxcox=boxcox,
            opens_average_first_day=opens_average_first_day,
            total_cap=total_cap,
        )
        com1000, net1000 = make_relative_comments(
            shen.shen.group_rets.group1, zz1000=1, show_nets=1
        )
        if gz2000:
            fi2000 = daily_factor_on300500(df, gz2000=1)
            shen = pure_moonnight(
                fi2000,
                groups_num=group_num,
                trade_cost_double_side=trade_cost_double_side,
                iplot=iplot,
                boxcox=boxcox,
                opens_average_first_day=opens_average_first_day,
                total_cap=total_cap,
            )
            com2000, net2000 = make_relative_comments(
                shen.shen.group_rets.group1, gz2000=1, show_nets=1
            )
    else:
        com300, net300 = make_relative_comments(
            shen.shen.group_rets[f"group{group_num}"], hs300=1, show_nets=1
        )
        fi500 = daily_factor_on300500(df, zz500=1)
        shen = pure_moonnight(
            fi500,
            groups_num=group_num,
            value_weighted=value_weighted,
            trade_cost_double_side=trade_cost_double_side,
            iplot=iplot,
            boxcox=boxcox,
            opens_average_first_day=opens_average_first_day,
            total_cap=total_cap,
        )
        com500, net500 = make_relative_comments(
            shen.shen.group_rets[f"group{group_num}"], zz500=1, show_nets=1
        )
        fi1000 = daily_factor_on300500(df, zz1000=1)
        shen = pure_moonnight(
            fi1000,
            groups_num=group_num,
            value_weighted=value_weighted,
            trade_cost_double_side=trade_cost_double_side,
            iplot=iplot,
            boxcox=boxcox,
            opens_average_first_day=opens_average_first_day,
            total_cap=total_cap,
        )
        com1000, net1000 = make_relative_comments(
            shen.shen.group_rets[f"group{group_num}"], zz1000=1, show_nets=1
        )
        if gz2000:
            fi2000 = daily_factor_on300500(df, gz2000=1)
            shen = pure_moonnight(
                fi2000,
                groups_num=group_num,
                value_weighted=value_weighted,
                trade_cost_double_side=trade_cost_double_side,
                iplot=iplot,
                boxcox=boxcox,
                opens_average_first_day=opens_average_first_day,
                total_cap=total_cap,
            )
            com2000, net2000 = make_relative_comments(
                shen.shen.group_rets[f"group{group_num}"], gz2000=1, show_nets=1
            )
    com300 = com300.to_frame("300è¶…é¢")
    com500 = com500.to_frame("500è¶…é¢")
    com1000 = com1000.to_frame("1000è¶…é¢")
    if gz2000:
        com2000 = com2000.to_frame("2000è¶…é¢")
        coms = pd.concat([com300, com500, com1000, com2000], axis=1)
    else:
        coms = pd.concat([com300, com500, com1000], axis=1)
    coms = np.around(coms, 3)
    if gz2000:
        nets = pd.concat([net300, net500, net1000, net2000], axis=1)
        nets.columns = ["300è¶…é¢", "500è¶…é¢", "1000è¶…é¢", "2000è¶…é¢"]
    else:
        nets = pd.concat([net300, net500, net1000], axis=1)
        nets.columns = ["300è¶…é¢", "500è¶…é¢", "1000è¶…é¢"]
    coms = coms.reset_index()
    if iplot:
        figs = cf.figures(
            nets,
            [dict(kind="line", y=list(nets.columns))],
            asList=True,
        )
        coms = coms.rename(columns={list(coms)[0]: "ç»©æ•ˆæŒ‡æ ‡"})
        table = FF.create_table(coms.iloc[::-1])
        table.update_yaxes(matches=None)
        figs.append(table)
        figs = [figs[-1]] + figs[:-1]
        figs[1].update_layout(
            legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
        )
        base_layout = cf.tools.get_base_layout(figs)
        if gz2000:
            sp = cf.subplots(
                figs,
                shape=(2, 10),
                base_layout=base_layout,
                vertical_spacing=0.15,
                horizontal_spacing=0.03,
                shared_yaxes=False,
                specs=[
                    [
                        None,
                        {"rowspan": 2, "colspan": 4},
                        None,
                        None,
                        None,
                        {"rowspan": 2, "colspan": 3},
                        None,
                        None,
                        None,
                        None,
                    ],
                    [
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                    ],
                ],
            )
        else:
            sp = cf.subplots(
                figs,
                shape=(2, 10),
                base_layout=base_layout,
                vertical_spacing=0.15,
                horizontal_spacing=0.03,
                shared_yaxes=False,
                specs=[
                    [
                        None,
                        {"rowspan": 2, "colspan": 3},
                        None,
                        None,
                        {"rowspan": 2, "colspan": 3},
                        None,
                        None,
                        None,
                        None,
                        None,
                    ],
                    [
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                    ],
                ],
            )
        sp["layout"].update(showlegend=True)
        cf.iplot(sp)
    else:
        nets.plot()
        plt.show()
        tb = Texttable()
        tb.set_cols_width([8] + [7] + [8] * 2 + [7] * 2 + [8])
        tb.set_cols_dtype(["f"] * 7)
        tb.header(list(coms.T.reset_index().columns))
        tb.add_rows(coms.T.reset_index().to_numpy(), header=True)
        print(tb.draw())


@do_on_dfs
class pure_star(object):
    def __init__(
        self,
        fac: pd.Series,
        code: str = None,
        price_opens: pd.Series = None,
        iplot: bool = 1,
        comments_writer: pd.ExcelWriter = None,
        net_values_writer: pd.ExcelWriter = None,
        sheetname: str = None,
        questdb_host: str = "127.0.0.1",
    ):
        """æ‹©æ—¶å›æµ‹æ¡†æ¶ï¼Œè¾“å…¥ä»“ä½æ¯”ä¾‹æˆ–ä¿¡å·å€¼ï¼Œä¾æ®ä¿¡å·ä¹°å…¥å¯¹åº”çš„è‚¡ç¥¨æˆ–æŒ‡æ•°ï¼Œå¹¶è€ƒå¯Ÿç»å¯¹æ”¶ç›Šã€è¶…é¢æ”¶ç›Šå’ŒåŸºå‡†æ”¶ç›Š
        å›æµ‹æ–¹å¼ä¸ºï¼Œtæ—¥æ”¶ç›˜æ—¶è·å¾—ä¿¡å·ï¼Œt+1æ—¥å¼€ç›˜æ—¶ä»¥å¼€ç›˜ä»·ä¹°å…¥ï¼Œt+2å¼€ç›˜æ—¶ä»¥å¼€ç›˜ä»·å–å‡º

        Parameters
        ----------
        fac : pd.Series
            ä»“ä½æ¯”ä¾‹åºåˆ—ï¼Œæˆ–ä¿¡å·åºåˆ—ï¼Œè¾“å…¥ä¿¡å·åºåˆ—æ—¶å³ä¸º0å’Œ1ï¼Œè¾“å…¥ä»“ä½æ¯”ä¾‹æ—¶ï¼Œå°†æ¯ä¸€æœŸçš„æ”¶ç›ŠæŒ‰ç…§å¯¹åº”æ¯”ä¾‹ç¼©å°
        code : str, optional
            å›æµ‹çš„èµ„äº§ä»£ç ï¼Œå¯ä»¥ä¸ºè‚¡ç¥¨ä»£ç æˆ–åŸºé‡‘ä»£ç , by default None
        price_opens : pd.Series, optional
            èµ„äº§çš„å¼€ç›˜ä»·åºåˆ—, by default None
        iplot : bool, optional
            ä½¿ç”¨cufflinkså‘ˆç°å›æµ‹ç»©æ•ˆå’Œèµ°åŠ¿å›¾, by default 1
        comments_writer : pd.ExcelWriter, optional
            ç»©æ•ˆè¯„ä»·çš„å­˜å‚¨æ–‡ä»¶, by default None
        net_values_writer : pd.ExcelWriter, optional
            å‡€å€¼åºåˆ—çš„å­˜å‚¨æ–‡ä»¶, by default None
        sheetname : str, optional
            å­˜å‚¨æ–‡ä»¶çš„å·¥ä½œè¡¨çš„åå­—, by default None
        questdb_host: str, optional
            questdbçš„hostï¼Œä½¿ç”¨NASæ—¶æ”¹ä¸º'192.168.1.3', by default '127.0.0.1'
        """
        if code is not None:
            x1 = code.split(".")[0]
            x2 = code.split(".")[1]
            if (x1[0] == "0" or x1[:2] == "30") and x2 == "SZ":
                kind = "stock"
            elif x1[0] == "6" and x2 == "SH":
                kind = "stock"
            else:
                kind = "index"
            self.kind = kind
            if kind == "index":
                qdb = Questdb(host=questdb_host)
                price_opens = qdb.get_data(
                    f"select date,num,close from minute_data_{kind} where code='{code}'"
                )
                price_opens = price_opens[price_opens.num == "1"]
                price_opens = price_opens.set_index("date").close
                price_opens.index = pd.to_datetime(price_opens.index, format="%Y%m%d")
            else:
                price_opens = read_daily(open=1)[code]
        price_opens = price_opens[price_opens.index >= fac.index.min()]
        self.price_opens = price_opens
        self.price_rets = price_opens.pct_change()
        self.fac = fac
        self.fac_rets = (self.fac.shift(2) * self.price_rets).dropna()
        self.ab_rets = (self.fac_rets - self.price_rets).dropna()
        self.price_rets = self.price_rets.dropna()
        self.fac_nets = (1 + self.fac_rets).cumprod()
        self.ab_nets = (1 + self.ab_rets).cumprod()
        self.price_nets = (1 + self.price_rets).cumprod()
        self.fac_nets = self.fac_nets / self.fac_nets.iloc[0]
        self.ab_nets = self.ab_nets / self.ab_nets.iloc[0]
        self.price_nets = self.price_nets / self.price_nets.iloc[0]
        self.fac_comments = comments_on_twins(self.fac_nets, self.fac_rets)
        self.ab_comments = comments_on_twins(self.ab_nets, self.ab_rets)
        self.price_comments = comments_on_twins(self.price_nets, self.price_rets)
        self.total_comments = pd.concat(
            [self.fac_comments, self.ab_comments, self.price_comments], axis=1
        )
        self.total_nets = pd.concat(
            [self.fac_nets, self.ab_nets, self.price_nets], axis=1
        )
        self.total_rets = pd.concat(
            [self.fac_rets, self.ab_rets, self.price_rets], axis=1
        )
        self.total_comments.columns = (
            self.total_nets.columns
        ) = self.total_rets.columns = ["å› å­ç»å¯¹", "å› å­è¶…é¢", "ä¹°å…¥æŒæœ‰"]
        self.total_comments = np.around(self.total_comments, 3)
        self.iplot = iplot
        self.plot()
        if comments_writer is None and sheetname is not None:
            from pure_ocean_breeze.state.states import COMMENTS_WRITER

            comments_writer = COMMENTS_WRITER
            self.total_comments.to_excel(comments_writer, sheet_name=sheetname)
        if net_values_writer is None and sheetname is not None:
            from pure_ocean_breeze.state.states import NET_VALUES_WRITER

            net_values_writer = NET_VALUES_WRITER
            self.total_nets.to_excel(net_values_writer, sheet_name=sheetname)

    def plot(self):
        coms = self.total_comments.copy().reset_index()
        if self.iplot:
            figs = cf.figures(
                self.total_nets,
                [dict(kind="line", y=list(self.total_nets.columns))],
                asList=True,
            )
            coms = coms.rename(columns={list(coms)[0]: "ç»©æ•ˆæŒ‡æ ‡"})
            table = FF.create_table(coms.iloc[::-1])
            table.update_yaxes(matches=None)
            figs.append(table)
            figs = [figs[-1]] + figs[:-1]
            figs[1].update_layout(
                legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
            )
            base_layout = cf.tools.get_base_layout(figs)
            sp = cf.subplots(
                figs,
                shape=(2, 10),
                base_layout=base_layout,
                vertical_spacing=0.15,
                horizontal_spacing=0.03,
                shared_yaxes=False,
                specs=[
                    [
                        None,
                        {"rowspan": 2, "colspan": 3},
                        None,
                        None,
                        {"rowspan": 2, "colspan": 6},
                        None,
                        None,
                        None,
                        None,
                        None,
                    ],
                    [
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                    ],
                ],
            )
            sp["layout"].update(showlegend=True)
            cf.iplot(sp)
        else:
            self.total_nets.plot()
            plt.show()
            tb = Texttable()
            tb.set_cols_width([8] + [7] + [8] * 2 + [7] * 2 + [8])
            tb.set_cols_dtype(["f"] * 7)
            tb.header(list(coms.T.reset_index().columns))
            tb.add_rows(coms.T.reset_index().to_numpy(), header=True)
            print(tb.draw())


@do_on_dfs
def get_group(df: pd.DataFrame, group_num: int = 10) -> pd.DataFrame:
    """ä½¿ç”¨groupbyçš„æ–¹æ³•ï¼Œå°†ä¸€ç»„å› å­å€¼æ”¹ä¸ºæˆªé¢ä¸Šçš„åˆ†ç»„å€¼ï¼Œæ­¤æ–¹æ³•ç›¸æ¯”qcutçš„æ–¹æ³•æ›´åŠ ç¨³å¥ï¼Œä½†é€Ÿåº¦æ›´æ…¢ä¸€äº›

    Parameters
    ----------
    df : pd.DataFrame
        å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
    group_num : int, optional
        åˆ†ç»„çš„æ•°é‡, by default 10

    Returns
    -------
    pd.DataFrame
        è½¬åŒ–ä¸ºåˆ†ç»„å€¼åçš„dfï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºåˆ†ç»„å€¼
    """
    a = pure_moon(no_read_indu=1)
    df = df.stack().reset_index()
    df.columns = ["date", "code", "fac"]
    df = a.get_groups(df, group_num).pivot(index="date", columns="code", values="group")
    return df


class pure_linprog(object):
    def __init__(
        self,
        facs: pd.DataFrame,
        total_caps: pd.DataFrame = None,
        indu_dummys: pd.DataFrame = None,
        index_weights_hs300: pd.DataFrame = None,
        index_weights_zz500: pd.DataFrame = None,
        index_weights_zz1000: pd.DataFrame = None,
        opens: pd.DataFrame = None,
        closes: pd.DataFrame = None,
        hs300_closes: pd.DataFrame = None,
        zz500_closes: pd.DataFrame = None,
        zz1000_closes: pd.DataFrame = None,
    ) -> None:
        """çº¿æ€§è§„åˆ’æ±‚è§£ï¼Œç›®æ ‡ä¸ºé¢„æœŸæ”¶ç›Šç‡æœ€å¤§ï¼ˆå³å› å­æ–¹å‘ä¸ºè´Ÿæ—¶ï¼Œç»„åˆå› å­å€¼æœ€å°ï¼‰
        æ¡ä»¶ä¸ºï¼Œä¸¥æ ¼æ§åˆ¶å¸‚å€¼ä¸­æ€§ï¼ˆæ•°æ®ï¼šæ€»å¸‚å€¼çš„å¯¹æ•°ï¼›å«ä¹‰ï¼šç»„åˆåœ¨å¸‚å€¼ä¸Šçš„æš´éœ²ä¸æŒ‡æ•°åœ¨å¸‚å€¼ä¸Šçš„æš´éœ²ç›¸ç­‰ï¼‰
        ä¸¥æ ¼æ§åˆ¶è¡Œä¸šä¸­æ€§ï¼ˆæ•°æ®ï¼šä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸šå“‘å˜é‡ï¼‰ï¼Œä¸ªè‚¡åç¦»åœ¨1%ä»¥å†…ï¼Œæˆåˆ†è‚¡æƒé‡ä¹‹å’Œåœ¨80%ä»¥ä¸Š
        åˆ†åˆ«åœ¨æ²ªæ·±300ã€ä¸­è¯500ã€ä¸­è¯1000ä¸Šä¼˜åŒ–æ±‚è§£

        Parameters
        ----------
        facs : pd.DataFrame
            å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        total_caps : pd.DataFrame, optional
            æ€»å¸‚å€¼æ•°æ®ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºæ€»å¸‚å€¼, by default None
        indu_dummys : pd.DataFrame, optional
            è¡Œä¸šå“‘å˜é‡ï¼ŒåŒ…å«ä¸¤åˆ—åä¸ºdateçš„æ—¶é—´å’Œcodeçš„è‚¡ç¥¨ä»£ç ï¼Œä»¥åŠ30+åˆ—è¡Œä¸šå“‘å˜é‡, by default None
        index_weights_hs300 : pd.DataFrame, optional
            æ²ªæ·±300æŒ‡æ•°æˆåˆ†è‚¡æƒé‡ï¼Œæœˆé¢‘æ•°æ®, by default None
        index_weights_zz500 : pd.DataFrame, optional
            ä¸­è¯500æŒ‡æ•°æˆåˆ†è‚¡æƒé‡ï¼Œæœˆé¢‘æ•°æ®, by default None
        index_weights_zz1000 : pd.DataFrame, optional
            ä¸­è¯1000æŒ‡æ•°æˆåˆ†è‚¡æƒé‡ï¼Œæœˆé¢‘æ•°æ®, by default None
        opens : pd.DataFrame, optional
            æ¯æœˆæœˆåˆå¼€ç›˜ä»·æ•°æ®, by default None
        closes : pd.DataFrame, optional
            æ¯æœˆæœˆæœ«æ”¶ç›˜ä»·æ•°æ®, by default None
        hs300_closes : pd.DataFrame, optional
            æ²ªæ·±300æ¯æœˆæ”¶ç›˜ä»·æ•°æ®, by default None
        zz500_closes : pd.DataFrame, optional
            ä¸­è¯500æ¯æœˆæ”¶ç›˜ä»·æ•°æ®,, by default None
        zz1000_closes : pd.DataFrame, optional
            ä¸­è¯1000æ¯æœˆæ”¶ç›˜ä»·æ•°æ®,, by default None
        """
        self.facs = facs.resample("M").last()
        if total_caps is None:
            total_caps = standardlize(
                np.log(read_daily(total_cap=1).resample("M").last())
            )
        if indu_dummys is None:
            indu_dummys = read_daily(zxindustry_dummy_code=1)
        if index_weights_hs300 is None:
            index_weights_hs300 = read_daily(hs300_member_weight=1)
        if index_weights_zz500 is None:
            index_weights_zz500 = read_daily(zz500_member_weight=1)
        if index_weights_zz1000 is None:
            index_weights_zz1000 = read_daily(zz1000_member_weight=1)
        if opens is None:
            opens = read_daily(open=1).resample("M").first()
        if closes is None:
            closes = read_daily(close=1).resample("M").last()
        if hs300_closes is None:
            hs300_closes = read_index_single("000300.SH").resample("M").last()
        if zz500_closes is None:
            zz500_closes = read_index_single("000905.SH").resample("M").last()
        if zz1000_closes is None:
            zz1000_closes = read_index_single("000852.SH").resample("M").last()
        self.total_caps = total_caps
        self.indu_dummys = indu_dummys
        self.index_weights_hs300 = index_weights_hs300
        self.index_weights_zz500 = index_weights_zz500
        self.index_weights_zz1000 = index_weights_zz1000
        self.hs300_weights = []
        self.zz500_weights = []
        self.zz1000_weights = []
        self.ret_next = closes / opens - 1
        self.ret_hs300 = hs300_closes.pct_change()
        self.ret_zz500 = zz500_closes.pct_change()
        self.ret_zz1000 = zz1000_closes.pct_change()

    def optimize_one_day(
        self,
        fac: pd.DataFrame,
        flow_cap: pd.DataFrame,
        indu_dummy: pd.DataFrame,
        index_weight: pd.DataFrame,
        name: str,
    ) -> pd.DataFrame:
        """ä¼˜åŒ–å•æœŸæ±‚è§£

        Parameters
        ----------
        fac : pd.DataFrame
            å•æœŸå› å­å€¼ï¼Œindexä¸ºcodeï¼Œcolumnsä¸ºdateï¼Œvaluesä¸ºå› å­å€¼
        flow_cap : pd.DataFrame
            æµé€šå¸‚å€¼ï¼Œindexä¸ºcodeï¼Œcolumnsä¸ºdateï¼Œvaluesä¸ºæˆªé¢æ ‡å‡†åŒ–çš„æµé€šå¸‚å€¼
        indu_dummy : pd.DataFrame
            è¡Œä¸šå“‘å˜é‡ï¼Œindexä¸ºcodeï¼Œcolumnsä¸ºè¡Œä¸šä»£ç ï¼Œvaluesä¸ºå“‘å˜é‡
        index_weight : pd.DataFrame
            æŒ‡æ•°æˆåˆ†è‚¡æƒé‡ï¼Œindexä¸ºcodeï¼Œcolumnsä¸ºdateï¼Œvaluesä¸ºæƒé‡

        Returns
        -------
        pd.DataFrame
            å½“æœŸæœ€ä½³æƒé‡
        """
        if fac.shape[0] > 0 and index_weight.shape[1] > 0:
            date = fac.columns.tolist()[0]
            codes = list(
                set(fac.index)
                | set(flow_cap.index)
                | set(indu_dummy.index)
                | set(index_weight.index)
            )
            fac, flow_cap, indu_dummy, index_weight = list(
                map(
                    lambda x: x.reindex(codes).fillna(0).to_numpy(),
                    [fac, flow_cap, indu_dummy, index_weight],
                )
            )
            sign_index_weight = np.sign(index_weight)
            # ä¸ªè‚¡æƒé‡å¤§äºé›¶ã€åç¦»1%
            bounds = list(
                zip(
                    select_max(index_weight - 0.01, 0).flatten(),
                    select_min(index_weight + 0.01, 1).flatten(),
                )
            )
            # å¸‚å€¼ä¸­æ€§+è¡Œä¸šä¸­æ€§+æƒé‡å’Œä¸º1
            huge = np.vstack([flow_cap.T, indu_dummy.T, np.array([1] * len(codes))])
            target = (
                list(flow_cap.T @ index_weight.flatten())
                + list((indu_dummy.T @ index_weight).flatten())
                + [np.sum(index_weight)]
            )
            # å†™çº¿æ€§æ¡ä»¶
            c = fac.T.flatten().tolist()
            a = sign_index_weight.reshape((1, -1)).tolist()
            b = [0.8]
            # ä¼˜åŒ–æ±‚è§£
            res = linprog(c, a, b, huge, target, bounds)
            if res.success:
                return pd.DataFrame({date: res.x.tolist()}, index=codes)
            else:
                # raise NotImplementedError(f"{date}è¿™ä¸€æœŸçš„ä¼˜åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥")
                logger.warning(f"{name}åœ¨{date}è¿™ä¸€æœŸçš„ä¼˜åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥")
                return None
        else:
            return None

    def optimize_many_days(self, startdate: int = STATES["START"]):
        dates = [i for i in self.facs.index if i >= pd.Timestamp(str(startdate))]
        for date in tqdm.auto.tqdm(dates):
            fac = self.facs[self.facs.index == date].T.dropna()
            total_cap = self.total_caps[self.total_caps.index == date].T.dropna()
            indu_dummy = self.indu_dummys[self.indu_dummys.date <= date]
            indu_dummy = (
                indu_dummy[indu_dummy.date == indu_dummy.date.max()]
                .drop(columns=["date"])
                .set_index("code")
            )
            index_weight_hs300 = self.index_weights_hs300[
                self.index_weights_hs300.index == date
            ].T.dropna()
            index_weight_zz500 = self.index_weights_zz500[
                self.index_weights_zz500.index == date
            ].T.dropna()
            index_weight_zz1000 = self.index_weights_zz1000[
                self.index_weights_zz1000.index == date
            ].T.dropna()
            weight_hs300 = self.optimize_one_day(
                fac, total_cap, indu_dummy, index_weight_hs300, "hs300"
            )
            weight_zz500 = self.optimize_one_day(
                fac, total_cap, indu_dummy, index_weight_zz500, "zz500"
            )
            weight_zz1000 = self.optimize_one_day(
                fac, total_cap, indu_dummy, index_weight_zz1000, "zz1000"
            )
            self.hs300_weights.append(weight_hs300)
            self.zz500_weights.append(weight_zz500)
            self.zz1000_weights.append(weight_zz1000)
        self.hs300_weights = pd.concat(self.hs300_weights, axis=1).T
        self.zz500_weights = pd.concat(self.zz500_weights, axis=1).T
        self.zz1000_weights = pd.concat(self.zz1000_weights, axis=1).T

    def make_contrast(self, weight, index, name) -> list[pd.DataFrame]:
        ret = (weight.shift(1) * self.ret_next).sum(axis=1)
        abret = ret - index
        rets = pd.concat([ret, index, abret], axis=1).dropna()
        rets.columns = [f"{name}å¢å¼ºç»„åˆå‡€å€¼", f"{name}æŒ‡æ•°å‡€å€¼", f"{name}å¢å¼ºç»„åˆè¶…é¢å‡€å€¼"]
        rets = (rets + 1).cumprod()
        rets = rets.apply(lambda x: x / x.iloc[0])
        comments = comments_on_twins(rets[f"{name}å¢å¼ºç»„åˆè¶…é¢å‡€å€¼"], abret.dropna())
        return comments, rets

    def run(self, startdate: int = STATES["START"]) -> pd.DataFrame:
        """è¿è¡Œè§„åˆ’æ±‚è§£

        Parameters
        ----------
        startdate : int, optional
            èµ·å§‹æ—¥æœŸ, by default 20130101

        Returns
        -------
        pd.DataFrame
            è¶…é¢ç»©æ•ˆæŒ‡æ ‡
        """
        self.optimize_many_days(startdate=startdate)
        self.hs300_comments, self.hs300_nets = self.make_contrast(
            self.hs300_weights, self.ret_hs300, "æ²ªæ·±300"
        )
        self.zz500_comments, self.zz500_nets = self.make_contrast(
            self.zz500_weights, self.ret_zz500, "ä¸­è¯500"
        )
        self.zz1000_comments, self.zz1000_nets = self.make_contrast(
            self.zz1000_weights, self.ret_zz1000, "ä¸­è¯1000"
        )

        figs = cf.figures(
            pd.concat([self.hs300_nets, self.zz500_nets, self.zz1000_nets]),
            [
                dict(kind="line", y=list(self.hs300_nets.columns)),
                dict(kind="line", y=list(self.zz500_nets.columns)),
                dict(kind="line", y=list(self.zz1000_nets.columns)),
            ],
            asList=True,
        )
        base_layout = cf.tools.get_base_layout(figs)

        sp = cf.subplots(
            figs,
            shape=(1, 3),
            base_layout=base_layout,
            vertical_spacing=0.15,
            horizontal_spacing=0.03,
            shared_yaxes=False,
            subplot_titles=["æ²ªæ·±300å¢å¼º", "ä¸­è¯500å¢å¼º", "ä¸­è¯1000å¢å¼º"],
        )
        sp["layout"].update(showlegend=True)
        cf.iplot(sp)

        self.comments = pd.concat(
            [self.hs300_comments, self.zz500_comments, self.zz1000_comments], axis=1
        )
        self.comments.columns = ["æ²ªæ·±300è¶…é¢", "ä¸­è¯500è¶…é¢", "ä¸­è¯1000è¶…é¢"]

        from pure_ocean_breeze.state.states import COMMENTS_WRITER, NET_VALUES_WRITER

        comments_writer = COMMENTS_WRITER
        net_values_writer = NET_VALUES_WRITER
        if comments_writer is not None:
            self.hs300_comments.to_excel(comments_writer, sheet_name="æ²ªæ·±300ç»„åˆä¼˜åŒ–è¶…é¢ç»©æ•ˆ")
            self.zz500_comments.to_excel(comments_writer, sheet_name="ä¸­è¯500ç»„åˆä¼˜åŒ–è¶…é¢ç»©æ•ˆ")
            self.zz1000_comments.to_excel(comments_writer, sheet_name="ä¸­è¯1000ç»„åˆä¼˜åŒ–è¶…é¢ç»©æ•ˆ")
        if net_values_writer is not None:
            self.hs300_nets.to_excel(net_values_writer, sheet_name="æ²ªæ·±300ç»„åˆä¼˜åŒ–å‡€å€¼")
            self.zz500_nets.to_excel(net_values_writer, sheet_name="ä¸­è¯500ç»„åˆä¼˜åŒ–å‡€å€¼")
            self.zz1000_nets.to_excel(net_values_writer, sheet_name="ä¸­è¯1000ç»„åˆä¼˜åŒ–å‡€å€¼")

        return self.comments.T


def symmetrically_orthogonalize(dfs: list[pd.DataFrame]) -> list[pd.DataFrame]:
    """å¯¹å¤šä¸ªå› å­åšå¯¹ç§°æ­£äº¤ï¼Œæ¯ä¸ªå› å­å¾—åˆ°æ­£äº¤å…¶ä»–å› å­åçš„ç»“æœ

    Parameters
    ----------
    dfs : list[pd.DataFrame]
        å¤šä¸ªè¦åšæ­£äº¤çš„å› å­ï¼Œæ¯ä¸ªdféƒ½æ˜¯indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼çš„df

    Returns
    -------
    list[pd.DataFrame]
        å¯¹ç§°æ­£äº¤åçš„å„ä¸ªå› å­
    """

    def sing(dfs: list[pd.DataFrame], date: pd.Timestamp):
        dds = []
        for num, i in enumerate(dfs):
            i = i[i.index == date]
            i.index = [f"fac{num}"]
            i = i.T
            dds.append(i)
        dds = pd.concat(dds, axis=1)
        cov = dds.cov()
        d, u = np.linalg.eig(cov)
        d = np.diag(d ** (-0.5))
        new_facs = pd.DataFrame(
            np.dot(dds, np.dot(np.dot(u, d), u.T)), columns=dds.columns, index=dds.index
        )
        new_facs = new_facs.stack().reset_index()
        new_facs.columns = ["code", "fac_number", "fac"]
        new_facs = new_facs.assign(date=date)
        dds = []
        for num, i in enumerate(dfs):
            i = new_facs[new_facs.fac_number == f"fac{num}"]
            i = i.pivot(index="date", columns="code", values="fac")
            dds.append(i)
        return dds

    dfs = [standardlize(i) for i in dfs]
    date_first = max([i.index.min() for i in dfs])
    date_last = min([i.index.max() for i in dfs])
    dfs = [i[(i.index >= date_first) & (i.index <= date_last)] for i in dfs]
    fac_num = len(dfs)
    ddss = [[] for i in range(fac_num)]
    for date in tqdm.auto.tqdm(dfs[0].index):
        dds = sing(dfs, date)
        for num, i in enumerate(dds):
            ddss[num].append(i)
    ds = []
    for i in tqdm.auto.tqdm(ddss):
        ds.append(pd.concat(i))
    return ds


def icir_weight(
    facs: list[pd.DataFrame],
    backsee: int = 6,
    boxcox: bool = 0,
    rank_corr: bool = 0,
    only_ic: bool = 0,
) -> pd.DataFrame:
    """ä½¿ç”¨iciræ»šåŠ¨åŠ æƒçš„æ–¹å¼ï¼ŒåŠ æƒåˆæˆå‡ ä¸ªå› å­

    Parameters
    ----------
    facs : list[pd.DataFrame]
        è¦åˆæˆçš„è‹¥å¹²å› å­ï¼Œæ¯ä¸ªdféƒ½æ˜¯indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼çš„df
    backsee : int, optional
        ç”¨æ¥è®¡ç®—icirçš„è¿‡å»æœŸæ•°, by default 6
    boxcox : bool, optional
        æ˜¯å¦å¯¹å› å­è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 0
    rank_corr : bool, optional
        æ˜¯å¦è®¡ç®—rankicir, by default 0
    only_ic : bool, optional
        æ˜¯å¦åªè®¡ç®—ICæˆ–Rank IC, by default 0

    Returns
    -------
    pd.DataFrame
        åˆæˆåçš„å› å­

    Raises
    ------
    ValueError
        å› å­æœŸæ•°å°‘äºå›çœ‹æœŸæ•°æ—¶å°†æŠ¥é”™
    """
    date_first_max = max([i.index[0] for i in facs])
    facs = [i[i.index >= date_first_max] for i in facs]
    date_last_min = min([i.index[-1] for i in facs])
    facs = [i[i.index <= date_last_min] for i in facs]
    facs = [i.shift(1) for i in facs]
    ret = read_daily(
        close=1, start=datetime.datetime.strftime(date_first_max, "%Y%m%d")
    )
    ret = ret / ret.shift(20) - 1
    if boxcox:
        facs = [decap_industry(i) for i in facs]
    facs = [((i.T - i.T.mean()) / i.T.std()).T for i in facs]
    dates = list(facs[0].index)
    fis = []
    for num, date in tqdm.auto.tqdm(list(enumerate(dates))):
        if num < backsee:
            ...
        else:
            nears = [i.iloc[num - backsee : num, :] for i in facs]
            targets = [i[i.index == date] for i in facs]
            if rank_corr:
                weights = [
                    show_corr(
                        i, ret[ret.index.isin(i.index)], plt_plot=0, show_series=1
                    )
                    for i in nears
                ]
            else:
                weights = [
                    show_corr(
                        i,
                        ret[ret.index.isin(i.index)],
                        plt_plot=0,
                        show_series=1,
                        method="pearson",
                    )
                    for i in nears
                ]
            if only_ic:
                weights = [i.mean() for i in weights]
            else:
                weights = [i.mean() / i.std() for i in weights]
            fi = sum([i * j for i, j in zip(weights, targets)])
            fis.append(fi)
    if len(fis) > 0:
        return pd.concat(fis).shift(-1)
    else:
        raise ValueError("è¾“å…¥çš„å› å­å€¼é•¿åº¦ä¸å¤ªå¤Ÿå§ï¼Ÿ")


def scipy_weight(
    facs: list[pd.DataFrame],
    backsee: int = 6,
    boxcox: bool = 0,
    rank_corr: bool = 0,
    only_ic: bool = 0,
    upper_bound: float = None,
    lower_bound: float = 0,
) -> pd.DataFrame:
    """ä½¿ç”¨scipyçš„minimizeä¼˜åŒ–æ±‚è§£çš„æ–¹å¼ï¼Œå¯»æ‰¾æœ€ä¼˜çš„å› å­åˆæˆæƒé‡ï¼Œé»˜è®¤ä¼˜åŒ–æ¡ä»¶ä¸ºæœ€å¤§ICIR

    Parameters
    ----------
    facs : list[pd.DataFrame]
        è¦åˆæˆçš„å› å­ï¼Œæ¯ä¸ªdféƒ½æ˜¯indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼çš„df
    backsee : int, optional
        ç”¨æ¥è®¡ç®—icirçš„è¿‡å»æœŸæ•°, by default 6
    boxcox : bool, optional
        æ˜¯å¦å¯¹å› å­è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 0
    rank_corr : bool, optional
        æ˜¯å¦è®¡ç®—rankicir, by default 0
    only_ic : bool, optional
        æ˜¯å¦åªè®¡ç®—ICæˆ–Rank IC, by default 0
    upper_bound : float, optional
        æ¯ä¸ªå› å­çš„æƒé‡ä¸Šé™ï¼Œå¦‚æœä¸æŒ‡å®šï¼Œåˆ™ä¸ºæ¯ä¸ªå› å­å¹³å‡æƒé‡çš„2å€ï¼Œå³2é™¤ä»¥å› å­æ•°é‡, by default None
    lower_bound : float, optional
        æ¯ä¸ªå› å­çš„æƒé‡ä¸‹é™, by default 0

    Returns
    -------
    pd.DataFrame
        åˆæˆåçš„å› å­
    """
    date_first_max = max([i.index[0] for i in facs])
    facs = [i[i.index >= date_first_max] for i in facs]
    date_last_min = min([i.index[-1] for i in facs])
    facs = [i[i.index <= date_last_min] for i in facs]
    facs = [i.shift(1) for i in facs]
    ret = read_daily(
        close=1, start=datetime.datetime.strftime(date_first_max, "%Y%m%d")
    )
    ret = ret / ret.shift(20) - 1
    if boxcox:
        facs = [decap_industry(i) for i in facs]
    facs = [((i.T - i.T.mean()) / i.T.std()).T for i in facs]
    if upper_bound is None:
        upper_bound = 2 / len(facs)
    dates = list(facs[0].index)
    fis = []
    for num, date in tqdm.auto.tqdm(list(enumerate(dates))):
        if num <= backsee:
            ...
        else:
            nears = [i.iloc[num - backsee : num, :] for i in facs]
            targets = [i[i.index == date] for i in facs]
            if rank_corr:
                weights = [
                    show_corr(
                        i, ret[ret.index.isin(i.index)], plt_plot=0, show_series=1
                    )
                    for i in nears
                ]
            else:
                weights = [
                    show_corr(
                        i,
                        ret[ret.index.isin(i.index)],
                        plt_plot=0,
                        show_series=1,
                        method="pearson",
                    )
                    for i in nears
                ]
            if only_ic:
                weights = [i.mean() for i in weights]
            else:
                weights = [i.mean() / i.std() for i in weights]
            weights = pd.concat(weights, axis=1)

            def func(x):
                w = np.array(x).reshape((-1, 1))
                y = weights @ w
                return np.mean(y) / np.std(y)

            cons = {"type": "eq", "fun": lambda x: np.sum(x) - 1}
            res = minimize(
                func,
                np.random.rand(weights.shape[1], 1),
                constraints=cons,
                bounds=[(lower_bound, upper_bound)] * weights.shape[1],
            )
            xs = res.x.tolist()
            fac = sum([i * j for i, j in zip(xs, targets)])
            fis.append(fac)
    return pd.concat(fis).shift(-1)


# æ­¤å¤„æœªå®Œæˆï¼Œå¾…æ”¹å†™
class pure_fall_second(object):
    """å¯¹å•åªè‚¡ç¥¨å•æ—¥è¿›è¡Œæ“ä½œ"""

    def __init__(
        self,
        factor_file: str,
        project: str = None,
        startdate: int = None,
        enddate: int = None,
        questdb_host: str = "127.0.0.1",
        ignore_history_in_questdb: bool = 0,
        groupby_target: list = ["date", "code"],
    ) -> None:
        """åŸºäºclickhouseçš„åˆ†é’Ÿæ•°æ®ï¼Œè®¡ç®—å› å­å€¼ï¼Œæ¯å¤©çš„å› å­å€¼åªç”¨åˆ°å½“æ—¥çš„æ•°æ®

        Parameters
        ----------
        factor_file : str
            ç”¨äºä¿å­˜å› å­å€¼çš„æ–‡ä»¶åï¼Œéœ€ä¸ºparquetæ–‡ä»¶ï¼Œä»¥'.parquet'ç»“å°¾
        project : str, optional
            è¯¥å› å­æ‰€å±é¡¹ç›®ï¼Œå³å­æ–‡ä»¶å¤¹åç§°, by default None
        startdate : int, optional
            èµ·å§‹æ—¶é—´ï¼Œå½¢å¦‚20121231ï¼Œä¸ºå¼€åŒºé—´, by default None
        enddate : int, optional
            æˆªæ­¢æ—¶é—´ï¼Œå½¢å¦‚20220814ï¼Œä¸ºé—­åŒºé—´ï¼Œä¸ºç©ºåˆ™è®¡ç®—åˆ°æœ€è¿‘æ•°æ®, by default None
        questdb_host: str, optional
            questdbçš„hostï¼Œä½¿ç”¨NASæ—¶æ”¹ä¸º'192.168.1.3', by default '127.0.0.1'
        ignore_history_in_questdb : bool, optional
            æ‰“æ–­åé‡æ–°ä»å¤´è®¡ç®—ï¼Œæ¸…é™¤åœ¨questdbä¸­çš„è®°å½•
        groupby_target: list, optional
            groupbyè®¡ç®—æ—¶ï¼Œåˆ†ç»„çš„ä¾æ®ï¼Œä½¿ç”¨æ­¤å‚æ•°æ—¶ï¼Œè‡ªå®šä¹‰å‡½æ•°çš„éƒ¨åˆ†ï¼Œå¦‚æœæŒ‡å®šæŒ‰ç…§['date']åˆ†ç»„groupbyè®¡ç®—ï¼Œ
            åˆ™è¿”å›æ—¶ï¼Œåº”å½“è¿”å›ä¸€ä¸ªä¸¤åˆ—çš„dataframeï¼Œç¬¬ä¸€åˆ—ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç¬¬äºŒåˆ—ä¸ºä¸ºå› å­å€¼, by default ['date','code']
        """
        homeplace = HomePlace()
        self.groupby_target = groupby_target
        self.chc = ClickHouseClient("second_data")
        # å°†è®¡ç®—åˆ°ä¸€åŠçš„å› å­ï¼Œå­˜å…¥questdbä¸­ï¼Œé¿å…ä¸­é€”è¢«æ‰“æ–­åé‡æ–°è®¡ç®—ï¼Œè¡¨åå³ä¸ºå› å­æ–‡ä»¶åçš„æ±‰è¯­æ‹¼éŸ³
        pinyin = Pinyin()
        self.factor_file_pinyin = pinyin.get_pinyin(
            factor_file.replace(".parquet", ""), ""
        )
        self.factor_steps = Questdb(host=questdb_host)
        if project is not None:
            if not os.path.exists(homeplace.factor_data_file + project):
                os.makedirs(homeplace.factor_data_file + project)
            else:
                logger.info(f"å½“å‰æ­£åœ¨{project}é¡¹ç›®ä¸­â€¦â€¦")
        else:
            logger.warning("å½“å‰å› å­ä¸å±äºä»»ä½•é¡¹ç›®ï¼Œè¿™å°†é€ æˆå› å­æ•°æ®æ–‡ä»¶å¤¹çš„æ··ä¹±ï¼Œä¸ä¾¿äºç®¡ç†ï¼Œå»ºè®®æŒ‡å®šä¸€ä¸ªé¡¹ç›®åç§°")
        # å®Œæ•´çš„å› å­æ–‡ä»¶è·¯å¾„
        if project is not None:
            factor_file = homeplace.factor_data_file + project + "/" + factor_file
        else:
            factor_file = homeplace.factor_data_file + factor_file
        self.factor_file = factor_file
        # è¯»å…¥ä¹‹å‰çš„å› å­
        if os.path.exists(factor_file):
            factor_old = drop_duplicates_index(pd.read_parquet(self.factor_file))
            self.factor_old = factor_old
            # å·²ç»ç®—å¥½çš„æ—¥å­
            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
            self.dates_old = dates_old
        elif (not ignore_history_in_questdb) and self.factor_file_pinyin in list(
            self.factor_steps.get_data("show tables").table
        ):
            logger.info(
                f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œç°åœ¨å°†è¯»å–ä¸Šæ¬¡çš„æ•°æ®ï¼Œç»§ç»­è®¡ç®—"
            )
            factor_old = self.factor_steps.get_data_with_tuple(
                f"select * from '{self.factor_file_pinyin}'"
            ).drop_duplicates(subset=["date", "code"])
            factor_old = factor_old.pivot(index="date", columns="code", values="fac")
            factor_old = factor_old.sort_index()
            self.factor_old = factor_old
            # å·²ç»ç®—å¥½çš„æ—¥å­
            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
            self.dates_old = dates_old
        elif ignore_history_in_questdb and self.factor_file_pinyin in list(
            self.factor_steps.get_data("show tables").table
        ):
            logger.info(
                f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œä½†æ‚¨é€‰æ‹©é‡æ–°è®¡ç®—ï¼Œæ‰€ä»¥æ­£åœ¨åˆ é™¤åŸæ¥çš„æ•°æ®ï¼Œä»å¤´è®¡ç®—"
            )
            factor_old = self.factor_steps.do_order(
                f"drop table '{self.factor_file_pinyin}'"
            )
            self.factor_old = None
            self.dates_old = []
            logger.info("åˆ é™¤å®Œæ¯•ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
        else:
            self.factor_old = None
            self.dates_old = []
            logger.info("è¿™ä¸ªå› å­ä»¥å‰æ²¡æœ‰ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
        # è¯»å–å½“å‰æ‰€æœ‰çš„æ—¥å­
        dates_all = self.chc.show_all_dates(f"second_data_stock_10s")
        dates_all = [int(i) for i in dates_all]
        if startdate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i >= startdate]
        if enddate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i <= enddate]
        self.dates_all = dates_all
        # éœ€è¦æ–°è¡¥å……çš„æ—¥å­
        self.dates_new = sorted([i for i in dates_all if i not in self.dates_old])
        if len(self.dates_new) == 0:
            ...
        elif len(self.dates_new) == 1:
            self.dates_new_intervals = [[pd.Timestamp(str(self.dates_new[0]))]]
            print(f"åªç¼ºä¸€å¤©{self.dates_new[0]}")
        else:
            dates = [pd.Timestamp(str(i)) for i in self.dates_new]
            intervals = [[]] * len(dates)
            interbee = 0
            intervals[0] = intervals[0] + [dates[0]]
            for i in range(len(dates) - 1):
                val1 = dates[i]
                val2 = dates[i + 1]
                if val2 - val1 < pd.Timedelta(days=30):
                    ...
                else:
                    interbee = interbee + 1
                intervals[interbee] = intervals[interbee] + [val2]
            intervals = [i for i in intervals if len(i) > 0]
            print(f"å…±{len(intervals)}ä¸ªæ—¶é—´åŒºé—´ï¼Œåˆ†åˆ«æ˜¯")
            for date in intervals:
                print(f"ä»{date[0]}åˆ°{date[-1]}")
            self.dates_new_intervals = intervals
        self.factor_new = []

    def __call__(self) -> pd.DataFrame:
        """è·å¾—ç»è¿ç®—äº§ç”Ÿçš„å› å­

        Returns
        -------
        `pd.DataFrame`
            ç»è¿ç®—äº§ç”Ÿçš„å› å­å€¼
        """
        return self.factor.copy()

    def forward_dates(self, dates, many_days):
        dates_index = [self.dates_all.index(i) for i in dates]

        def value(x, a):
            if x >= 0:
                return a[x]
            else:
                return None

        return [value(i - many_days, self.dates_all) for i in dates_index]

    def select_one_calculate(
        self,
        date: pd.Timestamp,
        func: Callable,
        fields: str = "*",
    ) -> None:
        the_func = partial(func)
        if not isinstance(date, int):
            date = int(datetime.datetime.strftime(date, "%Y%m%d"))
        # å¼€å§‹è®¡ç®—å› å­å€¼

        sql_order = f"select {fields} from second_data.second_data_stock_10s where toYYYYMMDD(date)=date order by code,date"
        df = self.chc.get_data(sql_order)
        df = ((df.set_index(["code", "date"])) / 100).reset_index()
        df = df.groupby(self.groupby_target).apply(the_func)
        if self.groupby_target == ["date", "code"]:
            df = df.to_frame("fac").reset_index()
            df.columns = ["date", "code", "fac"]
        else:
            df = df.reset_index()
        if (df is not None) and (df.shape[0] > 0):
            df = df.pivot(columns="code", index="date", values="fac")
            df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
            to_save = df.stack().reset_index()
            to_save.columns = ["date", "code", "fac"]
            self.factor_steps.write_via_df(
                to_save, self.factor_file_pinyin, tuple_col="fac"
            )
            return df

    def select_many_calculate(
        self,
        dates: List[pd.Timestamp],
        func: Callable,
        fields: str = "*",
        chunksize: int = 10,
        many_days: int = 1,
        n_jobs: int = 1,
    ) -> None:
        the_func = partial(func)
        factor_new = []
        dates = [int(datetime.datetime.strftime(i, "%Y%m%d")) for i in dates]
        if many_days == 1:
            # å°†éœ€è¦æ›´æ–°çš„æ—¥å­åˆ†å—ï¼Œæ¯200å¤©ä¸€ç»„ï¼Œä¸€èµ·è¿ç®—
            dates_new_len = len(dates)
            cut_points = list(range(0, dates_new_len, chunksize)) + [dates_new_len - 1]
            if cut_points[-1] == cut_points[-2]:
                cut_points = cut_points[:-1]
            cuts = tuple(zip(cut_points[:-many_days], cut_points[many_days:]))
            df_first = self.select_one_calculate(
                date=dates[0],
                func=func,
                fields=fields,
            )
            factor_new.append(df_first)

            def cal_one(date1, date2):
                if self.clickhouse == 1:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{dates[date1] * 100} and date<={dates[date2] * 100} order by code,date,num"
                else:
                    sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{dates[date1]} and cast(date as int)<={dates[date2]} order by code,date,num"

                df = self.chc.get_data(sql_order)
                if self.clickhouse == 1:
                    df = ((df.set_index("code")) / 100).reset_index()
                else:
                    df.num = df.num.astype(int)
                    df.date = df.date.astype(int)
                    df = df.sort_values(["date", "num"])
                df = df.groupby(self.groupby_target).apply(the_func)
                if self.groupby_target == ["date", "code"]:
                    df = df.to_frame("fac").reset_index()
                    df.columns = ["date", "code", "fac"]
                else:
                    df = df.reset_index()
                df = df.pivot(columns="code", index="date", values="fac")
                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                to_save = df.stack().reset_index()
                to_save.columns = ["date", "code", "fac"]
                self.factor_steps.write_via_df(
                    to_save, self.factor_file_pinyin, tuple_col="fac"
                )
                return df

            if n_jobs > 1:
                with concurrent.futures.ThreadPoolExecutor(
                    max_workers=n_jobs
                ) as executor:
                    factor_new_more = list(
                        tqdm.auto.tqdm(executor.map(cal_one, cuts), total=len(cuts))
                    )
                factor_new = factor_new + factor_new_more
            else:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                for date1, date2 in tqdm.auto.tqdm(cuts, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
                    df = cal_one(date1, date2)
                    factor_new.append(df)
        else:

            def cal_two(date1, date2):
                if date1 is not None:
                    if self.clickhouse == 1:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{date1*100} and date<={date2*100} order by code,date,num"
                    else:
                        sql_order = f"select {fields} from minute_data_{self.kind} where cast(date as int)>{date1} and cast(date as int)<={date2} order by code,date,num"

                    df = self.chc.get_data(sql_order)
                    if self.clickhouse == 1:
                        df = ((df.set_index("code")) / 100).reset_index()
                    else:
                        df.num = df.num.astype(int)
                        df.date = df.date.astype(int)
                        df = df.sort_values(["date", "num"])
                    if self.groupby_target == [
                        "date",
                        "code",
                    ] or self.groupby_target == ["code"]:
                        df = df.groupby(["code"]).apply(the_func).reset_index()
                    else:
                        df = the_func(df)
                    df = df.assign(date=date2)
                    df.columns = ["code", "fac", "date"]
                    df = df.pivot(columns="code", index="date", values="fac")
                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                    to_save = df.stack().reset_index()
                    to_save.columns = ["date", "code", "fac"]
                    self.factor_steps.write_via_df(
                        to_save, self.factor_file_pinyin, tuple_col="fac"
                    )
                    return df

            pairs = self.forward_dates(dates, many_days=many_days)
            cuts2 = tuple(zip(pairs, dates))
            if n_jobs > 1:
                with concurrent.futures.ThreadPoolExecutor(
                    max_workers=n_jobs
                ) as executor:
                    factor_new_more = list(
                        tqdm.auto.tqdm(executor.map(cal_two, cuts2), total=len(cuts2))
                    )
                factor_new = factor_new + factor_new_more
            else:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                for date1, date2 in tqdm.auto.tqdm(cuts2, desc="çŸ¥ä¸å¯ä¹éª¤å¾—ï¼Œæ‰˜é—å“äºæ‚²é£ã€‚"):
                    df = cal_two(date1, date2)
                    factor_new.append(df)

        if len(factor_new) > 0:
            factor_new = pd.concat(factor_new)
            return factor_new
        else:
            return None

    def select_any_calculate(
        self,
        dates: List[pd.Timestamp],
        func: Callable,
        fields: str = "*",
        chunksize: int = 10,
        show_time: bool = 0,
        many_days: int = 1,
        n_jobs: int = 1,
    ) -> None:
        if len(dates) == 1 and many_days == 1:
            res = self.select_one_calculate(
                dates[0],
                func=func,
                fields=fields,
                show_time=show_time,
            )
        else:
            res = self.select_many_calculate(
                dates=dates,
                func=func,
                fields=fields,
                chunksize=chunksize,
                show_time=show_time,
                many_days=many_days,
                n_jobs=n_jobs,
            )
        if res is not None:
            self.factor_new.append(res)
        return res

    @staticmethod
    def for_cross_via_str(func):
        """è¿”å›å€¼ä¸ºä¸¤å±‚çš„listï¼Œæ¯ä¸€ä¸ªé‡Œå±‚çš„å°listä¸ºå•ä¸ªè‚¡ç¥¨åœ¨è¿™ä¸€å¤©çš„è¿”å›å€¼
        ä¾‹å¦‚
        ```python
        return [[0.11,0.24,0.55],[2.59,1.99,0.43],[1.32,8.88,7.77]â€¦â€¦]
        ```
        ä¸Šä¾‹ä¸­ï¼Œæ¯ä¸ªè‚¡ç¥¨ä¸€å¤©è¿”å›ä¸‰ä¸ªå› å­å€¼ï¼Œé‡Œå±‚çš„listæŒ‰ç…§è‚¡ç¥¨ä»£ç é¡ºåºæ’åˆ—"""

        def full_run(df, *args, **kwargs):
            codes = sorted(list(set(df.code)))
            res = func(df, *args, **kwargs)
            if isinstance(res[0], list):
                kind = 1
                res = [",".join(i) for i in res]
            else:
                kind = 0
            df = pd.DataFrame({"code": codes, "fac": res})
            if kind:
                df.fac = df.fac.apply(lambda x: [float(i) for i in x.split(",")])
            return df

        return full_run

    @staticmethod
    def for_cross_via_zip(func):
        """è¿”å›å€¼ä¸ºå¤šä¸ªpd.Seriesï¼Œæ¯ä¸ªpd.Seriesçš„indexä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå•ä¸ªå› å­å€¼
        ä¾‹å¦‚
        ```python
        return (
                    pd.Series([1.54,8.77,9.99â€¦â€¦],index=['000001.SZ','000002.SZ','000004.SZ'â€¦â€¦]),
                    pd.Series([3.54,6.98,9.01â€¦â€¦],index=['000001.SZ','000002.SZ','000004.SZ'â€¦â€¦]),
                )
        ```
        ä¸Šä¾‹ä¸­ï¼Œæ¯ä¸ªè‚¡ç¥¨ä¸€å¤©è¿”å›ä¸¤ä¸ªå› å­å€¼ï¼Œæ¯ä¸ªpd.Serieså¯¹åº”ä¸€ä¸ªå› å­å€¼
        """

        def full_run(df, *args, **kwargs):
            res = func(df, *args, **kwargs)
            if isinstance(res, pd.Series):
                res = res.reset_index()
                res.columns = ["code", "fac"]
                return res
            elif isinstance(res, pd.DataFrame):
                res.columns = [f"fac{i}" for i in range(len(res.columns))]
                res = res.assign(fac=list(zip(*[res[i] for i in list(res.columns)])))
                res = res[["fac"]].reset_index()
                res.columns = ["code", "fac"]
                return res
            elif res is None:
                ...
            else:
                res = pd.concat(res, axis=1)
                res.columns = [f"fac{i}" for i in range(len(res.columns))]
                res = res.assign(fac=list(zip(*[res[i] for i in list(res.columns)])))
                res = res[["fac"]].reset_index()
                res.columns = ["code", "fac"]
                return res

        return full_run

    @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½ğŸˆ")
    def get_daily_factors(
        self,
        func: Callable,
        fields: str = "*",
        chunksize: int = 10,
        show_time: bool = 0,
        many_days: int = 1,
        n_jobs: int = 1,
    ) -> None:
        """æ¯æ¬¡æŠ½å–chunksizeå¤©çš„æˆªé¢ä¸Šå…¨éƒ¨è‚¡ç¥¨çš„åˆ†é’Ÿæ•°æ®
        å¯¹æ¯å¤©çš„è‚¡ç¥¨çš„æ•°æ®è®¡ç®—å› å­å€¼

        Parameters
        ----------
        func : Callable
            ç”¨äºè®¡ç®—å› å­å€¼çš„å‡½æ•°
        fields : str, optional
            è‚¡ç¥¨æ•°æ®æ¶‰åŠåˆ°å“ªäº›å­—æ®µï¼Œæ’é™¤ä¸å¿…è¦çš„å­—æ®µï¼Œå¯ä»¥èŠ‚çº¦è¯»å–æ•°æ®çš„æ—¶é—´ï¼Œå½¢å¦‚'date,code,num,close,amount,open'
            æå–å‡ºçš„æ•°æ®ï¼Œè‡ªåŠ¨æŒ‰ç…§code,date,numæ’åºï¼Œå› æ­¤code,date,numæ˜¯å¿…ä¸å¯å°‘çš„å­—æ®µ, by default "*"
        chunksize : int, optional
            æ¯æ¬¡è¯»å–çš„æˆªé¢ä¸Šçš„å¤©æ•°, by default 10
        show_time : bool, optional
            å±•ç¤ºæ¯æ¬¡è¯»å–æ•°æ®æ‰€éœ€è¦çš„æ—¶é—´, by default 0
        many_days : int, optional
            è®¡ç®—æŸå¤©çš„å› å­å€¼æ—¶ï¼Œéœ€è¦ä½¿ç”¨ä¹‹å‰å¤šå°‘å¤©çš„æ•°æ®
        n_jobs : int, optional
            å¹¶è¡Œæ•°é‡ï¼Œä¸å»ºè®®è®¾ç½®ä¸ºå¤§äº2çš„æ•°ï¼Œæ­¤å¤–å½“æ­¤å‚æ•°å¤§äº1æ—¶ï¼Œè¯·ä½¿ç”¨questdbæ•°æ®åº“æ¥è¯»å–åˆ†é’Ÿæ•°æ®, by default 1
        """
        if len(self.dates_new) > 0:
            for interval in self.dates_new_intervals:
                df = self.select_any_calculate(
                    dates=interval,
                    func=func,
                    fields=fields,
                    chunksize=chunksize,
                    show_time=show_time,
                    many_days=many_days,
                    n_jobs=n_jobs,
                )
            self.factor_new = pd.concat(self.factor_new)
            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
            self.factor = pd.concat([self.factor_old, self.factor_new]).sort_index()
            self.factor = drop_duplicates_index(self.factor.dropna(how="all"))
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            # å­˜å…¥æœ¬åœ°
            self.factor.to_parquet(self.factor_file)
            logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
            # åˆ é™¤å­˜å‚¨åœ¨questdbçš„ä¸­é€”å¤‡ä»½æ•°æ®
            try:
                self.factor_steps.do_order(f"drop table '{self.factor_file_pinyin}'")
                logger.info("å¤‡ä»½åœ¨questdbçš„è¡¨æ ¼å·²åˆ é™¤")
            except Exception:
                logger.warning("åˆ é™¤questdbä¸­è¡¨æ ¼æ—¶ï¼Œå­˜åœ¨æŸä¸ªæœªçŸ¥é”™è¯¯ï¼Œè¯·å½“å¿ƒ")

        else:
            self.factor = drop_duplicates_index(self.factor_old)
            # å­˜å…¥æœ¬åœ°
            self.factor.to_parquet(self.factor_file)
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            logger.info(f"å½“å‰æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼å·²ç»æ˜¯æœ€æ–°çš„äº†")

    def drop_table(self):
        """ç›´æ¥åˆ é™¤å­˜å‚¨åœ¨questdbä¸­çš„æš‚å­˜æ•°æ®"""
        try:
            self.factor_steps.do_order(f"drop table '{self.factor_file_pinyin}'")
            logger.success(f"æš‚å­˜åœ¨questdbä¸­çš„æ•°æ®è¡¨æ ¼'{self.factor_file_pinyin}'å·²ç»åˆ é™¤")
        except Exception:
            logger.warning(f"æ‚¨è¦åˆ é™¤çš„è¡¨æ ¼'{self.factor_file_pinyin}'å·²ç»ä¸å­˜åœ¨äº†ï¼Œè¯·æ£€æŸ¥")


class pure_fall_nature:
    def __init__(
        self,
        factor_file: str,
        project: str = None,
        startdate: int = None,
        enddate: int = None,
        questdb_host: str = "127.0.0.1",
        ignore_history_in_questdb: bool = 0,
        groupby_code: bool = 1,
    ) -> None:
        """åŸºäºè‚¡ç¥¨é€ç¬”æ•°æ®ï¼Œè®¡ç®—å› å­å€¼ï¼Œæ¯å¤©çš„å› å­å€¼åªç”¨åˆ°å½“æ—¥çš„æ•°æ®

        Parameters
        ----------
        factor_file : str
            ç”¨äºä¿å­˜å› å­å€¼çš„æ–‡ä»¶åï¼Œéœ€ä¸ºparquetæ–‡ä»¶ï¼Œä»¥'.parquet'ç»“å°¾
        project : str, optional
            è¯¥å› å­æ‰€å±é¡¹ç›®ï¼Œå³å­æ–‡ä»¶å¤¹åç§°, by default None
        startdate : int, optional
            èµ·å§‹æ—¶é—´ï¼Œå½¢å¦‚20121231ï¼Œä¸ºå¼€åŒºé—´, by default None
        enddate : int, optional
            æˆªæ­¢æ—¶é—´ï¼Œå½¢å¦‚20220814ï¼Œä¸ºé—­åŒºé—´ï¼Œä¸ºç©ºåˆ™è®¡ç®—åˆ°æœ€è¿‘æ•°æ®, by default None
        questdb_host: str, optional
            questdbçš„hostï¼Œä½¿ç”¨NASæ—¶æ”¹ä¸º'192.168.1.3', by default '127.0.0.1'
        ignore_history_in_questdb : bool, optional
            æ‰“æ–­åé‡æ–°ä»å¤´è®¡ç®—ï¼Œæ¸…é™¤åœ¨questdbä¸­çš„è®°å½•
        groupby_target: list, optional
            groupbyè®¡ç®—æ—¶ï¼Œåˆ†ç»„çš„ä¾æ®, by default ['code']
        """
        homeplace = HomePlace()
        self.groupby_code = groupby_code
        # å°†è®¡ç®—åˆ°ä¸€åŠçš„å› å­ï¼Œå­˜å…¥questdbä¸­ï¼Œé¿å…ä¸­é€”è¢«æ‰“æ–­åé‡æ–°è®¡ç®—ï¼Œè¡¨åå³ä¸ºå› å­æ–‡ä»¶åçš„æ±‰è¯­æ‹¼éŸ³
        pinyin = Pinyin()
        self.factor_file_pinyin = pinyin.get_pinyin(
            factor_file.replace(".parquet", ""), ""
        )
        self.factor_steps = Questdb(host=questdb_host)
        if project is not None:
            if not os.path.exists(homeplace.factor_data_file + project):
                os.makedirs(homeplace.factor_data_file + project)
            else:
                logger.info(f"å½“å‰æ­£åœ¨{project}é¡¹ç›®ä¸­â€¦â€¦")
        else:
            logger.warning("å½“å‰å› å­ä¸å±äºä»»ä½•é¡¹ç›®ï¼Œè¿™å°†é€ æˆå› å­æ•°æ®æ–‡ä»¶å¤¹çš„æ··ä¹±ï¼Œä¸ä¾¿äºç®¡ç†ï¼Œå»ºè®®æŒ‡å®šä¸€ä¸ªé¡¹ç›®åç§°")
        # å®Œæ•´çš„å› å­æ–‡ä»¶è·¯å¾„
        if project is not None:
            factor_file = homeplace.factor_data_file + project + "/" + factor_file
        else:
            factor_file = homeplace.factor_data_file + factor_file
        self.factor_file = factor_file
        # è¯»å…¥ä¹‹å‰çš„å› å­
        if os.path.exists(factor_file):
            factor_old = drop_duplicates_index(pd.read_parquet(self.factor_file))
            self.factor_old = factor_old
            # å·²ç»ç®—å¥½çš„æ—¥å­
            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
            self.dates_old = dates_old
        elif (not ignore_history_in_questdb) and self.factor_file_pinyin in list(
            self.factor_steps.get_data("show tables").table
        ):
            logger.info(
                f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œç°åœ¨å°†è¯»å–ä¸Šæ¬¡çš„æ•°æ®ï¼Œç»§ç»­è®¡ç®—"
            )
            factor_old = self.factor_steps.get_data_with_tuple(
                f"select * from '{self.factor_file_pinyin}'"
            ).drop_duplicates(subset=["date", "code"])
            factor_old = factor_old.pivot(index="date", columns="code", values="fac")
            factor_old = factor_old.sort_index()
            self.factor_old = factor_old
            # å·²ç»ç®—å¥½çš„æ—¥å­
            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
            self.dates_old = dates_old
        elif ignore_history_in_questdb and self.factor_file_pinyin in list(
            self.factor_steps.get_data("show tables").table
        ):
            logger.info(
                f"ä¸Šæ¬¡è®¡ç®—é€”ä¸­è¢«æ‰“æ–­ï¼Œå·²ç»å°†æ•°æ®å¤‡ä»½åœ¨questdbæ•°æ®åº“çš„è¡¨{self.factor_file_pinyin}ä¸­ï¼Œä½†æ‚¨é€‰æ‹©é‡æ–°è®¡ç®—ï¼Œæ‰€ä»¥æ­£åœ¨åˆ é™¤åŸæ¥çš„æ•°æ®ï¼Œä»å¤´è®¡ç®—"
            )
            factor_old = self.factor_steps.do_order(
                f"drop table '{self.factor_file_pinyin}'"
            )
            self.factor_old = None
            self.dates_old = []
            logger.info("åˆ é™¤å®Œæ¯•ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
        else:
            self.factor_old = None
            self.dates_old = []
            logger.info("è¿™ä¸ªå› å­ä»¥å‰æ²¡æœ‰ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
        # è¯»å–å½“å‰æ‰€æœ‰çš„æ—¥å­
        dates_all = os.listdir(homeplace.tick_by_tick_data)
        dates_all = [i.split(".")[0] for i in dates_all if i.endswith(".parquet")]
        dates_all = [i.replace("-", "") for i in dates_all]
        dates_all = [int(i) for i in dates_all if "20" if i]
        if startdate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i >= startdate]
        if enddate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i <= enddate]
        self.dates_all = dates_all
        # éœ€è¦æ–°è¡¥å……çš„æ—¥å­
        self.dates_new = sorted([i for i in dates_all if i not in self.dates_old])
        if len(self.dates_new) == 0:
            ...
        elif len(self.dates_new) == 1:
            self.dates_new_intervals = [[pd.Timestamp(str(self.dates_new[0]))]]
            print(f"åªç¼ºä¸€å¤©{self.dates_new[0]}")
        else:
            dates = [pd.Timestamp(str(i)) for i in self.dates_new]
            intervals = [[]] * len(dates)
            interbee = 0
            intervals[0] = intervals[0] + [dates[0]]
            for i in range(len(dates) - 1):
                val1 = dates[i]
                val2 = dates[i + 1]
                if val2 - val1 < pd.Timedelta(days=30):
                    ...
                else:
                    interbee = interbee + 1
                intervals[interbee] = intervals[interbee] + [val2]
            intervals = [i for i in intervals if len(i) > 0]
            print(f"å…±{len(intervals)}ä¸ªæ—¶é—´åŒºé—´ï¼Œåˆ†åˆ«æ˜¯")
            for date in intervals:
                print(f"ä»{date[0]}åˆ°{date[-1]}")
            self.dates_new_intervals = intervals
        self.factor_new = []
        self.age = read_daily(age=1)
        self.state = read_daily(state=1)
        self.closes_unadj = read_daily(close=1, unadjust=1).shift(1)

    def __call__(self) -> pd.DataFrame:
        """è·å¾—ç»è¿ç®—äº§ç”Ÿçš„å› å­

        Returns
        -------
        `pd.DataFrame`
            ç»è¿ç®—äº§ç”Ÿçš„å› å­å€¼
        """
        return self.factor.copy()

    def select_one_calculate(
        self,
        date: pd.Timestamp,
        func: Callable,
        fields: str = "*",
        resample_frequency: str = None,
        opens_in: bool = 0,
        highs_in: bool = 0,
        lows_in: bool = 0,
        amounts_in: bool = 0,
        merge_them: bool = 0,
    ) -> None:
        the_func = partial(func)
        if not isinstance(date, int):
            date = int(datetime.datetime.strftime(date, "%Y%m%d"))
        parquet_name = (
            homeplace.tick_by_tick_data
            + str(date)[:4]
            + "-"
            + str(date)[4:6]
            + "-"
            + str(date)[6:]
            + ".parquet"
        )
        if resample_frequency is not None:
            fields = "date,code,price,amount"
        # å¼€å§‹è®¡ç®—å› å­å€¼
        cursor = duckdb.connect()
        df = (
            cursor.execute(f"select {fields} from '{parquet_name}';")
            .arrow()
            .to_pandas()
        )
        date = df.date.iloc[0]
        date0 = pd.Timestamp(year=date.year, month=date.month, day=date.day)
        age_here = self.age.loc[pd.Timestamp(pd.Timestamp(df.date.iloc[0]).date())]
        age_here = age_here.where(age_here > 180, np.nan).dropna()
        state_here = self.state.loc[pd.Timestamp(pd.Timestamp(df.date.iloc[0]).date())]
        state_here = state_here.where(state_here > 0, np.nan).dropna()
        df = df[df.code.isin(age_here.index)]
        df = df[df.code.isin(state_here.index)]

        if resample_frequency is not None:
            date = df.date.iloc[0]
            date0 = pd.Timestamp(year=date.year, month=date.month, day=date.day)
            head = self.closes_unadj.loc[date0].to_frame("head_temp").T
            df = df[df.code.isin(head.columns)]
            price = df.drop_duplicates(subset=["code", "date"], keep="last").pivot(
                index="date", columns="code", values="price"
            )
            closes = price.resample(resample_frequency).last()
            head = head[[i for i in head.columns if i in closes.columns]]
            price = pd.concat([head, closes])
            closes = closes.ffill().iloc[1:, :]
            self.closes = closes
            names = []

            if opens_in:
                price = df.drop_duplicates(subset=["code", "date"], keep="first").pivot(
                    index="date", columns="code", values="price"
                )
                opens = price.resample(resample_frequency).first()
                opens = np.isnan(opens).replace(True, 1).replace(
                    False, 0
                ) * closes.shift(1) + opens.fillna(0)
                self.opens = opens
                names.append("open")
            else:
                self.opens = None

            if highs_in:
                price = (
                    df.sort_values(["code", "date", "price"])
                    .drop_duplicates(subset=["code", "date"], keep="last")
                    .pivot(index="date", columns="code", values="price")
                )
                highs = price.resample(resample_frequency).max()
                highs = np.isnan(highs).replace(True, 1).replace(
                    False, 0
                ) * closes.shift(1) + highs.fillna(0)
                self.highs = highs
                names.append("high")
            else:
                self.highs = None

            if lows_in:
                price = (
                    df.sort_values(["code", "date", "price"])
                    .drop_duplicates(subset=["code", "date"], keep="first")
                    .pivot(index="date", columns="code", values="price")
                )
                lows = price.resample(resample_frequency).min()
                lows = np.isnan(lows).replace(True, 1).replace(False, 0) * closes.shift(
                    1
                ) + lows.fillna(0)
                self.lows = lows
                names.append("low")
            else:
                self.low = None

            names.append("close")
            if amounts_in:
                amounts = df.groupby(["code", "date"]).amount.sum().reset_index()
                amounts = amounts.pivot(index="date", columns="code", values="amount")
                amounts = amounts.resample(resample_frequency).sum().fillna(0)
                self.amounts = amounts
                names.append("amount")
            else:
                self.amounts = None

            if merge_them:
                self.data = merge_many(
                    [
                        i
                        for i in [
                            self.opens,
                            self.highs,
                            self.lows,
                            self.closes,
                            self.amounts,
                        ]
                        if i is not None
                    ],
                    names,
                )

        if self.groupby_code:
            df = df.groupby(["code"]).apply(the_func)
        else:
            df = the_func(df)
            if isinstance(df, pd.DataFrame):
                df.columns = [f"fac{i}" for i in range(len(df.columns))]
                df = df.assign(fac=list(zip(*[df[i] for i in list(df.columns)])))
                df = df[["fac"]]
            elif isinstance(df, list) or isinstance(df, tuple):
                df = pd.concat(list(df), axis=1)
                df.columns = [f"fac{i}" for i in range(len(df.columns))]
                df = df.assign(fac=list(zip(*[df[i] for i in list(df.columns)])))
                df = df[["fac"]]
        df = df.reset_index()
        df.columns = ["code", "fac"]
        df.insert(
            0, "date", pd.Timestamp(year=date.year, month=date.month, day=date.day)
        )
        if (df is not None) and (df.shape[0] > 0):
            df1 = df.pivot(columns="code", index="date", values="fac")
            self.factor_steps.write_via_df(df, self.factor_file_pinyin, tuple_col="fac")
            return df1

    def get_daily_factors(
        self,
        func: Callable,
        n_jobs: int = 1,
        fields: str = "*",
        resample_frequency: str = None,
        opens_in: bool = 0,
        highs_in: bool = 0,
        lows_in: bool = 0,
        amounts_in: bool = 0,
        merge_them: bool = 0,
        use_mpire: bool = 0,
    ) -> None:
        """æ¯æ¬¡æŠ½å–chunksizeå¤©çš„æˆªé¢ä¸Šå…¨éƒ¨è‚¡ç¥¨çš„åˆ†é’Ÿæ•°æ®
        å¯¹æ¯å¤©çš„è‚¡ç¥¨çš„æ•°æ®è®¡ç®—å› å­å€¼

        Parameters
        ----------
        func : Callable
            ç”¨äºè®¡ç®—å› å­å€¼çš„å‡½æ•°
        n_jobs : int, optional
            å¹¶è¡Œæ•°é‡, by default 1
        fields : str, optional
            è¦è¯»å–çš„å­—æ®µï¼Œå¯é€‰åŒ…å«`date,code,price,amount,saleamount,buyamount,action,saleid,saleprice,buyid,buyprice`ï¼Œå…¶ä¸­date,codeå¿…é¡»åŒ…å«, by default `'*'`
        resample_frequency : str, optional
            å°†é€ç¬”æ•°æ®è½¬åŒ–ä¸ºç§’çº§æˆ–åˆ†é’Ÿé¢‘æ•°æ®ï¼Œå¯ä»¥å¡«å†™è¦è½¬åŒ–çš„é¢‘ç‡ï¼Œå¦‚'3s'ï¼ˆ3ç§’æ•°æ®ï¼‰ï¼Œ'1m'ï¼ˆ1åˆ†é’Ÿæ•°æ®ï¼‰ï¼Œ
            æŒ‡å®šæ­¤å‚æ•°åï¼Œå°†è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªself.closesçš„æ”¶ç›˜ä»·çŸ©é˜µ(indexä¸ºæ—¶é—´,columnsä¸ºè‚¡ç¥¨ä»£ç ,valuesä¸ºæ”¶ç›˜ä»·)ï¼Œ
            å¯åœ¨å¾ªç¯è®¡ç®—çš„å‡½æ•°ä¸­ä½¿ç”¨`self.closes`æ¥è°ƒç”¨è®¡ç®—å¥½çš„å€¼, by default None
        opens_in : bool, optional
            åœ¨resample_frequencyä¸ä¸ºNoneçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨æ­¤å‚æ•°ï¼Œæå‰è®¡ç®—å¥½å¼€ç›˜ä»·çŸ©é˜µ(indexä¸ºæ—¶é—´,columnsä¸ºè‚¡ç¥¨ä»£ç ,valuesä¸ºå¼€ç›˜ä»·)ï¼Œ
            å¯åœ¨å¾ªç¯è®¡ç®—çš„å‡½æ•°ä¸­ä½¿ç”¨`self.opens`æ¥è°ƒç”¨è®¡ç®—å¥½çš„å€¼ï¼Œby default 0
        highs_in : bool, optional
            åœ¨resample_frequencyä¸ä¸ºNoneçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨æ­¤å‚æ•°ï¼Œæå‰è®¡ç®—å¥½æœ€é«˜ä»·çŸ©é˜µ(indexä¸ºæ—¶é—´,columnsä¸ºè‚¡ç¥¨ä»£ç ,valuesä¸ºæœ€é«˜ä»·)ï¼Œ
            å¯åœ¨å¾ªç¯è®¡ç®—çš„å‡½æ•°ä¸­ä½¿ç”¨`self.highs`æ¥è°ƒç”¨è®¡ç®—å¥½çš„å€¼ï¼Œby default 0
        lows_in : bool, optional
            åœ¨resample_frequencyä¸ä¸ºNoneçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨æ­¤å‚æ•°ï¼Œæå‰è®¡ç®—å¥½æœ€ä½ä»·çŸ©é˜µ(indexä¸ºæ—¶é—´,columnsä¸ºè‚¡ç¥¨ä»£ç ,valuesä¸ºæœ€ä½ä»·)ï¼Œ
            å¯åœ¨å¾ªç¯è®¡ç®—çš„å‡½æ•°ä¸­ä½¿ç”¨`self.lows`æ¥è°ƒç”¨è®¡ç®—å¥½çš„å€¼ï¼Œby default 0
        amounts_in : bool, optional
            åœ¨resample_frequencyä¸ä¸ºNoneçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨æ­¤å‚æ•°ï¼Œæå‰è®¡ç®—å¥½æˆäº¤é¢çŸ©é˜µ(indexä¸ºæ—¶é—´,columnsä¸ºè‚¡ç¥¨ä»£ç ,valuesä¸ºæˆäº¤é‡)ï¼Œ
            å¯åœ¨å¾ªç¯è®¡ç®—çš„å‡½æ•°ä¸­ä½¿ç”¨`self.amounts`æ¥è°ƒç”¨è®¡ç®—å¥½çš„å€¼ï¼Œby default 0
        merge_them : bool, optional
            åœ¨resample_frequencyä¸ä¸ºNoneçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨æ­¤å‚æ•°ï¼Œå°†è®¡ç®—å¥½çš„å› å­å€¼åˆå¹¶åˆ°ä¸€èµ·ï¼Œç”Ÿæˆç±»ä¼¼äºåˆ†é’Ÿæ•°æ®çš„sqlå½¢å¼ï¼Œby default 0
        use_mpire : bool, optional
            å¹¶è¡Œæ˜¯å¦ä½¿ç”¨mpireï¼Œé»˜è®¤ä½¿ç”¨concurrentï¼Œby default 0
        """
        if len(self.dates_new) > 0:
            if n_jobs > 1:
                if use_mpire:
                    with WorkerPool(n_jobs=n_jobs) as pool:
                        self.factor_new = pool.map(
                            lambda x: self.select_one_calculate(
                                date=x,
                                func=func,
                                fields=fields,
                                resample_frequency=resample_frequency,
                                opens_in=opens_in,
                                highs_in=highs_in,
                                lows_in=lows_in,
                                amounts_in=amounts_in,
                                merge_them=merge_them,
                            ),
                            self.dates_new,
                            progress_bar=True,
                        )
                else:
                    with concurrent.futures.ThreadPoolExecutor(
                        max_workers=n_jobs
                    ) as executor:
                        self.factor_new = list(
                            tqdm.auto.tqdm(
                                executor.map(
                                    lambda x: self.select_one_calculate(
                                        date=x,
                                        func=func,
                                        fields=fields,
                                        resample_frequency=resample_frequency,
                                        opens_in=opens_in,
                                        highs_in=highs_in,
                                        lows_in=lows_in,
                                        amounts_in=amounts_in,
                                        merge_them=merge_them,
                                    ),
                                    self.dates_new,
                                ),
                                total=len(self.dates_new),
                            )
                        )
            else:
                for date in tqdm.auto.tqdm(self.dates_new, "æ‚¨ç°åœ¨å¤„äºå•æ ¸è¿ç®—çŠ¶æ€ï¼Œå»ºè®®ä»…åœ¨è°ƒè¯•æ—¶ä½¿ç”¨å•æ ¸"):
                    df = self.select_one_calculate(
                        date=date,
                        func=func,
                        resample_frequency=resample_frequency,
                        opens_in=opens_in,
                        highs_in=highs_in,
                        lows_in=lows_in,
                        amounts_in=amounts_in,
                        merge_them=merge_them,
                    )
                    self.factor_new.append(df)
            # æ‹¼æ¥æ–°çš„å’Œæ—§çš„
            if self.factor_old is not None:
                self.factor = pd.concat(
                    [self.factor_old] + self.factor_new
                ).sort_index()
            else:
                self.factor = pd.concat(self.factor_new).sort_index()
            self.factor = drop_duplicates_index(self.factor.dropna(how="all"))
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            # å­˜å…¥æœ¬åœ°
            self.factor.to_parquet(self.factor_file)
            logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
            # åˆ é™¤å­˜å‚¨åœ¨questdbçš„ä¸­é€”å¤‡ä»½æ•°æ®
            try:
                self.factor_steps.do_order(f"drop table '{self.factor_file_pinyin}'")
                logger.info("å¤‡ä»½åœ¨questdbçš„è¡¨æ ¼å·²åˆ é™¤")
            except Exception:
                logger.warning("åˆ é™¤questdbä¸­è¡¨æ ¼æ—¶ï¼Œå­˜åœ¨æŸä¸ªæœªçŸ¥é”™è¯¯ï¼Œè¯·å½“å¿ƒ")

        else:
            self.factor = drop_duplicates_index(self.factor_old)
            # å­˜å…¥æœ¬åœ°
            self.factor.to_parquet(self.factor_file)
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            logger.info(f"å½“å‰æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼å·²ç»æ˜¯æœ€æ–°çš„äº†")

    def drop_table(self):
        """ç›´æ¥åˆ é™¤å­˜å‚¨åœ¨questdbä¸­çš„æš‚å­˜æ•°æ®"""
        try:
            self.factor_steps.do_order(f"drop table '{self.factor_file_pinyin}'")
            logger.success(f"æš‚å­˜åœ¨questdbä¸­çš„æ•°æ®è¡¨æ ¼'{self.factor_file_pinyin}'å·²ç»åˆ é™¤")
        except Exception:
            logger.warning(f"æ‚¨è¦åˆ é™¤çš„è¡¨æ ¼'{self.factor_file_pinyin}'å·²ç»ä¸å­˜åœ¨äº†ï¼Œè¯·æ£€æŸ¥")


