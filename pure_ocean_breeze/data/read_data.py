__updated__ = "2023-07-26 16:42:17"

import os
import numpy as np
import pandas as pd
import datetime
import deprecation
from typing import Any, Union, Dict, Tuple
from loguru import logger

from pure_ocean_breeze import __version__
from pure_ocean_breeze.state.states import STATES
from pure_ocean_breeze.state.homeplace import HomePlace
from pure_ocean_breeze.state.decorators import *
from pure_ocean_breeze.data.database import ClickHouseClient, Questdb

try:
    homeplace = HomePlace()
except Exception:
    print("ÊÇ®ÊöÇÊú™ÂàùÂßãÂåñÔºåÂäüËÉΩÂ∞ÜÂèóÈôê")


def read_daily(
    path: str = None,
    open: bool = 0,
    close: bool = 0,
    high: bool = 0,
    low: bool = 0,
    vwap: bool = 0,
    tr: bool = 0,
    sharenum: bool = 0,
    total_sharenum: bool = 0,
    amount: bool = 0,
    money: bool = 0,
    age: bool = 0,
    flow_cap: bool = 0,
    total_cap: bool = 0,
    adjfactor: bool = 0,
    st: bool = 0,
    state: bool = 0,
    unadjust: bool = 0,
    ret: bool = 0,
    ret_inday: bool = 0,
    ret_night: bool = 0,
    vol_daily: bool = 0,
    vol: bool = 0,
    vol_inday: bool = 0,
    vol_night: bool = 0,
    swing: bool = 0,
    pb: bool = 0,
    pe: bool = 0,
    pettm: bool = 0,
    iret: bool = 0,
    ivol: bool = 0,
    illiquidity: bool = 0,
    swindustry_ret: bool = 0,
    zxindustry_ret: bool = 0,
    stop_up: bool = 0,
    stop_down: bool = 0,
    zxindustry_dummy_code: bool = 0,
    zxindustry_dummy_name: bool = 0,
    swindustry_dummy: bool = 0,
    hs300_member_weight: bool = 0,
    zz500_member_weight: bool = 0,
    zz1000_member_weight: bool = 0,
    start: Union[int, str] = STATES["START"],
) -> pd.DataFrame:
    """Áõ¥Êé•ËØªÂèñÂ∏∏Áî®ÁöÑÈáè‰ª∑ËØªÂèñÊó•È¢ëÊï∞ÊçÆÔºåÈªòËÆ§‰∏∫Â§çÊùÉ‰ª∑Ê†ºÔºå
    Âú® open,close,high,low,tr,sharenum,volume ‰∏≠ÈÄâÊã©‰∏Ä‰∏™ÂèÇÊï∞ÊåáÂÆö‰∏∫1

    Parameters
    ----------
    path : str, optional
        Ë¶ÅËØªÂèñÊñá‰ª∂ÁöÑË∑ØÂæÑÔºåÁî±‰∫éÂ∏∏Áî®ÁöÑÈ´òÂºÄ‰ΩéÊî∂Êç¢ÊâãÁéáÁ≠âÈÉΩÂ∑≤ÁªèÂ∞ÅË£ÖÔºåÂõ†Ê≠§Ê≠§Â§ÑÈÄöÂ∏∏‰∏∫None, by default None
    open : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÂºÄÁõò‰ª∑, by default 0
    close : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÊî∂Áõò‰ª∑, by default 0
    high : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÊúÄÈ´ò‰ª∑, by default 0
    low : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÊúÄ‰Ωé‰ª∑, by default 0
    vwap : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÊó•ÂùáÊàê‰∫§‰ª∑, by default 0
    tr : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÊç¢ÊâãÁéá, by default 0
    sharenum : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÊµÅÈÄöËÇ°Êï∞, by default 0
    total_sharenum : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÊÄªËÇ°Êï∞, by default 0
    amount : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÊàê‰∫§Èáè, by default 0
    money : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÊàê‰∫§È¢ù, by default 0
    age : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñ‰∏äÂ∏ÇÂ§©Êï∞, by default 0
    flow_cap : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÊµÅÈÄöÂ∏ÇÂÄº, by default 0
    total_cap : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÊÄªÂ∏ÇÂÄº, by default 0
    adjfactor : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÂ§çÊùÉÂõ†Â≠ê, by default 0
    st : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÂΩìÊó•ÊòØÂê¶‰∏∫stËÇ°Ôºå1Ë°®Á§∫ÊòØstËÇ°ÔºåÁ©∫ÂÄºÂàô‰∏çÊòØ, by default 0
    state : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÂΩìÊó•‰∫§ÊòìÁä∂ÊÄÅÊòØÂê¶Ê≠£Â∏∏Ôºå1Ë°®Á§∫Ê≠£Â∏∏‰∫§ÊòìÔºåÁ©∫ÂÄºÂàô‰∏çÊòØ, by default 0
    unadjust : bool, optional
        ‰∏∫1ÂàôÂ∞Ü‰∏äËø∞‰ª∑Ê†ºÊîπ‰∏∫‰∏çÂ§çÊùÉ‰ª∑Ê†º, by default 0
    ret : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÊó•Èó¥Êî∂ÁõäÁéá, by default 0
    ret_inday : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÊó•ÂÜÖÊî∂ÁõäÁéá, by default 0
    ret_night : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÈöîÂ§úÊ≥¢Âä®Áéá, by default 0
    vol_daily : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñ‰ΩøÁî®ÂàÜÈíüÊî∂Áõò‰ª∑ÁöÑÊ†áÂáÜÂ∑ÆËÆ°ÁÆóÁöÑÊ≥¢Âä®Áéá, by default 0
    vol : bool, optional
        ‰∏∫1ÂàôÈÄâÊã©ËØªÂèñÊªöÂä®20Êó•Êó•Èó¥Ê≥¢Âä®Áéá, by default 0
    vol_inday : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÊªöÂä®20Êó•Êó•ÂÜÖÊî∂ÁõäÁéáÊ≥¢Âä®Áéá, by default 0
    vol_night : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÊªöÂä®20Êó•ÈöîÂ§úÊî∂ÁõäÁéáÊ≥¢Âä®Áéá, by default 0
    swing : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÊåØÂπÖ, by default 0
    pb : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÂ∏ÇÂáÄÁéá, by default 0
    pe : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÂ∏ÇÁõàÁéá, by default 0
    pettm : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÂ∏ÇÁõàÁéá, by default 0
    iret : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñ20Êó•ÂõûÂΩíÁöÑfama‰∏âÂõ†Â≠êÔºàÂ∏ÇÂú∫„ÄÅÊµÅÈÄöÂ∏ÇÂÄº„ÄÅÂ∏ÇÂáÄÁéáÔºâÁâπË¥®Êî∂ÁõäÁéá, by default 0
    ivol : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñ20Êó•ÂõûÂΩíÁöÑ20Êó•fama‰∏âÂõ†Â≠êÔºàÂ∏ÇÂú∫„ÄÅÊµÅÈÄöÂ∏ÇÂÄº„ÄÅÂ∏ÇÂáÄÁéáÔºâÁâπË¥®Ê≥¢Âä®Áéá, by default 0
    illiquidity : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÂΩìÊó•amihudÈùûÊµÅÂä®ÊÄßÊåáÊ†á, by default 0
    swindustry_ret : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÊØèÂè™ËÇ°Á•®ÂØπÂ∫îÁî≥‰∏á‰∏ÄÁ∫ßË°å‰∏öÂΩìÊó•Êî∂ÁõäÁéá, by default 0
    zxindustry_ret : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÊØèÂè™ËÇ°Á•®ÂØπÂ∫îÁî≥‰∏á‰∏ÄÁ∫ßË°å‰∏öÂΩìÊó•Êî∂ÁõäÁéá, by default 0
    stop_up : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÊØèÂè™ËÇ°Á•®Ê∂®ÂÅú‰ª∑, by default 0
    stop_down : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÊØèÂè™ËÇ°Á•®Ë∑åÂÅú‰ª∑, by default 0
    zxindustry_dummy_code : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñ‰∏≠‰ø°‰∏ÄÁ∫ßË°å‰∏öÂìëÂèòÈáèË°®‰ª£Á†ÅÁâà, by default 0
    zxindustry_dummy_name : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñ‰∏≠‰ø°‰∏ÄÁ∫ßË°å‰∏öÂìëÂèòÈáèË°®ÂêçÁß∞Áâà, by default 0
    swindustry_dummy : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÁî≥‰∏á‰∏ÄÁ∫ßË°å‰∏öÂìëÂèòÈáè, by default 0
    hs300_member_weight : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñÊ≤™Ê∑±300ÊàêÂàÜËÇ°ÊùÉÈáçÔºàÊúàÈ¢ëÔºâ, by default 0
    zz500_member_weight : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñ‰∏≠ËØÅ500ÊàêÂàÜËÇ°ÊùÉÈáçÔºàÊúàÈ¢ëÔºâ, by default 0
    zz1000_member_weight : bool, optional
        ‰∏∫1ÂàôË°®Á§∫ËØªÂèñ‰∏≠ËØÅ1000ÊàêÂàÜËÇ°ÊùÉÈáçÔºàÊúàÈ¢ëÔºâ, by default 0
    start : Union[int,str], optional
        Ëµ∑ÂßãÊó•ÊúüÔºåÂΩ¢Â¶Ç20130101, by default STATES["START"]

    Returns
    -------
    `pd.DataFrame`
        ‰∏Ä‰∏™columns‰∏∫ËÇ°Á•®‰ª£Á†ÅÔºåindex‰∏∫Êó∂Èó¥Ôºåvalues‰∏∫ÁõÆÊ†áÊï∞ÊçÆÁöÑpd.DataFrame

    Raises
    ------
    `IOError`
        open,close,high,low,tr,sharenum,volume ÈÉΩ‰∏∫0Êó∂ÔºåÂ∞ÜÊä•Èîô
    """

    if not unadjust:
        if path:
            return pd.read_parquet(homeplace.daily_data_file + path)
        elif open:
            opens = pd.read_parquet(
                homeplace.daily_data_file + "opens.parquet"
            ) * read_daily(state=1, start=start)
            df = opens
        elif close:
            closes = pd.read_parquet(
                homeplace.daily_data_file + "closes.parquet"
            ) * read_daily(state=1, start=start)
            df = closes
        elif high:
            highs = pd.read_parquet(
                homeplace.daily_data_file + "highs.parquet"
            ) * read_daily(state=1, start=start)
            df = highs
        elif low:
            lows = pd.read_parquet(
                homeplace.daily_data_file + "lows.parquet"
            ) * read_daily(state=1, start=start)
            df = lows
        elif vwap:
            df = (
                pd.read_parquet(homeplace.daily_data_file + "vwaps.parquet")
                * read_daily(adjfactor=1, start=start)
                * read_daily(state=1, start=start)
            )
        elif tr:
            trs = pd.read_parquet(homeplace.daily_data_file + "trs.parquet").replace(
                0, np.nan
            ) * read_daily(state=1, start=start)
            df = trs
        elif sharenum:
            sharenums = pd.read_parquet(homeplace.daily_data_file + "sharenums.parquet")
            df = sharenums
        elif total_sharenum:
            df = pd.read_parquet(homeplace.daily_data_file + "total_sharenums.parquet")
        elif amount:
            volumes = pd.read_parquet(
                homeplace.daily_data_file + "amounts.parquet"
            ) * read_daily(state=1, start=start)
            df = volumes
        elif money:
            df = pd.read_parquet(
                homeplace.factor_data_file + "Êó•È¢ëÊï∞ÊçÆ-ÊØèÊó•Êàê‰∫§È¢ù/ÊØèÊó•Êàê‰∫§È¢ù.parquet"
            )
        elif age:
            age = pd.read_parquet(homeplace.daily_data_file + "ages.parquet")
            df = age
        elif flow_cap:
            closes = pd.read_parquet(
                homeplace.daily_data_file + "closes_unadj.parquet"
            ) * read_daily(state=1, start=start)
            sharenums = pd.read_parquet(homeplace.daily_data_file + "sharenums.parquet")
            flow_cap = closes * sharenums
            df = flow_cap
        elif total_cap:
            closes = pd.read_parquet(
                homeplace.daily_data_file + "closes_unadj.parquet"
            ) * read_daily(state=1, start=start)
            sharenums = pd.read_parquet(
                homeplace.daily_data_file + "total_sharenums.parquet"
            )
            flow_cap = closes * sharenums
            df = flow_cap
        elif adjfactor:
            # df=pd.read_parquet(homeplace.daily_data_file+'adjfactors.parquet')
            df = (
                read_daily(close=1, start=start)
                * read_daily(state=1, start=start)
                / read_daily(close=1, start=start, unadjust=1)
                * read_daily(state=1, start=start)
            )
        elif st:
            st = pd.read_parquet(homeplace.daily_data_file + "sts.parquet")
            df = st
        elif state:
            state = pd.read_parquet(homeplace.daily_data_file + "states.parquet")
            state = state.where(state == 1, np.nan)
            df = state
        elif ret:
            df = read_daily(close=1, start=start)
            df = df / df.shift(1) - 1
        elif ret_inday:
            df = read_daily(close=1, start=start) / read_daily(open=1, start=start) - 1
        elif ret_night:
            df = (
                read_daily(open=1, start=start)
                / read_daily(close=1, start=start).shift(1)
                - 1
            )
        elif vol_daily:
            df = pd.read_parquet(
                homeplace.factor_data_file + "ËçâÊú®ÁöÜÂÖµ/ËçâÊú®ÁöÜÂÖµ_ÂàùÁ∫ß.parquet"
            ) * read_daily(state=1)
        elif vol:
            df = read_daily(ret=1, start=start)
            df = df.rolling(20, min_periods=10).std()
        elif vol_inday:
            df = read_daily(ret_inday=1, start=start)
            df = df.rolling(20, min_periods=10).std()
        elif vol_night:
            df = read_daily(ret_night=1, start=start)
            df = df.rolling(20, min_periods=10).std()
        elif swing:
            df = (
                read_daily(high=1, start=start) - read_daily(low=1, start=start)
            ) / read_daily(close=1, start=start).shift(1)
        elif pb:
            df = pd.read_parquet(homeplace.daily_data_file + "pb.parquet") * read_daily(
                state=1, start=start
            )
        elif pe:
            df = pd.read_parquet(homeplace.daily_data_file + "pe.parquet") * read_daily(
                state=1, start=start
            )
        elif pettm:
            df = pd.read_parquet(
                homeplace.daily_data_file + "pettm.parquet"
            ) * read_daily(state=1, start=start)
        elif iret:
            df = pd.read_parquet(
                homeplace.daily_data_file + "idiosyncratic_ret.parquet"
            ) * read_daily(state=1, start=start)
        elif ivol:
            df = read_daily(iret=1, start=start)
            df = df.rolling(20, min_periods=10).std()
        elif illiquidity:
            df = pd.read_parquet(
                homeplace.daily_data_file + "illiquidity.parquet"
            ) * read_daily(state=1, start=start)
        elif swindustry_ret:
            df = pd.read_parquet(
                homeplace.daily_data_file + "ËÇ°Á•®ÂØπÂ∫îÁî≥‰∏á‰∏ÄÁ∫ßË°å‰∏öÊØèÊó•Êî∂ÁõäÁéá.parquet"
            ) * read_daily(state=1, start=start)
        elif zxindustry_ret:
            df = pd.read_parquet(
                homeplace.daily_data_file + "ËÇ°Á•®ÂØπÂ∫î‰∏≠‰ø°‰∏ÄÁ∫ßË°å‰∏öÊØèÊó•Êî∂ÁõäÁéá.parquet"
            ) * read_daily(state=1, start=start)
        elif stop_up:
            df = (
                pd.read_parquet(homeplace.daily_data_file + "stop_ups.parquet")
                * read_daily(adjfactor=1, start=start)
                * read_daily(state=1, start=start)
            )
        elif stop_down:
            df = (
                pd.read_parquet(homeplace.daily_data_file + "stop_downs.parquet")
                * read_daily(adjfactor=1, start=start)
                * read_daily(state=1, start=start)
            )
        elif zxindustry_dummy_code:
            df = pd.read_parquet(homeplace.daily_data_file + "‰∏≠‰ø°‰∏ÄÁ∫ßË°å‰∏öÂìëÂèòÈáè‰ª£Á†ÅÁâà.parquet")
        elif zxindustry_dummy_name:
            df = pd.read_parquet(homeplace.daily_data_file + "‰∏≠‰ø°‰∏ÄÁ∫ßË°å‰∏öÂìëÂèòÈáèÂêçÁß∞Áâà.parquet")
        elif swindustry_dummy:
            df = pd.read_parquet(homeplace.daily_data_file + "Áî≥‰∏áË°å‰∏ö2021ÁâàÂìëÂèòÈáè.parquet")
        elif hs300_member_weight:
            df = (
                pd.read_parquet(homeplace.daily_data_file + "Ê≤™Ê∑±300ÊàêÂàÜËÇ°ÊùÉÈáç.parquet")
                .resample("M")
                .last()
            )
        elif zz500_member_weight:
            df = (
                pd.read_parquet(homeplace.daily_data_file + "‰∏≠ËØÅ500ÊàêÂàÜËÇ°ÊùÉÈáç.parquet")
                .resample("M")
                .last()
            )
        elif zz1000_member_weight:
            df = (
                pd.read_parquet(homeplace.daily_data_file + "‰∏≠ËØÅ1000ÊàêÂàÜËÇ°ÊùÉÈáç.parquet")
                .resample("M")
                .last()
            )
        else:
            raise IOError("ÈòÅ‰∏ãÊÄªÂæóËØªÁÇπ‰ªÄ‰πàÂêßÔºüü§í")
    else:
        if open:
            opens = pd.read_parquet(
                homeplace.daily_data_file + "opens_unadj.parquet"
            ) * read_daily(state=1, start=start)
            df = opens
        elif close:
            closes = pd.read_parquet(
                homeplace.daily_data_file + "closes_unadj.parquet"
            ) * read_daily(state=1, start=start)
            df = closes
        elif high:
            highs = pd.read_parquet(
                homeplace.daily_data_file + "highs_unadj.parquet"
            ) * read_daily(state=1, start=start)
            df = highs
        elif low:
            lows = pd.read_parquet(
                homeplace.daily_data_file + "lows_unadj.parquet"
            ) * read_daily(state=1, start=start)
            df = lows
        elif vwap:
            df = pd.read_parquet(
                homeplace.daily_data_file + "vwaps.parquet"
            ) * read_daily(state=1, start=start)
        elif stop_up:
            df = pd.read_parquet(
                homeplace.daily_data_file + "stop_ups.parquet"
            ) * read_daily(state=1, start=start)
        elif stop_down:
            df = pd.read_parquet(
                homeplace.daily_data_file + "stop_downs.parquet"
            ) * read_daily(state=1, start=start)
        else:
            raise IOError("ÈòÅ‰∏ãÊÄªÂæóËØªÁÇπ‰ªÄ‰πàÂêßÔºüü§í")
    if "date" not in df.columns:
        df = df[df.index >= pd.Timestamp(str(start))]
    return df.dropna(how="all")


def read_market(
    open: bool = 0,
    close: bool = 0,
    high: bool = 0,
    low: bool = 0,
    start: int = STATES["START"],
    every_stock: bool = 1,
    market_code: str = "000985.SH",
    questdb_host: str = "127.0.0.1",
) -> Union[pd.DataFrame, pd.Series]:
    """ËØªÂèñ‰∏≠ËØÅÂÖ®ÊåáÊó•Ë°åÊÉÖÊï∞ÊçÆ

    Parameters
    ----------
    open : bool, optional
        ËØªÂèñÂºÄÁõòÁÇπÊï∞, by default 0
    close : bool, optional
        ËØªÂèñÊî∂ÁõòÁÇπÊï∞, by default 0
    high : bool, optional
        ËØªÂèñÊúÄÈ´òÁÇπÊï∞, by default 0
    low : bool, optional
        ËØªÂèñÊúÄ‰ΩéÁÇπÊï∞, by default 0
    start : int, optional
        ËØªÂèñÁöÑËµ∑ÂßãÊó•Êúü, by default STATES["START"]
    every_stock : bool, optional
        ÊòØÂê¶‰øÆÊîπ‰∏∫indexÊòØÊó∂Èó¥ÔºåcolumnsÊòØÊØèÂè™ËÇ°Á•®‰ª£Á†ÅÔºåÊØè‰∏ÄÂàóÂÄºÈÉΩÁõ∏ÂêåÁöÑÂΩ¢Âºè, by default 1
    market_code : str, optional
        ÈÄâÁî®Âì™‰∏™ÊåáÊï∞‰Ωú‰∏∫Â∏ÇÂú∫ÊåáÊï∞ÔºåÈªòËÆ§‰ΩøÁî®‰∏≠ËØÅÂÖ®Êåá
    questdb_host: str, optional
        questdbÁöÑhostÔºå‰ΩøÁî®NASÊó∂Êîπ‰∏∫'192.168.1.3', by default '127.0.0.1'

    Returns
    -------
    Union[pd.DataFrame,pd.Series]
        ‰∏≠ËØÅÂÖ®ÊåáÊØèÂ§©ÁöÑÊåáÊï∞

    Raises
    ------
    IOError
        Â¶ÇÊûúÊ≤°ÊúâÊåáÂÆö‰ªª‰ΩïÊåáÊï∞ÔºåÂ∞ÜÊä•Èîô
    """
    try:
        chc = ClickHouseClient("minute_data")
        df = (
            chc.get_data(
                f"select date,num,close,high,low from minute_data.minute_data_index where code='{market_code}' and date>={start}00 order by date,num"
            )
            / 100
        )
    except Exception:
        try:
            qdb = Questdb(host=questdb_host)
            df = qdb.get_data(
                f"select date,num,close,high,low from minute_data_index where code='{market_code}' and cast(date as int)>={start}"
            )
        except Exception:
            qdb = Questdb(host="192.168.1.3")
            df = qdb.get_data(
                f"select date,num,close,high,low from minute_data_index where code='{market_code}' and cast(date as int)>={start}"
            )
        df.num = df.num.astype(int)
    df = df.set_index("date")
    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
    if open:
        # Á±≥Á≠êÁöÑÁ¨¨‰∏ÄÂàÜÈíüÊòØÈõÜÂêàÁ´û‰ª∑ÔºåÁ¨¨‰∏ÄÂàÜÈíüÁöÑÊî∂Áõò‰ª∑Âç≥‰∏∫ÂΩìÂ§©ÂºÄÁõò‰ª∑
        df = df[df.num == 1].open
    elif close:
        df = df[df.num == 240].close
    elif high:
        df = df[df.num > 1]
        df = df.groupby("date").max()
        df = df.high
    elif low:
        df = df[df.num > 1]
        df = df.groupby("date").min()
        df = df.low
    else:
        raise IOError("ÊÄªÂæóÊåáÂÆö‰∏Ä‰∏™ÊåáÊ†áÂêßÔºüü§í")
    if every_stock:
        tr = read_daily(tr=1, start=start)
        df = pd.DataFrame({k: list(df) for k in list(tr.columns)}, index=df.index)
    return df


def read_money_flow(
    buy: bool = 0,
    sell: bool = 0,
    exlarge: bool = 0,
    large: bool = 0,
    median: bool = 0,
    small: bool = 0,
    whole: bool = 0,
) -> pd.DataFrame:
    """‰∏ÄÈîÆËØªÂÖ•ËµÑÈáëÊµÅÂêëÊï∞ÊçÆÔºåÂåÖÊã¨Ë∂ÖÂ§ßÂçï„ÄÅÂ§ßÂçï„ÄÅ‰∏≠Âçï„ÄÅÂ∞èÂçïÁöÑ‰π∞ÂÖ•ÂíåÂçñÂá∫ÊÉÖÂÜµ

    Parameters
    ----------
    buy : bool, optional
        ÊñπÂêë‰∏∫‰π∞, by default 0
    sell : bool, optional
        ÊñπÂêë‰∏∫Âçñ, by default 0
    exlarge : bool, optional
        Ë∂ÖÂ§ßÂçïÔºåÈáëÈ¢ùÂ§ß‰∫é100‰∏áÔºå‰∏∫Êú∫ÊûÑÊìç‰Ωú, by default 0
    large : bool, optional
        Â§ßÂçïÔºåÈáëÈ¢ùÂú®20‰∏áÂà∞100‰∏á‰πãÈó¥Ôºå‰∏∫Â§ßÊà∑ÁâπÂ§ßÂçï, by default 0
    median : bool, optional
        ‰∏≠ÂçïÔºåÈáëÈ¢ùÂú®4‰∏áÂà∞20‰∏á‰πãÈó¥Ôºå‰∏∫‰∏≠Êà∑Â§ßÂçï, by default 0
    small : bool, optional
        Â∞èÂçïÔºåÈáëÈ¢ùÂú®4‰∏á‰ª•‰∏ãÔºå‰∏∫Êï£Êà∑‰∏≠Âçï, by default 0
    whole : bool, optional
        ËØªÂÖ•ÂΩìÂ§©ÁöÑÊÄª‰∫§ÊòìÈ¢ù, by default 0

    Returns
    -------
    pd.DataFrame
        index‰∏∫Êó∂Èó¥Ôºåcolumns‰∏∫ËÇ°Á•®‰ª£Á†ÅÔºåvalues‰∏∫ÂØπÂ∫îÁ±ªÂûãËÆ¢ÂçïÂΩìÊó•ÁöÑÊàê‰∫§ÈáëÈ¢ù

    Raises
    ------
    IOError
        buyÂíåsellÂøÖÈ°ªÊåáÂÆö‰∏Ä‰∏™ÔºåÂê¶Âàô‰ºöÊä•Èîô
    IOError
        exlargeÔºålargeÔºåmedianÂíåsmallÂøÖÈ°ªÊåáÂÆö‰∏Ä‰∏™ÔºåÂê¶Âàô‰ºöÊä•Èîô
    """
    if not whole:
        if buy:
            if exlarge:
                name = "buy_value_exlarge"
            elif large:
                name = "buy_value_large"
            elif median:
                name = "buy_value_med"
            elif small:
                name = "buy_value_small"
            else:
                raise IOError("ÊÇ®ÊÄªÂæóÊåáÂÆö‰∏ÄÁßçËßÑÊ®°ÂêßÔºüü§í")
        elif sell:
            if exlarge:
                name = "sell_value_exlarge"
            elif large:
                name = "sell_value_large"
            elif median:
                name = "sell_value_med"
            elif small:
                name = "sell_value_small"
            else:
                raise IOError("ÊÇ®ÊÄªÂæóÊåáÂÆö‰∏ÄÁßçËßÑÊ®°ÂêßÔºüü§í")
        else:
            raise IOError("ÊÇ®ÊÄªÂæóÊåáÂÆö‰∏Ä‰∏ãÊòØ‰π∞ËøòÊòØÂçñÂêßÔºüü§í")
        name = homeplace.daily_data_file + name + ".parquet"
        df = pd.read_parquet(name)
        return df
    else:
        dfs = [
            pd.read_parquet(homeplace.daily_data_file + name + ".parquet")
            for name in [
                "buy_value_exlarge",
                "buy_value_large",
                "buy_value_med",
                "buy_value_small",
                "sell_value_exlarge",
                "sell_value_large",
                "sell_value_med",
                "sell_value_small",
            ]
        ]
        dfs = sum(dfs)
        return dfs


def read_index_single(code: str, questdb_host: str = "127.0.0.1") -> pd.Series:
    """ËØªÂèñÊüê‰∏™ÊåáÊï∞ÁöÑÊó•Ë°åÊÉÖÊî∂Áõò‰ª∑Êï∞ÊçÆ

    Parameters
    ----------
    code : str
        ÊåáÊï∞ÁöÑwind‰ª£Á†Å
    questdb_host: str, optional
        questdbÁöÑhostÔºå‰ΩøÁî®NASÊó∂Êîπ‰∏∫'192.168.1.3', by default '127.0.0.1'

    Returns
    -------
    pd.Series
        Êó•Ë°åÊÉÖÊï∞ÊçÆ
    """
    try:
        chc = ClickHouseClient("minute_data")
        hs300 = (
            chc.get_data(
                f"select date,num,close FROM minute_data.minute_data_index WHERE code='{code}'"
            )
            / 100
        )
        hs300.date = pd.to_datetime(hs300.date, format="%Y%m%d")
        hs300 = (
            hs300.sort_values(["date", "num"])
            .groupby("date")
            .last()
            .drop(columns=["num"])
            .close
        )
        return hs300
    except Exception:
        try:
            qdb = Questdb(host=questdb_host)
            hs300 = qdb.get_data(
                f"select date,num,close FROM 'minute_data_index' WHERE code='{code}'"
            )
        except Exception:
            qdb = Questdb(host="192.168.1.3")
            hs300 = qdb.get_data(
                f"select date,num,close FROM 'minute_data_index' WHERE code='{code}'"
            )
        hs300.date = pd.to_datetime(hs300.date, format="%Y%m%d")
        hs300.num = hs300.num.astype(int)
        hs300 = (
            hs300.sort_values(["date", "num"])
            .groupby("date")
            .last()
            .drop(columns=["num"])
            .close
        )
        return hs300


def read_index_three(day: int = None) -> Tuple[pd.DataFrame]:
    """ËØªÂèñ‰∏âÂ§ßÊåáÊï∞ÁöÑÂéüÂßãË°åÊÉÖÊï∞ÊçÆÔºåËøîÂõûÂπ∂‰øùÂ≠òÂú®Êú¨Âú∞

    Parameters
    ----------
    day : int, optional
        Ëµ∑ÂßãÊó•ÊúüÔºåÂΩ¢Â¶Ç20130101, by default None

    Returns
    -------
    `Tuple[pd.DataFrame]`
        ÂàÜÂà´ËøîÂõûÊ≤™Ê∑±300„ÄÅ‰∏≠ËØÅ500„ÄÅ‰∏≠ËØÅ1000ÁöÑË°åÊÉÖÊï∞ÊçÆ
    """
    if day is None:
        day = STATES["START"]

    hs300, zz500, zz1000, zz2000 = (
        read_index_single("000300.SH").resample("M").last(),
        read_index_single("000905.SH").resample("M").last(),
        read_index_single("000852.SH").resample("M").last(),
        read_index_single("399303.SZ").resample("M").last(),
    )
    hs300 = hs300[hs300.index >= pd.Timestamp(str(day))]
    zz500 = zz500[zz500.index >= pd.Timestamp(str(day))]
    zz1000 = zz1000[zz1000.index >= pd.Timestamp(str(day))]
    zz2000 = zz2000[zz2000.index >= pd.Timestamp(str(day))]

    return hs300, zz500, zz1000, zz2000


def read_swindustry_prices(
    day: int = None, monthly: bool = 1, start: int = STATES["START"]
) -> pd.DataFrame:
    """ËØªÂèñÁî≥‰∏á‰∏ÄÁ∫ßË°å‰∏öÊåáÊï∞ÁöÑÊó•Ë°åÊÉÖÊàñÊúàË°åÊÉÖ

    Parameters
    ----------
    day : int, optional
        Ëµ∑ÂßãÊó•ÊúüÔºåÂΩ¢Â¶Ç20130101, by default None
    monthly : bool, optional
        ÊòØÂê¶‰∏∫ÊúàË°åÊÉÖ, by default 1

    Returns
    -------
    `pd.DataFrame`
        Áî≥‰∏á‰∏ÄÁ∫ßË°å‰∏öÁöÑË°åÊÉÖÊï∞ÊçÆ
    """
    if day is None:
        day = STATES["START"]
    df = pd.read_parquet(homeplace.daily_data_file + "Áî≥‰∏áÂêÑË°å‰∏öË°åÊÉÖÊï∞ÊçÆ.parquet")
    df = df[df.index >= pd.Timestamp(str(start))]
    if monthly:
        df = df.resample("M").last()
    return df


def read_zxindustry_prices(
    day: int = None, monthly: bool = 1, start: int = STATES["START"]
) -> pd.DataFrame:
    """ËØªÂèñ‰∏≠‰ø°‰∏ÄÁ∫ßË°å‰∏öÊåáÊï∞ÁöÑÊó•Ë°åÊÉÖÊàñÊúàË°åÊÉÖ

    Parameters
    ----------
    day : int, optional
        Ëµ∑ÂßãÊó•ÊúüÔºåÂΩ¢Â¶Ç20130101, by default None
    monthly : bool, optional
        ÊòØÂê¶‰∏∫ÊúàË°åÊÉÖ, by default 1

    Returns
    -------
    `pd.DataFrame`
        Áî≥‰∏á‰∏ÄÁ∫ßË°å‰∏öÁöÑË°åÊÉÖÊï∞ÊçÆ
    """
    if day is None:
        day = STATES["START"]
    df = pd.read_parquet(homeplace.daily_data_file + "‰∏≠‰ø°ÂêÑË°å‰∏öË°åÊÉÖÊï∞ÊçÆ.parquet")
    df = df[df.index >= pd.Timestamp(str(start))]
    if monthly:
        df = df.resample("M").last()
    return df


def get_industry_dummies(
    daily: bool = 0,
    monthly: bool = 0,
    start: int = STATES["START"],
    swindustry: bool = 0,
    zxindustry: bool = 0,
) -> Dict:
    """ÁîüÊàê30/31‰∏™Ë°å‰∏öÁöÑÂìëÂèòÈáèÁü©ÈòµÔºåËøîÂõû‰∏Ä‰∏™Â≠óÂÖ∏

    Parameters
    ----------
    daily : bool, optional
        ËøîÂõûÊó•È¢ëÁöÑÂìëÂèòÈáè, by default 0
    monthly : bool, optional
        ËøîÂõûÊúàÈ¢ëÁöÑÂìëÂèòÈáè, by default 0
    start : int, optional
        Ëµ∑ÂßãÊó•Êúü, by default STATES["START"]
    swindustry : bool, optional
        ÊòØÂê¶‰ΩøÁî®Áî≥‰∏á‰∏ÄÁ∫ßË°å‰∏ö, by default 0
    zxindustry : bool, optional
        ÊòØÂê¶‰ΩøÁî®‰∏≠‰ø°‰∏ÄÁ∫ßË°å‰∏ö, by default 0

    Returns
    -------
    `Dict`
        ÂêÑ‰∏™Ë°å‰∏öÂèäÂÖ∂ÂìëÂèòÈáèÊûÑÊàêÁöÑÂ≠óÂÖ∏

    Raises
    ------
    `ValueError`
        Â¶ÇÊûúÊú™ÊåáÂÆöÈ¢ëÁéáÔºåÂ∞ÜÊä•Èîô
    """
    homeplace = HomePlace()
    if swindustry:
        name = "Áî≥‰∏áË°å‰∏ö2021ÁâàÂìëÂèòÈáè.parquet"
    else:
        name = "‰∏≠‰ø°‰∏ÄÁ∫ßË°å‰∏öÂìëÂèòÈáèÂêçÁß∞Áâà.parquet"
    if monthly:
        industry_dummy = pd.read_parquet(homeplace.daily_data_file + name)
        industry_dummy = (
            industry_dummy.set_index("date")
            .groupby("code")
            .resample("M")
            .last()
            .fillna(0)
            .drop(columns=["code"])
            .reset_index()
        )
    elif daily:
        industry_dummy = pd.read_parquet(homeplace.daily_data_file + name).fillna(0)
    else:
        raise ValueError("ÊÇ®ÊÄªÂæóÊåáÂÆö‰∏Ä‰∏™È¢ëÁéáÂêßÔºüü§í")
    industry_dummy = industry_dummy[industry_dummy.date >= pd.Timestamp(str(start))]
    ws = list(industry_dummy.columns)[2:]
    ress = {}
    for w in ws:
        df = industry_dummy[["date", "code", w]]
        df = df.pivot(index="date", columns="code", values=w)
        df = df.replace(0, np.nan)
        ress[w] = df
    return ress


@deprecation.deprecated(
    deprecated_in="4.0",
    removed_in="5.0",
    current_version=__version__,
    details="Áî±‰∫éÂõ†Â≠êÊàêÊûúÊï∞ÊçÆÂ∫ìÂçáÁ∫ßÔºå3.xÁâàÊú¨ÁöÑÂõ†Â≠êÊàêÊûúËØªÂèñÂáΩÊï∞Â∞Ü‰∏ãÁ∫ø",
)
def database_read_final_factors(
    name: str = None,
    order: int = None,
    freq: str = "Êúà",
    output: bool = 0,
    new: bool = 0,
) -> Tuple[pd.DataFrame, str]:
    """Ê†πÊçÆÂõ†Â≠êÂêçÂ≠óÔºåÊàñÂõ†Â≠êÂ∫èÂè∑ÔºåËØªÂèñÊúÄÁªàÂõ†Â≠êÁöÑÂõ†Â≠êÂÄº

    Parameters
    ----------
    name : str, optional
        Âõ†Â≠êÁöÑÂêçÂ≠ó, by default None
    order : int, optional
        Âõ†Â≠êÁöÑÂ∫èÂè∑, by default None
    freq : str, optional
        Âõ†Â≠êÁöÑÈ¢ëÁéáÔºåÁõÆÂâçÊîØÊåÅ`'Êúà'`Âíå`'Âë®'`
    output : bool, optional
        ÊòØÂê¶ËæìÂá∫Âà∞csvÊñá‰ª∂, by default 0
    new : bool, optional
        ÊòØÂê¶Âè™ËæìÂá∫ÊúÄÊñ∞‰∏ÄÊúüÁöÑÂõ†Â≠êÂÄº, by default 0

    Returns
    -------
    `Tuple[pd.DataFrame,str]`
        ÊúÄÁªàÂõ†Â≠êÂÄºÂíåÊñá‰ª∂Ë∑ØÂæÑ
    """
    homeplace = HomePlace()
    facs = os.listdir(homeplace.final_factor_file)
    if name is None and order is None:
        raise IOError("ËØ∑ÊåáÂÆöÂõ†Â≠êÂêçÂ≠óÊàñËÄÖÂõ†Â≠êÂ∫èÂè∑")
    elif name is None and order is not None:
        key = "Â§öÂõ†Â≠ê" + str(order) + "_" + freq
        ans = [i for i in facs if ((key in i) and (freq in i))][0]
    elif name is not None and name is None:
        key = name
        ans = [i for i in facs if ((key in i) and (freq in i))]
        if len(ans) > 0:
            ans = ans[0]
        else:
            raise IOError(f"ÊÇ®ÂêçÂ≠óËÆ∞Èîô‰∫ÜÔºå‰∏çÂ≠òÂú®Âè´{name}ÁöÑÂõ†Â≠ê")
    else:
        key1 = name
        key2 = "Â§öÂõ†Â≠ê" + str(order) + "_" + freq
        ans1 = [i for i in facs if ((key1 in i) and (freq in i))]
        if len(ans1) > 0:
            ans1 = ans1[0]
        else:
            raise IOError(f"ÊÇ®ÂêçÂ≠óËÆ∞Èîô‰∫ÜÔºå‰∏çÂ≠òÂú®Âè´{name}ÁöÑÂõ†Â≠ê")
        ans2 = [i for i in facs if ((key2 in i) and (freq in i))][0]
        if ans1 != ans2:
            ans = ans1
            logger.warning("ÊÇ®ËæìÂÖ•ÁöÑÂêçÂ≠óÂíåÂ∫èÂè∑‰∏ç‰∏ÄËá¥ÔºåÊÄÄÁñëÊÇ®ËÆ∞Èîô‰∫ÜÂ∫èÂè∑ÔºåÁ®ãÂ∫èÈªòËÆ§‰ª•ÂêçÂ≠ó‰∏∫ÂáÜ‰∫ÜÂìà")
        else:
            ans = ans1
    path = homeplace.final_factor_file + ans
    df = pd.read_parquet(path)
    df = df[sorted(list(df.columns))]
    final_date = df.index.max()
    final_date = datetime.datetime.strftime(final_date, "%Y%m%d")
    if output:
        if new:
            if os.path.exists(ans.split("_")[0]):
                fac_name = (
                    ans.split("_")[0]
                    + "/"
                    + ans.split("_")[0]
                    + "Âõ†Â≠ê"
                    + final_date
                    + "_"
                    + freq
                    + "È¢ë"
                    + "Âõ†Â≠êÂÄº.csv"
                )
            else:
                os.makedirs(ans.split("_")[0])
                fac_name = (
                    ans.split("_")[0]
                    + "/"
                    + ans.split("_")[0]
                    + "Âõ†Â≠ê"
                    + final_date
                    + "_"
                    + freq
                    + "È¢ë"
                    + "Âõ†Â≠êÂÄº.csv"
                )
            df.tail(1).T.to_csv(fac_name)
            logger.success(f"{final_date}ÁöÑÂõ†Â≠êÂÄºÂ∑≤‰øùÂ≠ò")
        else:
            if os.path.exists(ans.split("_")[0]):
                fac_name = (
                    ans.split("_")[0]
                    + "/"
                    + ans.split("_")[0]
                    + "Âõ†Â≠êÊà™Ëá≥"
                    + final_date
                    + "_"
                    + freq
                    + "È¢ë"
                    + "Âõ†Â≠êÂÄº.csv"
                )
            else:
                os.makedirs(ans.split("_")[0])
                fac_name = (
                    ans.split("_")[0]
                    + "/"
                    + ans.split("_")[0]
                    + "Âõ†Â≠êÊà™Ëá≥"
                    + final_date
                    + "_"
                    + freq
                    + "È¢ë"
                    + "Âõ†Â≠êÂÄº.csv"
                )
            df.to_csv(fac_name)
            logger.success(f"Êà™Ëá≥{final_date}ÁöÑÂõ†Â≠êÂÄºÂ∑≤‰øùÂ≠ò")
        return df, fac_name
    else:
        return df, ""


@deprecation.deprecated(
    deprecated_in="4.0",
    removed_in="5.0",
    current_version=__version__,
    details="Áî±‰∫éÂõ†Â≠êÊàêÊûúÊï∞ÊçÆÂ∫ìÂçáÁ∫ßÔºå3.xÁâàÊú¨ÁöÑÂõ†Â≠êÊàêÊûúËØªÂèñÂáΩÊï∞Â∞Ü‰∏ãÁ∫ø",
)
def database_read_primary_factors(name: str, name2: str = None) -> pd.DataFrame:
    """Ê†πÊçÆÂõ†Â≠êÂêçÂ≠óÔºåËØªÂèñÂàùÁ∫ßÂõ†Â≠êÁöÑÂõ†Â≠êÂÄº

    Parameters
    ----------
    name : str, optional
        Âõ†Â≠êÁöÑÂêçÂ≠ó, by default None
    name2 : str, optional
        Â≠êÂõ†Â≠êÁöÑÂêçÂ≠óÔºåÂΩìÊúâÂ§ö‰∏™ÂàÜÊîØÂõ†Â≠êÔºåÂàÜÂà´ÂÇ®Â≠òÊó∂Ôºå‰ΩøÁî®Ëøô‰∏™ÂèÇÊï∞Êù•ÊåáÂÆöÂÖ∑‰ΩìÁöÑÂ≠êÂõ†Â≠ê, by default None

    Returns
    -------
    `pd.DataFrame`
        ÂàùÁ∫ßÂõ†Â≠êÁöÑÂõ†Â≠êÂÄº
    """
    homeplace = HomePlace()
    if name2 is None:
        name = name + "/" + name + "_ÂàùÁ∫ß.parquet"
    else:
        name = name + "/" + name + "_ÂàùÁ∫ß_" + name2 + ".parquet"
    df = pd.read_parquet(homeplace.factor_data_file + name)
    df = df[sorted(list(df.columns))]
    return df


class FactorDone(object):
    def __init__(
        self,
        order: str,
        name: str = None,
        place: str = None,
        son_name: str = None,
        freq: str = "Êúà",
    ) -> None:
        self.homeplace = HomePlace()
        self.order = order
        self.freq = freq
        self.qdb = Questdb()
        try:
            self.factor_infos = self.qdb.get_data(
                f"select * from factor_infos where order='{self.order}' and freq='{self.freq}'"
            )
        except Exception:
            self.factor_infos = pd.DataFrame()
        self.name = name
        self.place = place
        self.son_name = son_name
        if (self.place is None) or (self.name is None):
            final_line = self.qdb.get_data(
                f"select * from factor_infos where order='{self.order}' and freq='{self.freq}'"
            )
            self.name = final_line.name.iloc[0]
            self.place = final_line.place.iloc[0]
        if son_name is None:
            self.file = f"Âõ†Â≠ê{self.order}_{self.name}_{self.freq}_{self.place}.parquet"
            self.son_factors = {}
            if self.factor_infos.shape[0] > 0:
                for row in self.factor_infos.dropna().itertuples():
                    self.son_factors[row.son_name] = FactorDone(
                        order=row.order,
                        name=row.name,
                        place=row.place,
                        son_name=row.son_name,
                        freq=row.freq,
                    )
        else:
            self.file = f"Âõ†Â≠ê{self.order}_{self.name}_{self.son_name}_{self.freq}_{self.place}.parquet"


    def __call__(self, son_name: str = None) -> Union[pd.DataFrame, dict]:
        if son_name is None:
            return pd.read_parquet(self.homeplace.final_factor_file + self.file)
        else:
            return self.son_factors[son_name]()

    def save_factor(self, factor: pd.DataFrame):
        try:
            son_info = self.qdb.get_data(
                f"select * from factor_infos where order='{self.order}' and son_name='{self.son_name}' and freq='{self.freq}'"
            )
        except Exception:
            logger.warning(f"Êú¨Ê¨°‰∏∫Á¨¨‰∏ÄÊ¨°ÂÜôÂÖ•{self.name}_{self.son_name}Âõ†Â≠ê")
            son_info = pd.DataFrame()
        if son_info.shape[0] == 0:
            self.qdb.write_via_df(
                pd.DataFrame(
                    {
                        "order": [self.order],
                        "name": [self.name],
                        "place": [self.place],
                        "son_name": [self.son_name],
                        "freq": [self.freq],
                    }
                ),
                "factor_infos",
            )
        factor.to_parquet(self.homeplace.final_factor_file + self.file)
