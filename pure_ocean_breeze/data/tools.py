"""
é’ˆå¯¹ä¸€äº›ä¸å¸¸è§çš„æ–‡ä»¶æ ¼å¼ï¼Œè¯»å–æ•°æ®æ–‡ä»¶çš„ä¸€äº›å·¥å…·å‡½æ•°ï¼Œä»¥åŠå…¶ä»–æ•°æ®å·¥å…·
"""

__updated__ = "2023-07-10 12:46:10"

import os
import pandas as pd
import tqdm.auto
import datetime
import scipy.io as scio
import numpy as np
import numpy_ext as npext
import knockknock as kk
import scipy.stats as ss
from functools import reduce, partial
from loguru import logger
from typing import Callable, Union, Dict, List, Tuple

try:
    import rqdatac

    rqdatac.init()
except Exception:
    print("æš‚æ—¶æœªè¿æ¥ç±³ç­")
from pure_ocean_breeze.state.homeplace import HomePlace
import deprecation
from pure_ocean_breeze import __version__
from pure_ocean_breeze.state.decorators import do_on_dfs

try:
    homeplace = HomePlace()
except Exception:
    print("æ‚¨æš‚æœªåˆå§‹åŒ–ï¼ŒåŠŸèƒ½å°†å—é™")


def is_notebook() -> bool:
    try:
        shell = get_ipython().__class__.__name__
        if shell == "ZMQInteractiveShell":
            return True  # Jupyter notebook or qtconsole
        elif shell == "TerminalInteractiveShell":
            return False  # Terminal running IPython
        else:
            return False  # Other type (?)
    except NameError:
        return False


@do_on_dfs
def read_mat(path: str) -> pd.DataFrame:
    """è¯»å–matæ–‡ä»¶

    Parameters
    ----------
    path : str
        matæ–‡ä»¶è·¯å¾„

    Returns
    -------
    `pd.DataFrame`
        å­—å…¸çš„ç¬¬4ä¸ªvalue
    """
    return list(scio.loadmat(path).values())[3]


@do_on_dfs
def convert_code(x: str) -> Tuple[str, str]:
    """å°†ç±³ç­ä»£ç è½¬æ¢ä¸ºwindä»£ç ï¼Œå¹¶è¯†åˆ«å…¶æ˜¯è‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°

    Parameters
    ----------
    x : str
        ç±³ç­çš„è‚¡ç¥¨/æŒ‡æ•°ä»£ç ï¼Œä»¥ XSHE æˆ– XSHG ç»“å°¾

    Returns
    -------
    `Tuple[str,str]`
        è½¬æ¢åçš„è‚¡ç¥¨/æŒ‡æ•°ä»£ç ï¼Œä»¥åŠè¯¥ä»£ç å±äºè‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°
    """
    x1 = x.split("/")[-1].split(".")[0]
    x2 = x.split("/")[-1].split(".")[1]
    if x2 == "XSHE":
        x2 = ".SZ"
    elif x2 == "XSHG":
        x2 = ".SH"
    elif x2 == "SZ":
        x2 = ".XSHE"
    elif x2 == "SH":
        x2 = ".XSHG"
    x = x1 + x2
    if (x1[0] == "0" or x1[:2] == "30") and x2 in [".SZ", ".XSHE"]:
        kind = "stock"
    elif x1[0] == "6" and x2 in [".SH", ".XSHG"]:
        kind = "stock"
    else:
        kind = "index"
    return x, kind


@do_on_dfs
def add_suffix(code: str) -> str:
    """ç»™è‚¡ç¥¨ä»£ç åŠ ä¸Šåç¼€

    Parameters
    ----------
    code : str
        çº¯æ•°å­—ç»„æˆçš„å­—ç¬¦ä¸²ç±»å‹çš„è‚¡ç¥¨ä»£ç ï¼Œå¦‚000001

    Returns
    -------
    str
        æ·»åŠ å®Œåç¼€åçš„è‚¡ç¥¨ä»£ç ï¼Œå¦‚000001.SZ
    """
    if not isinstance(code, str):
        code = str(code)
    if len(code) < 6:
        code = "0" * (6 - len(code)) + code
    if code.startswith("0") or code.startswith("3"):
        code = ".".join([code, "SZ"])
    elif code.startswith("6"):
        code = ".".join([code, "SH"])
    elif code.startswith("8"):
        code = ".".join([code, "BJ"])
    return code


@do_on_dfs
def get_value(df: pd.DataFrame, n: int) -> pd.DataFrame:
    """å¾ˆå¤šå› å­è®¡ç®—æ—¶ï¼Œä¼šä¸€æ¬¡æ€§ç”Ÿæˆå¾ˆå¤šå€¼ï¼Œä½¿ç”¨æ—¶åªå–å‡ºä¸€ä¸ªå€¼

    Parameters
    ----------
    df : pd.DataFrame
        æ¯ä¸ªvalueæ˜¯ä¸€ä¸ªåˆ—è¡¨æˆ–å…ƒç»„çš„pd.DataFrame
    n : int
        å–ç¬¬nä¸ªå€¼

    Returns
    -------
    `pd.DataFrame`
        ä»…æœ‰ç¬¬nä¸ªå€¼æ„æˆçš„pd.DataFrame
    """

    def get_value_single(x, n):
        try:
            return x[n]
        except Exception:
            return np.nan

    df = df.applymap(lambda x: get_value_single(x, n))
    return df


@do_on_dfs
def indus_name(df: pd.DataFrame, col_name: str = None) -> pd.DataFrame:
    """å°†2021ç‰ˆç”³ä¸‡è¡Œä¸šçš„ä»£ç ï¼Œè½¬åŒ–ä¸ºå¯¹åº”è¡Œä¸šçš„åå­—

    Parameters
    ----------
    df : pd.DataFrame
        ä¸€ä¸ªåŒ…å«ç”³ä¸‡ä¸€çº§è¡Œä¸šä»£ç çš„pd.DataFrameï¼Œå…¶ä¸­æŸä¸€åˆ—æˆ–indexä¸ºè¡Œä¸šä»£ç 
    col_name : str, optional
        ä»…æŸåˆ—ä¸ºè¡Œä¸šä»£ç æ—¶æŒ‡å®šè¯¥å‚æ•°ï¼Œè¯¥åˆ—çš„åå­—ï¼Œå¦åˆ™é»˜è®¤è½¬åŒ–index, by default None

    Returns
    -------
    `pd.DataFrame`
        è½¬åŒ–åçš„pd.DataFrame
    """
    names = pd.DataFrame(
        {
            "indus_we_cant_same": [
                "801170.SI",
                "801010.SI",
                "801140.SI",
                "801080.SI",
                "801780.SI",
                "801110.SI",
                "801230.SI",
                "801950.SI",
                "801180.SI",
                "801040.SI",
                "801740.SI",
                "801890.SI",
                "801770.SI",
                "801960.SI",
                "801200.SI",
                "801120.SI",
                "801710.SI",
                "801720.SI",
                "801880.SI",
                "801750.SI",
                "801050.SI",
                "801790.SI",
                "801150.SI",
                "801980.SI",
                "801030.SI",
                "801730.SI",
                "801160.SI",
                "801130.SI",
                "801210.SI",
                "801970.SI",
                "801760.SI",
            ],
            "è¡Œä¸šåç§°": [
                "äº¤é€šè¿è¾“",
                "å†œæ—ç‰§æ¸”",
                "è½»å·¥åˆ¶é€ ",
                "ç”µå­",
                "é“¶è¡Œ",
                "å®¶ç”¨ç”µå™¨",
                "ç»¼åˆ",
                "ç…¤ç‚­",
                "æˆ¿åœ°äº§",
                "é’¢é“",
                "å›½é˜²å†›å·¥",
                "æœºæ¢°è®¾å¤‡",
                "é€šä¿¡",
                "çŸ³æ²¹çŸ³åŒ–",
                "å•†è´¸é›¶å”®",
                "é£Ÿå“é¥®æ–™",
                "å»ºç­‘ææ–™",
                "å»ºç­‘è£…é¥°",
                "æ±½è½¦",
                "è®¡ç®—æœº",
                "æœ‰è‰²é‡‘å±",
                "éé“¶é‡‘è",
                "åŒ»è¯ç”Ÿç‰©",
                "ç¾å®¹æŠ¤ç†",
                "åŸºç¡€åŒ–å·¥",
                "ç”µåŠ›è®¾å¤‡",
                "å…¬ç”¨äº‹ä¸š",
                "çººç»‡æœé¥°",
                "ç¤¾ä¼šæœåŠ¡",
                "ç¯ä¿",
                "ä¼ åª’",
            ],
        }
    ).sort_values(["indus_we_cant_same"])
    if col_name:
        names = names.rename(columns={"indus_we_cant_same": col_name})
        df = pd.merge(df, names, on=[col_name])
    else:
        df = df.reset_index()
        df = df.rename(columns={list(df.columns)[0]: "indus_we_cant_same"})
        df = (
            pd.merge(df, names, on=["indus_we_cant_same"])
            .set_index("è¡Œä¸šåç§°")
            .drop(columns=["indus_we_cant_same"])
        )
    return df


def rqdatac_show_used() -> float:
    """æŸ¥è¯¢æµé‡ä½¿ç”¨æƒ…å†µ

    Returns
    -------
    `float`
        å½“æ—¥å·²ç»ä½¿ç”¨çš„æµé‡MBæ•°
    """
    user2 = round(rqdatac.user.get_quota()["bytes_used"] / 1024 / 1024, 2)
    print(f"ä»Šæ—¥å·²ä½¿ç”¨rqsdkæµé‡{user2}MB")
    return user2


@do_on_dfs
def add_suffix(code: str) -> str:
    """ç»™æ²¡æœ‰åç¼€çš„è‚¡ç¥¨ä»£ç åŠ ä¸Šwindåç¼€

    Parameters
    ----------
    code : str
        æ²¡æœ‰åç¼€çš„è‚¡ç¥¨ä»£ç 

    Returns
    -------
    `str`
        åŠ å®Œwindåç¼€çš„è‚¡ç¥¨ä»£ç 
    """
    if code.startswith("0") or code.startswith("3"):
        code = code + ".SZ"
    elif code.startswith("6"):
        code = code + ".SH"
    elif code.startswith("8"):
        code = code + ".BJ"
    else:
        code = code + ".UN"
    return code


@do_on_dfs
def ç”Ÿæˆæ¯æ—¥åˆ†ç±»è¡¨(
    df: pd.DataFrame, code: str, entry: str, exit: str, kind: str
) -> pd.DataFrame:
    """
    ```
    dfæ˜¯è¦åŒ…å«ä»»æ„å¤šåˆ—çš„è¡¨æ ¼ï¼Œä¸ºdataframeæ ¼å¼ï¼Œä¸»è¦å†…å®¹ä¸ºï¼Œæ¯ä¸€è¡Œæ˜¯
    ä¸€åªè‚¡ç¥¨æˆ–ä¸€åªåŸºé‡‘çš„ä»£ç ã€åˆ†ç±»ã€è¿›å…¥è¯¥åˆ†ç±»çš„æ—¶é—´ã€ç§»é™¤è¯¥åˆ†ç±»çš„æ—¶é—´ï¼Œ
    é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜å¯ä»¥åŒ…å«å¾ˆå¤šå…¶ä»–å†…å®¹
    codeæ˜¯è‚¡ç¥¨ä»£ç åˆ—çš„åˆ—åï¼Œä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼›
    entryæ˜¯è‚¡ç¥¨è¿›å…¥è¯¥åˆ†ç±»çš„æ—¥æœŸçš„åˆ—åï¼Œä¸ºå­—ç¬¦ä¸²æ ¼å¼
    exitæ˜¯è‚¡ç¥¨é€€å‡ºè¯¥åˆ†ç±»çš„æ—¥æœŸçš„åˆ—åï¼Œä¸ºå­—ç¬¦ä¸²æ ¼å¼
    kindæ˜¯åˆ†ç±»åˆ—çš„åˆ—åï¼Œä¸ºå­—ç¬¦ä¸²æ ¼å¼
    ```
    """
    df = df[[code, entry, exit, kind]]
    df = df.fillna(int(datetime.datetime.now().strftime("%Y%m%d")))
    try:
        if type(df[entry].iloc[0]) == str:
            df[entry] = df[entry].astype(str)
            df[exit] = df[exit].astype(str)
        else:
            df[entry] = df[entry].astype(int).astype(str)
            df[exit] = df[exit].astype(int).astype(str)
    except Exception:
        print("æ‚¨çš„è¿›å…¥æ—¥æœŸå’Œæ¨å‡ºæ—¥æœŸï¼Œæ—¢ä¸æ˜¯å­—ç¬¦ä¸²ï¼Œåˆä¸æ˜¯æ•°å­—æ ¼å¼ï¼Œå¥½å¥½æ£€æŸ¥ä¸€ä¸‹å§")
    df = df.set_index([code, kind])
    df = df.stack().to_frame(name="date")

    def fill_middle(df1):
        min_time = df1.date.min()
        max_time = df1.date.max()
        df2 = pd.DataFrame({"date": pd.date_range(min_time, max_time)})
        return df2

    ff = df.reset_index().groupby([code, kind]).apply(fill_middle)
    ff = ff.reset_index()
    ff = ff[[code, kind, "date"]]
    ff = ff[ff.date >= pd.Timestamp("2004-01-01")]
    return ff


@do_on_dfs
def set_index_first(df: pd.DataFrame) -> pd.DataFrame:
    """å°†dataframeçš„ç¬¬ä¸€åˆ—ï¼Œæ— è®ºå…¶æ˜¯ä»€ä¹ˆåå­—ï¼Œéƒ½è®¾ç½®ä¸ºindex

    Parameters
    ----------
    df : pd.DataFrame
        è¦ä¿®æ”¹çš„dataframe
    Returns
    -------
    pd.DataFrame
        ä¿®æ”¹åçš„dataframe
    """
    df = df.set_index(list(df.columns)[0])
    return df


def merge_many(
    dfs: List[pd.DataFrame], names: list = None, how: str = "outer"
) -> pd.DataFrame:
    """å°†å¤šä¸ªå®½dataframeä¾æ®columnså’Œindexï¼Œæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œæ‹¼æˆä¸€ä¸ªé•¿dataframe

    Parameters
    ----------
    dfs : List[pd.DataFrame]
        å°†æ‰€æœ‰è¦æ‹¼æ¥çš„å®½è¡¨æ”¾åœ¨ä¸€ä¸ªåˆ—è¡¨é‡Œ
    names : list, optional
        æ‹¼æ¥åï¼Œæ¯ä¸€åˆ—å®½è¡¨å¯¹åº”çš„åå­—, by default None
    how : str, optional
        æ‹¼æ¥çš„æ–¹å¼, by default 'outer'

    Returns
    -------
    pd.DataFrame
        æ‹¼æ¥åçš„dataframe
    """
    num = len(dfs)
    if names is None:
        names = [f"fac{i+1}" for i in range(num)]
    dfs = [i.stack().reset_index() for i in dfs]
    dfs = [i.rename(columns={list(i.columns)[-1]: j}) for i, j in zip(dfs, names)]
    dfs = [
        i.rename(columns={list(i.columns)[-2]: "code", list(i.columns)[0]: "date"})
        for i in dfs
    ]
    df = reduce(lambda x, y: pd.merge(x, y, on=["date", "code"], how=how), dfs)
    return df


class pure_dawn(object):
    """
    å› å­åˆ‡å‰²è®ºçš„æ¯æ¡†æ¶ï¼Œå¯ä»¥å¯¹ä¸¤ä¸ªå› å­è¿›è¡Œç±»ä¼¼äºå› å­åˆ‡å‰²çš„æ“ä½œ
    å¯ç”¨äºæ´¾ç”Ÿä»»ä½•"ä»¥ä¸¤ä¸ªå› å­ç”Ÿæˆä¸€ä¸ªå› å­"çš„å­ç±»
    ä½¿ç”¨ä¸¾ä¾‹
    cutå‡½æ•°é‡Œï¼Œå¿…é¡»å¸¦æœ‰è¾“å…¥å˜é‡df,dfæœ‰ä¸¤ä¸ªcolumnsï¼Œä¸€ä¸ªåä¸º'fac1'ï¼Œä¸€ä¸ªåä¸º'fac2'ï¼Œdfæ˜¯æœ€è¿‘ä¸€ä¸ªå›çœ‹æœŸå†…çš„æ•°æ®
    ```python
    class Cut(pure_dawn):

    def cut(self,df):
        df=df.sort_values('fac1')
        df=df.assign(fac3=df.fac1*df.fac2)
        ret0=df.fac2.iloc[:4].mean()
        ret1=df.fac2.iloc[4:8].mean()
        ret2=df.fac2.iloc[8:12].mean()
        ret3=df.fac2.iloc[12:16].mean()
        ret4=df.fac2.iloc[16:].mean()
        aret0=df.fac3.iloc[:4].mean()
        aret1=df.fac3.iloc[4:8].mean()
        aret2=df.fac3.iloc[8:12].mean()
        aret3=df.fac3.iloc[12:16].mean()
        aret4=df.fac3.iloc[16:].mean()
        return ret0,ret1,ret2,ret3,ret4,aret0,aret1,aret2,aret3,aret4

    cut=Cut(ct,ret_inday)
    cut.run(cut.cut)

    cut0=get_value(cut(),0)
    cut1=get_value(cut(),1)
    cut2=get_value(cut(),2)
    cut3=get_value(cut(),3)
    cut4=get_value(cut(),4)
    ```
    """

    def __init__(self, fac1: pd.DataFrame, fac2: pd.DataFrame, *args: list) -> None:
        """å‡ ä¸ªå› å­çš„æ“ä½œï¼Œæ¯ä¸ªæœˆæ“ä½œä¸€æ¬¡

        Parameters
        ----------
        fac1 : pd.DataFrame
            å› å­å€¼1ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        fac2 : pd.DataFrame
            å› å­2ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        """
        self.fac1 = fac1
        self.fac1 = self.fac1.stack().reset_index()
        self.fac1.columns = ["date", "code", "fac1"]
        self.fac2 = fac2
        self.fac2 = self.fac2.stack().reset_index()
        self.fac2.columns = ["date", "code", "fac2"]
        fac_all = pd.merge(self.fac1, self.fac2, on=["date", "code"])
        for i, fac in enumerate(args):
            fac = fac.stack().reset_index()
            fac.columns = ["date", "code", f"fac{i+3}"]
            fac_all = pd.merge(fac_all, fac, on=["date", "code"])
        fac_all = fac_all.sort_values(["date", "code"])
        self.fac = fac_all.copy()

    def __call__(self) -> pd.DataFrame:
        """è¿”å›æœ€ç»ˆæœˆåº¦å› å­å€¼

        Returns
        -------
        `pd.DataFrame`
            æœ€ç»ˆå› å­å€¼
        """
        return self.fac.copy()

    def get_fac_long_and_tradedays(self):
        """å°†ä¸¤ä¸ªå› å­çš„çŸ©é˜µè½¬åŒ–ä¸ºé•¿åˆ—è¡¨"""
        self.tradedays = sorted(list(set(self.fac.date)))

    def get_month_starts_and_ends(self, backsee=20):
        """è®¡ç®—å‡ºæ¯ä¸ªæœˆå›çœ‹æœŸé—´çš„èµ·ç‚¹æ—¥å’Œç»ˆç‚¹æ—¥"""
        self.month_ends = [
            i
            for i, j in zip(self.tradedays[:-1], self.tradedays[1:])
            if i.month != j.month
        ]
        self.month_ends.append(self.tradedays[-1])
        self.month_starts = [
            self.find_begin(self.tradedays, i, backsee=backsee) for i in self.month_ends
        ]
        self.month_starts[0] = self.tradedays[0]

    def find_begin(self, tradedays, end_day, backsee=20):
        """æ‰¾å‡ºå›çœ‹è‹¥å¹²å¤©çš„å¼€å§‹æ—¥ï¼Œé»˜è®¤ä¸º20"""
        end_day_index = tradedays.index(end_day)
        start_day_index = end_day_index - backsee + 1
        start_day = tradedays[start_day_index]
        return start_day

    def make_monthly_factors_single_code(self, df, func, daily):
        """
        å¯¹å•ä¸€è‚¡ç¥¨æ¥è®¡ç®—æœˆåº¦å› å­
        funcä¸ºå•æœˆæ‰§è¡Œçš„å‡½æ•°ï¼Œè¿”å›å€¼åº”ä¸ºæœˆåº¦å› å­ï¼Œå¦‚ä¸€ä¸ªfloatæˆ–ä¸€ä¸ªlist
        dfä¸ºä¸€ä¸ªè‚¡ç¥¨çš„å››åˆ—è¡¨ï¼ŒåŒ…å«æ—¶é—´ã€ä»£ç ã€å› å­1å’Œå› å­2
        """
        res = {}
        if daily:
            ones = [self.find_begin(i) for i in self.tradedays[self.backsee - 1 :]]
            twos = self.tradedays[self.backsee - 1 :]
        else:
            ones = self.month_starts
            twos = self.month_ends
        for start, end in zip(ones, twos):
            this_month = df[(df.date >= start) & (df.date <= end)]
            res[end] = func(this_month)
        dates = list(res.keys())
        corrs = list(res.values())
        part = pd.DataFrame({"date": dates, "corr": corrs})
        return part

    @staticmethod
    def for_cross_via_zip(func):
        """è¿”å›å€¼ä¸ºå¤šä¸ªpd.Seriesï¼Œæ¯ä¸ªpd.Seriesçš„indexä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå•ä¸ªå› å­å€¼
        ä¾‹å¦‚
        ```python
        return (
                    pd.Series([1.54,8.77,9.99â€¦â€¦],index=['000001.SZ','000002.SZ','000004.SZ'â€¦â€¦]),
                    pd.Series([3.54,6.98,9.01â€¦â€¦],index=['000001.SZ','000002.SZ','000004.SZ'â€¦â€¦]),
                )
        ```
        ä¸Šä¾‹ä¸­ï¼Œæ¯ä¸ªè‚¡ç¥¨ä¸€å¤©è¿”å›ä¸¤ä¸ªå› å­å€¼ï¼Œæ¯ä¸ªpd.Serieså¯¹åº”ä¸€ä¸ªå› å­å€¼
        """

        def full_run(df, *args, **kwargs):
            res = func(df, *args, **kwargs)
            if isinstance(res, pd.Series):
                return res
            else:
                res = pd.concat(res, axis=1)
                res.columns = [f"fac{i}" for i in range(len(res.columns))]
                res = res.assign(fac=list(zip(*[res[i] for i in list(res.columns)])))
                return res.fac

        return full_run

    def get_monthly_factor(
        self, func, whole_cross: bool = 0, daily: bool = 0, history_file: str = None
    ):
        """è¿è¡Œè‡ªå·±å†™çš„å‡½æ•°ï¼Œè·å¾—æœˆåº¦å› å­"""
        if daily:
            iter_item = self.tradedays[self.backsee - 1 :]
        else:
            iter_item = self.month_ends
        res = []
        if history_file is not None:
            if os.path.exists(homeplace.update_data_file + history_file):
                old = pd.read_parquet(homeplace.update_data_file + history_file)
                old_date = old.index.max()
                if old_date == self.fac.date.max():
                    logger.info(f"æœ¬åœ°æ–‡ä»¶å·²ç»æ˜¯æœ€æ–°çš„äº†ï¼Œæ— éœ€è®¡ç®—")
                    self.fac = old
                else:
                    try:
                        new_date = self.find_begin(
                            self.tradedays, old_date, self.backsee
                        )
                        fac = self.fac[self.fac.date > new_date]
                        iter_item = [i for i in iter_item if i > new_date]
                        if whole_cross:
                            for end_date in tqdm.auto.tqdm(iter_item):
                                start_date = self.find_begin(
                                    self.tradedays, end_date, self.backsee
                                )
                                if start_date < end_date:
                                    df = self.fac[
                                        (self.fac.date >= start_date)
                                        & (self.fac.date <= end_date)
                                    ]
                                else:
                                    df = self.fac[self.fac.date <= end_date]
                                df = func(df)
                                df = df.to_frame().T
                                df.index = [end_date]
                                res.append(df)
                            fac = pd.concat(res).resample("M").last()
                            self.fac = pd.concat([old, fac])
                        else:
                            tqdm.auto.tqdm.pandas(
                                desc="when the dawn comes, tonight will be a memory too."
                            )
                            fac = fac.groupby(["code"]).progress_apply(
                                lambda x: self.make_monthly_factors_single_code(
                                    x, func, daily=daily
                                )
                            )
                            fac = (
                                fac.reset_index(level=1, drop=True)
                                .reset_index()
                                .set_index(["date", "code"])
                                .unstack()
                            )
                            fac.columns = [i[1] for i in list(fac.columns)]
                            fac = fac.resample("M").last()
                            self.fac = pd.concat([old, fac])
                        self.fac.to_parquet(homeplace.update_data_file + history_file)
                        logger.success(f"æœ¬åœ°æ–‡ä»¶å·²ç»æ›´æ–°å®Œæˆ")
                    except Exception:
                        logger.info(f"æœ¬åœ°æ–‡ä»¶å·²ç»æ˜¯æœ€æ–°çš„äº†ï¼Œæ— éœ€è®¡ç®—")
            else:
                logger.info("ç¬¬ä¸€æ¬¡è®¡ç®—ï¼Œè¯·è€å¿ƒç­‰å¾…â€¦â€¦")
                if whole_cross:
                    for end_date in tqdm.auto.tqdm(iter_item):
                        start_date = self.find_begin(
                            self.tradedays, end_date, self.backsee
                        )
                        if start_date < end_date:
                            df = self.fac[
                                (self.fac.date >= start_date)
                                & (self.fac.date <= end_date)
                            ]
                        else:
                            df = self.fac[self.fac.date <= end_date]
                        df = func(df)
                        df = df.to_frame().T
                        df.index = [end_date]
                        res.append(df)
                    self.fac = pd.concat(res).resample("M").last()
                else:
                    tqdm.auto.tqdm.pandas(
                        desc="when the dawn comes, tonight will be a memory too."
                    )
                    self.fac = self.fac.groupby(["code"]).progress_apply(
                        lambda x: self.make_monthly_factors_single_code(
                            x, func, daily=daily
                        )
                    )
                    self.fac = (
                        self.fac.reset_index(level=1, drop=True)
                        .reset_index()
                        .set_index(["date", "code"])
                        .unstack()
                    )
                    self.fac.columns = [i[1] for i in list(self.fac.columns)]
                    self.fac = self.fac.resample("M").last()
                self.fac.to_parquet(homeplace.update_data_file + history_file)
                logger.success(f"æœ¬åœ°æ–‡ä»¶å·²ç»å†™å…¥å®Œæˆ")
        else:
            logger.warning(
                "æ‚¨æœ¬æ¬¡è®¡ç®—æ²¡æœ‰æŒ‡å®šä»»ä½•æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œè¿™å¾ˆå¯èƒ½ä¼šå¯¼è‡´å¤§é‡çš„é‡å¤è®¡ç®—å’Œä¸å¿…è¦çš„æ—¶é—´æµªè´¹ï¼Œè¯·æ³¨æ„ï¼"
            )
            if daily:
                logger.warning(
                    "æ‚¨æŒ‡å®šçš„æ˜¯æ—¥é¢‘è®¡ç®—ï¼Œéæœˆé¢‘è®¡ç®—ï¼Œå› æ­¤å¼ºçƒˆå»ºè®®æ‚¨æŒ‡å®šhistory_fileå‚æ•°ï¼ï¼"
                )
            if whole_cross:
                for end_date in tqdm.auto.tqdm(iter_item):
                    start_date = self.find_begin(self.tradedays, end_date, self.backsee)
                    if start_date < end_date:
                        df = self.fac[
                            (self.fac.date >= start_date) & (self.fac.date <= end_date)
                        ]
                    else:
                        df = self.fac[self.fac.date <= end_date]
                    df = func(df)
                    df = df.to_frame().T
                    df.index = [end_date]
                    res.append(df)
                self.fac = pd.concat(res).resample("M").last()
            else:
                tqdm.auto.tqdm.pandas(
                    desc="when the dawn comes, tonight will be a memory too."
                )
                self.fac = self.fac.groupby(["code"]).progress_apply(
                    lambda x: self.make_monthly_factors_single_code(
                        x, func, daily=daily
                    )
                )
                self.fac = (
                    self.fac.reset_index(level=1, drop=True)
                    .reset_index()
                    .set_index(["date", "code"])
                    .unstack()
                )
                self.fac.columns = [i[1] for i in list(self.fac.columns)]
                self.fac = self.fac.resample("M").last()

    @kk.desktop_sender(title="å˜¿ï¼Œåˆ‡å‰²å®Œæˆå•¦ğŸ›")
    def run(
        self,
        func: Callable,
        backsee: int = 20,
        whole_cross: bool = 0,
        daily: bool = 0,
        history_file: str = None,
    ) -> None:
        """æ‰§è¡Œè®¡ç®—çš„æ¡†æ¶ï¼Œäº§ç”Ÿå› å­å€¼

        Parameters
        ----------
        func : Callable
            æ¯ä¸ªæœˆè¦è¿›è¡Œçš„æ“ä½œ
        backsee : int, optional
            å›çœ‹æœŸï¼Œå³æ¯ä¸ªæœˆæœˆåº•å¯¹è¿‡å»å¤šå°‘å¤©è¿›è¡Œè®¡ç®—, by default 20
        whole_cross : bool, optional
            æ˜¯å¦åŒæ—¶å–æ¨ªæˆªé¢ä¸Šæ‰€æœ‰è‚¡ç¥¨è¿›è¡Œè®¡ç®—, by default 20
        daily : bool, optional
            æ˜¯å¦æ¯æ—¥è®¡ç®—, by default 20
        history_file : str, optional
            å­˜å‚¨å†å²æ•°æ®çš„æ–‡ä»¶å, by default None
        """
        self.backsee = backsee
        self.get_fac_long_and_tradedays()
        self.get_month_starts_and_ends(backsee=backsee)
        self.get_monthly_factor(
            func, whole_cross=whole_cross, daily=daily, history_file=history_file
        )


def corr_two_daily(
    df1: pd.DataFrame,
    df2: pd.DataFrame,
    history: str = None,
    rolling_window: int = 20,
    n_jobs: int = 1,
    daily: bool = 1,
    method: str = "pearson",
) -> pd.DataFrame:
    """æ±‚ä¸¤ä¸ªå› å­ï¼Œåœ¨ç›¸åŒè‚¡ç¥¨ä¸Šï¼Œæ—¶åºä¸Šæ»šåŠ¨çª—å£ä¸‹çš„ç›¸å…³ç³»æ•°

    Parameters
    ----------
    df1 : pd.DataFrame
        ç¬¬ä¸€ä¸ªå› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    df2 : pd.DataFrame
        ç¬¬äºŒä¸ªå› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    history : str, optional
        ä»æŸå¤„è¯»å–è®¡ç®—å¥½çš„å†å²æ–‡ä»¶
    rolling_window : int, optional
        æ»šåŠ¨çª—å£, by default 20
    n_jobs : int, optional
        å¹¶è¡Œæ•°é‡, by default 1
    daily : bool, optional
        æ˜¯å¦æ¯å¤©è®¡ç®—, by default 1
    method : str, optional
        ä½¿ç”¨å“ªç§æ–¹æ³•è®¡ç®—ç›¸å…³ç³»æ•°, by default 'pearson'

    Returns
    -------
    pd.DataFrame
        ç›¸å…³ç³»æ•°åçš„ç»“æœï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    """
    if daily:
        if method == "pearson":

            def corr_in(a, b, c):
                return c.iloc[-1], np.corrcoef(a, b)[0, 1]

            return func_two_daily(
                df1=df1,
                df2=df2,
                func=corr_in,
                history=history,
                rolling_window=rolling_window,
                n_jobs=n_jobs,
            )
        elif method == "spearman":

            def corr_in(a, b, c):
                return c.iloc[-1], np.corrcoef(np.argsort(a), np.argsort(b))[0, 1]

            return func_two_daily(
                df1=df1,
                df2=df2,
                func=corr_in,
                history=history,
                rolling_window=rolling_window,
                n_jobs=n_jobs,
            )
        else:
            raise ValueError("æ‚¨è¾“å…¥çš„æ–¹æ³•æš‚ä¸æ”¯æŒ")
    else:
        if method == "pearson":

            class Cut(pure_dawn):
                def cut(self, df: pd.DataFrame):
                    return df[["fac1", "fac2"]].corr().iloc[0, 1]

            cut = Cut(df1, df2)
            cut.run(cut.cut, backsee=rolling_window, history_file=history)
            return cut()
        elif method == "spearman":

            class Cut(pure_dawn):
                def cut(self, df: pd.DataFrame):
                    return df[["fac1", "fac2"]].rank().corr().iloc[0, 1]

            cut = Cut(df1, df2)
            cut.run(cut.cut, backsee=rolling_window, history_file=history)
            return cut()
        else:
            raise ValueError("æ‚¨è¾“å…¥çš„æ–¹æ³•æš‚ä¸æ”¯æŒ")


def cov_two_daily(
    df1: pd.DataFrame,
    df2: pd.DataFrame,
    history: str = None,
    rolling_window: int = 20,
    n_jobs: int = 1,
    daily: bool = 1,
) -> pd.DataFrame:
    """æ±‚ä¸¤ä¸ªå› å­ï¼Œåœ¨ç›¸åŒè‚¡ç¥¨ä¸Šï¼Œæ—¶åºä¸Šæ»šåŠ¨çª—å£ä¸‹çš„åæ–¹å·®

    Parameters
    ----------
    df1 : pd.DataFrame
        ç¬¬ä¸€ä¸ªå› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    df2 : pd.DataFrame
        ç¬¬äºŒä¸ªå› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    history : str, optional
        ä»æŸå¤„è¯»å–è®¡ç®—å¥½çš„å†å²æ–‡ä»¶
    rolling_window : int, optional
        æ»šåŠ¨çª—å£, by default 20
    n_jobs : int, optional
        å¹¶è¡Œæ•°é‡, by default 1
    daily : bool, optional
        æ˜¯å¦æ¯å¤©è®¡ç®—, by default 1

    Returns
    -------
    pd.DataFrame
        æ±‚åæ–¹å·®åçš„ç»“æœï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    """
    if daily:

        def cov_in(a, b, c):
            return c.iloc[-1], np.cov(a, b)[0, 1]

        return func_two_daily(
            df1=df1,
            df2=df2,
            func=cov_in,
            history=history,
            rolling_window=rolling_window,
            n_jobs=n_jobs,
        )
    else:

        class Cut(pure_dawn):
            def cut(self, df: pd.DataFrame):
                return df[["fac1", "fac2"]].cov().iloc[0, 1]

        cut = Cut(df1, df2)
        cut.run(cut.cut, backsee=rolling_window, history_file=history)
        return cut()


def func_two_daily(
    df1: pd.DataFrame,
    df2: pd.DataFrame,
    func: Callable,
    history: str = None,
    rolling_window: int = 20,
    n_jobs: int = 1,
) -> pd.DataFrame:
    """æ±‚ä¸¤ä¸ªå› å­ï¼Œåœ¨ç›¸åŒè‚¡ç¥¨ä¸Šï¼Œæ—¶åºä¸Šæ»šåŠ¨çª—å£ä¸‹çš„ç›¸å…³ç³»æ•°

    Parameters
    ----------
    df1 : pd.DataFrame
        ç¬¬ä¸€ä¸ªå› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    df2 : pd.DataFrame
        ç¬¬äºŒä¸ªå› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    func : Callable
        è¦å¯¹ä¸¤åˆ—æ•°è¿›è¡Œæ“ä½œçš„å‡½æ•°
    history : str, optional
        ä»æŸå¤„è¯»å–è®¡ç®—å¥½çš„å†å²æ–‡ä»¶
    rolling_window : int, optional
        æ»šåŠ¨çª—å£, by default 20
    n_jobs : int, optional
        å¹¶è¡Œæ•°é‡, by default 1

    Returns
    -------
    pd.DataFrame
        è®¡ç®—åçš„ç»“æœï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    """

    the_func = partial(func)

    def func_rolling(df):
        df = df.sort_values(["date"])
        if df.shape[0] > rolling_window:
            df = npext.rolling_apply(
                the_func, rolling_window, df.fac1, df.fac2, df.date, n_jobs=n_jobs
            )
            return df

    homeplace = HomePlace()
    if history is not None:
        if os.path.exists(homeplace.update_data_file + history):
            old = pd.read_parquet(homeplace.update_data_file + history)
            new_end = min(df1.index.max(), df2.index.max())
            if new_end > old.index.max():
                old_end = datetime.datetime.strftime(old.index.max(), "%Y%m%d")
                logger.info(f"ä¸Šæ¬¡æ›´æ–°åˆ°äº†{old_end}")
                df1a = df1[df1.index <= old.index.max()].tail(rolling_window - 1)
                df1b = df1[df1.index > old.index.max()]
                df1 = pd.concat([df1a, df1b])
                df2a = df2[df2.index <= old.index.max()].tail(rolling_window - 1)
                df2b = df2[df2.index > old.index.max()]
                df2 = pd.concat([df2a, df2b])
                twins = merge_many([df1, df2])

                tqdm.auto.tqdm.pandas()
                corrs = twins.groupby(["code"]).progress_apply(func_rolling)
                cor = []
                for i in range(len(corrs)):
                    df = (
                        pd.DataFrame(corrs.iloc[i]).dropna().assign(code=corrs.index[i])
                    )
                    cor.append(df)
                cors = pd.concat(cor)
                cors.columns = ["date", "corr", "code"]
                cors = cors.pivot(index="date", columns="code", values="corr")
                if history is not None:
                    if os.path.exists(homeplace.update_data_file + history):
                        cors = pd.concat([old, cors])
                    cors = drop_duplicates_index(cors)
                    cors.to_parquet(homeplace.update_data_file + history)
                    new_end = datetime.datetime.strftime(cors.index.max(), "%Y%m%d")
                    logger.info(f"å·²ç»æ›´æ–°è‡³{new_end}")
                return cors
            else:
                logger.info(f"å·²ç»æ˜¯æœ€æ–°çš„äº†")
                return old
        else:
            logger.info("ç¬¬ä¸€æ¬¡è®¡ç®—ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼Œè®¡ç®—å®Œæˆåå°†å­˜å‚¨")
            twins = merge_many([df1, df2])
            tqdm.auto.tqdm.pandas()
            corrs = twins.groupby(["code"]).progress_apply(func_rolling)
            cor = []
            for i in range(len(corrs)):
                df = pd.DataFrame(corrs.iloc[i]).dropna().assign(code=corrs.index[i])
                cor.append(df)
            cors = pd.concat(cor)
            cors.columns = ["date", "corr", "code"]
            cors = cors.pivot(index="date", columns="code", values="corr")
            if history is not None:
                if os.path.exists(homeplace.update_data_file + history):
                    cors = pd.concat([old, cors])
                cors = drop_duplicates_index(cors)
                cors.to_parquet(homeplace.update_data_file + history)
                new_end = datetime.datetime.strftime(cors.index.max(), "%Y%m%d")
                logger.info(f"å·²ç»æ›´æ–°è‡³{new_end}")
            return cors
    else:
        logger.warning(
            "æ‚¨æœ¬æ¬¡è®¡ç®—æ²¡æœ‰æŒ‡å®šä»»ä½•æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œè¿™å¾ˆå¯èƒ½ä¼šå¯¼è‡´å¤§é‡çš„é‡å¤è®¡ç®—å’Œä¸å¿…è¦çš„æ—¶é—´æµªè´¹ï¼Œè¯·æ³¨æ„ï¼"
        )
        twins = merge_many([df1, df2])
        tqdm.auto.tqdm.pandas()
        corrs = twins.groupby(["code"]).progress_apply(func_rolling)
        cor = []
        for i in range(len(corrs)):
            df = pd.DataFrame(corrs.iloc[i]).dropna().assign(code=corrs.index[i])
            cor.append(df)
        cors = pd.concat(cor)
        cors.columns = ["date", "corr", "code"]
        cors = cors.pivot(index="date", columns="code", values="corr")
        return cors


@do_on_dfs
def drop_duplicates_index(new: pd.DataFrame) -> pd.DataFrame:
    """å¯¹dataframeä¾ç…§å…¶indexè¿›è¡Œå»é‡ï¼Œå¹¶ä¿ç•™æœ€ä¸Šé¢çš„è¡Œ

    Parameters
    ----------
    new : pd.DataFrame
        è¦å»é‡çš„dataframe

    Returns
    -------
    pd.DataFrame
        å»é‡åçš„dataframe
    """
    pri_name = new.index.name
    new = new.reset_index()
    new = new.rename(
        columns={
            list(new.columns)[0]: "tmp_name_for_this_function_never_same_to_others"
        }
    )
    new = new.drop_duplicates(
        subset=["tmp_name_for_this_function_never_same_to_others"], keep="first"
    )
    new = new.set_index("tmp_name_for_this_function_never_same_to_others")
    if pri_name == "tmp_name_for_this_function_never_same_to_others":
        new.index.name = "date"
    else:
        new.index.name = pri_name
    return new


def select_max(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:
    """ä¸¤ä¸ªcolumnsä¸indexå®Œå…¨ç›¸åŒçš„dfï¼Œæ¯ä¸ªå€¼éƒ½æŒ‘å‡ºè¾ƒå¤§å€¼

    Parameters
    ----------
    df1 : pd.DataFrame
        ç¬¬ä¸€ä¸ªdf
    df2 : pd.DataFrame
        ç¬¬äºŒä¸ªdf

    Returns
    -------
    `pd.DataFrame`
        ä¸¤ä¸ªdfæ¯ä¸ªvalueä¸­çš„è¾ƒå¤§è€…
    """
    return (df1 + df2 + np.abs(df1 - df2)) / 2


def select_min(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:
    """ä¸¤ä¸ªcolumnsä¸indexå®Œå…¨ç›¸åŒçš„dfï¼Œæ¯ä¸ªå€¼éƒ½æŒ‘å‡ºè¾ƒå°å€¼

    Parameters
    ----------
    df1 : pd.DataFrame
        ç¬¬ä¸€ä¸ªdf
    df2 : pd.DataFrame
        ç¬¬äºŒä¸ªdf

    Returns
    -------
    `pd.DataFrame`
        ä¸¤ä¸ªdfæ¯ä¸ªvalueä¸­çš„è¾ƒå°è€…
    """
    return (df1 + df2 - np.abs(df1 - df2)) / 2


@do_on_dfs
def debj(df: pd.DataFrame) -> pd.DataFrame:
    """å»é™¤å› å­ä¸­çš„åŒ—äº¤æ‰€æ•°æ®

    Parameters
    ----------
    df : pd.DataFrame
        åŒ…å«åŒ—äº¤æ‰€çš„å› å­dataframeï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 

    Returns
    -------
    pd.DataFrame
        å»é™¤åŒ—äº¤æ‰€è‚¡ç¥¨çš„å› å­dataframe
    """
    df = df[[i for i in list(df.columns) if i[0] in ["0", "3", "6"]]]
    return df


@do_on_dfs
def standardlize(df: pd.DataFrame, all_pos: bool = 0) -> pd.DataFrame:
    """å¯¹å› å­dataframeåšæ¨ªæˆªé¢z-scoreæ ‡å‡†åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        è¦åšä¸­æ€§åŒ–çš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    all_pos : bool, optional
        æ˜¯å¦è¦å°†å€¼éƒ½å˜æˆæ­£æ•°ï¼Œé€šè¿‡å‡å»æˆªé¢çš„æœ€å°å€¼å®ç°, by default 0

    Returns
    -------
    pd.DataFrame
        æ ‡å‡†åŒ–ä¹‹åçš„å› å­
    """
    df = ((df.T - df.T.mean()) / df.T.std()).T
    if all_pos:
        df = (df.T - df.T.min()).T
    return df


@do_on_dfs
def count_value(df: pd.DataFrame, with_zero: bool = 0) -> int:
    """è®¡ç®—dataframeä¸­æ€»å…±æœ‰å¤šå°‘ï¼ˆé0ï¼‰éç©ºçš„å€¼

    Parameters
    ----------
    df : pd.DataFrame
        è¦æ£€æµ‹çš„dataframe
    with_zero : bool, optional
        ç»Ÿè®¡æ•°é‡æ—¶ï¼Œæ˜¯å¦ä¹ŸæŠŠå€¼ä¸º0çš„æ•°æ®ç»Ÿè®¡è¿›å», by default 0

    Returns
    -------
    int
        ï¼ˆé0ï¼‰éç©ºçš„æ•°æ®çš„ä¸ªæ•°
    """
    y = np.sign(np.abs(df))
    if with_zero:
        y = np.sign(y + 1)
    return y.sum().sum()


@do_on_dfs
def detect_nan(df: pd.DataFrame) -> bool:
    """æ£€æŸ¥ä¸€ä¸ªpd.DataFrameä¸­æ˜¯å¦å­˜åœ¨ç©ºå€¼

    Parameters
    ----------
    df : pd.DataFrame
        å¾…æ£€æŸ¥çš„pd.DataFrame

    Returns
    -------
    `bool`
        æ£€æŸ¥ç»“æœï¼Œæœ‰ç©ºå€¼ä¸ºTrueï¼Œå¦åˆ™ä¸ºFalse
    """
    x = df.isna() + 0
    if x.sum().sum():
        print("å­˜åœ¨ç©ºå€¼")
        return True
    else:
        print("ä¸å­˜åœ¨ç©ºå€¼")
        return False


@do_on_dfs
def get_abs(df: pd.DataFrame, quantile: float = None, square: bool = 0) -> pd.DataFrame:
    """å‡å€¼è·ç¦»åŒ–ï¼šè®¡ç®—å› å­ä¸æˆªé¢å‡å€¼çš„è·ç¦»

    Parameters
    ----------
    df : pd.DataFrame
        æœªå‡å€¼è·ç¦»åŒ–çš„å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 
    quantile : bool, optional
        ä¸º1åˆ™è®¡ç®—åˆ°æŸä¸ªåˆ†ä½ç‚¹çš„è·ç¦», by default None
    square : bool, optional
        ä¸º1åˆ™è®¡ç®—è·ç¦»çš„å¹³æ–¹, by default 0

    Returns
    -------
    `pd.DataFrame`
        å‡å€¼è·ç¦»åŒ–ä¹‹åçš„å› å­å€¼
    """
    if not square:
        if quantile is not None:
            return np.abs((df.T - df.T.quantile(quantile)).T)
        else:
            return np.abs((df.T - df.T.mean()).T)
    else:
        if quantile is not None:
            return ((df.T - df.T.quantile(quantile)).T) ** 2
        else:
            return ((df.T - df.T.mean()).T) ** 2


@do_on_dfs
def get_normal(df: pd.DataFrame) -> pd.DataFrame:
    """å°†å› å­æ¨ªæˆªé¢æ­£æ€åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        åŸå§‹å› å­ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 

    Returns
    -------
    `pd.DataFrame`
        æ¯ä¸ªæ¨ªæˆªé¢éƒ½å‘ˆç°æ­£æ€åˆ†å¸ƒçš„å› å­
    """
    df = df.replace(0, np.nan)
    df = df.T.apply(lambda x: ss.boxcox(x)[0]).T
    return df


@do_on_dfs
def coin_reverse(
    ret20: pd.DataFrame, vol20: pd.DataFrame, mean: bool = 1, positive_negtive: bool = 0
) -> pd.DataFrame:
    """çƒé˜Ÿç¡¬å¸æ³•ï¼šæ ¹æ®vol20çš„å¤§å°ï¼Œç¿»è½¬ä¸€åŠret20ï¼ŒæŠŠvol20è¾ƒå¤§çš„éƒ¨åˆ†ï¼Œç»™ret20æ·»åŠ è´Ÿå·

    Parameters
    ----------
    ret20 : pd.DataFrame
        è¦è¢«ç¿»è½¬çš„å› å­ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    vol20 : pd.DataFrame
        ç¿»è½¬çš„ä¾æ®ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    mean : bool, optional
        ä¸º1åˆ™ä»¥æ˜¯å¦å¤§äºæˆªé¢å‡å€¼ä¸ºæ ‡å‡†ç¿»è½¬ï¼Œå¦åˆ™ä»¥æ˜¯å¦å¤§äºæˆªé¢ä¸­ä½æ•°ä¸ºæ ‡å‡†, by default 1
    positive_negtive : bool, optional
        æ˜¯å¦æˆªé¢ä¸Šæ­£è´Ÿå€¼çš„ä¸¤éƒ¨åˆ†ï¼Œå„ç¿»è½¬ä¸€åŠ, by default 0

    Returns
    -------
    `pd.DataFrame`
        ç¿»è½¬åçš„å› å­å€¼
    """
    if positive_negtive:
        if not mean:
            down20 = np.sign(ret20)
            down20 = down20.replace(1, np.nan)
            down20 = down20.replace(-1, 1)

            vol20_down = down20 * vol20
            vol20_down = (vol20_down.T - vol20_down.T.median()).T
            vol20_down = np.sign(vol20_down)
            ret20_down = ret20[ret20 < 0]
            ret20_down = vol20_down * ret20_down

            up20 = np.sign(ret20)
            up20 = up20.replace(-1, np.nan)

            vol20_up = up20 * vol20
            vol20_up = (vol20_up.T - vol20_up.T.median()).T
            vol20_up = np.sign(vol20_up)
            ret20_up = ret20[ret20 > 0]
            ret20_up = vol20_up * ret20_up

            ret20_up = ret20_up.replace(np.nan, 0)
            ret20_down = ret20_down.replace(np.nan, 0)
            new_ret20 = ret20_up + ret20_down
            new_ret20_tr = new_ret20.replace(0, np.nan)
            return new_ret20_tr
        else:
            down20 = np.sign(ret20)
            down20 = down20.replace(1, np.nan)
            down20 = down20.replace(-1, 1)

            vol20_down = down20 * vol20
            vol20_down = (vol20_down.T - vol20_down.T.mean()).T
            vol20_down = np.sign(vol20_down)
            ret20_down = ret20[ret20 < 0]
            ret20_down = vol20_down * ret20_down

            up20 = np.sign(ret20)
            up20 = up20.replace(-1, np.nan)

            vol20_up = up20 * vol20
            vol20_up = (vol20_up.T - vol20_up.T.mean()).T
            vol20_up = np.sign(vol20_up)
            ret20_up = ret20[ret20 > 0]
            ret20_up = vol20_up * ret20_up

            ret20_up = ret20_up.replace(np.nan, 0)
            ret20_down = ret20_down.replace(np.nan, 0)
            new_ret20 = ret20_up + ret20_down
            new_ret20_tr = new_ret20.replace(0, np.nan)
            return new_ret20_tr
    else:
        if not mean:
            vol20_dummy = np.sign((vol20.T - vol20.T.median()).T)
            ret20 = ret20 * vol20_dummy
            return ret20
        else:
            vol20_dummy = np.sign((vol20.T - vol20.T.mean()).T)
            ret20 = ret20 * vol20_dummy
            return ret20


@do_on_dfs
def multidfs_to_one(*args: list) -> pd.DataFrame:
    """å¾ˆå¤šä¸ªdfï¼Œå„æœ‰ä¸€éƒ¨åˆ†ï¼Œå…¶ä½™ä½ç½®éƒ½æ˜¯ç©ºï¼Œ
    æƒ³æŠŠå„è‡ªdfæœ‰å€¼çš„éƒ¨åˆ†ä¿ç•™ï¼Œéƒ½æ²¡æœ‰å€¼çš„éƒ¨åˆ†ç»§ç»­è®¾ä¸ºç©º

    Returns
    -------
    `pd.DataFrame`
        åˆå¹¶åçš„df
    """
    dfs = [i.fillna(0) for i in args]
    background = np.sign(np.abs(np.sign(sum(dfs))) + 1).replace(1, 0)
    dfs = [(i + background).fillna(0) for i in dfs]
    df_nans = [i.isna() for i in dfs]
    nan = reduce(lambda x, y: x * y, df_nans)
    nan = nan.replace(1, np.nan)
    nan = nan.replace(0, 1)
    df_final = sum(dfs) * nan
    return df_final


@do_on_dfs
def to_percent(x: float) -> Union[float, str]:
    """æŠŠå°æ•°è½¬åŒ–ä¸º2ä½å°æ•°çš„ç™¾åˆ†æ•°

    Parameters
    ----------
    x : float
        è¦è½¬æ¢çš„å°æ•°

    Returns
    -------
    Union[float,str]
        ç©ºå€¼åˆ™ä¾ç„¶ä¸ºç©ºï¼Œå¦åˆ™è¿”å›å¸¦%çš„å­—ç¬¦ä¸²
    """
    if np.isnan(x):
        return x
    else:
        x = str(round(x * 100, 2)) + "%"
        return x


@do_on_dfs
def calc_exp_list(window: int, half_life: int) -> np.ndarray:
    """ç”ŸæˆåŠè¡°åºåˆ—

    Parameters
    ----------
    window : int
        çª—å£æœŸ
    half_life : int
        åŠè¡°æœŸ

    Returns
    -------
    `np.ndarray`
        åŠè¡°åºåˆ—
    """
    exp_wt = np.asarray([0.5 ** (1 / half_life)] * window) ** np.arange(window)
    return exp_wt[::-1] / np.sum(exp_wt)


@do_on_dfs
def calcWeightedStd(series: pd.Series, weights: Union[pd.Series, np.ndarray]) -> float:
    """è®¡ç®—åŠè¡°åŠ æƒæ ‡å‡†å·®

    Parameters
    ----------
    series : pd.Series
        ç›®æ ‡åºåˆ—
    weights : Union[pd.Series,np.ndarray]
        æƒé‡åºåˆ—

    Returns
    -------
    `float`
        åŠè¡°åŠ æƒæ ‡å‡†å·®
    """
    weights /= np.sum(weights)
    return np.sqrt(np.sum((series - np.mean(series)) ** 2 * weights))


def get_list_std(delta_sts: List[pd.DataFrame]) -> pd.DataFrame:
    """åŒä¸€å¤©å¤šä¸ªå› å­ï¼Œè®¡ç®—è¿™äº›å› å­åœ¨å½“å¤©çš„æ ‡å‡†å·®

    Parameters
    ----------
    delta_sts : List[pd.DataFrame]
        å¤šä¸ªå› å­æ„æˆçš„listï¼Œæ¯ä¸ªå› å­indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç 

    Returns
    -------
    `pd.DataFrame`
        æ¯å¤©æ¯åªè‚¡ç¥¨å¤šä¸ªå› å­çš„æ ‡å‡†å·®
    """
    delta_sts_mean = sum(delta_sts) / len(delta_sts)
    delta_sts_std = [(i - delta_sts_mean) ** 2 for i in delta_sts]
    delta_sts_std = sum(delta_sts_std)
    delta_sts_std = delta_sts_std**0.5 / len(delta_sts) ** 0.5
    return delta_sts_std


def get_list_std_weighted(delta_sts: List[pd.DataFrame], weights: list) -> pd.DataFrame:
    """å¯¹å¤šä¸ªdfå¯¹åº”ä½ç½®ä¸Šçš„å€¼æ±‚åŠ æƒæ ‡å‡†å·®

    Parameters
    ----------
    delta_sts : List[pd.DataFrame]
        å¤šä¸ªdataframe
    weights : list
        æƒé‡åºåˆ—

    Returns
    -------
    pd.DataFrame
        æ ‡å‡†å·®åºåˆ—
    """
    weights = [i / sum(weights) for i in weights]
    delta_sts_mean = sum(delta_sts) / len(delta_sts)
    delta_sts_std = [(i - delta_sts_mean) ** 2 for i in delta_sts]
    delta_sts_std = sum([i * j for i, j in zip(delta_sts_std, weights)])
    return delta_sts_std**0.5


@do_on_dfs
def to_group(df: pd.DataFrame, group: int = 10) -> pd.DataFrame:
    """æŠŠä¸€ä¸ªindexä¸ºæ—¶é—´ï¼Œcodeä¸ºæ—¶é—´çš„dfï¼Œæ¯ä¸ªæˆªé¢ä¸Šçš„å€¼ï¼ŒæŒ‰ç…§æ’åºåˆ†ä¸ºgroupç»„ï¼Œå°†å€¼æ”¹ä¸ºç»„å·ï¼Œä»0å¼€å§‹

    Parameters
    ----------
    df : pd.DataFrame
        è¦æ”¹ä¸ºç»„å·çš„df
    group : int, optional
        åˆ†ä¸ºå¤šå°‘ç»„, by default 10

    Returns
    -------
    pd.DataFrame
        ç»„å·ç»„æˆçš„dataframe
    """
    df = df.T.apply(lambda x: pd.qcut(x, group, labels=False, duplicates="drop")).T
    return df


def same_columns(dfs: List[pd.DataFrame]) -> List[pd.DataFrame]:
    """ä¿ç•™å¤šä¸ªdataframeå…±åŒcolumnsçš„éƒ¨åˆ†

    Parameters
    ----------
    dfs : List[pd.DataFrame]
        å¤šä¸ªdataframe

    Returns
    -------
    List[pd.DataFrame]
        ä¿ç•™å…±åŒéƒ¨åˆ†åçš„ç»“æœ
    """
    dfs = [i.T for i in dfs]
    res = []
    for i, df in enumerate(dfs):
        others = dfs[:i] + dfs[i + 1 :]

        for other in others:
            df = df[df.index.isin(other.index)]
        res.append(df.T)
    return res


def same_index(dfs: List[pd.DataFrame]) -> List[pd.DataFrame]:
    """ä¿ç•™å¤šä¸ªdataframeå…±åŒindexçš„éƒ¨åˆ†

    Parameters
    ----------
    dfs : List[pd.DataFrame]
        å¤šä¸ªdataframe

    Returns
    -------
    List[pd.DataFrame]
        ä¿ç•™å…±åŒéƒ¨åˆ†åçš„ç»“æœ
    """
    res = []
    for i, df in enumerate(dfs):
        others = dfs[:i] + dfs[i + 1 :]

        for other in others:
            df = df[df.index.isin(other.index)]
        res.append(df)
    return res


@do_on_dfs
def feather_to_parquet(folder: str):
    """å°†æŸä¸ªè·¯å¾„ä¸‹çš„æ‰€æœ‰featheræ–‡ä»¶éƒ½è½¬åŒ–ä¸ºparquetæ–‡ä»¶

    Parameters
    ----------
    folder : str
        è¦è½¬åŒ–çš„æ–‡ä»¶å¤¹è·¯å¾„
    """
    files = os.listdir(folder)
    files = [folder + i for i in files]
    for file in tqdm.auto.tqdm(files):
        try:
            df = pd.read_feather(file)
            if (("date" in list(df.columns)) and ("code" not in list(df.columns))) or (
                "index" in list(df.columns)
            ):
                df = df.set_index(list(df.columns)[0])
            df.to_parquet(file.split(".")[0] + ".parquet")
        except Exception:
            logger.warning(f"{file}ä¸æ˜¯parquetæ–‡ä»¶")


def feather_to_parquet_all():
    """å°†æ•°æ®åº“ä¸­æ‰€æœ‰çš„featheræ–‡ä»¶éƒ½è½¬åŒ–ä¸ºparquetæ–‡ä»¶"""
    homeplace = HomePlace()
    feather_to_parquet(homeplace.daily_data_file)
    feather_to_parquet(homeplace.barra_data_file)
    feather_to_parquet(homeplace.final_factor_file)
    feather_to_parquet(homeplace.update_data_file)
    feather_to_parquet(homeplace.factor_data_file)
    logger.success(
        "æ•°æ®åº“ä¸­çš„featheræ–‡ä»¶å…¨éƒ¨è¢«è½¬åŒ–ä¸ºäº†parquetæ–‡ä»¶ï¼Œæ‚¨å¯ä»¥æ‰‹åŠ¨åˆ é™¤æ‰€æœ‰çš„featheræ–‡ä»¶äº†"
    )


def zip_many_dfs(dfs: List[pd.DataFrame]) -> pd.DataFrame:
    """å°†å¤šä¸ªdataframeï¼Œæ‹¼åœ¨ä¸€èµ·ï¼Œç›¸åŒindexå’ŒcolumnsæŒ‡å‘çš„é‚£ä¸ªvaluesï¼Œå˜ä¸ºå¤šä¸ªdataframeçš„å€¼çš„åˆ—è¡¨
    é€šå¸¸ç”¨äºå­˜å‚¨æ•´åˆåˆ†é’Ÿæ•°æ®è®¡ç®—çš„å› å­å€¼

    Parameters
    ----------
    dfs : List[pd.DataFrame]
        å¤šä¸ªdataframeï¼Œæ¯ä¸€ä¸ªçš„valueséƒ½æ˜¯floatå½¢å¼

    Returns
    -------
    pd.DataFrame
        æ•´åˆåçš„dataframeï¼Œæ¯ä¸€ä¸ªvalueséƒ½æ˜¯listçš„å½¢å¼
    """
    df = merge_many(dfs)
    cols = [df[f"fac{i}"] for i in range(1, len(dfs) + 1)]
    df = df.assign(fac=pd.Series(zip(*cols)))
    df = df.pivot(index="date", columns="code", values="fac")
    return df


def get_values(df: pd.DataFrame) -> List[pd.DataFrame]:
    """ä»ä¸€ä¸ªvaluesä¸ºåˆ—è¡¨çš„dataframeä¸­ï¼Œä¸€æ¬¡æ€§å–å‡ºæ‰€æœ‰å€¼ï¼Œåˆ†åˆ«è®¾ç½®ä¸ºä¸€ä¸ªdataframeï¼Œå¹¶ä¾ç…§é¡ºåºå­˜å‚¨åœ¨åˆ—è¡¨ä¸­

    Parameters
    ----------
    df : pd.DataFrame
        ä¸€ä¸ªvaluesä¸ºlistçš„dataframe

    Returns
    -------
    List[pd.DataFrame]
        å¤šä¸ªdataframeï¼Œæ¯ä¸€ä¸ªçš„valueséƒ½æ˜¯floatå½¢å¼
    """
    d = df.dropna(how="all", axis=1)
    d = d.iloc[:, 0].dropna()
    num = len(d.iloc[0])
    facs = list(map(lambda x: get_value(df, x), range(num)))
    return facs


@do_on_dfs
def get_fac_via_corr(
    df: pd.DataFrame,
    history_file: str = None,
    backsee: int = 20,
    fillna_method: Union[float, str] = "ffill",
    corr_method: str = "pearson",
    daily: bool = 0,
    abs: bool = 0,
    riskmetrics: bool = 0,
    riskmetrics_lambda: float = 0.94,
) -> pd.DataFrame:
    """å¯¹ä¸€ä¸ªæ—¥é¢‘å› å­ï¼Œå¯¹å…¶æ»šåŠ¨æ—¶é—´çª—å£è¿›è¡Œå› å­æœˆåº¦åŒ–è®¡ç®—ã€‚
    å…·ä½“æ“ä½œä¸ºæ¯å¤©ï¼ˆæˆ–æ¯æœˆæœˆåº•ï¼‰è®¡ç®—è¿‡å»20å¤©å› å­å€¼çš„ç›¸å…³æ€§çŸ©é˜µï¼Œ
    ç„¶åå¯¹æ¯ä¸ªè‚¡ç¥¨çš„æ‰€æœ‰ç›¸å…³ç³»æ•°æ±‚å‡å€¼

    Parameters
    ----------
    df : pd.DataFrame
        æ—¥é¢‘å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºæ—¶é—´
    history_file : str, optional
        ç”¨äºå­˜å‚¨å†å²æ•°æ®çš„æœ¬åœ°æ–‡ä»¶, by default None
    backsee : int, optional
        æ»šåŠ¨çª—å£é•¿åº¦, by default 20
    fillna_method : Union[float, str], optional
        ç”±äºå­˜åœ¨ç¼ºå¤±å€¼æ—¶ï¼Œç›¸å…³æ€§çŸ©é˜µçš„è®¡ç®—å­˜åœ¨é—®é¢˜ï¼Œå› æ­¤è¿™é‡Œå¯¹å…¶è¿›è¡Œè¡¥å…¨ï¼Œå¯é€‰æ‹©è¡¥å…¨æ–¹å¼ï¼Œè¾“å…¥`â€˜ffill'`æˆ–`'bfill'`å³ä¸ºå–å‰å–åå¡«å……,è¾“å…¥æ•°å­—åˆ™ä¸ºç”¨å›ºå®šæ•°å­—å¡«å…… by default "ffill"
    corr_method : str, optional
        æ±‚ç›¸å…³æ€§çš„æ–¹æ³•ï¼Œå¯ä»¥æŒ‡å®š`'pearson'`ã€`'spearman'`ã€`'kendall'`, default 'pearson'
    daily : bool, optional
        æ˜¯å¦æ¯å¤©æ»šåŠ¨, by default 0
    abs : bool, optional
        æ˜¯å¦è¦å¯¹ç›¸å…³ç³»æ•°çŸ©é˜µå–ç»å¯¹å€¼, by default 0
    riskmetrics : bool, optional
        ä½¿ç”¨RiskMetricsæ–¹æ³•ï¼Œå¯¹ç›¸å…³æ€§è¿›è¡Œè°ƒæ•´ï¼Œå¢åŠ ä¸´è¿‘äº¤æ˜“æ—¥çš„æƒé‡, by default 0
    riskmetrics_lambda : float, optional
        ä½¿ç”¨RiskMetricsæ–¹æ³•æ—¶çš„lambdaå‚æ•°, by default 0.94

    Returns
    -------
    pd.DataFrame
        æœˆåº¦åŒ–åçš„å› å­å€¼
    """
    homeplace = HomePlace()
    if history_file is not None:
        if os.path.exists(homeplace.update_data_file + history_file):
            old = pd.read_parquet(homeplace.update_data_file + history_file)
        else:
            old = None
            logger.info("è¿™ä¸€ç»“æœæ˜¯æ–°çš„ï¼Œå°†ä»å¤´è®¡ç®—")
    else:
        old = None
    if old is not None:
        old_end = old.index.max()
        pastpart = df[df.index <= old_end]
        old_tail = pastpart.tail(backsee - 1)
        old_end_str = datetime.datetime.strftime(old_end, "%Y%m%d")
        logger.info(f"ä¸Šæ¬¡è®¡ç®—åˆ°äº†{old_end_str}")
        df = df[df.index > old_end]
        if df.shape[0] > 0:
            df = pd.concat([old_tail, df])
            ends = list(df.index)
            ends = pd.Series(ends, index=ends)
            ends = ends.resample("M").last()
            ends = list(ends)
            if daily:
                iters = list(df.index)
            else:
                iters = ends
            dfs = []
            for end in tqdm.auto.tqdm(iters):
                if isinstance(fillna_method, float):
                    df0 = (
                        df.loc[:end]
                        .tail(backsee)
                        .dropna(how="all", axis=1)
                        .fillna(fillna_method)
                        .dropna(axis=1)
                    )
                else:
                    df0 = (
                        df.loc[:end]
                        .tail(backsee)
                        .dropna(how="all", axis=1)
                        .fillna(method=fillna_method)
                        .dropna(axis=1)
                    )
                if riskmetrics:
                    df0 = (
                        (df0 - df0.mean()).T
                        * (
                            pd.Series(
                                [
                                    riskmetrics_lambda ** (backsee - i)
                                    for i in range(df0.shape[0])
                                ],
                                index=df0.index,
                            )
                            ** 0.5
                        )
                    ).T
                if corr_method == "spearman":
                    corr = df0.rank().corr()
                else:
                    corr = df0.corr(method=corr_method)
                if abs:
                    corr = corr.abs()
                df0 = corr.mean().to_frame(end)
                dfs.append(df0)
            dfs = pd.concat(dfs, axis=1).T
            dfs = drop_duplicates_index(pd.concat([old, dfs]))
            dfs.to_parquet(homeplace.update_data_file + history_file)
            if daily:
                return dfs
            else:
                return dfs.resample("M").last()
        else:
            logger.info("å·²ç»æ˜¯æœ€æ–°çš„äº†")
            return old
    else:
        ends = list(df.index)
        ends = pd.Series(ends, index=ends)
        ends = ends.resample("M").last()
        ends = list(ends)
        if daily:
            iters = list(df.index)
        else:
            iters = ends
        dfs = []
        for end in tqdm.auto.tqdm(iters):
            if isinstance(fillna_method, float):
                df0 = (
                    df.loc[:end]
                    .tail(backsee)
                    .dropna(how="all", axis=1)
                    .fillna(fillna_method)
                    .dropna(axis=1)
                )
            else:
                df0 = (
                    df.loc[:end]
                    .tail(backsee)
                    .dropna(how="all", axis=1)
                    .fillna(method=fillna_method)
                    .dropna(axis=1)
                )
            if riskmetrics:
                df0 = (
                    (df0 - df0.mean()).T
                    * (
                        pd.Series(
                            [
                                riskmetrics_lambda ** (backsee - i)
                                for i in range(df0.shape[0])
                            ],
                            index=df0.index,
                        )
                        ** 0.5
                    )
                ).T
            if corr_method == "spearman":
                corr = df0.rank().corr()
            else:
                corr = df0.corr(method=corr_method)
            if abs:
                corr = corr.abs()
            df0 = corr.mean().to_frame(end)
            dfs.append(df0)
        dfs = pd.concat(dfs, axis=1).T
        if history_file is not None:
            dfs.to_parquet(homeplace.update_data_file + history_file)
        if daily:
            return dfs
        else:
            return dfs.resample("M").last()


@do_on_dfs
def get_fac_cross_via_func(
    df: pd.DataFrame,
    func: Callable,
    history_file: str = None,
    backsee: int = 20,
    fillna_method: Union[float, str, None] = "ffill",
    daily: bool = 0,
) -> pd.DataFrame:
    """å¯¹ä¸€ä¸ªæ—¥é¢‘å› å­ï¼Œå¯¹å…¶æ»šåŠ¨æ—¶é—´çª—å£è¿›è¡Œå› å­æœˆåº¦åŒ–è®¡ç®—ã€‚
    å…·ä½“æ“ä½œä¸ºæ¯å¤©ï¼ˆæˆ–æ¯æœˆæœˆåº•ï¼‰æˆªå–è¿‡å»ä¸€æ®µçª—å£ï¼Œå¹¶è¿›è¡ŒæŸä¸ªè‡ªå®šä¹‰çš„æ“ä½œï¼Œ

    Parameters
    ----------
    df : pd.DataFrame
        æ—¥é¢‘å› å­ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºæ—¶é—´
    func : Callable
        è‡ªå®šä¹‰çš„æ“ä½œå‡½æ•°ï¼Œéœ€è¦å¯¹ä¸€ä¸ªçª—å£æ—¶é—´å†…çš„é¢æ¿æ•°æ®è¿›è¡Œå¤„ç†ï¼Œæœ€ç»ˆè¦è¿”å›ä¸€ä¸ªseriesï¼Œindexä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºæœˆåº¦åŒ–çš„å› å­å€¼ï¼Œnameæ— æ‰€è°“
    history_file : str, optional
        ç”¨äºå­˜å‚¨å†å²æ•°æ®çš„æœ¬åœ°æ–‡ä»¶, by default None
    backsee : int, optional
        æ»šåŠ¨çª—å£é•¿åº¦, by default 20
    fillna_method : Union[float, str], optional
        å¯¹ç¼ºå¤±å€¼è¿›è¡Œè¡¥å…¨ï¼Œå¯é€‰æ‹©è¡¥å…¨æ–¹å¼ï¼Œè¾“å…¥`'ffill'`æˆ–`'bfill'`å³ä¸ºå–å‰å–åå¡«å……ï¼›è¾“å…¥æ•°å­—åˆ™ä¸ºç”¨å›ºå®šæ•°å­—å¡«å……ï¼›è¾“å…¥Noneåˆ™ä¸å¡«å……ç¼ºå¤±å€¼ by default "ffill"
    daily : bool, optional
        æ˜¯å¦æ¯å¤©æ»šåŠ¨, by default 0

    Returns
    -------
    pd.DataFrame
        æœˆåº¦åŒ–åçš„å› å­å€¼
    """
    homeplace = HomePlace()
    if history_file is not None:
        if os.path.exists(homeplace.update_data_file + history_file):
            old = pd.read_parquet(homeplace.update_data_file + history_file)
        else:
            old = None
            logger.info("è¿™ä¸€ç»“æœæ˜¯æ–°çš„ï¼Œå°†ä»å¤´è®¡ç®—")
    else:
        old = None
    if old is not None:
        old_end = old.index.max()
        pastpart = df[df.index <= old_end]
        old_tail = pastpart.tail(backsee - 1)
        old_end_str = datetime.datetime.strftime(old_end, "%Y%m%d")
        logger.info(f"ä¸Šæ¬¡è®¡ç®—åˆ°äº†{old_end_str}")
        df = df[df.index > old_end]
        if df.shape[0] > 0:
            df = pd.concat([old_tail, df])
            ends = list(df.index)
            ends = pd.Series(ends, index=ends)
            ends = ends.resample("M").last()
            ends = list(ends)
            if daily:
                iters = list(df.index)
            else:
                iters = ends
            dfs = []
            for end in tqdm.auto.tqdm(iters):
                if isinstance(fillna_method, float):
                    df0 = (
                        df.loc[:end]
                        .tail(backsee)
                        .dropna(how="all", axis=1)
                        .fillna(fillna_method)
                        .dropna(axis=1)
                    )
                elif isinstance(fillna_method, str):
                    df0 = (
                        df.loc[:end]
                        .tail(backsee)
                        .dropna(how="all", axis=1)
                        .fillna(method=fillna_method)
                        .dropna(axis=1)
                    )
                else:
                    df0 = df.loc[:end].tail(backsee).dropna(how="all", axis=1)
                corr = func(df0).to_frame(end)
                dfs.append(corr)
            dfs = pd.concat(dfs, axis=1).T
            dfs = drop_duplicates_index(pd.concat([old, dfs]))
            dfs.to_parquet(homeplace.update_data_file + history_file)
            return dfs
        else:
            logger.info("å·²ç»æ˜¯æœ€æ–°çš„äº†")
            return old
    else:
        ends = list(df.index)
        ends = pd.Series(ends, index=ends)
        ends = ends.resample("M").last()
        ends = list(ends)
        if daily:
            iters = list(df.index)
        else:
            iters = ends
        dfs = []
        for end in tqdm.auto.tqdm(iters):
            if isinstance(fillna_method, float):
                df0 = (
                    df.loc[:end]
                    .tail(backsee)
                    .dropna(how="all", axis=1)
                    .fillna(fillna_method)
                    .dropna(axis=1)
                )
            elif isinstance(fillna_method, str):
                df0 = (
                    df.loc[:end]
                    .tail(backsee)
                    .dropna(how="all", axis=1)
                    .fillna(method=fillna_method)
                    .dropna(axis=1)
                )
            else:
                df0 = df.loc[:end].tail(backsee).dropna(how="all", axis=1)
            corr = func(df0).to_frame(end)
            dfs.append(corr)
        dfs = pd.concat(dfs, axis=1).T
        if history_file is not None:
            dfs.to_parquet(homeplace.update_data_file + history_file)
        return dfs


@do_on_dfs
def è®¡ç®—è¿ç»­æœŸæ•°(ret0: pd.Series, point: float = 0) -> pd.Series:
    """è®¡ç®—ä¸€åˆ—æ•°ï¼ŒæŒç»­å¤§äºæˆ–æŒç»­å°äºæŸä¸ªä¸´ç•Œç‚¹çš„æœŸæ•°

    Parameters
    ----------
    ret0 : pd.Series
        æ”¶ç›Šç‡åºåˆ—ã€æˆ–è€…æŸä¸ªæŒ‡æ ‡çš„åºåˆ—
    point : float, optional
        ä¸´ç•Œå€¼, by default 0

    Returns
    -------
    pd.Series
        æŒç»­å¤§äºæˆ–å°äºçš„æœŸæ•°
    """
    ret = ret0.copy()
    ret = ((ret >= point) + 0).replace(0, -1)
    ret = ret.to_frame("signal").assign(num=range(ret.shape[0]))
    ret.signal = ret.signal.diff().shift(-1)
    ret1 = ret[ret.signal != 0]
    ret1 = ret1.assign(duration=ret1.num.diff())
    ret = pd.concat([ret, ret1[["duration"]]], axis=1)
    ret.signal = ret.signal.diff()
    ret2 = ret[ret.signal.abs() == 2]
    ret2 = ret2.assign(add_duration=1)
    ret = pd.concat([ret, ret2[["add_duration"]]], axis=1)
    ret.duration = ret.duration.fillna(0)
    ret.add_duration = ret.add_duration.fillna(0)
    ret.duration = select_max(ret.duration, ret.add_duration)
    ret.duration = ret.duration.replace(0, np.nan).interpolate()
    return ret.duration


@do_on_dfs
def all_pos(df: pd.DataFrame) -> pd.DataFrame:
    """å°†å› å­å€¼æ¯ä¸ªæˆªé¢ä¸Šå‡å»æœ€å°å€¼ï¼Œä»è€Œéƒ½å˜æˆéè´Ÿæ•°

    Parameters
    ----------
    df : pd.DataFrame
        å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼

    Returns
    -------
    pd.DataFrame
        å˜åŒ–åéè´Ÿçš„å› å­å€¼
    """
    return (df.T - df.T.min()).T


@do_on_dfs
def clip_mad(
    df: pd.DataFrame, n: float = 5, replace: bool = 1, keep_trend: bool = 1
) -> pd.DataFrame:
    if keep_trend:
        df = df.stack().reset_index()
        df.columns = ["date", "code", "fac"]

        def clip_sing(x: pd.DataFrame, n: float = 3):
            median = x.fac.quantile(0.5)
            diff_median = ((x.fac - median).abs()).quantile(0.5)
            max_range1 = median + n * diff_median
            min_range1 = median - n * diff_median
            max_range2 = median + (n + 0.5) * diff_median
            min_range2 = median - (n + 0.5) * diff_median
            x = x.sort_values(["fac"])
            x_min = x[x.fac <= min_range1]
            x_max = x[x.fac >= max_range1]
            x_middle = x[(x.fac > min_range1) & (x.fac < max_range1)]
            x_min.fac = np.nan
            x_max.fac = np.nan
            if x_min.shape[0] >= 1:
                x_min.fac.iloc[-1] = min_range1
                if x_min.shape[0] >= 2:
                    x_min.fac.iloc[0] = min_range2
                    x_min.fac = x_min.fac.interpolate()
            if x_max.shape[0] >= 1:
                x_max.fac.iloc[-1] = max_range2
                if x_max.shape[0] >= 2:
                    x_max.fac.iloc[0] = max_range1
                    x_max.fac = x_max.fac.interpolate()
            x = pd.concat([x_min, x_middle, x_max]).sort_values(["code"])
            return x

        df = df.groupby(["date"]).apply(lambda x: clip_sing(x, n))
        try:
            df = df.reset_index()
        except Exception:
            ...
        df = df.drop_duplicates(subset=["date", "code"]).pivot(
            index="date", columns="code", values="fac"
        )
        return df
    elif replace:

        def clip_sing(x: pd.Series, n: float = 3):
            median = x.quantile(0.5)
            diff_median = ((x - median).abs()).quantile(0.5)
            max_range = median + n * diff_median
            min_range = median - n * diff_median
            x = x.where(x < max_range, max_range)
            x = x.where(x > min_range, min_range)
            return x

        df1 = df.T.apply(lambda x: clip_sing(x, n)).T
        df = np.abs(np.sign(df)) * df1
        return df
    else:
        df0 = df.T
        median = df0.quantile(0.5)
        diff_median = ((df0 - median).abs()).quantile(0.5)
        max_range = median + n * diff_median
        min_range = median - n * diff_median
        mid1 = (((df0 - min_range) >= 0) + 0).replace(0, np.nan)
        mid2 = (((df0 - max_range) <= 0) + 0).replace(0, np.nan)
        return (df0 * mid1 * mid2).T


@do_on_dfs
def clip_three_sigma(df: pd.DataFrame, n: float = 3) -> pd.DataFrame:
    df0 = df.T
    mean = df0.mean()
    std = df0.std()
    max_range = mean + n * std
    min_range = mean - n * std
    mid1 = (((df0 - min_range) >= 0) + 0).replace(0, np.nan)
    mid2 = (((df0 - max_range) <= 0) + 0).replace(0, np.nan)
    return (df0 * mid1 * mid2).T


@do_on_dfs
def clip_percentile(
    df: pd.DataFrame, min_percent: float = 0.025, max_percent: float = 0.975
) -> pd.DataFrame:
    df0 = df.T
    max_range = df0.quantile(max_percent)
    min_range = df0.quantile(min_percent)
    mid1 = (((df0 - min_range) >= 0) + 0).replace(0, np.nan)
    mid2 = (((df0 - max_range) <= 0) + 0).replace(0, np.nan)
    return (df0 * mid1 * mid2).T


@do_on_dfs
def clip(
    df: pd.DataFrame,
    mad: bool = 0,
    three_sigma: bool = 0,
    percentile: bool = 0,
    parameter: Union[float, tuple] = None,
) -> pd.DataFrame:
    """å¯¹å› å­å€¼è¿›è¡Œæˆªé¢å»æå€¼çš„æ“ä½œ

    Parameters
    ----------
    df : pd.DataFrame
        è¦å¤„ç†çš„å› å­è¡¨ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œindexä¸ºæ—¶é—´
    mad : bool, optional
        ä½¿ç”¨madæ³•å»æå€¼ï¼Œå…ˆè®¡ç®—æ‰€æœ‰å› å­ä¸å¹³å‡å€¼ä¹‹é—´çš„è·ç¦»æ€»å’Œæ¥æ£€æµ‹ç¦»ç¾¤å€¼, by default 0
    three_sigma : bool, optional
        æ ¹æ®å‡å€¼å’Œå‡ å€æ ‡å‡†å·®åšè°ƒæ•´, by default 0
    percentile : bool, optional
        æ ¹æ®ä¸Šä¸‹é™çš„åˆ†ä½æ•°å»æå€¼, by default 0
    parameter : Union[float,tuple], optional
        å‚æ•°ï¼Œmadå’Œthree_sigmaé»˜è®¤å‚æ•°ä¸º3ï¼Œè¾“å…¥floatå½¢å¼ï¼›è€Œpercentileé»˜è®¤å‚æ•°ä¸º(0.025,0.975)ï¼Œè¾“å…¥tupleå½¢å¼, by default None
    [å‚è€ƒèµ„æ–™](https://blog.csdn.net/The_Time_Runner/article/details/100118505)

    Returns
    -------
    pd.DataFrame
        å»æå€¼åçš„å‚æ•°

    Raises
    ------
    ValueError
        ä¸æŒ‡å®šæ–¹æ³•æˆ–å‚æ•°ç±»å‹é”™è¯¯ï¼Œå°†æŠ¥é”™
    """
    if mad and ((isinstance(parameter, float)) or (parameter is None)):
        return clip_mad(df, parameter)
    elif three_sigma and ((isinstance(parameter, float)) or (parameter is None)):
        return clip_three_sigma(df, parameter)
    elif percentile and ((isinstance(parameter, tuple)) or (parameter is None)):
        return clip_percentile(df, parameter[0], parameter[1])
    else:
        raise ValueError("å‚æ•°è¾“å…¥é”™è¯¯")


def judge_factor_by_third(
    fac1: pd.DataFrame, fac2: pd.DataFrame, judge: Union[pd.DataFrame, pd.Series]
) -> pd.DataFrame:
    """å¯¹äºfac1å’Œfac2ä¸¤ä¸ªå› å­ï¼Œä¾æ®judgeè¿™ä¸ªseriesæˆ–dataframeè¿›è¡Œåˆ¤æ–­ï¼Œ
    judgeå¯èƒ½ä¸ºå…¨å¸‚åœºçš„æŸä¸ªæ—¶åºæŒ‡æ ‡ï¼Œä¹Ÿå¯èƒ½æ˜¯æ¯ä¸ªè‚¡ç¥¨å„ä¸€ä¸ªçš„æŒ‡æ ‡ï¼Œ
    å¦‚æœjudgeè¿™ä¸€æœŸçš„å€¼å¤§äº0ï¼Œåˆ™å–fac1çš„å€¼ï¼Œå°äº0åˆ™å–fac2çš„å€¼

    Parameters
    ----------
    fac1 : pd.DataFrame
        å› å­1ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
    fac2 : pd.DataFrame
        å› å­2ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
    judge : Union[pd.DataFrame,pd.Series]
        å¸‚åœºæŒ‡æ ‡æˆ–ä¸ªè‚¡æŒ‡æ ‡ï¼Œä¸ºå¸‚åœºæŒ‡æ ‡æ—¶ï¼Œåˆ™è¾“å…¥serieså½¢å¼ï¼Œindexä¸ºæ—¶é—´ï¼Œvaluesä¸ºæŒ‡æ ‡å€¼
        ä¸ºä¸ªè‚¡æŒ‡æ ‡æ—¶ï¼Œåˆ™è¾“å…¥dataframeå½¢å¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼

    Returns
    -------
    pd.DataFrame
        åˆæˆåçš„å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
    """
    if isinstance(judge, pd.Series):
        judge = pd.DataFrame(
            {k: list(judge) for k in list(fac1.columns)}, index=judge.index
        )
    s1 = (judge > 0) + 0
    s2 = (judge < 0) + 0
    fac1 = fac1 * s1
    fac2 = fac2 * s2
    fac = fac1 + fac2
    have = np.sign(fac1.abs() + 1)
    return fac * have
