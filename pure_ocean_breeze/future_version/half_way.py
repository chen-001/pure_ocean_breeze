__updated__ = "2023-02-23 12:36:54"

import numpy as np
import pandas as pd
import knockknock as kk
import matplotlib.pyplot as plt

plt.style.use(["science", "no-latex", "notebook"])
plt.rcParams["axes.unicode_minus"] = False
import plotly.express as pe
import plotly.io as pio
import tqdm
import os
from loguru import logger
import datetime
from typing import Callable
from functools import reduce,partial
from pure_ocean_breeze.data.database import ClickHouseClient,Questdb
from pure_ocean_breeze.data.tools import drop_duplicates_index
from pure_ocean_breeze.labor.process import pure_moon, decap_industry
from pure_ocean_breeze.data.read_data import read_daily
from pure_ocean_breeze.state.homeplace import HomePlace


class pure_cloud(object):
    """
    ä¸ºäº†æµ‹è¯•å…¶ä»–ä¸åŒçš„é¢‘çŽ‡è€Œè®¾è®¡çš„ç±»ï¼Œä»…è€ƒè™‘äº†ä¸Šå¸‚æ»¡60å¤©è¿™ä¸€è¦ç´ 
    è¿™ä¸€å›žæµ‹é‡‡å–çš„æ–¹æ¡ˆæ˜¯ï¼Œå¯¹äºŽå›žæµ‹é¢‘çŽ‡nå¤©ï¼Œå°†åˆå§‹èµ„é‡‘ç­‰åˆ†æˆnç¬”ï¼Œæ¯å¤©ä»¥1/nçš„èµ„é‡‘è°ƒä»“
    æ¯ç¬”èµ„é‡‘ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œæœ€ç»ˆæ±‡èšæˆä¸€ä¸ªæ”¶ç›ŠçŽ‡åºåˆ—
    """

    def __init__(
        self,
        fac,
        freq,
        group=10,
        boxcox=1,
        trade_cost=0,
        print_comments=1,
        plt_plot=1,
        plotly_plot=0,
        filename="å‡€å€¼èµ°åŠ¿å›¾",
        comments_writer=None,
        nets_writer=None,
        sheet_name=None,
    ):
        """næ˜¯å›žæµ‹çš„é¢‘çŽ‡ï¼Œç­‰åˆ†æˆnä»½ï¼Œgroupæ˜¯å›žæµ‹çš„ç»„æ•°ï¼Œboxcoxæ˜¯æ˜¯å¦åšè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–"""
        self.fac = fac
        self.freq = freq
        self.group = group
        self.boxcox = boxcox
        self.trade_cost = trade_cost
        ages = read_daily(age=1)
        sts = read_daily(st=1)
        states = read_daily(state=1)
        opens = read_daily(open=1)
        closes = read_daily(close=1)
        capitals = read_daily(flow_cap=1).resample("M").last()
        moon = pure_moon()
        moon.set_basic_data(
            age=ages,
            st=sts,
            state=states,
            open=opens,
            close=closes,
            capital=capitals,
        )
        moon.prerpare()
        ages = moon.ages.copy()
        ages = (ages >= 60) + 0
        self.ages = ages.replace(0, np.nan)
        self.closes = read_daily(close=1)
        self.rets = (
            (self.closes.shift(-self.freq) / self.closes - 1) * self.ages
        ) / self.freq
        self.run(
            print_comments=print_comments,
            plt_plot=plt_plot,
            plotly_plot=plotly_plot,
            filename=filename,
        )
        if comments_writer:
            if sheet_name:
                self.long_short_comments.to_excel(
                    comments_writer, sheet_name=sheet_name
                )
            else:
                raise AttributeError("å¿…é¡»åˆ¶å®šsheet_nameå‚æ•°ðŸ¤’")
        if nets_writer:
            if sheet_name:
                self.group_nets.to_excel(nets_writer, sheet_name=sheet_name)
            else:
                raise AttributeError("å¿…é¡»åˆ¶å®šsheet_nameå‚æ•°ðŸ¤’")

    def comments(self, series, series1):
        """å¯¹twinsä¸­çš„ç»“æžœç»™å‡ºè¯„ä»·
        è¯„ä»·æŒ‡æ ‡åŒ…æ‹¬å¹´åŒ–æ”¶ç›ŠçŽ‡ã€æ€»æ”¶ç›ŠçŽ‡ã€å¹´åŒ–æ³¢åŠ¨çŽ‡ã€å¹´åŒ–å¤æ™®æ¯”çŽ‡ã€æœ€å¤§å›žæ’¤çŽ‡ã€èƒœçŽ‡"""
        ret = (series.iloc[-1] - series.iloc[0]) / series.iloc[0]
        duration = (series.index[-1] - series.index[0]).days
        year = duration / 365
        ret_yearly = (series.iloc[-1] / series.iloc[0]) ** (1 / year) - 1
        max_draw = -(series / series.expanding(1).max() - 1).min()
        vol = np.std(series1) * (250**0.5)
        sharpe = ret_yearly / vol
        wins = series1[series1 > 0]
        win_rate = len(wins) / len(series1)
        return pd.Series(
            [ret, ret_yearly, vol, sharpe, max_draw, win_rate],
            index=["æ€»æ”¶ç›ŠçŽ‡", "å¹´åŒ–æ”¶ç›ŠçŽ‡", "å¹´åŒ–æ³¢åŠ¨çŽ‡", "ä¿¡æ¯æ¯”çŽ‡", "æœ€å¤§å›žæ’¤çŽ‡", "èƒœçŽ‡"],
        )

    @kk.desktop_sender(title="å˜¿ï¼Œå˜é¢‘å›žæµ‹ç»“æŸå•¦ï½žðŸ—“")
    def run(self, print_comments, plt_plot, plotly_plot, filename):
        """å¯¹å› å­å€¼åˆ†ç»„å¹¶åŒ¹é…"""
        if self.boxcox:
            self.fac = decap_industry(self.fac,daily=1)
        self.fac = self.fac.T.apply(
            lambda x: pd.qcut(x, self.group, labels=False, duplicates="drop")
        ).T
        self.fac = self.fac.shift(1)
        self.vs = [
            (((self.fac == i) + 0).replace(0, np.nan) * self.rets).mean(axis=1)
            for i in range(self.group)
        ]
        self.group_rets = pd.DataFrame(
            {f"group{k}": list(v) for k, v in zip(range(1, self.group + 1), self.vs)},
            index=self.vs[0].index,
        )
        self.group_rets = self.group_rets.dropna(how="all")
        self.group_rets = self.group_rets
        self.group_nets = (self.group_rets + 1).cumprod()
        self.group_nets = self.group_nets.apply(lambda x: x / x.iloc[0])
        self.one = self.group_nets["group1"]
        self.end = self.group_nets[f"group{self.group}"]
        if self.one.iloc[-1] > self.end.iloc[-1]:
            self.long_name = "group1"
            self.short_name = f"group{self.group}"
        else:
            self.long_name = f"group{self.group}"
            self.short_name = "group1"
        self.long_short_ret = (
            self.group_rets[self.long_name] - self.group_rets[self.short_name]
        )
        self.long_short_net = (self.long_short_ret + 1).cumprod()
        self.long_short_net = self.long_short_net / self.long_short_net.iloc[0]
        if self.long_short_net.iloc[-1] < 1:
            self.long_short_ret = (
                self.group_rets[self.short_name] - self.group_rets[self.long_name]
            )
            self.long_short_net = (self.long_short_ret + 1).cumprod()
            self.long_short_net = self.long_short_net / self.long_short_net.iloc[0]
            self.long_short_ret = (
                self.group_rets[self.short_name]
                - self.group_rets[self.long_name]
                - 2 * self.trade_cost / self.freq
            )
            self.long_short_net = (self.long_short_ret + 1).cumprod()
            self.long_short_net = self.long_short_net / self.long_short_net.iloc[0]
        else:
            self.long_short_ret = (
                self.group_rets[self.long_name]
                - self.group_rets[self.short_name]
                - 2 * self.trade_cost / self.freq
            )
            self.long_short_net = (self.long_short_ret + 1).cumprod()
            self.long_short_net = self.long_short_net / self.long_short_net.iloc[0]
        self.group_rets = pd.concat(
            [self.group_rets, self.long_short_ret.to_frame("long_short")], axis=1
        )
        self.group_nets = pd.concat(
            [self.group_nets, self.long_short_net.to_frame("long_short")], axis=1
        )
        self.long_short_comments = self.comments(
            self.long_short_net, self.long_short_ret
        )
        if print_comments:
            print(self.long_short_comments)
        if plt_plot:
            self.group_nets.plot(rot=60)
            plt.savefig(filename + ".png")
            plt.show()
        if plotly_plot:
            fig = pe.line(self.group_nets)
            filename_path = filename + ".html"
            pio.write_html(fig, filename_path, auto_open=True)


class pure_moonson(object):
    """è¡Œä¸šè½®åŠ¨å›žæµ‹æ¡†æž¶"""

    def __init__(self, fac, group_num=5):
        homeplace = HomePlace()
        pindu = (
            pd.read_parquet(homeplace.daily_data_file + "å„è¡Œä¸šè¡Œæƒ…æ•°æ®.parquet")
            .resample("M")
            .last()
        )
        rindu = pindu / pindu.shift(1) - 1
        self.rindu = rindu
        self.fac = fac
        self.group = self.get_groups(fac, group_num)
        print("æœªå®Œå·¥ï¼Œå¾…å®Œå–„ï¼Œæš‚æ—¶è¯·å‹¿ä½¿ç”¨âš ï¸")

    def get_groups(self, df, groups_num):
        """ä¾æ®å› å­å€¼ï¼Œåˆ¤æ–­æ˜¯åœ¨ç¬¬å‡ ç»„"""
        if "group" in list(df.columns):
            df = df.drop(columns=["group"])
        df = df.sort_values(["fac"], ascending=True)
        each_group = round(df.shape[0] / groups_num)
        l = list(
            map(
                lambda x, y: [x] * y,
                list(range(1, groups_num + 1)),
                [each_group] * groups_num,
            )
        )
        l = reduce(lambda x, y: x + y, l)
        if len(l) < df.shape[0]:
            l = l + [groups_num] * (df.shape[0] - len(l))
        l = l[: df.shape[0]]
        df.insert(0, "group", l)
        return df


class pure_fall_flexible(object):
    def __init__(
        self,
        factor_file: str,
        startdate: int = None,
        enddate: int = None,
        kind: str = "stock",
        clickhouse: bool = 0,
        questdb: bool = 0,
    ) -> None:
        """åŸºäºŽclickhouseçš„åˆ†é’Ÿæ•°æ®ï¼Œè®¡ç®—å› å­å€¼ï¼Œæ¯å¤©çš„å› å­å€¼ç”¨åˆ°å¤šæ—¥çš„æ•°æ®ï¼Œæˆ–è€…ç”¨åˆ°æˆªé¢çš„æ•°æ®
        å¯¹ä¸€æ®µæ—¶é—´çš„æˆªé¢æ•°æ®è¿›è¡Œæ“ä½œï¼Œåœ¨get_daily_factorsçš„funcå‡½æ•°ä¸­
        è¯·å†™å…¥df=df.groupby([xxx]).apply(fff)ä¹‹ç±»çš„è¯­å¥
        ç„¶åŽå•ç‹¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä½œä¸ºè¦applyçš„fffï¼Œå¯ä»¥åœ¨applyä¸ŠåŠ è¿›åº¦æ¡

        Parameters
        ----------
        factor_file : str
            ç”¨äºŽå­˜å‚¨å› å­çš„æ–‡ä»¶åç§°ï¼Œè¯·ä»¥'.parquet'ç»“å°¾
        startdate : int, optional
            è®¡ç®—å› å­çš„èµ·å§‹æ—¥æœŸï¼Œå½¢å¦‚20220816, by default None
        enddate : int, optional
            è®¡ç®—å› å­çš„ç»ˆæ­¢æ—¥æœŸï¼Œå½¢å¦‚20220816, by default None
        kind : str, optional
            æŒ‡å®šè®¡ç®—è‚¡ç¥¨è¿˜æ˜¯æŒ‡æ•°ï¼ŒæŒ‡æ•°åˆ™ä¸º'index', by default "stock"
        clickhouse : bool, optional
            ä½¿ç”¨clickhouseä½œä¸ºæ•°æ®æºï¼Œå¦‚æžœpostgresqlä¸Žæœ¬å‚æ•°éƒ½ä¸º0ï¼Œå°†ä¾ç„¶ä»Žclickhouseä¸­è¯»å–, by default 0
        questdb : bool, optional
            ä½¿ç”¨questdbä½œä¸ºæ•°æ®æº, by default 0
        """
        homeplace = HomePlace()
        self.kind = kind
        if clickhouse == 0 and questdb == 0:
            clickhouse = 1
        self.clickhouse = clickhouse
        self.questdb = questdb
        if clickhouse == 1:
            # è¿žæŽ¥clickhouse
            self.chc = ClickHouseClient("minute_data")
        elif questdb:
            self.chc = Questdb()
        # å®Œæ•´çš„å› å­æ–‡ä»¶è·¯å¾„
        factor_file = homeplace.factor_data_file + factor_file
        self.factor_file = factor_file
        # è¯»å…¥ä¹‹å‰çš„å› å­
        if os.path.exists(factor_file):
            factor_old = drop_duplicates_index(pd.read_parquet(self.factor_file))
            self.factor_old = factor_old
            # å·²ç»ç®—å¥½çš„æ—¥å­
            dates_old = sorted(list(factor_old.index.strftime("%Y%m%d").astype(int)))
            self.dates_old = dates_old
        else:
            self.factor_old = None
            self.dates_old = []
            logger.info("è¿™ä¸ªå› å­ä»¥å‰æ²¡æœ‰ï¼Œæ­£åœ¨é‡æ–°è®¡ç®—")
        # è¯»å–å½“å‰æ‰€æœ‰çš„æ—¥å­
        dates_all = self.chc.show_all_dates(f"minute_data_{kind}")
        if startdate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i >= startdate]
        if enddate is None:
            ...
        else:
            dates_all = [i for i in dates_all if i <= enddate]
        self.dates_all = dates_all
        # éœ€è¦æ–°è¡¥å……çš„æ—¥å­
        self.dates_new = sorted([i for i in dates_all if i not in dates_old])

    def __call__(self) -> pd.DataFrame:
        """ç›´æŽ¥è¿”å›žå› å­å€¼çš„pd.DataFrame

        Returns
        -------
        `pd.DataFrame`
            è®¡ç®—å‡ºçš„å› å­å€¼
        """
        return self.factor.copy()

    @kk.desktop_sender(title="å˜¿ï¼Œåˆ†é’Ÿæ•°æ®å¤„ç†å®Œå•¦ï½žðŸŽˆ")
    def get_daily_factors(
        self,
        func: Callable,
        fields: str = "*",
        chunksize: int = 250,
        show_time: bool = 0,
        tqdm_inside: bool = 0,
    ) -> None:
        """æ¯æ¬¡æŠ½å–chunksizeå¤©çš„æˆªé¢ä¸Šå…¨éƒ¨è‚¡ç¥¨çš„åˆ†é’Ÿæ•°æ®
        ä¾ç…§å®šä¹‰çš„å‡½æ•°è®¡ç®—å› å­å€¼

        Parameters
        ----------
        func : Callable
            ç”¨äºŽè®¡ç®—å› å­å€¼çš„å‡½æ•°
        fields : str, optional
            è‚¡ç¥¨æ•°æ®æ¶‰åŠåˆ°å“ªäº›å­—æ®µï¼ŒæŽ’é™¤ä¸å¿…è¦çš„å­—æ®µï¼Œå¯ä»¥èŠ‚çº¦è¯»å–æ•°æ®çš„æ—¶é—´ï¼Œå½¢å¦‚'date,code,num,close,amount,open'
            æå–å‡ºçš„æ•°æ®ï¼Œè‡ªåŠ¨æŒ‰ç…§code,date,numæŽ’åºï¼Œå› æ­¤code,date,numæ˜¯å¿…ä¸å¯å°‘çš„å­—æ®µ, by default "*"
        chunksize : int, optional
            æ¯æ¬¡è¯»å–çš„æˆªé¢ä¸Šçš„å¤©æ•°, by default 10
        show_time : bool, optional
            å±•ç¤ºæ¯æ¬¡è¯»å–æ•°æ®æ‰€éœ€è¦çš„æ—¶é—´, by default 0
        tqdm_inside : bool, optional
            å°†è¿›åº¦æ¡åŠ åœ¨å†…éƒ¨ï¼Œè€Œéžå¤–éƒ¨ï¼Œå»ºè®®ä»…chunksizeè¾ƒå¤§æ—¶ä½¿ç”¨, by default 0
        """
        the_func = partial(func)
        # å°†éœ€è¦æ›´æ–°çš„æ—¥å­åˆ†å—ï¼Œæ¯200å¤©ä¸€ç»„ï¼Œä¸€èµ·è¿ç®—
        dates_new_len = len(self.dates_new)
        if dates_new_len > 0:
            cut_points = list(range(0, dates_new_len, chunksize)) + [dates_new_len - 1]
            if cut_points[-1] == cut_points[-2]:
                cut_points = cut_points[:-1]
            self.cut_points = cut_points
            self.factor_new = []
            # å¼€å§‹è®¡ç®—å› å­å€¼
            if tqdm_inside:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                for date1, date2 in cut_points:
                    if self.clickhouse == 1:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]*100} and date<={self.dates_new[date2]*100} order by code,date,num"
                    else:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]} and date<={self.dates_new[date2]}"
                    if show_time:
                        df = self.chc.get_data_show_time(sql_order)
                    else:
                        df = self.chc.get_data(sql_order)
                    if self.clickhouse == 1:
                        df = ((df.set_index("code")) / 100).reset_index()
                    tqdm.auto.tqdm.pandas()
                    df = the_func(df)
                    if isinstance(df, pd.Series):
                        df = df.reset_index()
                    df.columns = ["date", "code", "fac"]
                    df = df.pivot(columns="code", index="date", values="fac")
                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                    self.factor_new.append(df)
            else:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                for date1, date2 in tqdm.auto.tqdm(cut_points, desc="ä¸çŸ¥ä¹˜æœˆå‡ äººå½’ï¼Œè½æœˆæ‘‡æƒ…æ»¡æ±Ÿæ ‘ã€‚"):
                    if self.clickhouse == 1:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]*100} and date<={self.dates_new[date2]*100} order by code,date,num"
                    else:
                        sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date>{self.dates_new[date1]} and date<={self.dates_new[date2]} order by code,date,num"
                    if show_time:
                        df = self.chc.get_data_show_time(sql_order)
                    else:
                        df = self.chc.get_data(sql_order)
                    if self.clickhouse == 1:
                        df = ((df.set_index("code")) / 100).reset_index()
                    df = the_func(df)
                    if isinstance(df, pd.Series):
                        df = df.reset_index()
                    df.columns = ["date", "code", "fac"]
                    df = df.pivot(columns="code", index="date", values="fac")
                    df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                    self.factor_new.append(df)
            self.factor_new = pd.concat(self.factor_new)
            # æ‹¼æŽ¥æ–°çš„å’Œæ—§çš„
            self.factor = pd.concat([self.factor_old, self.factor_new]).sort_index()
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            # å­˜å…¥æœ¬åœ°
            self.factor.to_parquet(self.factor_file)
            logger.info(f"æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
        elif dates_new_len == 1:
            print("å…±1å¤©")
            if tqdm_inside:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                if self.clickhouse == 1:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]*100} order by code,date,num"
                else:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]} order by code,date,num"
                if show_time:
                    df = self.chc.get_data_show_time(sql_order)
                else:
                    df = self.chc.get_data(sql_order)
                if self.clickhouse == 1:
                    df = ((df.set_index("code")) / 100).reset_index()
                tqdm.auto.tqdm.pandas()
                df = the_func(df)
                if isinstance(df, pd.Series):
                    df = df.reset_index()
                df.columns = ["date", "code", "fac"]
                df = df.pivot(columns="code", index="date", values="fac")
                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
            else:
                # å¼€å§‹è®¡ç®—å› å­å€¼
                if self.clickhouse == 1:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]*100} order by code,date,num"
                else:
                    sql_order = f"select {fields} from minute_data.minute_data_{self.kind} where date={self.dates_new[0]} order by code,date,num"
                if show_time:
                    df = self.chc.get_data_show_time(sql_order)
                else:
                    df = self.chc.get_data(sql_order)
                if self.clickhouse == 1:
                    df = ((df.set_index("code")) / 100).reset_index()
                df = the_func(df)
                if isinstance(df, pd.Series):
                    df = df.reset_index()
                df.columns = ["date", "code", "fac"]
                df = df.pivot(columns="code", index="date", values="fac")
                df.index = pd.to_datetime(df.index.astype(str), format="%Y%m%d")
                self.factor_new.append(df)
            self.factor_new = df
            # æ‹¼æŽ¥æ–°çš„å’Œæ—§çš„
            self.factor = (
                pd.concat([self.factor_old, self.factor_new])
                .sort_index()
                .drop_duplicates()
            )
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            # å­˜å…¥æœ¬åœ°
            self.factor.to_parquet(self.factor_file)
            logger.info(f"è¡¥å……{self.dates_new[0]}æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼è®¡ç®—å®Œäº†")
        else:
            self.factor = self.factor_old
            new_end_date = datetime.datetime.strftime(self.factor.index.max(), "%Y%m%d")
            logger.info(f"å½“å‰æˆªæ­¢åˆ°{new_end_date}çš„å› å­å€¼å·²ç»æ˜¯æœ€æ–°çš„äº†")