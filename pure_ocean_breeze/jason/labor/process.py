__updated__ = "2023-10-14 15:31:01"

import warnings

warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd
import os
import tqdm.auto
import scipy.stats as ss
from scipy.optimize import linprog
import statsmodels.formula.api as smf
# import matplotlib as mpl

# mpl.rcParams.update(mpl.rcParamsDefault)
import matplotlib.pyplot as plt

plt.rcParams["axes.unicode_minus"] = False

from functools import reduce, lru_cache, partial
from dateutil.relativedelta import relativedelta
from loguru import logger
import datetime
from collections.abc import Iterable
import plotly.express as pe
import plotly.io as pio
from plotly.tools import FigureFactory as FF
import plotly.graph_objects as go
import mpire

from texttable import Texttable

import cufflinks as cf

try:
    cf.set_config_file(offline=True)
except Exception:
    pass
from IPython.display import display
from typing import Callable, Union, Dict, List, Tuple
from pure_ocean_breeze.jason.data.read_data import (
    read_daily,
    read_market,
    get_industry_dummies,
)
from pure_ocean_breeze.jason.state.homeplace import HomePlace

try:
    homeplace = HomePlace()
except Exception:
    print("æ‚¨æš‚æœªåˆå§‹åŒ–ï¼ŒåŠŸèƒ½å°†å—é™")
from pure_ocean_breeze.jason.state.states import STATES
from pure_ocean_breeze.jason.state.decorators import do_on_dfs
from pure_ocean_breeze.jason.data.dicts import INDUS_DICT
from pure_ocean_breeze.jason.data.tools import (
    drop_duplicates_index,
    to_percent,
    to_group,
    standardlize,
    select_max,
    select_min,
    merge_many,
)
from pure_ocean_breeze.jason.labor.comment import (
    comments_on_twins,
    make_relative_comments,
    make_relative_comments_plot,
)


@do_on_dfs
def daily_factor_on_industry(df: pd.DataFrame) -> dict:
    """å°†ä¸€ä¸ªå› å­å˜ä¸ºä»…åœ¨æŸä¸ªç”³ä¸‡ä¸€çº§è¡Œä¸šä¸Šçš„è‚¡ç¥¨

    Parameters
    ----------
    df : pd.DataFrame
        å…¨å¸‚åœºçš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 

    Returns
    -------
    dict
        keyä¸ºè¡Œä¸šä»£ç ï¼Œvalueä¸ºå¯¹åº”çš„è¡Œä¸šä¸Šçš„å› å­å€¼
    """
    df1 = df.resample("W").last()
    if df1.shape[0] * 2 > df.shape[0]:
        daily = 0
        weekly = 1
    else:
        daily = 1
        weekly = 0
    start = int(datetime.datetime.strftime(df.index.min(), "%Y%m%d"))
    ress = get_industry_dummies(
        daily=daily,
        weekly=weekly,
        start=start,
    )
    ress = {k: v * df for k, v in ress.items()}
    return ress


@do_on_dfs
def decap(df: pd.DataFrame, daily: bool = 0, monthly: bool = 0) -> pd.DataFrame:
    """å¯¹å› å­åšå¸‚å€¼ä¸­æ€§åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        æœªä¸­æ€§åŒ–çš„å› å­ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
    daily : bool, optional
        æœªä¸­æ€§åŒ–å› å­æ˜¯æ—¥é¢‘çš„åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0, by default 0
    monthly : bool, optional
        æœªä¸­æ€§åŒ–å› å­æ˜¯æœˆé¢‘çš„åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0, by default 0

    Returns
    -------
    `pd.DataFrame`
        å¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­

    Raises
    ------
    `NotImplementedError`
        å¦‚æœæœªæŒ‡å®šæ—¥é¢‘æˆ–æœˆé¢‘ï¼Œå°†æŠ¥é”™
    """
    tqdm.auto.tqdm.pandas()
    cap = read_daily(flow_cap=1).stack().reset_index()
    cap.columns = ["date", "code", "cap"]
    cap.cap = ss.boxcox(cap.cap)[0]

    def single(x):
        x.cap = ss.boxcox(x.cap)[0]
        return x

    cap = cap.groupby(["date"]).apply(single)
    cap = cap.set_index(["date", "code"]).unstack()
    cap.columns = [i[1] for i in list(cap.columns)]
    cap_monthly = cap.resample("W").last()
    last = df.resample("W").last()
    if df.shape[0] / last.shape[0] < 2:
        monthly = True
    else:
        daily = True
    if daily:
        df = (pure_fallmount(df) - (pure_fallmount(cap),))()
    elif monthly:
        df = (pure_fallmount(df) - (pure_fallmount(cap_monthly),))()
    else:
        raise NotImplementedError("å¿…é¡»æŒ‡å®šé¢‘ç‡")
    return df


@do_on_dfs
def decap_industry(df: pd.DataFrame) -> pd.DataFrame:
    """å¯¹å› å­åšè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        æœªä¸­æ€§åŒ–çš„å› å­ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 

    Returns
    -------
    `pd.DataFrame`
        è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­

    Raises
    ------
    `NotImplementedError`
        å¦‚æœæœªæŒ‡å®šæ—¥é¢‘æˆ–æœˆé¢‘ï¼Œå°†æŠ¥é”™
    """
    start_date = int(datetime.datetime.strftime(df.index.min(), "%Y%m%d"))
    last = df.resample("W").last()
    homeplace = HomePlace()
    if df.shape[0] / last.shape[0] < 2:
        weekly = True
    else:
        daily = True
    if weekly:
        cap = read_daily(flow_cap=1, start=start_date).resample("W").last()
    else:
        cap = read_daily(flow_cap=1, start=start_date)
    cap = cap.stack().reset_index()
    cap.columns = ["date", "code", "cap"]
    cap.cap = ss.boxcox(cap.cap)[0]

    def single(x):
        x.cap = ss.boxcox(x.cap)[0]
        return x

    cap = cap.groupby(["date"]).apply(single)
    df = df.stack().reset_index()
    df.columns = ["date", "code", "fac"]
    df = pd.merge(df, cap, on=["date", "code"])

    def neutralize_factors(df):
        """ç»„å†…å¯¹å› å­è¿›è¡Œå¸‚å€¼ä¸­æ€§åŒ–"""
        industry_codes = list(df.columns)
        industry_codes = [i for i in industry_codes if i.startswith("w")]
        industry_codes_str = "+".join(industry_codes)
        ols_result = smf.ols("fac~cap+" + industry_codes_str, data=df).fit()
        ols_w = ols_result.params["cap"]
        ols_b = ols_result.params["Intercept"]
        ols_bs = {}
        for ind in industry_codes:
            ols_bs[ind] = ols_result.params[ind]
        df.fac = df.fac - ols_w * df.cap - ols_b
        for k, v in ols_bs.items():
            df.fac = df.fac - v * df[k]
        df = df[["fac"]]
        return df

    file_name = "sw_industry_level1_dummies.parquet"

    if weekly:
        industry_dummy = (
            pd.read_parquet(homeplace.daily_data_file + file_name)
            .fillna(0)
            .set_index("date")
            .groupby("code")
            .resample("W")
            .last()
        )
        industry_dummy = industry_dummy.fillna(0).drop(columns=["code"]).reset_index()
        industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
        col = ["code", "date"] + industry_ws
    elif daily:
        industry_dummy = pd.read_parquet(homeplace.daily_data_file + file_name).fillna(
            0
        )
        industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
        col = ["date", "code"] + industry_ws
    else:
        raise NotImplementedError("å¿…é¡»æŒ‡å®šé¢‘ç‡")
    industry_dummy.columns = col
    df = pd.merge(df, industry_dummy, on=["date", "code"])
    df = df.set_index(["date", "code"])
    tqdm.auto.tqdm.pandas()
    df = df.groupby(["date"]).progress_apply(neutralize_factors)
    df = df.unstack()
    df.columns = [i[1] for i in list(df.columns)]
    return df


@do_on_dfs
def deboth(df: pd.DataFrame) -> pd.DataFrame:
    """é€šè¿‡å›æµ‹çš„æ–¹å¼ï¼Œå¯¹æœˆé¢‘å› å­åšè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        æœªä¸­æ€§åŒ–çš„å› å­

    Returns
    -------
    `pd.DataFrame`
        è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­
    """
    shen = pure_moonnight(df, boxcox=1, plt_plot=0, print_comments=0)
    return shen()


@do_on_dfs
def boom_one(
    df: pd.DataFrame, backsee: int = 5, daily: bool = 0, min_periods: int = None
) -> pd.DataFrame:
    if min_periods is None:
        min_periods = int(backsee * 0.5)
    if not daily:
        df_mean = (
            df.rolling(backsee, min_periods=min_periods).mean().resample("W").last()
        )
    else:
        df_mean = df.rolling(backsee, min_periods=min_periods).mean()
    return df_mean


@do_on_dfs
def boom_four(
    df: pd.DataFrame, backsee: int = 5, daily: bool = 0, min_periods: int = None
) -> Tuple[pd.DataFrame]:
    """ç”Ÿæˆ20å¤©å‡å€¼ï¼Œ20å¤©æ ‡å‡†å·®ï¼ŒåŠäºŒè€…æ­£å‘z-scoreåˆæˆï¼Œæ­£å‘æ’åºåˆæˆï¼Œè´Ÿå‘z-scoreåˆæˆï¼Œè´Ÿå‘æ’åºåˆæˆè¿™6ä¸ªå› å­

    Parameters
    ----------
    df : pd.DataFrame
        åŸæ—¥é¢‘å› å­
    backsee : int, optional
        å›çœ‹å¤©æ•°, by default 20
    daily : bool, optional
        ä¸º1æ˜¯æ¯å¤©éƒ½æ»šåŠ¨ï¼Œä¸º0åˆ™ä»…ä¿ç•™æœˆåº•å€¼, by default 0
    min_periods : int, optional
        rollingæ—¶çš„æœ€å°æœŸ, by default backseeçš„ä¸€åŠ

    Returns
    -------
    `Tuple[pd.DataFrame]`
        6ä¸ªå› å­çš„å…ƒç»„
    """
    if min_periods is None:
        min_periods = int(backsee * 0.5)
    if not daily:
        df_mean = (
            df.rolling(backsee, min_periods=min_periods).mean().resample("W").last()
        )
        df_std = df.rolling(backsee, min_periods=min_periods).std().resample("W").last()
        twins_add = (pure_fallmount(df_mean) + (pure_fallmount(df_std),))()
        rtwins_add = df_mean.rank(axis=1) + df_std.rank(axis=1)
        twins_minus = (pure_fallmount(df_mean) + (pure_fallmount(-df_std),))()
        rtwins_minus = df_mean.rank(axis=1) - df_std.rank(axis=1)
    else:
        df_mean = df.rolling(backsee, min_periods=min_periods).mean()
        df_std = df.rolling(backsee, min_periods=min_periods).std()
        twins_add = (pure_fallmount(df_mean) + (pure_fallmount(df_std),))()
        rtwins_add = df_mean.rank(axis=1) + df_std.rank(axis=1)
        twins_minus = (pure_fallmount(df_mean) + (pure_fallmount(-df_std),))()
        rtwins_minus = df_mean.rank(axis=1) - df_std.rank(axis=1)
    return df_mean, df_std, twins_add, rtwins_add, twins_minus, rtwins_minus


@do_on_dfs
def add_cross_standardlize(*args: list) -> pd.DataFrame:
    """å°†ä¼—å¤šå› å­æ¨ªæˆªé¢åšz-scoreæ ‡å‡†åŒ–ä¹‹åç›¸åŠ 

    Returns
    -------
    `pd.DataFrame`
        åˆæˆåçš„å› å­
    """
    fms = [pure_fallmount(i) for i in args]
    one = fms[0]
    others = fms[1:]
    final = one + others
    return final()


@do_on_dfs
def to_tradeends(df: pd.DataFrame) -> pd.DataFrame:
    """å°†æœ€åä¸€ä¸ªè‡ªç„¶æ—¥æ”¹å˜ä¸ºæœ€åä¸€ä¸ªäº¤æ˜“æ—¥

    Parameters
    ----------
    df : pd.DataFrame
        indexä¸ºæ—¶é—´ï¼Œä¸ºæ¯ä¸ªæœˆçš„æœ€åä¸€å¤©

    Returns
    -------
    `pd.DataFrame`
        ä¿®æ”¹ä¸ºäº¤æ˜“æ—¥æ ‡æ³¨åçš„pd.DataFrame
    """
    """"""
    start = df.index.min()
    start = start - pd.DateOffset(weeks=1)
    start = datetime.datetime.strftime(start, "%Y%m%d")
    trs = read_daily(tr=1, start=start)
    trs = trs.assign(tradeends=list(trs.index))
    trs = trs[["tradeends"]]
    trs = trs.resample("W").last()
    df = pd.concat([trs, df], axis=1)
    df = df.set_index(["tradeends"])
    return df


@do_on_dfs
def market_kind(
    df: pd.DataFrame,
    zhuban: bool = 0,
    chuangye: bool = 0,
    kechuang: bool = 0,
    beijing: bool = 0,
) -> pd.DataFrame:
    """ä¸å®½åŸºæŒ‡æ•°æˆåˆ†è‚¡çš„å‡½æ•°ç±»ä¼¼ï¼Œé™å®šè‚¡ç¥¨åœ¨æŸä¸ªå…·ä½“æ¿å—ä¸Š

    Parameters
    ----------
    df : pd.DataFrame
        åŸå§‹å…¨éƒ¨è‚¡ç¥¨çš„å› å­å€¼
    zhuban : bool, optional
        é™å®šåœ¨ä¸»æ¿èŒƒå›´å†…, by default 0
    chuangye : bool, optional
        é™å®šåœ¨åˆ›ä¸šæ¿èŒƒå›´å†…, by default 0
    kechuang : bool, optional
        é™å®šåœ¨ç§‘åˆ›æ¿èŒƒå›´å†…, by default 0
    beijing : bool, optional
        é™å®šåœ¨åŒ—äº¤æ‰€èŒƒå›´å†…, by default 0

    Returns
    -------
    `pd.DataFrame`
        é™åˆ¶èŒƒå›´åçš„å› å­å€¼ï¼Œå…¶ä½™ä¸ºç©º

    Raises
    ------
    `ValueError`
        å¦‚æœæœªæŒ‡å®šä»»ä½•è‚¡ç¥¨æ± ï¼Œå°†æŠ¥é”™
    """
    trs = read_daily(tr=1)
    codes = list(trs.columns)
    dates = list(trs.index)
    if chuangye and kechuang:
        dummys = [1 if code[:2] in ["30", "68"] else np.nan for code in codes]
    else:
        if zhuban:
            dummys = [1 if code[:2] in ["00", "60"] else np.nan for code in codes]
        elif chuangye:
            dummys = [1 if code.startswith("3") else np.nan for code in codes]
        elif kechuang:
            dummys = [1 if code.startswith("68") else np.nan for code in codes]
        elif beijing:
            dummys = [1 if code.startswith("8") else np.nan for code in codes]
        else:
            raise ValueError("ä½ æ€»å¾—é€‰ä¸€ä¸ªè‚¡ç¥¨æ± å§ï¼ŸğŸ¤’")
    dummy_dict = {k: v for k, v in zip(codes, dummys)}
    dummy_df = pd.DataFrame(dummy_dict, index=dates)
    df = df * dummy_df
    return df


def show_corr(
    fac1: pd.DataFrame,
    fac2: pd.DataFrame,
    method: str = "pearson",
    plt_plot: bool = 1,
    show_series: bool = 0,
    old_way: bool = 0,
) -> float:
    """å±•ç¤ºä¸¤ä¸ªå› å­çš„æˆªé¢ç›¸å…³æ€§

    Parameters
    ----------
    fac1 : pd.DataFrame
        å› å­1
    fac2 : pd.DataFrame
        å› å­2
    method : str, optional
        è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "pearson"
    plt_plot : bool, optional
        æ˜¯å¦ç”»å‡ºç›¸å…³ç³»æ•°çš„æ—¶åºå˜åŒ–å›¾, by default 1
    show_series : bool, optional
        è¿”å›ç›¸å…³æ€§çš„åºåˆ—ï¼Œè€Œéå‡å€¼
    old_way : bool, optional
        ä½¿ç”¨3.xç‰ˆæœ¬çš„æ–¹å¼æ±‚ç›¸å…³ç³»æ•°

    Returns
    -------
    `float`
        å¹³å‡æˆªé¢ç›¸å…³ç³»æ•°
    """
    if old_way:
        if method == "spearman":
            corr = show_x_with_func(fac1, fac2, lambda x: x.rank().corr().iloc[0, 1])
        else:
            corr = show_x_with_func(
                fac1, fac2, lambda x: x.corr(method=method).iloc[0, 1]
            )
    else:
        corr = fac1.corrwith(fac2, axis=1, method=method)
    if show_series:
        return corr
    else:
        if plt_plot:
            corr.plot(rot=60)
            plt.show()
        return corr.mean()


def show_corrs(
    factors: List[pd.DataFrame],
    factor_names: List[str] = None,
    print_bool: bool = True,
    show_percent: bool = True,
    method: str = "pearson",
) -> pd.DataFrame:
    """å±•ç¤ºå¾ˆå¤šå› å­ä¸¤ä¸¤ä¹‹é—´çš„æˆªé¢ç›¸å…³æ€§

    Parameters
    ----------
    factors : List[pd.DataFrame]
        æ‰€æœ‰å› å­æ„æˆçš„åˆ—è¡¨, by default None
    factor_names : List[str], optional
        ä¸Šè¿°å› å­ä¾æ¬¡çš„åå­—, by default None
    print_bool : bool, optional
        æ˜¯å¦æ‰“å°å‡ºä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼, by default True
    show_percent : bool, optional
        æ˜¯å¦ä»¥ç™¾åˆ†æ•°çš„å½¢å¼å±•ç¤º, by default True
    method : str, optional
        è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "pearson"

    Returns
    -------
    `pd.DataFrame`
        ä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼
    """
    corrs = []
    for i in range(len(factors)):
        main_i = factors[i]
        follows = factors[i + 1 :]
        corr = [show_corr(main_i, i, plt_plot=False, method=method) for i in follows]
        corr = [np.nan] * (i + 1) + corr
        corrs.append(corr)
    if factor_names is None:
        factor_names = [f"fac{i}" for i in list(range(1, len(factors) + 1))]
    corrs = pd.DataFrame(corrs, columns=factor_names, index=factor_names)
    np.fill_diagonal(corrs.to_numpy(), 1)
    corrs=pd.DataFrame(corrs.fillna(0).to_numpy()+corrs.fillna(0).to_numpy().T-np.diag(np.diag(corrs)),index=corrs.index,columns=corrs.columns)
    if show_percent:
        pcorrs = corrs.applymap(to_percent)
    else:
        pcorrs = corrs.copy()
    if print_bool:
        return pcorrs
    else:
        return corrs


def show_cov(
    fac1: pd.DataFrame,
    fac2: pd.DataFrame,
    plt_plot: bool = 1,
    show_series: bool = 0,
) -> float:
    """å±•ç¤ºä¸¤ä¸ªå› å­çš„æˆªé¢ç›¸å…³æ€§

    Parameters
    ----------
    fac1 : pd.DataFrame
        å› å­1
    fac2 : pd.DataFrame
        å› å­2
    method : str, optional
        è®¡ç®—ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default "spearman"
    plt_plot : bool, optional
        æ˜¯å¦ç”»å‡ºç›¸å…³ç³»æ•°çš„æ—¶åºå˜åŒ–å›¾, by default 1
    show_series : bool, optional
        è¿”å›ç›¸å…³æ€§çš„åºåˆ—ï¼Œè€Œéå‡å€¼

    Returns
    -------
    `float`
        å¹³å‡æˆªé¢ç›¸å…³ç³»æ•°
    """
    cov = show_x_with_func(fac1, fac2, lambda x: x.cov().iloc[0, 1])
    if show_series:
        return cov
    else:
        if plt_plot:
            cov.plot(rot=60)
            plt.show()
        return cov.mean()


def show_x_with_func(
    fac1: pd.DataFrame,
    fac2: pd.DataFrame,
    func: Callable,
) -> pd.Series:
    """å±•ç¤ºä¸¤ä¸ªå› å­çš„æŸç§æˆªé¢å…³ç³»

    Parameters
    ----------
    fac1 : pd.DataFrame
        å› å­1
    fac2 : pd.DataFrame
        å› å­2
    func : Callable
        è¦å¯¹ä¸¤ä¸ªå› å­åœ¨æˆªé¢ä¸Šçš„è¿›è¡Œçš„æ“ä½œ

    Returns
    -------
    `pd.Series`
        æˆªé¢å…³ç³»
    """
    the_func = partial(func)
    both1 = fac1.stack().reset_index()
    befo1 = fac2.stack().reset_index()
    both1.columns = ["date", "code", "both"]
    befo1.columns = ["date", "code", "befo"]
    twins = pd.merge(both1, befo1, on=["date", "code"]).set_index(["date", "code"])
    corr = twins.groupby("date").apply(the_func)
    return corr


def show_covs(
    factors: List[pd.DataFrame],
    factor_names: List[str] = None,
    print_bool: bool = True,
    show_percent: bool = True,
) -> pd.DataFrame:
    """å±•ç¤ºå¾ˆå¤šå› å­ä¸¤ä¸¤ä¹‹é—´çš„æˆªé¢ç›¸å…³æ€§

    Parameters
    ----------
    factors : List[pd.DataFrame]
        æ‰€æœ‰å› å­æ„æˆçš„åˆ—è¡¨, by default None
    factor_names : List[str], optional
        ä¸Šè¿°å› å­ä¾æ¬¡çš„åå­—, by default None
    print_bool : bool, optional
        æ˜¯å¦æ‰“å°å‡ºä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼, by default True
    show_percent : bool, optional
        æ˜¯å¦ä»¥ç™¾åˆ†æ•°çš„å½¢å¼å±•ç¤º, by default True

    Returns
    -------
    `pd.DataFrame`
        ä¸¤ä¸¤ä¹‹é—´ç›¸å…³ç³»æ•°çš„è¡¨æ ¼
    """
    corrs = []
    for i in range(len(factors)):
        main_i = factors[i]
        follows = factors[i + 1 :]
        corr = [show_cov(main_i, i, plt_plot=False) for i in follows]
        corr = [np.nan] * (i + 1) + corr
        corrs.append(corr)
    if factor_names is None:
        factor_names = [f"fac{i}" for i in list(range(1, len(factors) + 1))]
    corrs = pd.DataFrame(corrs, columns=factor_names, index=factor_names)
    np.fill_diagonal(corrs.to_numpy(), 1)
    if show_percent:
        pcorrs = corrs.applymap(to_percent)
    else:
        pcorrs = corrs.copy()
    if print_bool:
        print(pcorrs)
    return corrs


@do_on_dfs
def show_corrs_with_old(
    df: pd.DataFrame = None,
    method: str = "pearson",
) -> pd.DataFrame:
    """è®¡ç®—æ–°å› å­å’Œå·²æœ‰å› å­çš„ç›¸å…³ç³»æ•°

    Parameters
    ----------
    df : pd.DataFrame, optional
        æ–°å› å­, by default None
    method : str, optional
        æ±‚ç›¸å…³ç³»æ•°çš„æ–¹æ³•, by default 'pearson'


    Returns
    -------
    pd.DataFrame
        ç›¸å…³ç³»æ•°çŸ©é˜µ
    """
    files = os.listdir(homeplace.final_factor_file)
    names = [i[:-8] for i in files]
    files = [homeplace.final_factor_file + i for i in files]
    files = [pd.read_parquet(i) for i in files]
    if df is not None:
        corrs = show_corrs([df] + files, names, method=method)
    else:
        corrs = show_corrs(files, names, method=method)
    return corrs


@do_on_dfs
def remove_unavailable(df: pd.DataFrame) -> pd.DataFrame:
    """å¯¹æ—¥é¢‘æˆ–æœˆé¢‘å› å­å€¼ï¼Œå‰”é™¤stè‚¡ã€ä¸æ­£å¸¸äº¤æ˜“çš„è‚¡ç¥¨å’Œä¸Šå¸‚ä¸è¶³60å¤©çš„è‚¡ç¥¨

    Parameters
    ----------
    df : pd.DataFrame
        å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç ï¼Œvaluesæ˜¯å› å­å€¼

    Returns
    -------
    pd.DataFrame
        å‰”é™¤åçš„å› å­å€¼
    """
    df0 = df.resample("W").last()
    if df.shape[0] / df0.shape[0] > 2:
        daily = 1
    else:
        daily = 0
    state = read_daily(state=1).replace(0, np.nan)
    if daily:
        df = df * state
    else:
        df = state.resample("W").first() * df
    return df


class frequency_controller(object):
    def __init__(self, freq: str):
        self.homeplace = HomePlace()
        self.freq = freq

        if freq == "M":
            self.counts_one_year = 12
            self.time_shift = pd.DateOffset(months=1)
            self.states_files = (
                self.homeplace.daily_data_file + "states_monthly.parquet"
            )
            self.sts_files = self.homeplace.daily_data_file + "sts_monthly.parquet"
            self.comment_name = "æœˆ"
            self.days_in = 20
        elif freq == "W":
            self.counts_one_year = 50
            self.time_shift = pd.DateOffset(weeks=1)
            self.states_files = self.homeplace.daily_data_file + "states_weekly.parquet"
            self.sts_files = self.homeplace.daily_data_file + "sts_weekly.parquet"
            self.comment_name = "å‘¨"
            self.days_in = 5
        else:
            raise ValueError("'æš‚æ—¶ä¸æ”¯æŒæ­¤é¢‘ç‡å“ˆğŸ¤’ï¼Œç›®å‰ä»…æ”¯æŒæœˆé¢‘'M'ï¼Œå’Œå‘¨é¢‘'W'")

    def next_end(self, x):
        """æ‰¾åˆ°ä¸‹ä¸ªå‘¨æœŸçš„æœ€åä¸€å¤©"""
        if self.freq == "M":
            return x + pd.DateOffset(months=1) + pd.tseries.offsets.MonthEnd()
        elif self.freq == "W":
            return x + pd.DateOffset(weeks=1)
        else:
            raise ValueError("'æš‚æ—¶ä¸æ”¯æŒæ­¤é¢‘ç‡å“ˆğŸ¤’ï¼Œç›®å‰ä»…æ”¯æŒæœˆé¢‘'M'ï¼Œå’Œå‘¨é¢‘'W'")


class pure_moon(object):
    __slots__ = [
        "homeplace",
        "sts_monthly_file",
        "states_monthly_file",
        "factors",
        "codes",
        "tradedays",
        "ages",
        "amounts",
        "closes",
        "opens",
        "capital",
        "states",
        "sts",
        "turnovers",
        "sts_monthly",
        "states_monthly",
        "ages_monthly",
        "tris_monthly",
        "opens_monthly",
        "closes_monthly",
        "rets_monthly",
        "opens_monthly_shift",
        "rets_monthly_begin",
        "limit_ups",
        "limit_downs",
        "data",
        "ic_icir_and_rank",
        "big_small_rankic",
        "rets_monthly_limit_downs",
        "group_rets",
        "long_short_rets",
        "long_short_net_values",
        "group_net_values",
        "long_short_ret_yearly",
        "long_short_vol_yearly",
        "long_short_info_ratio",
        "long_short_win_times",
        "long_short_win_ratio",
        "retreats",
        "max_retreat",
        "long_short_comments",
        "total_comments",
        "square_rets",
        "cap",
        "cap_value",
        "industry_dummy",
        "industry_codes",
        "industry_codes_str",
        "industry_ws",
        "__factors_out",
        "ics",
        "rankics",
        "factor_turnover_rates",
        "factor_turnover_rate",
        "group_rets_std",
        "group_rets_stds",
        "group_rets_skews",
        "group_rets_skew",
        "wind_out",
        "swindustry_dummy",
        "zxindustry_dummy",
        "closes2_monthly",
        "rets_monthly_last",
        "freq_ctrl",
        "freq",
        "factor_cover",
        "factor_cross_skew",
        "factor_cross_skew_after_neu",
        "pos_neg_rate",
        "corr_itself",
        "factor_cross_stds",
        "corr_itself_shift2",
        "rets_all",
        "inner_long_ret_yearly",
        "inner_short_ret_yearly",
        "inner_long_net_values",
        "inner_short_net_values",
        "group_mean_rets_monthly",
        "not_ups",
        "not_downs",
        "group1_ret_yearly",
        "group10_ret_yearly",
        "market_ret",
        "long_minus_market_rets",
        "long_minus_market_nets",
        "inner_rets_long",
        "inner_rets_short",
    ]

    @classmethod
    @lru_cache(maxsize=None)
    def __init__(
        cls,
        freq: str = "W",
        no_read_indu: bool = 0,
    ):
        cls.homeplace = HomePlace()
        cls.freq = freq
        cls.freq_ctrl = frequency_controller(freq)

        def deal_dummy(industry_dummy):
            industry_dummy = industry_dummy.drop(columns=["code"]).reset_index()
            industry_ws = [f"w{i}" for i in range(1, industry_dummy.shape[1] - 1)]
            col = ["code", "date"] + industry_ws
            industry_dummy.columns = col
            industry_dummy = industry_dummy[
                industry_dummy.date >= pd.Timestamp(str(STATES["START"]))
            ]
            return industry_dummy

        if not no_read_indu:
            # week_here
            cls.swindustry_dummy = (
                pd.read_parquet(
                    cls.homeplace.daily_data_file + "sw_industry_level1_dummies.parquet"
                )
                .fillna(0)
                .set_index("date")
                .groupby("code")
                .resample(freq)
                .last()
            )
            cls.swindustry_dummy = deal_dummy(cls.swindustry_dummy)

    @property
    def factors_out(self):
        return self.__factors_out

    def __call__(self):
        """è°ƒç”¨å¯¹è±¡åˆ™è¿”å›å› å­å€¼"""
        return self.factors_out

    @classmethod
    @lru_cache(maxsize=None)
    def set_basic_data(
        cls,
        total_cap: bool = 0,
    ):
        states = read_daily(state=1, start=STATES["START"])
        opens = read_daily(vwap=1, start=STATES["START"])
        closes = read_daily(vwap=1, start=STATES["START"])
        if total_cap:
            capitals = (
                read_daily(total_cap=1, start=STATES["START"]).resample(cls.freq).last()
            )
        else:
            capitals = (
                read_daily(flow_cap=1, start=STATES["START"]).resample(cls.freq).last()
            )
        market=read_market(zz500=1,every_stock=0,close=1).resample(cls.freq).last()
        cls.market_ret=market/market.shift(1)-1
        # äº¤æ˜“çŠ¶æ€æ–‡ä»¶
        cls.states = states
        # Monday vwap
        cls.opens = opens
        # Friday vwap
        cls.closes = closes
        # æœˆåº•æµé€šå¸‚å€¼æ•°æ®
        cls.capital = capitals
        cls.opens = cls.opens.replace(0, np.nan)
        cls.closes = cls.closes.replace(0, np.nan)
        cls.states = read_daily(state=1)
        cls.states = cls.states.resample(cls.freq).first()
        # cls.states=np.sign(cls.states.where(cls.states==cls.states.max().max(),np.nan))
        up_downs = read_daily(up_down_limit_status=1)
        cls.not_ups = (
            np.sign(up_downs.where(up_downs != 1, np.nan).abs() + 1)
            .resample(cls.freq)
            .first()
        )
        cls.not_downs = (
            np.sign(up_downs.where(up_downs != -1, np.nan).abs() + 1)
            .resample(cls.freq)
            .last()
        )

    def set_factor_df_date_as_index(self, df: pd.DataFrame):
        """è®¾ç½®å› å­æ•°æ®çš„dataframeï¼Œå› å­è¡¨åˆ—ååº”ä¸ºè‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•åº”ä¸ºæ—¶é—´"""
        # week_here
        self.factors = df.resample(self.freq).last().dropna(how="all")
        self.factors = self.factors * self.states
        self.factor_cover = np.sign(self.factors.abs() + 1).sum().sum()
        opens = self.opens[self.opens.index >= self.factors.index.min()]
        total = np.sign(opens.resample(self.freq).last()).sum().sum()
        self.factor_cover = min(self.factor_cover / total, 1)
        self.factor_cross_skew = self.factors.skew(axis=1).mean()
        pos_num = ((self.factors > 0) + 0).sum().sum()
        neg_num = ((self.factors < 0) + 0).sum().sum()
        self.pos_neg_rate = pos_num / (neg_num + pos_num)
        self.corr_itself = show_corr(self.factors, self.factors.shift(1), plt_plot=0)
        self.corr_itself_shift2 = show_corr(
            self.factors, self.factors.shift(2), plt_plot=0
        )
        self.factor_cross_stds = self.factors.std(axis=1)
        

    @classmethod
    @lru_cache(maxsize=None)
    def get_rets_month(cls):
        """è®¡ç®—æ¯æœˆçš„æ”¶ç›Šç‡ï¼Œå¹¶æ ¹æ®æ¯æœˆåšå‡ºäº¤æ˜“çŠ¶æ€ï¼Œåšå‡ºåˆ å‡"""
        # week_here
        cls.opens_monthly = cls.opens.resample(cls.freq).first()
        # week_here
        cls.closes_monthly = cls.closes.resample(cls.freq).last()
        cls.rets_monthly = (cls.closes_monthly - cls.opens_monthly) / cls.opens_monthly
        cls.rets_monthly = cls.rets_monthly.stack().reset_index()
        cls.rets_monthly.columns = ["date", "code", "ret"]

    # @classmethod
    def neutralize_factors(self, date):
        """ç»„å†…å¯¹å› å­è¿›è¡Œå¸‚å€¼ä¸­æ€§åŒ–"""
        df=self.factors[self.factors.date==date].set_index(['date','code'])
        industry_codes = list(df.columns)
        industry_codes = [i for i in industry_codes if i.startswith("w")]
        industry_codes_str = "+".join(industry_codes)
        if len(industry_codes_str) > 0:
            ols_result = smf.ols("fac~cap_size+" + industry_codes_str, data=df).fit()
        else:
            ols_result = smf.ols("fac~cap_size", data=df).fit()
        df.fac=ols_result.resid
        df = df[["fac"]]
        return df

    @classmethod
    @lru_cache(maxsize=None)
    def get_log_cap(cls):
        """è·å¾—å¯¹æ•°å¸‚å€¼"""
        cls.cap = cls.capital.stack().reset_index()
        cls.cap.columns = ["date", "code", "cap_size"]
        cls.cap["cap_size"] = np.log(cls.cap["cap_size"])

    def get_neutral_factors(self, only_cap=0):
        """å¯¹å› å­è¿›è¡Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–"""
        self.factors = self.factors.stack().reset_index()
        self.factors.columns = ["date", "code", "fac"]
        self.factors = pd.merge(
            self.factors, self.cap, how="inner", on=["date", "code"]
        )
        if not only_cap:
            self.factors = pd.merge(
                self.factors, self.swindustry_dummy, on=["date", "code"]
            )

        dates=list(set(self.factors.date))
        with mpire.WorkerPool(20) as pool:
            res=pool.map(self.neutralize_factors,dates)
        self.factors = pd.concat(res)
        self.factors = self.factors.reset_index()
        self.factors = self.factors.pivot(index="date", columns="code", values="fac")

    def get_ic_rankic(cls, df):
        """è®¡ç®—ICå’ŒRankIC"""
        df1 = df[["ret", "fac"]]
        ic = df1.corr(method="pearson").iloc[0, 1]
        rankic = df1.rank().corr().iloc[0, 1]
        small_ic=df1[df1.fac<=df1.fac.median()].rank().corr().iloc[0, 1]
        big_ic=df1[df1.fac>=df1.fac.median()].rank().corr().iloc[0, 1]
        df2 = pd.DataFrame({"ic": [ic], "rankic": [rankic],"small_rankic":[small_ic],"big_rankic":[big_ic]})
        return df2

    def get_icir_rankicir(cls, df):
        """è®¡ç®—ICIRå’ŒRankICIR"""
        ic = df.ic.mean()
        rankic = df.rankic.mean()
        small_rankic=df.small_rankic.mean()
        big_rankic=df.big_rankic.mean()
        # week_here
        icir = ic / np.std(df.ic) * (cls.freq_ctrl.counts_one_year ** (0.5))
        # week_here
        rankicir = rankic / np.std(df.rankic) * (cls.freq_ctrl.counts_one_year ** (0.5))
        small_rankicir = small_rankic / np.std(df.small_rankic) * (cls.freq_ctrl.counts_one_year ** (0.5))
        big_rankicir = big_rankic / np.std(df.big_rankic) * (cls.freq_ctrl.counts_one_year ** (0.5))
        return pd.DataFrame(
            {"IC": [ic], "ICIR": [icir], "RankIC": [rankic], "RankICIR": [rankicir]},
            index=["è¯„ä»·æŒ‡æ ‡"],
        ),pd.DataFrame({"1-5RankIC":[small_rankic],"1-5ICIR":[small_rankicir],"6-10RankIC":[big_rankic],"6-10ICIR":[big_rankicir]},index=["è¯„ä»·æŒ‡æ ‡"]).T

    def get_ic_icir_and_rank(cls, df):
        """è®¡ç®—ICã€ICIRã€RankICã€RankICIR"""
        df1 = df.groupby("date").apply(cls.get_ic_rankic)
        cls.ics = df1.ic
        cls.rankics = df1.rankic
        cls.ics = cls.ics.reset_index(drop=True, level=1).to_frame()
        cls.rankics = cls.rankics.reset_index(drop=True, level=1).to_frame()
        df2,df5 = cls.get_icir_rankicir(df1)
        df2 = df2.T
        dura = (df.date.max() - df.date.min()).days / 365
        t_value = df2.iloc[3, 0] * (dura ** (1 / 2))
        df3 = pd.DataFrame({"è¯„ä»·æŒ‡æ ‡": [t_value]}, index=["RankIC.t"])
        df4 = pd.concat([df2, df3])
        df
        return df4,df5

    @classmethod
    def get_groups(cls, df, groups_num):
        """ä¾æ®å› å­å€¼ï¼Œåˆ¤æ–­æ˜¯åœ¨ç¬¬å‡ ç»„"""
        if "group" in list(df.columns):
            df = df.drop(columns=["group"])
        df = df.sort_values(["fac"], ascending=True)
        each_group = round(df.shape[0] / groups_num)
        l = list(
            map(
                lambda x, y: [x] * y,
                list(range(1, groups_num + 1)),
                [each_group] * groups_num,
            )
        )
        l = reduce(lambda x, y: x + y, l)
        if len(l) < df.shape[0]:
            l = l + [groups_num] * (df.shape[0] - len(l))
        l = l[: df.shape[0]]
        df.insert(0, "group", l)
        return df

    def get_data(self, groups_num):
        """æ‹¼æ¥å› å­æ•°æ®å’Œæ¯æœˆæ”¶ç›Šç‡æ•°æ®ï¼Œå¹¶å¯¹æ¶¨åœå’Œè·Œåœè‚¡åŠ ä»¥å¤„ç†"""
        self.data = pd.merge(
            self.rets_monthly, self.factors, how="inner", on=["date", "code"]
        )
        self.ic_icir_and_rank,self.big_small_rankic = self.get_ic_icir_and_rank(self.data)
        self.data = self.data.groupby("date").apply(
            lambda x: self.get_groups(x, groups_num)
        )
        self.wind_out = self.data.copy()
        self.factor_turnover_rates = self.data.pivot(
            index="date", columns="code", values="group"
        )
        rates = []
        for i in range(1, groups_num + 1):
            son = (self.factor_turnover_rates == i) + 0
            son1 = son.diff()
            # self.factor_turnover_rates = self.factor_turnover_rates.diff()
            change = ((np.abs(np.sign(son1)) == 1) + 0).sum(axis=1)
            still = (((son1 == 0) + 0) * son).sum(axis=1)
            rate = change / (change + still)
            rates.append(rate.to_frame(f"group{i}"))
        rates = pd.concat(rates, axis=1).fillna(0)
        self.factor_turnover_rates = rates
        self.data = self.data.reset_index(drop=True)

    def to_group_ret(self, l):
        """æ¯ä¸€ç»„çš„å¹´åŒ–æ”¶ç›Šç‡"""
        # week_here
        ret = l[-1] / len(l) * self.freq_ctrl.counts_one_year
        return ret

    def make_start_to_one(self, l):
        """è®©å‡€å€¼åºåˆ—çš„ç¬¬ä¸€ä¸ªæ•°å˜æˆ1"""
        min_date = self.factors.date.min()
        add_date = min_date - pd.DateOffset(weeks=1)
        add_l = pd.Series([0], index=[add_date])
        l = pd.concat([add_l, l])
        return l

    def get_group_rets_net_values(
        self, groups_num=10, value_weighted=False, trade_cost_double_side=0
    ):
        """è®¡ç®—ç»„å†…æ¯ä¸€æœŸçš„å¹³å‡æ”¶ç›Šï¼Œç”Ÿæˆæ¯æ—¥æ”¶ç›Šç‡åºåˆ—å’Œå‡€å€¼åºåˆ—"""
        if value_weighted:
            cap_value = self.capital.copy()
            # week_here
            cap_value = cap_value.resample(self.freq).last().shift(1)
            cap_value = cap_value * self.tris_monthly
            # cap_value=np.log(cap_value)
            cap_value = cap_value.stack().reset_index()
            cap_value.columns = ["date", "code", "cap_value"]
            self.data = pd.merge(self.data, cap_value, on=["date", "code"])

            def in_g(df):
                df.cap_value = df.cap_value / df.cap_value.sum()
                df.ret = df.ret * df.cap_value
                return df.ret.sum()

            self.group_rets = self.data.groupby(["date", "group"]).apply(in_g)
            self.rets_all = self.data.groupby(["date"]).apply(in_g)
            self.group_rets_std = "å¸‚å€¼åŠ æƒæš‚æœªè®¾ç½®è¯¥åŠŸèƒ½ï¼Œæ•¬è¯·æœŸå¾…ğŸŒ™"
        else:
            self.group_rets = self.data.groupby(["date", "group"]).apply(
                lambda x: x.ret.mean()
            )
            self.rets_all = self.data.groupby(["date"]).apply(lambda x: x.ret.mean())
            self.group_rets_stds = self.data.groupby(["date", "group"]).ret.std()
            self.group_rets_std = (
                self.group_rets_stds.reset_index().groupby("group").mean()
            )
            self.group_rets_skews = self.data.groupby(["date", "group"]).ret.skew()
            self.group_rets_skew = (
                self.group_rets_skews.reset_index().groupby("group").mean()
            )
        # dropnaæ˜¯å› ä¸ºå¦‚æœè‚¡ç¥¨è¡Œæƒ…æ•°æ®æ¯”å› å­æ•°æ®çš„æˆªæ­¢æ—¥æœŸæ™šï¼Œè€Œæœ€åä¸€ä¸ªæœˆå‘ç”Ÿæœˆåˆè·Œåœæ—¶ï¼Œä¼šé€ æˆæœ€åæŸç»„å¤šå‡ºä¸€ä¸ªæœˆçš„æ•°æ®
        self.group_rets = self.group_rets.unstack()
        self.group_rets = self.group_rets[
            self.group_rets.index <= self.factors.date.max()
        ]
        self.group_rets.columns = list(map(str, list(self.group_rets.columns)))
        self.group_rets = self.group_rets.add_prefix("group")
        self.group_rets = (
            self.group_rets - self.factor_turnover_rates * trade_cost_double_side
        )
        self.rets_all = (
            self.rets_all
            - self.factor_turnover_rates.mean(axis=1) * trade_cost_double_side
        ).dropna()
        self.long_short_rets = (
            self.group_rets["group1"] - self.group_rets["group" + str(groups_num)]
        )
        self.inner_rets_long = self.group_rets.group1 - self.rets_all
        self.inner_rets_short = (
            self.rets_all - self.group_rets["group" + str(groups_num)]
        )
        self.long_short_net_values = self.make_start_to_one(
            self.long_short_rets.cumsum()
        )
        self.long_minus_market_rets=self.group_rets.group1-self.market_ret
        
        if self.long_short_net_values[-1] <= self.long_short_net_values[0]:
            self.long_short_rets = (
                self.group_rets["group" + str(groups_num)] - self.group_rets["group1"]
            )
            self.long_short_net_values = self.make_start_to_one(
                self.long_short_rets.cumsum()
            )
            self.inner_rets_long = (
                self.group_rets["group" + str(groups_num)] - self.rets_all
            )
            self.inner_rets_short = self.rets_all - self.group_rets.group1
            self.long_minus_market_rets=self.group_rets['group'+str(groups_num)]-self.market_ret
        self.long_minus_market_nets=self.make_start_to_one(self.long_minus_market_rets.dropna().cumsum())
        self.inner_long_net_values = self.make_start_to_one(
            self.inner_rets_long.cumsum()
        )
        self.inner_short_net_values = self.make_start_to_one(
            self.inner_rets_short.cumsum()
        )
        self.group_rets = self.group_rets.assign(long_short=self.long_short_rets)
        self.group_net_values = self.group_rets.cumsum()
        self.group_net_values = self.group_net_values.apply(self.make_start_to_one)
        a = groups_num ** (0.5)
        # åˆ¤æ–­æ˜¯å¦è¦ä¸¤ä¸ªå› å­ç”»è¡¨æ ¼
        if a == int(a):
            self.square_rets = (
                self.group_net_values.iloc[:, :-1].apply(self.to_group_ret).to_numpy()
            )
            self.square_rets = self.square_rets.reshape((int(a), int(a)))
            self.square_rets = pd.DataFrame(
                self.square_rets,
                columns=list(range(1, int(a) + 1)),
                index=list(range(1, int(a) + 1)),
            )
            print("è¿™æ˜¯self.square_rets", self.square_rets)

    def get_long_short_comments(self, on_paper=False):
        """è®¡ç®—å¤šç©ºå¯¹å†²çš„ç›¸å…³è¯„ä»·æŒ‡æ ‡
        åŒ…æ‹¬å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€æœˆåº¦èƒœç‡ã€æœ€å¤§å›æ’¤ç‡"""
        # week_here
        self.long_short_ret_yearly = self.long_short_net_values[-1] * (
            self.freq_ctrl.counts_one_year / len(self.long_short_net_values)
        )
        self.inner_long_ret_yearly = self.inner_long_net_values[-1] * (
            self.freq_ctrl.counts_one_year / len(self.inner_long_net_values)
        )
        self.inner_short_ret_yearly = self.inner_short_net_values[-1] * (
            self.freq_ctrl.counts_one_year / len(self.inner_short_net_values)
        )
        self.group1_ret_yearly= self.group_net_values["group1"][-1] * (
            self.freq_ctrl.counts_one_year / len(self.group_net_values.group1)
        )
        self.group10_ret_yearly = self.group_net_values["group10"][-1] * (
            self.freq_ctrl.counts_one_year / len(self.group_net_values.group10)
        )
        # week_here
        self.long_short_vol_yearly = np.std(self.long_short_rets) * (
            self.freq_ctrl.counts_one_year**0.5
        )
        self.long_short_info_ratio = (
            self.long_short_ret_yearly / self.long_short_vol_yearly
        )
        self.long_short_win_times = len(self.long_short_rets[self.long_short_rets > 0])
        self.long_short_win_ratio = self.long_short_win_times / len(
            self.long_short_rets
        )
        self.max_retreat = -(
            (self.long_short_net_values + 1)
            / (self.long_short_net_values + 1).expanding(1).max()
            - 1
        ).min()
        if on_paper:
            self.long_short_comments = pd.DataFrame(
                {
                    "è¯„ä»·æŒ‡æ ‡": [
                        self.long_short_ret_yearly,
                        self.long_short_vol_yearly,
                        self.long_short_info_ratio,
                        self.long_short_win_ratio,
                        self.max_retreat,
                    ]
                },
                # week_here
                index=[
                    "å¹´åŒ–æ”¶ç›Šç‡",
                    "å¹´åŒ–æ³¢åŠ¨ç‡",
                    "æ”¶ç›Šæ³¢åŠ¨æ¯”",
                    f"{self.freq_ctrl.comment_name}åº¦èƒœç‡",
                    "æœ€å¤§å›æ’¤ç‡",
                ],
            )
        else:
            self.long_short_comments = pd.DataFrame(
                {
                    "è¯„ä»·æŒ‡æ ‡": [
                        self.long_short_ret_yearly,
                        self.long_short_vol_yearly,
                        self.long_short_info_ratio,
                        self.long_short_win_ratio,
                        self.max_retreat,
                    ]
                },
                # week_here
                index=[
                    "å¹´åŒ–æ”¶ç›Šç‡",
                    "å¹´åŒ–æ³¢åŠ¨ç‡",
                    "ä¿¡æ¯æ¯”ç‡",
                    f"{self.freq_ctrl.comment_name}åº¦èƒœç‡",
                    "æœ€å¤§å›æ’¤ç‡",
                ],
            )

    def get_total_comments(self, groups_num):
        """ç»¼åˆICã€ICIRã€RankICã€RankICIR,å¹´åŒ–æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€ä¿¡æ¯æ¯”ç‡ã€èƒœç‡ã€æœ€å¤§å›æ’¤ç‡"""
        rankic = self.rankics.mean()
        rankic_win = self.rankics[self.rankics * rankic > 0]
        rankic_win_ratio = rankic_win.dropna().shape[0] / self.rankics.dropna().shape[0]
        self.factor_cross_skew_after_neu = self.__factors_out.skew(axis=1).mean()
        if self.ic_icir_and_rank.iloc[2, 0] > 0:
            self.factor_turnover_rate = self.factor_turnover_rates[
                f"group{groups_num}"
            ].mean()
        else:
            self.factor_turnover_rate = self.factor_turnover_rates["group1"].mean()
        self.total_comments = pd.concat(
            [
                self.ic_icir_and_rank,
                pd.DataFrame(
                    {"è¯„ä»·æŒ‡æ ‡": [rankic_win_ratio]},
                    index=["RankICèƒœç‡"],
                ),
                self.long_short_comments,
                # week_here
                pd.DataFrame(
                    {
                        "è¯„ä»·æŒ‡æ ‡": [
                            self.factor_turnover_rate,
                            self.factor_cover,
                            self.pos_neg_rate,
                            self.factor_cross_skew,
                            self.inner_long_ret_yearly,
                            self.inner_long_ret_yearly
                            / (
                                self.inner_long_ret_yearly + self.inner_short_ret_yearly
                            ),
                            self.corr_itself,
                        ]
                    },
                    index=[
                        f"å¤šå¤´{self.freq_ctrl.comment_name}å‡æ¢æ‰‹",
                        "å› å­è¦†ç›–ç‡",
                        "å› å­æ­£å€¼å æ¯”",
                        "å› å­æˆªé¢ååº¦",
                        "å¤šå¤´è¶…å‡æ”¶ç›Š",
                        "å¤šå¤´æ”¶ç›Šå æ¯”",
                        "ä¸€é˜¶è‡ªç›¸å…³æ€§",
                    ],
                ),
                self.big_small_rankic,
                pd.DataFrame(
                    {
                        "è¯„ä»·æŒ‡æ ‡": [
                            self.group1_ret_yearly,
                            self.group10_ret_yearly,
                        ]
                    },
                    index=[
                        "1ç»„å¹´åŒ–æ”¶ç›Š",
                        "10ç»„å¹´åŒ–æ”¶ç›Š",
                    ]
                )
            ]
        )
        # print(self.total_comments)
        self.group_mean_rets_monthly = self.group_rets.drop(
            columns=["long_short"]
        ).mean()
        # self.group_mean_rets_monthly = (
        #     self.group_mean_rets_monthly - self.group_mean_rets_monthly.mean()
        # )
        mar=self.market_ret.loc[self.factors_out.index]
        self.group_mean_rets_monthly = (
            self.group_mean_rets_monthly - mar.mean()
        )*self.freq_ctrl.counts_one_year

    def plot_net_values(self, y2, filename, iplot=1, ilegend=1, without_breakpoint=0):
        """ä½¿ç”¨matplotlibæ¥ç”»å›¾ï¼Œy2ä¸ºæ˜¯å¦å¯¹å¤šç©ºç»„åˆé‡‡ç”¨åŒyè½´"""
        if not iplot:
            fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(33, 8))
            self.group_net_values.plot(secondary_y=y2, rot=60, ax=ax[0])
            self.group_net_values.plot(secondary_y=y2, ax=ax[0])
            b = self.rankics.copy()
            b.index = [int(i.year) if i.month == 1 else "" for i in list(b.index)]
            b.plot(kind="bar", rot=60, ax=ax[1])
            self.factor_cross_stds.plot(rot=60, ax=ax[2])

            filename_path = filename + ".png"
            if not STATES["NO_SAVE"]:
                plt.savefig(filename_path)
        else:

            tris = self.group_net_values.drop(columns=['long_short'])
            if without_breakpoint:
                tris = tris.dropna()
            figs = cf.figures(
                tris,
                [
                    dict(kind="line", y=list(tris.columns)),
                    # dict(kind="bar", y="å„ç»„æœˆå‡è¶…å‡æ”¶ç›Š"),
                    # dict(kind="bar", y="rankic"),
                ],
                asList=True,
            )
            comments = (
                self.total_comments.applymap(lambda x: round(x, 4))
                .rename(index={"RankICå‡å€¼tå€¼": "RankIC.t"})
                .reset_index()
            )
            here = pd.concat(
                [
                    comments.iloc[:6, :].reset_index(drop=True),
                    comments.iloc[6:12, :].reset_index(drop=True),
                    comments.iloc[12:18, :].reset_index(drop=True),
                    comments.iloc[18:, :].reset_index(drop=True),
                ],
                axis=1,
            )
            here.columns = ["ä¿¡æ¯ç³»æ•°", "ç»“æœ", "ç»©æ•ˆæŒ‡æ ‡", "ç»“æœ", "å…¶ä»–æŒ‡æ ‡", "ç»“æœ","å•ä¾§","ç»“æœ"]
            # here=here.to_numpy().tolist()+[['ä¿¡æ¯ç³»æ•°','ç»“æœ','ç»©æ•ˆæŒ‡æ ‡','ç»“æœ']]
            table = FF.create_table(here.iloc[::-1],xgap=0)
            table.update_yaxes(matches=None)
            pic2 = go.Figure(
                go.Bar(
                    y=list(self.group_mean_rets_monthly),
                    x=[
                        i.replace("roup", "")
                        for i in list(self.group_mean_rets_monthly.index)
                    ],
                )
            )
            # table=go.Figure([go.Table(header=dict(values=list(here.columns)),cells=dict(values=here.to_numpy().tolist()))])
            pic3_data = go.Bar(y=list(self.rankics.rankic), x=list(self.rankics.index),marker_color="red")
            pic3 = go.Figure(data=[pic3_data])
            pic5_data = go.Line(
                y=list(self.rankics.rankic.cumsum()),
                x=list(self.rankics.index),
                name="y2",
                yaxis="y2",
            )
            pic5_layout = go.Layout(yaxis2=dict(title="y2", side="right"))
            pic5 = go.Figure(data=[pic5_data], layout=pic5_layout)
            figs.append(table)
            figs = [figs[-1]] + figs[:-1]
            figs.append(pic2)
            figs = [figs[0], figs[1], figs[-1], pic3]
            figs[1].update_layout(
                legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
            )
            figs[3].update_layout(yaxis2=dict(title="y2", side="right"))
            # twins=pd.concat([self.long_short_net_values,self.long_minus_market_nets],axis=1)
            pic4=go.Figure()
            figs.append(pic4)
            figs[4].add_trace(go.Line(x=self.long_short_net_values.index,y=self.long_short_net_values,name='long_short'))
            figs[4].add_trace(go.Line(x=self.long_minus_market_nets.index,y=self.long_minus_market_nets,name='long_minus_market'))
            figs.append(pic5)
            base_layout = cf.tools.get_base_layout(figs)

            sp = cf.subplots(
                figs,
                shape=(2, 14),
                base_layout=base_layout,
                vertical_spacing=0.15,
                horizontal_spacing=0.025,
                shared_yaxes=False,
                specs=[
                    [
                        # None,
                        {"rowspan": 2, "colspan": 5},
                        None,
                        None,
                        None,
                        None,
                        {"rowspan": 2, "colspan": 3},
                        None,
                        None,
                        {"colspan": 3},
                        None,
                        None,
                        {"colspan": 3},
                        None,
                        None,
                    ],
                    [
                        # None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        None,
                        {"colspan": 3},
                        None,
                        None,
                        {"colspan": 3},
                        None,
                        None,
                    ],
                ],
                subplot_titles=[
                    "å‡€å€¼æ›²çº¿",
                    "å„ç»„æœˆå‡è¶…å‡æ”¶ç›Š",
                    "Rank ICæ—¶åºå›¾",
                    "ç»©æ•ˆæŒ‡æ ‡",
                ],
            )
            sp["layout"].update(showlegend=ilegend,width=1780,height=230,margin=dict(l=0, r=0, b=0, t=0, pad=0))
            # sp['colors']=['#FF5733', '#33FF57', '#3357FF']
            # los=sp['layout']['annotations']
            # los[0]['font']['color']='#000000'
            # los[1]['font']['color']='#000000'
            # los[2]['font']['color']='#000000'
            # los[3]['font']['color']='#000000'
            # los[-1]['font']['color']='#ffffff'
            # los[-2]['font']['color']='#ffffff'
            # los[-3]['font']['color']='#ffffff'
            # los[-4]['font']['color']='#ffffff'
            # los[0]['text']=los[0]['text'][3:-4]
            # los[1]['text']=los[1]['text'][3:-4]
            # los[2]['text']=los[2]['text'][3:-4]
            # los[3]['text']=los[3]['text'][3:-4]
            # los[-1]['text']='<b>'+los[-1]['text']+'</b>'
            # los[-2]['text']='<b>'+los[-2]['text']+'</b>'
            # los[-3]['text']='<b>'+los[-3]['text']+'</b>'
            # los[-4]['text']='<b>'+los[-4]['text']+'</b>'
            # sp['layout']['annotations']=los
            # print(sp['layout']['annotations'])
            # sp['layout']['annotations'][0]['yanchor']='top'

            cf.iplot(sp)
            # tris=pd.concat([self.group_net_values,self.rankics,self.factor_turnover_rates],axis=1).rename(columns={0:'turnover_rate'})
            # sp=plyoo.make_subplots(rows=2,cols=8,vertical_spacing=.15,horizontal_spacing=.03,
            #                specs=[[{'rowspan':2,'colspan':2,'type':'domain'},None,{'rowspan':2,'colspan':4,'type':'xy'},None,None,None,{'colspan':2,'type':'xy'},None],
            #                       [None,None,None,None,None,None,{'colspan':2,'type':'xy'},None]],
            #                subplot_titles=['å‡€å€¼æ›²çº¿','Rank ICæ—¶åºå›¾','æœˆæ¢æ‰‹ç‡','ç»©æ•ˆæŒ‡æ ‡'])
            # comments=self.total_comments.applymap(lambda x:round(x,4)).rename(index={'RankICå‡å€¼tå€¼':'RankIC.t'}).reset_index()
            # here=pd.concat([comments.iloc[:5,:].reset_index(drop=True),comments.iloc[5:,:].reset_index(drop=True)],axis=1)
            # here.columns=['ä¿¡æ¯ç³»æ•°','ç»“æœ','ç»©æ•ˆæŒ‡æ ‡','ç»“æœ']
            # table=FF.create_table(here)
            # sp.add_trace(table)

    def plotly_net_values(self, filename):
        """ä½¿ç”¨plotly.expressç”»å›¾"""
        fig = pe.line(self.group_net_values)
        filename_path = filename + ".html"
        pio.write_html(fig, filename_path, auto_open=True)

    @classmethod
    @lru_cache(maxsize=None)
    def prerpare(cls):
        """é€šç”¨æ•°æ®å‡†å¤‡"""
        cls.get_rets_month()

    def run(
        self,
        groups_num=10,
        neutralize=False,
        trade_cost_double_side=0,
        value_weighted=False,
        y2=False,
        plt_plot=True,
        plotly_plot=False,
        filename="åˆ†ç»„å‡€å€¼å›¾",
        print_comments=True,
        comments_writer=None,
        net_values_writer=None,
        rets_writer=None,
        comments_sheetname=None,
        net_values_sheetname=None,
        rets_sheetname=None,
        on_paper=False,
        sheetname=None,
        only_cap=0,
        iplot=1,
        ilegend=1,
        without_breakpoint=0,
        beauty_comments=0,
    ):
        """è¿è¡Œå›æµ‹éƒ¨åˆ†"""
        if comments_writer and not (comments_sheetname or sheetname):
            raise IOError("æŠŠtotal_commentsè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
        if net_values_writer and not (net_values_sheetname or sheetname):
            raise IOError("æŠŠgroup_net_valuesè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
        if rets_writer and not (rets_sheetname or sheetname):
            raise IOError("æŠŠgroup_retsè¾“å‡ºåˆ°excelä¸­æ—¶ï¼Œå¿…é¡»æŒ‡å®šsheetnameğŸ¤’")
        if neutralize:
            self.get_log_cap()
            self.get_neutral_factors(only_cap=only_cap)
        self.__factors_out = self.factors.copy()
        self.factors = self.factors.shift(1)
        self.factors = self.factors.stack().reset_index()
        self.factors.columns = ["date", "code", "fac"]
        self.get_data(groups_num)
        self.get_group_rets_net_values(
            groups_num=groups_num,
            value_weighted=value_weighted,
            trade_cost_double_side=trade_cost_double_side,
        )
        self.get_long_short_comments(on_paper=on_paper)
        self.get_total_comments(groups_num=groups_num)
        if on_paper:
            group1_ttest = ss.ttest_1samp(self.group_rets.group1, 0).pvalue
            group10_ttest = ss.ttest_1samp(
                self.group_rets[f"group{groups_num}"], 0
            ).pvalue
            group_long_short_ttest = ss.ttest_1samp(self.long_short_rets, 0).pvalue
            group1_ret = self.group_rets.group1.mean()
            group10_ret = self.group_rets[f"group{groups_num}"].mean()
            group_long_short_ret = self.long_short_rets.mean()
            papers = pd.DataFrame(
                {
                    "è¯„ä»·æŒ‡æ ‡": [
                        group1_ttest,
                        group10_ttest,
                        group_long_short_ttest,
                        group1_ret,
                        group10_ret,
                        group_long_short_ret,
                    ]
                },
                index=[
                    "åˆ†ç»„1på€¼",
                    f"åˆ†ç»„{groups_num}på€¼",
                    f"åˆ†ç»„1-åˆ†ç»„{groups_num}på€¼",
                    "åˆ†ç»„1æ”¶ç›Šç‡",
                    f"åˆ†ç»„{groups_num}æ”¶ç›Šç‡",
                    f"åˆ†ç»„1-åˆ†ç»„{groups_num}æ”¶ç›Šç‡",
                ],
            )
            self.total_comments = pd.concat([papers, self.total_comments])

        if plt_plot:
            if not STATES["NO_PLOT"]:
                if filename:
                    self.plot_net_values(
                        y2=y2,
                        filename=filename,
                        iplot=iplot,
                        ilegend=bool(ilegend),
                        without_breakpoint=without_breakpoint,
                    )
                else:
                    self.plot_net_values(
                        y2=y2,
                        filename=self.factors_file.split(".")[-2].split("/")[-1]
                        + str(groups_num)
                        + "åˆ†ç»„",
                        iplot=iplot,
                        ilegend=bool(ilegend),
                        without_breakpoint=without_breakpoint,
                    )
                plt.show()
        if plotly_plot:
            if not STATES["NO_PLOT"]:
                if filename:
                    self.plotly_net_values(filename=filename)
                else:
                    self.plotly_net_values(
                        filename=self.factors_file.split(".")[-2].split("/")[-1]
                        + str(groups_num)
                        + "åˆ†ç»„"
                    )
        if print_comments:
            if not STATES["NO_COMMENT"]:
                tb = Texttable()
                tb.set_cols_width(
                    [8] * 5 + [9] + [8] * 2 + [7] * 2 + [8] + [8] + [9] + [10] * 5
                )
                tb.set_cols_dtype(["f"] * 18)
                tb.header(list(self.total_comments.T.columns))
                tb.add_rows(self.total_comments.T.to_numpy(), header=False)
                print(tb.draw())
        if sheetname:
            if comments_writer:
                if not on_paper:
                    total_comments = self.total_comments.copy()
                    tc = list(total_comments.è¯„ä»·æŒ‡æ ‡)
                    if beauty_comments:
                        tc[0] = str(round(tc[0] * 100, 2)) + "%"
                        tc[1] = str(round(tc[1], 2))
                        tc[2] = str(round(tc[2] * 100, 2)) + "%"
                        tc[3] = str(round(tc[3], 2))
                        tc[4] = str(round(tc[4], 2))
                        tc[5] = str(round(tc[5] * 100, 2)) + "%"
                        tc[6] = str(round(tc[6] * 100, 2)) + "%"
                        tc[7] = str(round(tc[7] * 100, 2)) + "%"
                        tc[8] = str(round(tc[8], 2))
                        tc[9] = str(round(tc[9] * 100, 2)) + "%"
                        tc[10] = str(round(tc[10] * 100, 2)) + "%"
                        tc[11] = str(round(tc[11] * 100, 2)) + "%"
                        tc[12] = str(round(tc[12] * 100, 2)) + "%"
                        tc[13] = str(round(tc[13] * 100, 2)) + "%"
                        tc[14] = str(round(tc[14], 2))
                        tc[15] = str(round(tc[15], 2))
                        tc[16] = str(round(tc[16] * 100, 2)) + "%"
                        tc[17] = str(round(tc[17] * 100, 2)) + "%"
                    tc = tc + list(self.group_mean_rets_monthly)
                    new_total_comments = pd.DataFrame(
                        {sheetname: tc},
                        index=list(total_comments.index)
                        + [f"ç¬¬{i}ç»„" for i in range(1, groups_num + 1)],
                    )
                    new_total_comments.to_excel(comments_writer, sheet_name=sheetname)
                    rankic_twins = pd.concat(
                        [self.rankics.rankic, self.rankics.rankic.cumsum()], axis=1
                    )
                    rankic_twins.columns = ["RankIC", "RankICç´¯ç§¯"]
                    rankic_twins.to_excel(
                        comments_writer, sheet_name=sheetname + "RankIC"
                    )
                else:
                    self.total_comments.rename(
                        columns={"è¯„ä»·æŒ‡æ ‡": sheetname}
                    ).to_excel(comments_writer, sheet_name=sheetname)
            if net_values_writer:
                groups_net_values = self.group_net_values.copy()
                groups_net_values.index = groups_net_values.index.strftime("%Y/%m/%d")
                groups_net_values.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(groups_net_values.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                groups_net_values.to_excel(net_values_writer, sheet_name=sheetname)
            if rets_writer:
                group_rets = self.group_rets.copy()
                group_rets.index = group_rets.index.strftime("%Y/%m/%d")
                group_rets.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(group_rets.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                group_rets.to_excel(rets_writer, sheet_name=sheetname)
        else:
            if comments_writer and comments_sheetname:
                total_comments = self.total_comments.copy()
                tc = list(total_comments.è¯„ä»·æŒ‡æ ‡)
                tc[0] = str(round(tc[0] * 100, 2)) + "%"
                tc[1] = str(round(tc[1], 2))
                tc[2] = str(round(tc[2] * 100, 2)) + "%"
                tc[3] = str(round(tc[3], 2))
                tc[4] = str(round(tc[4], 2))
                tc[5] = str(round(tc[5] * 100, 2)) + "%"
                tc[6] = str(round(tc[6] * 100, 2)) + "%"
                tc[7] = str(round(tc[7] * 100, 2)) + "%"
                tc[8] = str(round(tc[8], 2))
                tc[9] = str(round(tc[9] * 100, 2)) + "%"
                tc[10] = str(round(tc[10] * 100, 2)) + "%"
                tc[11] = str(round(tc[11] * 100, 2)) + "%"
                tc[12] = str(round(tc[12] * 100, 2)) + "%"
                tc[13] = str(round(tc[13] * 100, 2)) + "%"
                tc[14] = str(round(tc[14], 2))
                tc[15] = str(round(tc[15], 2))
                tc[16] = str(round(tc[16] * 100, 2)) + "%"
                tc[17] = str(round(tc[17] * 100, 2)) + "%"
                new_total_comments = pd.DataFrame(
                    {comments_sheetname: tc}, index=total_comments.index
                )
                new_total_comments.T.to_excel(
                    comments_writer, sheet_name=comments_sheetname
                )
            if net_values_writer and net_values_sheetname:
                groups_net_values = self.group_net_values.copy()
                groups_net_values.index = groups_net_values.index.strftime("%Y/%m/%d")
                groups_net_values.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(groups_net_values.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                groups_net_values.to_excel(
                    net_values_writer, sheet_name=net_values_sheetname
                )
            if rets_writer and rets_sheetname:
                group_rets = self.group_rets.copy()
                group_rets.index = group_rets.index.strftime("%Y/%m/%d")
                group_rets.columns = [
                    f"åˆ†ç»„{i}" for i in range(1, len(list(group_rets.columns)))
                ] + ["å¤šç©ºå¯¹å†²ï¼ˆå³è½´ï¼‰"]
                group_rets.to_excel(rets_writer, sheet_name=rets_sheetname)


@do_on_dfs
class pure_moonnight(object):
    """å°è£…é€‰è‚¡æ¡†æ¶"""

    __slots__ = ["shen"]

    def __init__(
        self,
        factors: pd.DataFrame,
        groups_num: int = 10,
        freq: str = "W",
        neutralize: bool = 0,
        trade_cost_double_side: float = 0,
        value_weighted: bool = 0,
        y2: bool = 0,
        plt_plot: bool = 1,
        plotly_plot: bool = 0,
        filename: str = "åˆ†ç»„å‡€å€¼å›¾",
        time_start: int = None,
        time_end: int = None,
        print_comments: bool = 1,
        comments_writer: pd.ExcelWriter = None,
        net_values_writer: pd.ExcelWriter = None,
        rets_writer: pd.ExcelWriter = None,
        comments_sheetname: str = None,
        net_values_sheetname: str = None,
        rets_sheetname: str = None,
        on_paper: bool = 0,
        sheetname: str = None,
        no_read_indu: bool = 0,
        only_cap: bool = 0,
        iplot: bool = 1,
        ilegend: bool = 0,
        without_breakpoint: bool = 0,
        total_cap: bool = 0,
    ) -> None:
        """ä¸€é”®å›æµ‹æ¡†æ¶ï¼Œæµ‹è¯•å•å› å­çš„æœˆé¢‘è°ƒä»“çš„åˆ†ç»„è¡¨ç°
        æ¯æœˆæœˆåº•è®¡ç®—å› å­å€¼ï¼Œæœˆåˆç¬¬ä¸€å¤©å¼€ç›˜æ—¶ä¹°å…¥ï¼Œæœˆæœ«æ”¶ç›˜æœ€åä¸€å¤©æ”¶ç›˜æ—¶å–å‡º
        å‰”é™¤ä¸Šå¸‚ä¸è¶³60å¤©çš„ï¼Œåœç‰Œå¤©æ•°è¶…è¿‡ä¸€åŠçš„ï¼Œstå¤©æ•°è¶…è¿‡ä¸€åŠçš„
        æœˆæœ«æ”¶ç›˜è·Œåœçš„ä¸å–å‡ºï¼Œæœˆåˆå¼€ç›˜æ¶¨åœçš„ä¸ä¹°å…¥
        ç”±æœ€å¥½ç»„å’Œæœ€å·®ç»„çš„å¤šç©ºç»„åˆæ„æˆå¤šç©ºå¯¹å†²ç»„

        Parameters
        ----------
        factors : pd.DataFrame
            è¦ç”¨äºæ£€æµ‹çš„å› å­å€¼ï¼Œindexæ˜¯æ—¶é—´ï¼Œcolumnsæ˜¯è‚¡ç¥¨ä»£ç 
        groups_num : int, optional
            åˆ†ç»„æ•°é‡, by default 10
        freq : str, optional
            å›æµ‹é¢‘ç‡, by default 'M'
        neutralize : bool, optional
            å¯¹æµé€šå¸‚å€¼å–è‡ªç„¶å¯¹æ•°ï¼Œä»¥å®Œæˆè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–, by default 0
        trade_cost_double_side : float, optional
            äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
        value_weighted : bool, optional
            æ˜¯å¦ç”¨æµé€šå¸‚å€¼åŠ æƒ, by default 0
        y2 : bool, optional
            ç”»å›¾æ—¶æ˜¯å¦å¯ç”¨ç¬¬äºŒyè½´, by default 0
        plt_plot : bool, optional
            å°†åˆ†ç»„å‡€å€¼æ›²çº¿ç”¨matplotlibç”»å‡ºæ¥, by default 1
        plotly_plot : bool, optional
            å°†åˆ†ç»„å‡€å€¼æ›²çº¿ç”¨plotlyç”»å‡ºæ¥, by default 0
        filename : str, optional
            åˆ†ç»„å‡€å€¼æ›²çº¿çš„å›¾ä¿å­˜çš„åç§°, by default "åˆ†ç»„å‡€å€¼å›¾"
        time_start : int, optional
            å›æµ‹èµ·å§‹æ—¶é—´, by default None
        time_end : int, optional
            å›æµ‹ç»ˆæ­¢æ—¶é—´, by default None
        print_comments : bool, optional
            æ˜¯å¦æ‰“å°å‡ºè¯„ä»·æŒ‡æ ‡, by default 1
        comments_writer : pd.ExcelWriter, optional
            ç”¨äºè®°å½•è¯„ä»·æŒ‡æ ‡çš„xlsxæ–‡ä»¶, by default None
        net_values_writer : pd.ExcelWriter, optional
            ç”¨äºè®°å½•å‡€å€¼åºåˆ—çš„xlsxæ–‡ä»¶, by default None
        rets_writer : pd.ExcelWriter, optional
            ç”¨äºè®°å½•æ”¶ç›Šç‡åºåˆ—çš„xlsxæ–‡ä»¶, by default None
        comments_sheetname : str, optional
            åœ¨è®°å½•è¯„ä»·æŒ‡æ ‡çš„xlsxæ–‡ä»¶ä¸­ï¼Œè¯¥å·¥ä½œè¡¨çš„åç§°, by default None
        net_values_sheetname : str, optional
            åœ¨è®°å½•å‡€å€¼åºåˆ—çš„xlsxæ–‡ä»¶ä¸­ï¼Œè¯¥å·¥ä½œè¡¨çš„åç§°, by default None
        rets_sheetname : str, optional
            åœ¨è®°å½•æ”¶ç›Šç‡åºåˆ—çš„xlsxæ–‡ä»¶ä¸­ï¼Œè¯¥å·¥ä½œè¡¨çš„åç§°, by default None
        on_paper : bool, optional
            ä½¿ç”¨å­¦æœ¯åŒ–è¯„ä»·æŒ‡æ ‡, by default 0
        sheetname : str, optional
            å„ä¸ªpd.Excelwriterä¸­å·¥ä½œè¡¨çš„ç»Ÿä¸€åç§°, by default None
        no_read_indu : bool, optional
            ä¸è¯»å…¥è¡Œä¸šæ•°æ®, by default 0
        only_cap : bool, optional
            ä»…åšå¸‚å€¼ä¸­æ€§åŒ–, by default 0
        iplot : bool, optional
            ä½¿ç”¨cufflinkså‘ˆç°å›æµ‹ç»“æœ, by default 1
        ilegend : bool, optional
            ä½¿ç”¨cufflinksç»˜å›¾æ—¶ï¼Œæ˜¯å¦æ˜¾ç¤ºå›¾ä¾‹, by default 1
        without_breakpoint : bool, optional
            ç”»å›¾çš„æ—¶å€™æ˜¯å¦å»é™¤é—´æ–­ç‚¹, by default 0
        total_cap : bool, optional
            åŠ æƒå’Œè¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–æ—¶ä½¿ç”¨æ€»å¸‚å€¼, by default 0
        """

        if not isinstance(factors, pd.DataFrame):
            factors = factors()
        if comments_writer is None and sheetname is not None:
            from pure_ocean_breeze.state.states import COMMENTS_WRITER

            comments_writer = COMMENTS_WRITER
        if net_values_writer is None and sheetname is not None:
            from pure_ocean_breeze.state.states import NET_VALUES_WRITER

            net_values_writer = NET_VALUES_WRITER
        if not on_paper:
            from pure_ocean_breeze.state.states import ON_PAPER

            on_paper = ON_PAPER
        if time_start is None:
            from pure_ocean_breeze.state.states import MOON_START

            if MOON_START is not None:
                factors = factors[factors.index >= pd.Timestamp(str(MOON_START))]
        else:
            factors = factors[factors.index >= pd.Timestamp(str(time_start))]
        if time_end is None:
            from pure_ocean_breeze.state.states import MOON_END

            if MOON_END is not None:
                factors = factors[factors.index <= pd.Timestamp(str(MOON_END))]
        else:
            factors = factors[factors.index <= pd.Timestamp(str(time_end))]
        if neutralize == 0:
            no_read_indu = 1
        if only_cap + no_read_indu > 0:
            only_cap = no_read_indu = 1
        if iplot:
            print_comments = 0
        if total_cap:
            if freq == "M":
                self.shen = pure_moon_b(
                    freq=freq,
                    no_read_indu=no_read_indu,
                )
            elif freq == "W":
                self.shen = pure_week_b(
                    freq=freq,
                    no_read_indu=no_read_indu,
                )
        else:
            if freq == "M":
                self.shen = pure_moon(
                    freq=freq,
                    no_read_indu=no_read_indu,
                )
            elif freq == "W":
                self.shen = pure_week(
                    freq=freq,
                    no_read_indu=no_read_indu,
                )
            elif freq == "D":
                self.shen = pure_moon(
                    freq=freq,
                    no_read_indu=no_read_indu,
                )
        self.shen.set_basic_data(
            total_cap=total_cap,
        )
        self.shen.set_factor_df_date_as_index(factors)
        self.shen.prerpare()
        self.shen.run(
            groups_num=groups_num,
            neutralize=neutralize,
            trade_cost_double_side=trade_cost_double_side,
            value_weighted=value_weighted,
            y2=y2,
            plt_plot=plt_plot,
            plotly_plot=plotly_plot,
            filename=filename,
            print_comments=print_comments,
            comments_writer=comments_writer,
            net_values_writer=net_values_writer,
            rets_writer=rets_writer,
            comments_sheetname=comments_sheetname,
            net_values_sheetname=net_values_sheetname,
            rets_sheetname=rets_sheetname,
            on_paper=on_paper,
            sheetname=sheetname,
            only_cap=only_cap,
            iplot=iplot,
            ilegend=ilegend,
            without_breakpoint=without_breakpoint,
        )

    def __call__(self) -> pd.DataFrame:
        """å¦‚æœåšäº†è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ï¼Œåˆ™è¿”å›è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­æ•°æ®

        Returns
        -------
        `pd.DataFrame`
            å¦‚æœåšäº†è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ï¼Œåˆ™è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–ä¹‹åçš„å› å­æ•°æ®ï¼Œå¦åˆ™è¿”å›åŸå› å­æ•°æ®
        """
        return self.shen.factors_out

    def comments_ten(self) -> pd.DataFrame:
        """å¯¹å›æµ‹çš„ååˆ†ç»„ç»“æœåˆ†åˆ«ç»™å‡ºè¯„ä»·

        Returns
        -------
        `pd.DataFrame`
            è¯„ä»·æŒ‡æ ‡åŒ…æ‹¬å¹´åŒ–æ”¶ç›Šç‡ã€æ€»æ”¶ç›Šç‡ã€å¹´åŒ–æ³¢åŠ¨ç‡ã€å¹´åŒ–å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤ç‡ã€èƒœç‡
        """
        rets_cols = list(self.shen.group_rets.columns)
        rets_cols = rets_cols[:-1]
        coms = []
        for i in rets_cols:
            ret = self.shen.group_rets[i]
            net = self.shen.group_net_values[i]
            com = comments_on_twins(net, ret)
            com = com.to_frame(i)
            coms.append(com)
        df = pd.concat(coms, axis=1)
        return df.T

    def comment_yearly(self) -> pd.DataFrame:
        """å¯¹å›æµ‹çš„æ¯å¹´è¡¨ç°ç»™å‡ºè¯„ä»·

        Returns
        -------
        pd.DataFrame
            å„å¹´åº¦çš„æ”¶ç›Šç‡
        """
        df = self.shen.group_net_values.resample("Y").last().pct_change()
        df.index = df.index.year
        return df


class pure_week(pure_moon): ...


class pure_moon_a(pure_moon): ...


class pure_week_a(pure_moon): ...


class pure_moon_b(pure_moon): ...


class pure_week_b(pure_moon): ...


class pure_moon_c(pure_moon): ...


class pure_week_c(pure_moon): ...


class pure_fall(object):
    # DONEï¼šä¿®æ”¹ä¸ºå› å­æ–‡ä»¶åå¯ä»¥å¸¦â€œæ—¥é¢‘_â€œï¼Œä¹Ÿå¯ä»¥ä¸å¸¦â€œæ—¥é¢‘_â€œ
    def __init__(self, daily_path: str) -> None:
        """ä¸€ä¸ªä½¿ç”¨mysqlä¸­çš„åˆ†é’Ÿæ•°æ®ï¼Œæ¥æ›´æ–°å› å­å€¼çš„æ¡†æ¶

        Parameters
        ----------
        daily_path : str
            æ—¥é¢‘å› å­å€¼å­˜å‚¨æ–‡ä»¶çš„åå­—ï¼Œè¯·ä»¥'.parquet'ç»“å°¾
        """
        self.homeplace = HomePlace()
        # å°†åˆ†é’Ÿæ•°æ®æ‹¼æˆä¸€å¼ æ—¥é¢‘å› å­è¡¨
        self.daily_factors = None
        self.daily_factors_path = self.homeplace.factor_data_file + "æ—¥é¢‘_" + daily_path

    def __call__(self, monthly=False):
        """ä¸ºäº†é˜²æ­¢å±æ€§åå¤ªå¤šï¼Œå¿˜è®°äº†è¦è°ƒç”¨å“ªä¸ªæ‰æ˜¯ç»“æœï¼Œå› æ­¤å¯ä»¥ç›´æ¥è¾“å‡ºæœˆåº¦æ•°æ®è¡¨"""
        if monthly:
            return self.monthly_factors.copy()
        else:
            try:
                return self.daily_factors.copy()
            except Exception:
                return self.monthly_factors.copy()

    def wide_to_long(self, df, i):
        """å°†å®½æ•°æ®è½¬åŒ–ä¸ºé•¿æ•°æ®ï¼Œç”¨äºå› å­è¡¨è½¬åŒ–å’Œæ‹¼æ¥"""
        df = df.stack().reset_index()
        df.columns = ["date", "code", i]
        df = df.set_index(["date", "code"])
        return df

    def de_in_group(self, df, help_names):
        """å¯¹æ¯ä¸ªæ—¶é—´ï¼Œåˆ†åˆ«åšå›å½’ï¼Œå‰”é™¤ç›¸å…³å› å­"""
        ols_order = "fac~" + "+".join(help_names)
        ols_result = smf.ols(ols_order, data=df).fit()
        params = {i: ols_result.params[i] for i in help_names}
        predict = [params[i] * df[i] for i in help_names]
        predict = reduce(lambda x, y: x + y, predict)
        df.fac = df.fac - predict - ols_result.params["Intercept"]
        df = df[["fac"]]
        return df

    def standardlize_in_cross_section(self, df):
        """
        åœ¨æ¨ªæˆªé¢ä¸Šåšæ ‡å‡†åŒ–
        è¾“å…¥çš„dfåº”ä¸ºï¼Œåˆ—åæ˜¯è‚¡ç¥¨ä»£ç ï¼Œç´¢å¼•æ˜¯æ—¶é—´
        """
        df = df.T
        df = (df - df.mean()) / df.std()
        df = df.T
        return df


class pure_fallmount(pure_fall):
    """ç»§æ‰¿è‡ªçˆ¶ç±»ï¼Œä¸“ä¸ºåšå› å­æˆªé¢æ ‡å‡†åŒ–ä¹‹åç›¸åŠ å’Œå› å­å‰”é™¤å…¶ä»–è¾…åŠ©å› å­çš„ä½œç”¨"""

    def __init__(self, monthly_factors):
        """è¾“å…¥æœˆåº¦å› å­å€¼ï¼Œä»¥è®¾å®šæ–°çš„å¯¹è±¡"""
        super(pure_fall, self).__init__()
        self.monthly_factors = monthly_factors

    def __call__(self, monthly=False):
        """ä¸ºäº†é˜²æ­¢å±æ€§åå¤ªå¤šï¼Œå¿˜è®°äº†è¦è°ƒç”¨å“ªä¸ªæ‰æ˜¯ç»“æœï¼Œå› æ­¤å¯ä»¥ç›´æ¥è¾“å‡ºæœˆåº¦æ•°æ®è¡¨"""
        if monthly:
            return self.monthly_factors.copy()
        else:
            try:
                return self.daily_factors.copy()
            except Exception:
                return self.monthly_factors.copy()

    def __add__(self, selfas):
        """è¿”å›ä¸€ä¸ªå¯¹è±¡ï¼Œè€Œéä¸€ä¸ªè¡¨æ ¼ï¼Œå¦‚éœ€è¡¨æ ¼è¯·è°ƒç”¨å¯¹è±¡"""
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac2s = []
        if not isinstance(selfas, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfas} is changed into Iterable")
            selfas = (selfas,)
        for selfa in selfas:
            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2s.append(fac2)
        for i in fac2s:
            fac1 = fac1 + i
        new_pure = pure_fallmount(fac1)
        return new_pure

    def __mul__(self, selfas):
        """å°†å‡ ä¸ªå› å­æ¨ªæˆªé¢æ ‡å‡†åŒ–ä¹‹åï¼Œä½¿å…¶éƒ½ä¸ºæ­£æ•°ï¼Œç„¶åå› å­å€¼ç›¸ä¹˜"""
        fac1 = self.standardlize_in_cross_section(self.monthly_factors)
        fac1 = fac1 - fac1.min()
        fac2s = []
        if not isinstance(selfas, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfas} is changed into Iterable")
            selfas = (selfas,)
        for selfa in selfas:
            fac2 = self.standardlize_in_cross_section(selfa.monthly_factors)
            fac2 = fac2 - fac2.min()
            fac2s.append(fac2)
        for i in fac2s:
            fac1 = fac1 * i
        new_pure = pure_fall()
        new_pure.monthly_factors = fac1
        return new_pure

    def __sub__(self, selfa):
        """è¿”å›å¯¹è±¡ï¼Œå¦‚éœ€è¡¨æ ¼ï¼Œè¯·è°ƒç”¨å¯¹è±¡"""
        tqdm.auto.tqdm.pandas()
        if not isinstance(selfa, Iterable):
            if not STATES["NO_LOG"]:
                logger.warning(f"{selfa} is changed into Iterable")
            selfa = (selfa,)
        fac_main = self.wide_to_long(self.monthly_factors, "fac")
        fac_helps = [i.monthly_factors for i in selfa]
        help_names = ["help" + str(i) for i in range(1, (len(fac_helps) + 1))]
        fac_helps = list(map(self.wide_to_long, fac_helps, help_names))
        fac_helps = pd.concat(fac_helps, axis=1)
        facs = pd.concat([fac_main, fac_helps], axis=1).dropna()
        facs = facs.groupby("date").progress_apply(
            lambda x: self.de_in_group(x, help_names)
        )
        facs = facs.unstack()
        facs.columns = list(map(lambda x: x[1], list(facs.columns)))
        new_pure = pure_fallmount(facs)
        return new_pure

    def __gt__(self, selfa):
        """ç”¨äºè¾“å‡º25åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
        x = self.monthly_factors.copy()
        y = selfa.monthly_factors.copy()
        x = x.stack().reset_index()
        y = y.stack().reset_index()
        x.columns = ["date", "code", "fac"]
        y.columns = ["date", "code", "fac"]
        shen = pure_moon()
        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 5))
        x = (
            x.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupx"})
        )
        xy = pd.merge(x, y, on=["date", "code"])
        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 5))
        xy = (
            xy.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupy"})
        )
        xy = xy.assign(fac=xy.groupx * 5 + xy.groupy)
        xy = xy[["date", "code", "fac"]]
        xy = xy.set_index(["date", "code"]).unstack()
        xy.columns = [i[1] for i in list(xy.columns)]
        new_pure = pure_fallmount(xy)
        return new_pure

    def __rshift__(self, selfa):
        """ç”¨äºè¾“å‡º100åˆ†ç»„è¡¨æ ¼ï¼Œä½¿ç”¨æ—¶ï¼Œä»¥x>>yçš„å½¢å¼ä½¿ç”¨ï¼Œå…¶ä¸­x,yå‡ä¸ºpure_fallå¯¹è±¡
        è®¡ç®—æ—¶ä½¿ç”¨çš„æ˜¯ä»–ä»¬çš„æœˆåº¦å› å­è¡¨ï¼Œå³self.monthly_factorså±æ€§ï¼Œä¸ºå®½æ•°æ®å½¢å¼çš„dataframe
        xåº”ä¸ºé¦–å…ˆç”¨æ¥çš„åˆ†ç»„çš„ä¸»å› å­ï¼Œyä¸ºåœ¨xåˆ†ç»„åçš„ç»„å†…ç»§ç»­åˆ†ç»„çš„æ¬¡å› å­"""
        x = self.monthly_factors.copy()
        y = selfa.monthly_factors.copy()
        x = x.stack().reset_index()
        y = y.stack().reset_index()
        x.columns = ["date", "code", "fac"]
        y.columns = ["date", "code", "fac"]
        shen = pure_moon()
        x = x.groupby("date").apply(lambda df: shen.get_groups(df, 10))
        x = (
            x.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupx"})
        )
        xy = pd.merge(x, y, on=["date", "code"])
        xy = xy.groupby(["date", "groupx"]).apply(lambda df: shen.get_groups(df, 10))
        xy = (
            xy.reset_index(drop=True)
            .drop(columns=["fac"])
            .rename(columns={"group": "groupy"})
        )
        xy = xy.assign(fac=xy.groupx * 10 + xy.groupy)
        xy = xy[["date", "code", "fac"]]
        xy = xy.set_index(["date", "code"]).unstack()
        xy.columns = [i[1] for i in list(xy.columns)]
        new_pure = pure_fallmount(xy)
        return new_pure


class pure_coldwinter(object):
    # DONE: å¯ä»¥è‡ªç”±æ·»åŠ å…¶ä»–è¦å‰”é™¤çš„å› å­ï¼Œæˆ–è€…æ›¿æ¢æŸäº›è¦å‰”é™¤çš„å› å­

    @classmethod
    @lru_cache(maxsize=None)
    def __init__(
        cls,
        momentum: bool = 1,
        earningsyield: bool = 1,
        growth: bool = 1,
        liquidity: bool = 1,
        size: bool = 1,
        leverage: bool = 1,
        beta: bool = 1,
        nonlinearsize: bool = 1,
        residualvolatility: bool = 1,
        booktoprice: bool = 1,
    ) -> None:
        """è¯»å…¥10ç§å¸¸ç”¨é£æ ¼å› å­ï¼Œå¹¶å¯ä»¥é¢å¤–åŠ å…¥å…¶ä»–å› å­

        Parameters
        ----------
        facs_dict : Dict, optional
            é¢å¤–åŠ å…¥çš„å› å­ï¼Œåå­—ä¸ºkeyï¼Œå› å­çŸ©é˜µä¸ºvalueï¼Œå½¢å¦‚`{'åè½¬': ret20, 'æ¢æ‰‹': tr20}`, by default None
        momentum : bool, optional
            æ˜¯å¦åˆ å»åŠ¨é‡å› å­, by default 1
        earningsyield : bool, optional
            æ˜¯å¦åˆ å»ç›ˆåˆ©å› å­, by default 1
        growth : bool, optional
            æ˜¯å¦åˆ å»æˆé•¿å› å­, by default 1
        liquidity : bool, optional
            æ˜¯å¦åˆ å»æµåŠ¨æ€§å› å­, by default 1
        size : bool, optional
            æ˜¯å¦åˆ å»è§„æ¨¡å› å­, by default 1
        leverage : bool, optional
            æ˜¯å¦åˆ å»æ æ†å› å­, by default 1
        beta : bool, optional
            æ˜¯å¦åˆ å»è´å¡”å› å­, by default 1
        nonlinearsize : bool, optional
            æ˜¯å¦åˆ å»éçº¿æ€§å¸‚å€¼å› å­, by default 1
        residualvolatility : bool, optional
            æ˜¯å¦åˆ å»æ®‹å·®æ³¢åŠ¨ç‡å› å­, by default 1
        booktoprice : bool, optional
            æ˜¯å¦åˆ å»è´¦é¢å¸‚å€¼æ¯”å› å­, by default 1
        """
        cls.homeplace = HomePlace()
        # barraå› å­æ•°æ®
        styles = os.listdir(cls.homeplace.barra_data_file)
        styles = [i for i in styles if (i.endswith(".parquet")) and (i[0] != ".")]
        barras = {}
        for s in styles:
            k = s.split(".")[0]
            v = pd.read_parquet(cls.homeplace.barra_data_file + s).resample("W").last()
            barras[k] = v
        rename_dict = {
            "size": "å¸‚å€¼",
            "nonlinearsize": "éçº¿æ€§å¸‚å€¼",
            "booktoprice": "ä¼°å€¼",
            "earningsyield": "ç›ˆåˆ©",
            "growth": "æˆé•¿",
            "leverage": "æ æ†",
            "liquidity": "æµåŠ¨æ€§",
            "momentum": "åŠ¨é‡",
            "residualvolatility": "æ³¢åŠ¨ç‡",
            "beta": "è´å¡”",
        }
        if momentum == 0:
            barras = {k: v for k, v in barras.items() if k != "momentum"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "momentum"}
        if earningsyield == 0:
            barras = {k: v for k, v in barras.items() if k != "earningsyield"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "earningsyield"}
        if growth == 0:
            barras = {k: v for k, v in barras.items() if k != "growth"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "growth"}
        if liquidity == 0:
            barras = {k: v for k, v in barras.items() if k != "liquidity"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "liquidity"}
        if size == 0:
            barras = {k: v for k, v in barras.items() if k != "size"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "size"}
        if leverage == 0:
            barras = {k: v for k, v in barras.items() if k != "leverage"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "leverage"}
        if beta == 0:
            barras = {k: v for k, v in barras.items() if k != "beta"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "beta"}
        if nonlinearsize == 0:
            barras = {k: v for k, v in barras.items() if k != "nonlinearsize"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "nonlinearsize"}
        if residualvolatility == 0:
            barras = {k: v for k, v in barras.items() if k != "residualvolatility"}
            rename_dict = {
                k: v for k, v in rename_dict.items() if k != "residualvolatility"
            }
        if booktoprice == 0:
            barras = {k: v for k, v in barras.items() if k != "booktoprice"}
            rename_dict = {k: v for k, v in rename_dict.items() if k != "booktoprice"}
        facs_dict = {
            "åè½¬_5å¤©æ”¶ç›Šç‡å‡å€¼": boom_one(read_daily(ret=1)),
            "æ³¢åŠ¨_20å¤©æ”¶ç›Šç‡æ ‡å‡†å·®": read_daily(ret=1)
            .rolling(20, min_periods=10)
            .std()
            .resample("W")
            .last(),
            "æ¢æ‰‹_5å¤©æ¢æ‰‹ç‡å‡å€¼": boom_one(read_daily(tr=1)),
        }
        barras.update(facs_dict)
        rename_dict.update({k: k for k in facs_dict.keys()})
        cls.barras = barras
        cls.rename_dict = rename_dict
        sort_names = list(rename_dict.values())
        cls.sort_names = sort_names
        cls.barras_together = merge_many(
            list(barras.values()), list(barras.keys()), how="inner"
        )

    def __call__(self):
        """è¿”å›çº¯å‡€å› å­å€¼"""
        return self.snow_fac

    def set_factors_df_wide(self, df: pd.DataFrame, other_factors: dict = None):
        """ä¼ å…¥å› å­æ•°æ®ï¼Œæ—¶é—´ä¸ºç´¢å¼•ï¼Œä»£ç ä¸ºåˆ—å"""
        df = df.resample("W").last()
        self.__corr = [
            df.corrwith(i, axis=1).mean() for i in list(self.barras.values())
        ]
        self.__corr = (
            pd.Series(
                self.__corr, index=[self.rename_dict[i] for i in self.barras.keys()]
            )
            .to_frame("ç›¸å…³ç³»æ•°")
            .T
        )
        self.__corr = self.__corr[self.sort_names]
        df = df.stack().reset_index()
        df.columns = ["date", "code", "fac"]
        self.factors = df
        self.corr_pri = pd.merge(df, self.barras_together, on=["date", "code"]).dropna()
        if other_factors is not None:
            other_factors = merge_many(
                list(other_factors.values()), list(other_factors.keys()), how="inner"
            )
            self.corr_pri = pd.merge(self.corr_pri, other_factors, on=["date", "code"])

    @property
    def corr(self) -> pd.DataFrame:
        """å› å­å’Œ10ä¸ªå¸¸ç”¨é£æ ¼å› å­çš„ç›¸å…³ç³»æ•°

        Returns
        -------
        pd.DataFrame
            å› å­å’Œ10ä¸ªå¸¸ç”¨é£æ ¼å› å­çš„ç›¸å…³ç³»æ•°
        """
        return self.__corr.copy()

    def ols_in_group(self, date):
        """å¯¹æ¯ä¸ªæ—¶é—´æ®µè¿›è¡Œå›å½’ï¼Œå¹¶è®¡ç®—æ®‹å·®"""
        df=self.corr_pri[self.corr_pri.date==date].set_index(['date','code'])
        xs = list(df.columns)
        xs = [i for i in xs if i != "fac"]
        xs_join = "+".join(xs)
        ols_formula = "fac~" + xs_join
        ols_result = smf.ols(ols_formula, data=df).fit()
        df.fac = ols_result.resid
        return df[['fac']]

    def get_snow_fac(self):
        """è·å¾—çº¯å‡€å› å­"""
        dates=list(set(self.corr_pri.date))
        # dates=[self.corr_pri[self.corr_pri.date==i].set_index(['date','code']) for i in dates]
        # print(dates[0])
        with mpire.WorkerPool(20) as pool:
            res=pool.map(self.ols_in_group,dates)
        self.snow_fac = pd.concat(res)
        # self.snow_fac = (
        #     self.corr_pri.set_index(["date", "code"])
        #     .groupby(["date"],as_index=False)
        #     .apply(self.ols_in_group)
        # )
        if 'date' in self.snow_fac.columns:
            self.snow_fac=self.snow_fac.rename(columns={'date':'old_date'})
        # self.snow_fac = self.snow_fac.unstack()
        # self.snow_fac.columns = list(map(lambda x: x[1], list(self.snow_fac.columns)))
        self.snow_fac=self.snow_fac.reset_index().pivot(index='date',columns='code',values='fac')

def ols_in_group(df):
    """å¯¹æ¯ä¸ªæ—¶é—´æ®µè¿›è¡Œå›å½’ï¼Œå¹¶è®¡ç®—æ®‹å·®"""
    xs = list(df.columns)
    xs = [i for i in xs if i != "fac"]
    xs_join = "+".join(xs)
    ols_formula = "fac~" + xs_join
    ols_result = smf.ols(ols_formula, data=df).fit()
    df.fac = ols_result.resid
    return df[['fac']]

@do_on_dfs
class pure_snowtrain(object):
    """ç›´æ¥è¿”å›çº¯å‡€å› å­"""

    def __init__(
        self,
        factors: pd.DataFrame,
        facs_dict: Dict = None,
        momentum: bool = 1,
        earningsyield: bool = 1,
        growth: bool = 1,
        liquidity: bool = 1,
        size: bool = 1,
        leverage: bool = 1,
        beta: bool = 1,
        nonlinearsize: bool = 1,
        residualvolatility: bool = 1,
        booktoprice: bool = 1,
    ) -> None:
        """è®¡ç®—å› å­å€¼ä¸10ç§å¸¸ç”¨é£æ ¼å› å­ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶è¿›è¡Œçº¯å‡€åŒ–ï¼Œå¯ä»¥é¢å¤–åŠ å…¥å…¶ä»–å› å­

        Parameters
        ----------
        factors : pd.DataFrame
            è¦è€ƒå¯Ÿçš„å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        facs_dict : Dict, optional
            é¢å¤–åŠ å…¥çš„å› å­ï¼Œåå­—ä¸ºkeyï¼Œå› å­çŸ©é˜µä¸ºvalueï¼Œå½¢å¦‚`{'åè½¬': ret20, 'æ¢æ‰‹': tr20}`, by default None
        momentum : bool, optional
            æ˜¯å¦åˆ å»åŠ¨é‡å› å­, by default 1
        earningsyield : bool, optional
            æ˜¯å¦åˆ å»ç›ˆåˆ©å› å­, by default 1
        growth : bool, optional
            æ˜¯å¦åˆ å»æˆé•¿å› å­, by default 1
        liquidity : bool, optional
            æ˜¯å¦åˆ å»æµåŠ¨æ€§å› å­, by default 1
        size : bool, optional
            æ˜¯å¦åˆ å»è§„æ¨¡å› å­, by default 1
        leverage : bool, optional
            æ˜¯å¦åˆ å»æ æ†å› å­, by default 1
        beta : bool, optional
            æ˜¯å¦åˆ å»è´å¡”å› å­, by default 1
        nonlinearsize : bool, optional
            æ˜¯å¦åˆ å»éçº¿æ€§å¸‚å€¼å› å­, by default 1
        residualvolatility : bool, optional
            æ˜¯å¦åˆ å»æ®‹å·®æ³¢åŠ¨ç‡å› å­, by default 1
        booktoprice : bool, optional
            æ˜¯å¦åˆ å»è´¦é¢å¸‚å€¼æ¯”å› å­, by default 1
        """
        self.winter = pure_coldwinter(
            momentum=momentum,
            earningsyield=earningsyield,
            growth=growth,
            liquidity=liquidity,
            size=size,
            leverage=leverage,
            beta=beta,
            nonlinearsize=nonlinearsize,
            residualvolatility=residualvolatility,
            booktoprice=booktoprice,
        )
        self.winter.set_factors_df_wide(factors, facs_dict)
        self.winter.get_snow_fac()
        self.corr = self.winter.corr

    def __call__(self) -> pd.DataFrame:
        """è·å¾—çº¯å‡€åŒ–ä¹‹åçš„å› å­å€¼

        Returns
        -------
        pd.DataFrame
            çº¯å‡€åŒ–ä¹‹åçš„å› å­å€¼
        """
        return self.winter.snow_fac.copy()

    def show_corr(self) -> pd.DataFrame:
        """å±•ç¤ºå› å­ä¸barraé£æ ¼å› å­çš„ç›¸å…³ç³»æ•°

        Returns
        -------
        pd.DataFrame
            ç›¸å…³ç³»æ•°è¡¨æ ¼
        """
        return self.corr.applymap(lambda x: to_percent(x))


class pure_newyear(object):
    """è½¬ä¸ºç”Ÿæˆ25åˆ†ç»„å’Œç™¾åˆ†ç»„çš„æ”¶ç›ŠçŸ©é˜µè€Œå°è£…"""

    def __init__(
        self,
        facx: pd.DataFrame,
        facy: pd.DataFrame,
        group_num_single: int,
        trade_cost_double_side: float = 0,
        namex: str = "ä¸»",
        namey: str = "æ¬¡",
    ) -> None:
        """æ¡ä»¶åŒå˜é‡æ’åºæ³•ï¼Œå…ˆå¯¹æ‰€æœ‰è‚¡ç¥¨ï¼Œä¾ç…§å› å­facxè¿›è¡Œæ’åº
        ç„¶ååœ¨æ¯ä¸ªç»„å†…ï¼Œä¾ç…§facyè¿›è¡Œæ’åºï¼Œæœ€åç»Ÿè®¡å„ä¸ªç»„å†…çš„å¹³å‡æ”¶ç›Šç‡

        Parameters
        ----------
        facx : pd.DataFrame
            é¦–å…ˆè¿›è¡Œæ’åºçš„å› å­ï¼Œé€šå¸¸ä¸ºæ§åˆ¶å˜é‡ï¼Œç›¸å½“äºæ­£äº¤åŒ–ä¸­çš„è‡ªå˜é‡
            indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        facy : pd.DataFrame
            åœ¨facxçš„å„ä¸ªç»„å†…ï¼Œä¾ç…§facyè¿›è¡Œæ’åºï¼Œä¸ºä¸»è¦è¦ç ”ç©¶çš„å› å­
            indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
        group_num_single : int
            å•ä¸ªå› å­åˆ†æˆå‡ ç»„ï¼Œé€šå¸¸ä¸º5æˆ–10
        trade_cost_double_side : float, optional
            äº¤æ˜“çš„åŒè¾¹æ‰‹ç»­è´¹ç‡, by default 0
        namex : str, optional
            facxè¿™ä¸€å› å­çš„åå­—, by default "ä¸»"
        namey : str, optional
            facyè¿™ä¸€å› å­çš„åå­—, by default "æ¬¡"
        """
        homex = pure_fallmount(facx)
        homey = pure_fallmount(facy)
        if group_num_single == 5:
            homexy = homex > homey
        elif group_num_single == 10:
            homexy = homex >> homey
        shen = pure_moonnight(
            homexy(),
            group_num_single**2,
            trade_cost_double_side=trade_cost_double_side,
            plt_plot=False,
            print_comments=False,
        )
        sq = shen.shen.square_rets.copy()
        sq.index = [namex + str(i) for i in list(sq.index)]
        sq.columns = [namey + str(i) for i in list(sq.columns)]
        self.square_rets = sq

    def __call__(self) -> pd.DataFrame:
        """è°ƒç”¨å¯¹è±¡æ—¶ï¼Œè¿”å›æœ€ç»ˆç»“æœï¼Œæ­£æ–¹å½¢çš„åˆ†ç»„å¹´åŒ–æ”¶ç›Šç‡è¡¨

        Returns
        -------
        `pd.DataFrame`
            æ¯ä¸ªç»„çš„å¹´åŒ–æ”¶ç›Šç‡
        """
        return self.square_rets.copy()


class pure_helper(object):
    def __init__(
        self,
        df_main: pd.DataFrame,
        df_helper: pd.DataFrame,
        func: Callable = None,
        group: int = 10,
    ) -> None:
        """ä½¿ç”¨å› å­bçš„å€¼å¤§å°ï¼Œå¯¹å› å­aè¿›è¡Œåˆ†ç»„ï¼Œå¹¶å¯ä»¥åœ¨ç»„å†…è¿›è¡ŒæŸç§æ“ä½œ

        Parameters
        ----------
        df_main : pd.DataFrame
            è¦è¢«åˆ†ç»„å¹¶è¿›è¡Œæ“ä½œçš„å› å­
        df_helper : pd.DataFrame
            ç”¨æ¥åšåˆ†ç»„çš„ä¾æ®
        func : Callable, optional
            åˆ†ç»„åï¼Œç»„å†…è¦è¿›è¡Œçš„æ“ä½œ, by default None
        group : int, optional
            è¦åˆ†çš„ç»„æ•°, by default 10
        """
        self.df_main = df_main
        self.df_helper = df_helper
        self.func = func
        self.group = group
        if self.func is None:
            self.__data = self.sort_a_with_b()
        else:
            self.__data = self.sort_a_with_b_func()

    @property
    def data(self):
        return self.__data

    def __call__(self) -> pd.DataFrame:
        return self.data

    def sort_a_with_b(self):
        dfb = to_group(self.df_helper, group=self.group)
        dfb = dfb.stack().reset_index()
        dfb.columns = ["date", "code", "group"]
        dfa = self.df_main.stack().reset_index()
        dfa.columns = ["date", "code", "target"]
        df = pd.merge(dfa, dfb, on=["date", "code"])
        return df

    def sort_a_with_b_func(self):
        the_func = partial(self.func)
        df = self.sort_a_with_b().drop(columns=["code"])
        df = (
            df.groupby(["date", "group"])
            .apply(the_func)
            .drop(columns=["group"])
            .reset_index()
        )
        df = df.pivot(index="date", columns="group", values="target")
        df.columns = [f"group{str(int(i+1))}" for i in list(df.columns)]
        return df


@do_on_dfs
def get_group(df: pd.DataFrame, group_num: int = 10) -> pd.DataFrame:
    """ä½¿ç”¨groupbyçš„æ–¹æ³•ï¼Œå°†ä¸€ç»„å› å­å€¼æ”¹ä¸ºæˆªé¢ä¸Šçš„åˆ†ç»„å€¼ï¼Œæ­¤æ–¹æ³•ç›¸æ¯”qcutçš„æ–¹æ³•æ›´åŠ ç¨³å¥ï¼Œä½†é€Ÿåº¦æ›´æ…¢ä¸€äº›

    Parameters
    ----------
    df : pd.DataFrame
        å› å­å€¼ï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼
    group_num : int, optional
        åˆ†ç»„çš„æ•°é‡, by default 10

    Returns
    -------
    pd.DataFrame
        è½¬åŒ–ä¸ºåˆ†ç»„å€¼åçš„dfï¼Œindexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºåˆ†ç»„å€¼
    """
    a = pure_moon(no_read_indu=1)
    df = df.stack().reset_index()
    df.columns = ["date", "code", "fac"]
    df = a.get_groups(df, group_num).pivot(index="date", columns="code", values="group")
    return df


def symmetrically_orthogonalize(dfs: list[pd.DataFrame]) -> list[pd.DataFrame]:
    """å¯¹å¤šä¸ªå› å­åšå¯¹ç§°æ­£äº¤ï¼Œæ¯ä¸ªå› å­å¾—åˆ°æ­£äº¤å…¶ä»–å› å­åçš„ç»“æœ

    Parameters
    ----------
    dfs : list[pd.DataFrame]
        å¤šä¸ªè¦åšæ­£äº¤çš„å› å­ï¼Œæ¯ä¸ªdféƒ½æ˜¯indexä¸ºæ—¶é—´ï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºå› å­å€¼çš„df

    Returns
    -------
    list[pd.DataFrame]
        å¯¹ç§°æ­£äº¤åçš„å„ä¸ªå› å­
    """

    def sing(dfs: list[pd.DataFrame], date: pd.Timestamp):
        dds = []
        for num, i in enumerate(dfs):
            i = i[i.index == date]
            i.index = [f"fac{num}"]
            i = i.T
            dds.append(i)
        dds = pd.concat(dds, axis=1)
        cov = dds.cov()
        d, u = np.linalg.eig(cov)
        d = np.diag(d ** (-0.5))
        new_facs = pd.DataFrame(
            np.dot(dds, np.dot(np.dot(u, d), u.T)), columns=dds.columns, index=dds.index
        )
        new_facs = new_facs.stack().reset_index()
        new_facs.columns = ["code", "fac_number", "fac"]
        new_facs = new_facs.assign(date=date)
        dds = []
        for num, i in enumerate(dfs):
            i = new_facs[new_facs.fac_number == f"fac{num}"]
            i = i.pivot(index="date", columns="code", values="fac")
            dds.append(i)
        return dds

    dfs = [standardlize(i) for i in dfs]
    date_first = max([i.index.min() for i in dfs])
    date_last = min([i.index.max() for i in dfs])
    dfs = [i[(i.index >= date_first) & (i.index <= date_last)] for i in dfs]
    fac_num = len(dfs)
    ddss = [[] for i in range(fac_num)]
    for date in tqdm.auto.tqdm(dfs[0].index):
        dds = sing(dfs, date)
        for num, i in enumerate(dds):
            ddss[num].append(i)
    ds = []
    for i in tqdm.auto.tqdm(ddss):
        ds.append(pd.concat(i))
    return ds


@do_on_dfs
def sun(factor:pd.DataFrame,rolling_5:int=1):
    '''å…ˆå•å› å­æµ‹è¯•ï¼Œå†æµ‹è¯•å…¶ä¸å¸¸ç”¨é£æ ¼ä¹‹é—´çš„å…³ç³»'''
    if rolling_5:
        factor=boom_one(factor)
    shen=pure_moonnight(factor)
    pfi=pure_snowtrain(factor)
    shen=pure_moonnight(pfi,neutralize=1)
    display(pfi.show_corr())